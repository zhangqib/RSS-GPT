<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>AI Autonomy Coefficient ($\alpha$): Defining Boundaries for Responsible AI Systems</title>
<link>https://arxiv.org/abs/2512.11295</link>
<guid>https://arxiv.org/abs/2512.11295</guid>
<content:encoded><![CDATA[
<div> Human-Instead-of-AI, AI Autonomy Coefficient, AFHE paradigm, functional independence, sustainable AI systems<br><br>Summary:  
The paper identifies a critical issue in AI systems where human labor is concealed as part of the operation, termed Human-Instead-of-AI (HISOAI). This design is ethically questionable and economically unfeasible as it disguises systems heavily reliant on humans rather than true automation. To tackle this problem, the authors introduce the AI-First, Human-Empowered (AFHE) paradigm, which mandates AI systems to prove a measurable degree of functional independence before deployment. This is quantified using the AI Autonomy Coefficient, which calculates the fraction of tasks an AI completes without compulsory human involvement. Additionally, the paper proposes the AFHE Deployment Algorithm, an enforcement mechanism that ensures AI systems meet a minimum autonomy threshold during offline testing and shadow deployment phases. Experimental results demonstrate that the AI Autonomy Coefficient successfully detects HISOAI systems exhibiting autonomy levels as low as 0.38, whereas systems adopting the AFHE framework achieve significantly higher autonomy, measured at 0.85. Ultimately, the study advocates for AFHE as a metric-based approach promoting verifiable autonomy, operational transparency, and long-term sustainability of AI technologies. <div>
arXiv:2512.11295v3 Announce Type: replace-cross 
Abstract: The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence prior to deployment. This requirement is formalized through the AI Autonomy Coefficient, measuring the proportion of tasks completed without mandatory human intervention. We further propose the AFHE Deployment Algorithm, an algorithmic gate that enforces a minimum autonomy threshold during offline evaluation and shadow deployment. Our results show that the AI Autonomy Coefficient effectively identifies HISOAI systems with an autonomy level of 0.38, while systems governed by the AFHE framework achieve an autonomy level of 0.85. We conclude that AFHE provides a metric-driven approach for ensuring verifiable autonomy, transparency, and sustainable operational integrity in modern AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments</title>
<link>https://arxiv.org/abs/2512.15736</link>
<guid>https://arxiv.org/abs/2512.15736</guid>
<content:encoded><![CDATA[
<div> Keywords: Anubuddhi, quantum optics, multi-agent AI, experiment simulation, natural language prompts<br /><br />Summary:<br /><br />1. Anubuddhi is a multi-agent AI system designed to create and simulate quantum optics experiments using natural language prompts, eliminating the need for specialized programming skills.  
2. The system constructs optical setups by selecting components from a three-tier toolbox through semantic retrieval, then verifies designs via physics simulations enhanced by iterative refinement.  
3. Its architecture integrates intent routing, knowledge-augmented generation, and dual-mode validation using QuTiP and FreeSim simulators.  
4. Evaluations covered 13 experiments across fundamental optics (e.g., Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states), quantum information protocols (e.g., BB84 QKD, GHZ states, quantum teleportation), and advanced quantum technologies (e.g., boson sampling, electromagnetically induced transparency).  
5. Results show the system achieves high design-simulation alignment scores (8-9/10), confirming correct physics architecture, though numerical accuracy still requires expert evaluation. Free-form simulation with FreeSim outperformed more constrained frameworks in most cases, highlighting the need for flexible mathematical models in quantum optics.  
6. By enabling iterative conversational refinement, Anubuddhi democratizes computational experiment design for both research and educational use, providing strong initial experiment drafts that users can further develop. <div>
arXiv:2512.15736v1 Announce Type: new 
Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems</title>
<link>https://arxiv.org/abs/2512.15740</link>
<guid>https://arxiv.org/abs/2512.15740</guid>
<content:encoded><![CDATA[
<div> Keywords: ethical frameworks, uncertainty, Principle of Proportional Duty, epistemic state, AI decision systems  

<br /><br />Summary:  
This paper introduces the Principle of Proportional Duty (PPD), a novel ethical framework designed to better model decision-making under uncertainty. Traditional ethical approaches typically treat uncertainty as a constraint, but PPD reframes moral responsibility as dynamically scaling with an agent's epistemic state. The framework distinguishes between Action Duty (the obligation to act decisively) and Repair Duty (the obligation to verify and resolve uncertainty), showing that as uncertainty increases, the moral duty proportionally shifts from action to inquiry. The core relationship is encapsulated by the equation D_total = K[(1-HI) + HI * g(C_signal)], linking total duty to knowledge (K), humility/uncertainty (HI), and contextual signal strength (C_signal). Using Monte Carlo simulations, the authors demonstrate that maintaining a baseline humility coefficient (lambda > 0) leads to more stable duty allocations and mitigates risks of overconfident decisions. Importantly, the PPD framework formalizes humility as a system parameter, providing a mathematically tractable model for ethical responsibility that is suitable for auditable AI systems. Applications across clinical ethics, recipient-rights law, economic governance, and AI illustrate the framework’s broad relevance and its potential to stabilize complex systems. By balancing epistemic confidence with contextual risk, PPD prevents both ethical overreach and omission. <div>
arXiv:2512.15740v1 Announce Type: new 
Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions</title>
<link>https://arxiv.org/abs/2512.15743</link>
<guid>https://arxiv.org/abs/2512.15743</guid>
<content:encoded><![CDATA[
<div> assembly instructions, large language models, LDraw, discrete parts vocabulary, physical prototyping<br /><br />Summary:<br /><br />1. The paper introduces a framework to generate physically realizable assembly instructions from natural language descriptions, focusing on producing valid construction sequences for brick-based prototypes.  
2. Unlike traditional text-to-3D generation techniques, the method uses a discrete parts vocabulary ensuring geometric validity, connection constraints, and buildability order, thereby enabling physically meaningful outputs.  
3. LDraw, a text-rich intermediate representation, is leveraged to bridge natural language commands and the physical assembly process, guiding large language models (LLMs) to construct step-by-step instructions for assembling complex structures with over 3000 parts.  
4. The authors present a Python library supporting programmatic model generation, demonstrated through buildable outputs in challenging domains like satellites, aircraft, and architecture, highlighting the approach's scalability, modularity, and design fidelity.  
5. This novel "bag of bricks" approach acts as a physical API linking semantic design intent ("bag of words") to precise brick placements, overcoming limitations of pixel-based diffusion and CAD methods, and opening new avenues in AI-guided manufacturing and engineering prototyping from natural language inputs. <div>
arXiv:2512.15743v1 Announce Type: new 
Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying</title>
<link>https://arxiv.org/abs/2512.15776</link>
<guid>https://arxiv.org/abs/2512.15776</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Privileged Information Bias, Theory of Mind, Asymmetric Assistive Reasoning, Active Uncertainty Reduction  

<br /><br />Summary:  
This paper addresses the challenge that Large Language Models (LLMs) face with symbol grounding in embodied settings where information is unevenly distributed among agents. Specifically, it studies the Privileged Information Bias, also known as the "Curse of Knowledge," where a knowledgeable "Leader" agent struggles to effectively guide a sensor-limited "Follower" agent due to a lack of Theory of Mind, or the ability to infer the follower’s knowledge state. To analyze this, the authors propose a novel Asymmetric Assistive Reasoning framework implemented within the AI2-THOR environment. Their experiments reveal a significant "Success Gap": the Leader alone perceives the target in 35% of episodes, but the collaborative team only succeeds 17% of the time, indicating nearly half of potential task completions fail because of communication and grounding errors. Crucially, the study shows that a "Pull-based" communication protocol, where the Follower actively asks clarification questions, outperforms the traditional "Push-based" approach where the Leader only provides instructions. Successful episodes involve twice as many clarification requests, highlighting the importance of active uncertainty reduction. This mechanism is identified as essential for enabling safe and effective collaboration in human-AI and robot-robot interactions. <div>
arXiv:2512.15776v1 Announce Type: new 
Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title>
<link>https://arxiv.org/abs/2512.15783</link>
<guid>https://arxiv.org/abs/2512.15783</guid>
<content:encoded><![CDATA[
<div> AI Epidemiology, population-level surveillance, risk assessment, AI governance, expert interaction<br /><br />Summary:<br /><br />AI Epidemiology is a novel framework designed to govern and explain advanced AI systems by applying epidemiological surveillance methods at the population level to AI outputs. It draws parallels to public health epidemiology, which uses statistical evidence for interventions before molecular mechanisms are fully understood, thereby avoiding the complexity issues faced by current interpretability methods like SHAP. The framework standardizes the capture of AI-expert interactions into structured assessment fields such as risk level, alignment score, and accuracy score, which serve as exposure variables predicting AI output failures through statistical associations. These associations are validated via expert overrides and real-world outcomes, enhancing reliability. A key advantage is that AI Epidemiology imposes no additional burden on experts by passively tracking their convergence or divergence from AI recommendations and generating automatic audit trails. Because it analyzes outputs instead of internal model mechanics, it maintains governance continuity despite changes in underlying AI models or vendors. Furthermore, the system offers reliability scores and semantic assessments, helping experts identify unreliable AI outputs early, thus preventing harm. By democratizing AI oversight, AI Epidemiology empowers domain experts to govern and assess AI systems without requiring specialized machine learning knowledge. <div>
arXiv:2512.15783v1 Announce Type: new 
Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM</title>
<link>https://arxiv.org/abs/2512.15784</link>
<guid>https://arxiv.org/abs/2512.15784</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model agents, memory-centric architecture, Profile Memory, Experience Memory, Action Memory  

<br /><br />Summary:  
This paper addresses the challenge of enabling Large Language Model (LLM) agents to self-evolve post-deployment without relying on costly model retraining or fine-tuning. To overcome the trade-off between accuracy and inference efficiency, the authors propose MOBIMEM, a memory-centric agent system that decouples agent evolution from the underlying model weights by using three specialized memory primitives. First, Profile Memory employs a distance-graph structure to efficiently align with user preferences, significantly reducing latency during profile retrieval and enhancing personalization. Second, Experience Memory uses multi-level templates to generalize execution logic across new tasks, thereby improving the agent’s capabilities. Third, Action Memory records detailed interaction sequences to minimize reliance on expensive model inferences, boosting efficiency. MOBIMEM further incorporates OS-inspired services for robust task orchestration, including a scheduler to manage parallel sub-task executions, an agent record-and-replay mechanism (AgentRR) for safe action reuse, and context-aware exception handling to ensure graceful recovery from errors and interruptions. Experimental evaluation on AndroidWorld and the top-50 apps demonstrates MOBIMEM’s effectiveness, achieving 83.1% profile alignment with a retrieval time of 23.83 ms, which is 280 times faster than GraphRAG baselines. It also improves task success rates by up to 50.3% and reduces end-to-end latency by up to nine times on mobile devices, proving its practical value in mobile and desktop agent workflows. <div>
arXiv:2512.15784v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State-Augmented Graphs for Circular Economy Triage</title>
<link>https://arxiv.org/abs/2512.15824</link>
<guid>https://arxiv.org/abs/2512.15824</guid>
<content:encoded><![CDATA[
<div> Circular economy, disassembly sequencing, decision-making framework, electric vehicle batteries, condition-aware utility  

<br /><br />Summary:  
This paper introduces a novel decision-making framework designed to optimize circular economy (CE) triage, which assesses products to determine sustainable pathways after their useful life. The framework operates as a deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph that incorporates disassembly history into the system state. This encoding enforces the Markov property, allowing each decision to depend solely on the previous state, thus enabling optimal and recursive evaluation. The triage decisions involve choosing between continuing disassembly or committing to a specific CE option, with considerations based on diagnostic health scores and complex operational constraints. The model’s strength lies in its condition-aware utility function that evaluates retained value against costs and constraints related to processing and labor. The framework’s flexibility is demonstrated through a hierarchical triage example focused on electric vehicle (EV) batteries, where recursive component valuation drives decision-making. This example highlights the model’s ability to handle varying mechanical complexities, safety requirements, and economic factors. Overall, the proposed unified formalism offers a tractable, generalizable foundation for improving CE triage decisions across diverse products and operational contexts, supporting sustainable end-of-life management strategies. <div>
arXiv:2512.15824v1 Announce Type: new 
Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations</title>
<link>https://arxiv.org/abs/2512.15894</link>
<guid>https://arxiv.org/abs/2512.15894</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, pediatric guidance, adversarial queries, safety evaluation, PediatricAnxietyBench<br /><br />Summary:  
Large language models (LLMs) are increasingly used by parents seeking pediatric guidance, but their safety under real-world adversarial conditions is not well understood. To address this, the authors developed PediatricAnxietyBench, an open-source benchmark containing 300 high-quality queries across 10 pediatric topics, split evenly between patient-derived and adversarial queries. The adversarial set mimics real parental pressures such as urgency, economic challenges, and attempts to bypass model disclaimers. Two Llama models, with 70 billion and 8 billion parameters, were evaluated through a multi-dimensional safety framework assessing diagnostic restraint, referral adherence, hedging, and emergency recognition. The average safety score was low (5.50/15), with the larger 70B model performing significantly better than the 8B model (6.26 vs. 4.95) and showing fewer critical failures (4.8% vs. 12.0%). Adversarial queries caused an 8% reduction in safety, with urgent language having the most severe impact. Vulnerabilities were especially notable in seizure-related and post-vaccination queries. Hedging behavior strongly correlated with safety scores, but emergency recognition was notably absent across models. The study highlights that while model scale improves safety, all tested models remain vulnerable to realistic parental adversarial pressures. PediatricAnxietyBench offers a reproducible adversarial testing framework that reveals clinically significant failure modes missed by standard benchmarks. <div>
arXiv:2512.15894v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries</title>
<link>https://arxiv.org/abs/2512.15906</link>
<guid>https://arxiv.org/abs/2512.15906</guid>
<content:encoded><![CDATA[
<div> arXiv, large language models, knowledge graph, healthcare, open-source<br /><br />Summary: Darth Vecdor (DV) is a tool designed to extract and structure knowledge from large language models (LLMs) into a terminology-mapped SQL database, creating a knowledge graph. This approach aims to improve query efficiency, cost, speed, safety, and confidence when compared to direct LLM querying, particularly in high-volume settings like healthcare. DV addresses critical challenges such as erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as managing multi-element responses. The system includes a user-friendly browser-based graphical interface to enable domain experts with minimal technical skills to perform prompt engineering. Released as free and open-source software under an "as is" basis without any warranties, DV requires users to recognize potential risks and assume responsibility for safe and effective use. Despite potential serious bugs, the author hopes that DV’s ongoing development and application will positively impact healthcare by providing more reliable, standardized access to LLM-derived knowledge. <div>
arXiv:2512.15906v1 Announce Type: new 
Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems</title>
<link>https://arxiv.org/abs/2512.15922</link>
<guid>https://arxiv.org/abs/2512.15922</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, knowledge graphs, spreading activation, multi-hop question answering, language models<br /><br />Summary:<br />1. Retrieval-augmented generation (RAG) systems, despite various architectures, face challenges in reliably retrieving multi-step evidence for complex reasoning tasks due to treating retrieved information as equally reliable without considering the interconnected nature of text corpora.<br />2. GraphRAG approaches improve RAG by integrating knowledge graphs that structure information as nodes and edges, enabling multi-step logical traversal, but rely heavily on high-quality knowledge graphs which are costly or unreliable to build.<br />3. Existing GraphRAG systems typically use large language models to guide graph traversal, inheriting similar difficulties as standard RAG frameworks.<br />4. This paper proposes a novel RAG framework leveraging the spreading activation algorithm on automatically constructed knowledge graphs to better retrieve information from linked documents, enhancing performance on complex tasks such as multi-hop question answering.<br />5. Experimental results demonstrate the proposed method achieves superior or comparable performance to iterative RAG techniques, can be easily integrated as a plug-and-play module, and when combined with chain-of-thought iterative retrieval, yields up to a 39% absolute improvement in answer correctness using small open-weight language models, showcasing its efficiency in resource-limited environments. <div>
arXiv:2512.15922v1 Announce Type: new 
Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning</title>
<link>https://arxiv.org/abs/2512.15943</link>
<guid>https://arxiv.org/abs/2512.15943</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Large Language Models, OPT-350M, Fine-tuning, Cost Optimization<br /><br />Summary:<br /><br />1. As generative AI adoption scales in enterprises, model cost optimization and operational efficiency have become crucial for sustainability and accessibility.<br />2. Large Language Models (LLMs), despite their strong capabilities, have high computational demands making them expensive for routine use.<br />3. Small Language Models (SLMs) are explored as cost-effective alternatives that maintain competitive performance in specific tasks.<br />4. This study fine-tuned the facebook/opt-350m SLM model using Hugging Face’s Transformer Reinforcement Learning (TRL) framework with Supervised Fine-Tuning (SFT) on domain-specific tasks such as document summarization, query answering, and structured data interpretation.<br />5. Experimental results showed that the fine-tuned OPT-350M SLM achieved a 77.55% pass rate on the ToolBench evaluation, outperforming baseline models including ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%).<br />6. The findings demonstrate that carefully designed and targeted training of SLMs can substantially reduce deployment costs while enabling effective, large-scale generative AI integration in production settings. <div>
arXiv:2512.15943v1 Announce Type: new 
Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subjective functions</title>
<link>https://arxiv.org/abs/2512.15948</link>
<guid>https://arxiv.org/abs/2512.15948</guid>
<content:encoded><![CDATA[
<div> objective functions, subjective function, higher-order objectives, prediction error, agent-endogenous goals<br /><br />Summary:<br /><br />1. This paper explores the fundamental question of where objective functions originate and how agents decide which goals to pursue. 2. It introduces the concept of a subjective function, defined as a higher-order objective function that is internal or endogenous to the agent, meaning it is based on the agent’s own features and not imposed externally. 3. The authors use expected prediction error as a key example to concretely illustrate a subjective function, showing how it can serve as an internal guiding metric for goal selection. 4. The approach bridges several fields—psychology, neuroscience, and machine learning—highlighting interdisciplinary connections and suggesting that understanding subjective functions could illuminate both human cognition and artificial intelligence. 5. Ultimately, the paper proposes that endowing artificial systems with the ability to generate and use subjective functions could enable them to autonomously adapt and create new objectives dynamically, resembling the flexible goal-setting capacity of human intelligence. <div>
arXiv:2512.15948v1 Announce Type: new 
Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting</title>
<link>https://arxiv.org/abs/2512.16022</link>
<guid>https://arxiv.org/abs/2512.16022</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, foundation models, large language models, ensemble learning, SHAP-based finetuning<br /><br />Summary:<br /><br />1. The paper addresses the challenge in time series forecasting where no single foundation model consistently outperforms others, suggesting that an interpretable and optimized ensemble approach is more effective.<br /><br />2. While Large Language Models (LLMs) have strong reasoning abilities, their direct application to time series forecasting has been ineffective; the authors propose repositioning LLMs as intelligent judges rather than primary predictors.<br /><br />3. To enable LLMs to understand and evaluate time series models, an R1-style finetuning process is introduced, guided by SHAP (SHapley Additive exPlanations)-based faithfulness scores, teaching the LLM to interpret ensemble weights as causal statements that reflect temporal dynamics.<br /><br />4. The trained LLM agent conducts iterative, multi-turn conversations to perform assessments, provide causal explanations for model weighting decisions, and adaptively refine the ensemble optimization strategy.<br /><br />5. Experimental validation on the GIFT-Eval benchmark covering 23 datasets and 97 settings demonstrates that this ensemble coordination approach significantly outperforms leading time series foundation models using CRPS and MASE metrics, setting new state-of-the-art results. <div>
arXiv:2512.16022v1 Announce Type: new 
Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets</title>
<link>https://arxiv.org/abs/2512.16030</link>
<guid>https://arxiv.org/abs/2512.16030</guid>
<content:encoded><![CDATA[
<div> Keywords: epistemic calibration, large language models, KalshiBench, prediction markets, model overconfidence<br /><br />Summary:<br /><br />1. This study introduces KalshiBench, a novel benchmark containing 300 prediction market questions from Kalshi, a regulated exchange, designed to evaluate model calibration on future events with real, verifiable outcomes beyond the training data.<br /><br />2. The benchmark shifts focus from traditional accuracy on static knowledge to assessing how well models quantify uncertainty about genuinely unknown events.<br /><br />3. Five state-of-the-art large language models—Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2—were tested for epistemic calibration.<br /><br />4. Results show systematic overconfidence across all models, with the best-calibrated model, Claude Opus 4.5, having a notable calibration error (ECE=0.120), while reasoning-enhanced models like GPT-5.2-XHigh performed even worse (ECE=0.395) despite similar accuracy.<br /><br />5. Most models scored negatively on the Brier Skill Score, indicating poorer performance than naive base rate predictions, emphasizing that scaling and advanced reasoning do not inherently improve calibration and that epistemic calibration requires specific targeted development efforts. <div>
arXiv:2512.16030v1 Announce Type: new 
Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education</title>
<link>https://arxiv.org/abs/2512.16036</link>
<guid>https://arxiv.org/abs/2512.16036</guid>
<content:encoded><![CDATA[
<div> Generative AI, Educational Policy, Topic Modeling, Large Language Models, Academic Integrity<br /><br />Summary:<br /><br />1. Generative artificial intelligence (GenAI) is increasingly used by students for personalized learning and real-time feedback, helping clarify concepts and solve complex problems, though sometimes leading to concerns about plagiarism and undermining critical thinking.<br /><br />2. The rise of GenAI prompts educational institutions to develop varied policies that address its use, aiming to guide responsible and ethical integration in academic environments.<br /><br />3. These policies differ widely across universities and courses and often leave students uncertain about what is allowed and how to comply with best practices.<br /><br />4. To address this variability and uncertainty, the authors created an automated system that discovers and categorizes GenAI-related policies by analyzing course syllabi and institutional website content.<br /><br />5. The system employs unsupervised topic modeling to identify key themes and leverages GPT-4.0 large language models to classify policy stances on GenAI, achieving high coherence (0.73), precision (0.92–0.97), and recall (0.85–0.97) metrics across eight topics.<br /><br />6. This tool offers structured, interpretable information about AI policies that support safe, equitable, and pedagogically sound use of GenAI in education.<br /><br />7. Moreover, it can be integrated into educational technology platforms, helping students better understand and adhere to applicable GenAI guidelines. <div>
arXiv:2512.16036v1 Announce Type: new 
Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning</title>
<link>https://arxiv.org/abs/2512.16108</link>
<guid>https://arxiv.org/abs/2512.16108</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational music recommendation, large language model, agentic boundary learning, tool integration, personalized benchmarks<br /><br />Summary:  
This paper addresses the challenge of personalized music recommendation within conversational settings, which requires a nuanced understanding of user preferences and musical context. Existing methods often struggle to balance specialized musical domain knowledge and the flexible integration of external tools. To overcome this, the authors propose WeMusic-Agent, a novel training framework for large language model (LLM)-based conversational music recommendation. The framework leverages two main techniques: knowledge internalization, enabling the model to absorb extensive musical information, and agentic boundary learning, teaching the model when to use internalized knowledge versus when to call on external specialized tools such as music retrieval APIs or recommendation systems. A specific model, WeMusic-Agent-M1, is introduced, which undergoes continued pretraining on a massive 50 billion token music-related corpus to internalize deep musical knowledge while maintaining the ability to invoke external tools when necessary. Additionally, the authors identify a gap in available evaluation resources and thus develop a new open benchmark for personalized conversational music recommendation, sourced from real-world user data on WeChat Listen. This benchmark evaluates recommendation relevance, personalization, and diversity. Experimental results on real-world data demonstrate that WeMusic-Agent significantly outperforms existing models, establishing a strong foundation for future research in conversational music recommendation. <div>
arXiv:2512.16108v1 Announce Type: new 
Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs</title>
<link>https://arxiv.org/abs/2512.16149</link>
<guid>https://arxiv.org/abs/2512.16149</guid>
<content:encoded><![CDATA[
<div> ToolForge, synthetic data generation, multi-hop reasoning, tool invocation, validation framework<br /><br />Summary: Training large language models (LLMs) to effectively invoke external tools and utilize retrieved information requires diverse and high-quality data. Existing methods depend heavily on large-scale real API calls, which are costly and fail to adequately support multi-hop reasoning or self-reflection. To overcome these challenges, the authors propose ToolForge, an automated data synthesis framework that only uses a small set of virtual tools, eliminating the dependence on real API calls. ToolForge generates extensive tool-learning datasets from (question, golden context, answer) triples tailored for multi-hop search tasks, enhancing data richness via multi-hop reasoning and self-reflection processes. Ensuring the high quality of synthesized data, a Multi-Layer Validation Framework combining rule-based and model-based evaluations is employed. Experimental evaluations demonstrate that an 8-billion-parameter model trained solely on ToolForge-generated data surpasses GPT-4o’s performance across multiple benchmarks. The authors have made their code and dataset publicly accessible at the provided GitHub repository, facilitating further research and development in tool use by LLMs. <div>
arXiv:2512.16149v1 Announce Type: new 
Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Science Consultant Agent</title>
<link>https://arxiv.org/abs/2512.16171</link>
<guid>https://arxiv.org/abs/2512.16171</guid>
<content:encoded><![CDATA[
<div> Keywords: Science Consultant Agent, AI tool, modeling strategy, research-guided recommendation, prototype builder<br /><br />Summary: The Science Consultant Agent is an innovative web-based Artificial Intelligence tool designed to assist practitioners in selecting and implementing the most effective modeling strategies for AI solutions. It features four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder, each contributing to an efficient development workflow. The Questionnaire gathers structured input from users to understand specific project requirements and contexts. The Smart Fill component streamlines data entry by predicting and auto-completing relevant fields, enhancing user experience and accuracy. The Research-Guided Recommendation leverages up-to-date scientific literature to suggest optimal AI modeling approaches tailored to user inputs, ensuring solutions are grounded in validated research. Finally, the Prototype Builder enables rapid generation of functional prototypes based on the recommended strategies, allowing users ranging from Product Managers to Researchers to quickly transition from planning to implementation. Collectively, this pipeline accelerates AI development and democratizes access to cutting-edge modeling techniques by integrating expert knowledge with automated processes, as visually depicted in Figure 1 of the article. <div>
arXiv:2512.16171v1 Announce Type: new 
Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications</title>
<link>https://arxiv.org/abs/2512.16185</link>
<guid>https://arxiv.org/abs/2512.16185</guid>
<content:encoded><![CDATA[
<div> Weighted K-harmonic means, clustering algorithm, wireless networks, convergence guarantees, user association<br /><br />Summary:<br /><br />1. The paper introduces the weighted K-harmonic means (WKHM) clustering algorithm, a regularized version of the traditional K-harmonic means designed to provide numerical stability and enable soft assignments via inverse-distance weighting.<br /><br />2. WKHM differs from classical K-means and constrained K-means by offering a direct interpretation in wireless networks, where the algorithm’s weights correspond exactly to fractional user association based on received signal strength.<br /><br />3. The authors establish rigorous convergence guarantees for WKHM in both deterministic and stochastic contexts, tackling complex technical challenges due to the problem’s non-convexity and random initialization.<br /><br />4. Specifically, they prove that WKHM demonstrates monotone descent towards a local minimum under fixed initialization, convergence in probability when initialized by a Binomial Point Process, and almost sure convergence under mild decay conditions—marking the first stochastic convergence proof for harmonic-mean-based clustering.<br /><br />5. Extensive simulations with varied user distributions demonstrate that WKHM provides a superior balance between minimum signal strength and load fairness compared to existing classical and modern clustering methods, making it a robust tool for joint radio node placement and user association in wireless network design. <div>
arXiv:2512.16185v1 Announce Type: new 
Abstract: We propose the \emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving</title>
<link>https://arxiv.org/abs/2512.16214</link>
<guid>https://arxiv.org/abs/2512.16214</guid>
<content:encoded><![CDATA[
<div> Keywords: PDE solving, multi-agent collaboration, toolchain, LLM-driven agents, automated scientific computing<br /><br />Summary:<br /><br />1. Solving Partial Differential Equations (PDEs) is crucial in engineering and scientific research, but traditional methods require manual setup and domain expertise.  
2. Existing neural network approaches such as Physics-Informed Neural Networks (PINNs) and frameworks like DeepXDE improve automation but still depend heavily on expert knowledge and lack full autonomy.  
3. The paper introduces PDE-Agent, a novel framework that treats PDE solving as tool invocation managed by large language model (LLM)-driven agents, combining LLM reasoning with external tool controllability to fully automate PDE solving from natural language descriptions.  
4. PDE-Agent features two core innovations: (a) a Prog-Act framework with graph memory enabling multi-agent collaboration with effective dynamic planning and error correction through dual-loop mechanisms (localized fixes and global revisions), and (b) a Resource-Pool with a tool-parameter separation mechanism that centralizes runtime artifact management and resolves inter-tool dependencies.  
5. To benchmark and evaluate this paradigm, the authors develop PDE-Bench, a multi-type PDE benchmark for agent-based tool collaborative solving, alongside multi-level metrics assessing tool coordination. Experimental results show PDE-Agent’s superior performance and applicability in complex, multi-step, cross-step dependent PDE tasks, signaling a new direction for automated scientific computing.  
6. The source code and dataset will be publicly released, promoting further research and development in this area. <div>
arXiv:2512.16214v1 Announce Type: new 
Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis</title>
<link>https://arxiv.org/abs/2512.16237</link>
<guid>https://arxiv.org/abs/2512.16237</guid>
<content:encoded><![CDATA[
<div> Keywords: Embodied intelligence, Spatial reasoning, Vision-Language Models, Large Language Models, Dataset synthesis<br /><br />Summary:  
Embodied intelligence in AI faces fundamental limits due to insufficient spatial understanding and reasoning in current models. Existing approaches to improve Vision-Language Models (VLMs) struggle between scalable but rigid template-based datasets and linguistically diverse but unscalable and imprecise manual annotation. The paper introduces SPRITE, a novel framework that leverages simulators and large language models (LLMs) to synthesize scalable, diverse, and high-quality spatial reasoning data programmatically. SPRITE reframes ground-truth generation as a code-generation problem, using LLMs to compile spatial questions into executable programs verified by precise scene metadata from simulators. This ensures highly accurate and verifiable ground truth data alongside linguistic diversity. Employing this method, the authors curated a large dataset including 3 simulators, over 11,000 scenes, and more than 300,000 image/video instruction-tuning pairs. Experiments show that VLMs trained on this data significantly outperform models trained on other open-source datasets of comparable size on multiple spatial reasoning benchmarks. Scalability analysis confirms that overcoming the low linguistic diversity of traditional templates is key to robust and generalizable spatial intelligence. The SPRITE framework and full dataset will be publicly released to support further research in spatial reasoning and embodied AI. <div>
arXiv:2512.16237v1 Announce Type: new 
Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints</title>
<link>https://arxiv.org/abs/2512.16245</link>
<guid>https://arxiv.org/abs/2512.16245</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, model merging, Fisher-Rao geometry, AlignMerge<br /><br />Summary:<br /><br />1. The paper addresses the problem of merging large language models (LLMs) by combining multiple fine-tuned checkpoints without retraining, highlighting that common methods like linear weight soups and Fisher-weighted averaging may preserve loss but can degrade model alignment. <br /><br />2. The authors argue that merging should be treated as a geometry-constrained operation centered on a pre-aligned anchor model, emphasizing that the process must respect alignment-related geometry actively rather than verifying alignment only after merging.<br /><br />3. They introduce AlignMerge, a novel framework for geometry-aware merging that explicitly maintains alignment as an invariant by operating in a local Fisher information metric space around an instruction-tuned base model.<br /><br />4. AlignMerge optimizes a combined loss: L_geo to keep close proximity in Fisher-Rao geometry to expert models, L_align to penalize deviation along alignment-sensitive directions identified by a projector P_A, and L_bud to enforce an alignment budget constraint. It uses a latent-space Alignment Quality Index (AQI) to measure alignment effectiveness.<br /><br />5. Experiments on five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2) demonstrate that AlignMerge improves alignment metrics such as AQI, reduces toxicity, and performs well on tasks like instruction-following and reasoning, outperforming prior methods like Fisher soups, TIES, SafeMerge, and MergeAlign in preserving alignment and model quality.<br /><br />The results promote alignment-preserving merging as a crucial design goal and suggest geometry-aware composition as a promising direction in future foundation model development. <div>
arXiv:2512.16245v1 Announce Type: new 
Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding</title>
<link>https://arxiv.org/abs/2512.16250</link>
<guid>https://arxiv.org/abs/2512.16250</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, agentic reasoning, audio-visual understanding, AMUSE benchmark, RAFT framework<br /><br />Summary: Recent multimodal large language models (MLLMs) like GPT-4o and Qwen3-Omni demonstrate strong perception abilities but face challenges in multi-speaker, dialogue-centric scenarios that require agentic reasoning—tracking speakers, maintaining conversational roles, and grounding events over time. These challenges are especially critical in multimodal audio-video understanding applications such as conversational video assistants and meeting analytics. To address this, the authors present AMUSE, a benchmark designed to evaluate MLLMs on inherently agentic tasks that break down complex audio-visual interactions into planning, grounding, and reflection phases. AMUSE tests models in three modes—zero-shot, guided, and agentic—across six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Evaluations reveal that existing models struggle with multi-speaker reasoning and exhibit inconsistent performance in both agentic and non-agentic settings. To improve this, the authors propose RAFT, a data-efficient agentic alignment framework that combines reward optimization with intrinsic multimodal self-evaluation and selective parameter adaptation for efficient learning updates. Implementing RAFT leads to up to a 39.52% relative accuracy improvement on the AMUSE benchmark. Together, AMUSE and RAFT offer a robust platform to assess and enhance agentic reasoning capabilities in multimodal models. <div>
arXiv:2512.16250v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Wait: Synchronizing Agents with the Physical World</title>
<link>https://arxiv.org/abs/2512.16262</link>
<guid>https://arxiv.org/abs/2512.16262</guid>
<content:encoded><![CDATA[
<div> Temporal Gap, Large Language Models, Cognitive Timeline, Asynchronous Environment, Time Synchronization<br /><br />Summary:<br /><br />1. The paper addresses a key challenge in real-world agentic tasks where actions are non-blocking and have variable latencies, causing a temporal gap between action initiation and completion. 2. Traditional environment-side solutions like blocking wrappers or frequent polling have drawbacks such as reduced scalability and redundant observations that dilute the agent’s context window. 3. The authors propose an agent-side approach that allows Large Language Models (LLMs) to actively align their cognitive timeline with the real-world temporal dynamics, improving synchronization without excessive environment querying. 4. By extending the Code-as-Action paradigm into the temporal domain, the agents leverage semantic priors and In-Context Learning (ICL) to predict accurate waiting times using commands like time.sleep(t), thereby managing asynchronous actions efficiently. 5. Experimental validation in a simulated Kubernetes cluster shows that agents can finely tune their internal clocks to reduce both query overhead and execution latency, proving that temporal awareness is a learnable, critical skill for autonomous agents operating in open-ended, real-world environments. <div>
arXiv:2512.16262v1 Announce Type: new 
Abstract: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems</title>
<link>https://arxiv.org/abs/2512.16279</link>
<guid>https://arxiv.org/abs/2512.16279</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety policies, machine-checkable rules, multi-agent system, runtime enforcement  

<br /><br />Summary:  
1. The paper addresses safety risks in large language model (LLM)-based agents that perform complex tasks using tools, multi-step plans, and communication among agents.  
2. Current deployer-written safety policies, expressed in natural language, are ambiguous and context-dependent, making them difficult to translate into machine-checkable rules and leading to unreliable runtime enforcement.  
3. The authors introduce \textsc{QuadSentinel}, a novel safety mechanism composed of four agents: a state tracker, policy verifier, threat watcher, and referee. This multi-agent guard system compiles natural language policies into precise machine-checkable rules based on predicates over observable states.  
4. The referee component employs logic combined with an efficient top-$k$ predicate updating strategy to prioritize safety checks and resolve conflicts hierarchically, thereby maintaining low computational costs.  
5. Empirical evaluation on benchmarks ST-WebAgentBench and AgentHarm demonstrates that \textsc{QuadSentinel} outperforms single-agent baselines like ShieldAgent by improving guardrail accuracy, rule recall, and reducing false positives, offering better overall safety control.  
6. The approach allows near-term deployments to adopt this safety framework without modifying existing core agents by keeping policies separate and machine-checkable.  
7. The authors commit to releasing their code publicly at https://github.com/yyiliu/QuadSentinel to facilitate adoption and further research. <div>
arXiv:2512.16279v1 Announce Type: new 
Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models</title>
<link>https://arxiv.org/abs/2512.16295</link>
<guid>https://arxiv.org/abs/2512.16295</guid>
<content:encoded><![CDATA[
<div> GUI navigation, critic models, data synthesis, OS-Oracle, step-level evaluation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reliable step-level decision-making for VLM-powered computer-using agents (CUAs) navigating graphical user interfaces (GUIs), where errors in long-horizon workflows accumulate causing significant issues.<br /><br />2. Existing critic models, which assess the appropriateness of each action before execution, suffer from scarce diverse, high-quality GUI feedback data and lack public benchmarks for thorough step-level evaluation.<br /><br />3. To overcome these limitations, the authors introduce OS-Oracle, which includes: (a) a scalable data pipeline to synthesize cross-platform GUI critic data, (b) a two-stage training approach combining supervised fine-tuning (SFT) with consistency-preserving group relative policy optimization (CP-GRPO), and (c) OS-Critic Bench, a comprehensive benchmark spanning Mobile, Web, and Desktop platforms.<br /><br />4. Using this framework, the team curated a large dataset of 310,000 critic samples, enabling effective training of the OS-Oracle-7B model.<br /><br />5. OS-Oracle-7B achieves state-of-the-art performance on the OS-Critic Bench among open-source visual language models, surpasses proprietary models specifically in the mobile domain, and enhances performance when used as a pre-critic for native GUI agents like UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code and resources have been open-sourced for community use. <div>
arXiv:2512.16295v1 Announce Type: new 
Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection</title>
<link>https://arxiv.org/abs/2512.16300</link>
<guid>https://arxiv.org/abs/2512.16300</guid>
<content:encoded><![CDATA[
<div> Keywords: image forgery detection, multimodal large language models, Python-based tools, reinforcement fine-tuning, reasoning loop<br /><br />Summary:<br /><br />Existing image forgery detection (IFD) approaches either focus on low-level, semantics-agnostic artifacts or utilize multimodal large language models (MLLMs) to leverage high-level semantic information. However, integrating these heterogeneous information streams and their distinct reasoning paradigms remains challenging. To overcome this, the paper introduces ForenAgent, a novel multi-round interactive IFD framework where MLLMs autonomously generate, execute, and iteratively refine Python-based low-level tools to enhance forgery analysis flexibility and interpretability. ForenAgent's training involves two stages: Cold Start to bootstrap tool interaction capabilities, followed by Reinforcement Fine-Tuning to improve reasoning adaptability dynamically. The system features a human-inspired dynamic reasoning loop, encompassing global perception, local focusing, iterative probing, and holistic adjudication, which is implemented as a data-sampling strategy and task-aligned reward mechanism. To facilitate comprehensive development and assessment, the authors create FABench, a large-scale heterogeneous dataset with 100k images and nearly 200k agent-interaction question-answer pairs. Experimental results demonstrate that ForenAgent develops emergent tool-using skills and reflective reasoning, especially when supported by low-level tools, suggesting promising progress toward a general-purpose image forgery detection system. The source code is planned to be released post-review. <div>
arXiv:2512.16300v1 Announce Type: new 
Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation of Agentic AI</title>
<link>https://arxiv.org/abs/2512.16301</link>
<guid>https://arxiv.org/abs/2512.16301</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, adaptation, foundation models, tool adaptation, system design  

<br /><br />Summary:  
This paper addresses the development and improvement of agentic AI systems, which are built on foundation models capable of planning, reasoning, and interacting with external tools for complex tasks. It emphasizes the importance of adaptation as a central mechanism to enhance performance, reliability, and generalization of these systems. The authors propose a unified framework that categorizes adaptation strategies into agent adaptations and tool adaptations. Agent adaptations are further divided into tool-execution-signaled and agent-output-signaled forms, while tool adaptations are classified as agent-agnostic or agent-supervised. This framework clarifies the design space for adaptation strategies, making their trade-offs explicit and providing practical guidance for selecting or switching strategies during system design. The paper also reviews representative approaches within each category, analyzing their strengths and limitations. Finally, it identifies key open challenges and future opportunities in the field, aiming to provide both a conceptual foundation and practical roadmap. Overall, this work supports researchers and practitioners in building more capable, efficient, and reliable agentic AI systems by systematically organizing adaptation research and suggesting informed pathways for advancement. <div>
arXiv:2512.16301v1 Announce Type: new 
Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference</title>
<link>https://arxiv.org/abs/2512.16317</link>
<guid>https://arxiv.org/abs/2512.16317</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized LLM inference, Proof of Quality, cost-aware framework, evaluator architecture, quality-cost analysis  

<br /><br />Summary:  
This paper addresses the challenge of scaling verification for decentralized large language model (LLM) inference by proposing a cost-aware Proof of Quality (PoQ) framework, which incorporates efficiency metrics into the reward mechanism for both inference and evaluator nodes. The approach combines multiple evaluation techniques—ground truth token-level F1 scores, lightweight learned evaluators, and GPT-based judgments—into a single pipeline, utilizing a linear reward function that balances normalized output quality against computational cost. Experiments were conducted on tasks including extractive question answering and abstractive summarization, using five instruction-tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B alongside three evaluation models featuring cross encoder and bi-encoder architectures. Results demonstrate that semantic textual similarity bi-encoders better correlate with ground truth and GPT evaluations than cross encoders, underscoring evaluator architecture as a vital design consideration for PoQ. Quality-cost analysis showed that the largest models also yielded the best quality per unit latency, challenging assumptions about model size vs. efficiency. Monte Carlo simulations across 5,000 PoQ rounds confirmed that the cost-aware reward scheme favors high-quality, low-cost inference models and efficient evaluators while penalizing slower, lower-quality nodes. The work establishes cost-aware PoQ as a feasible and economically sustainable foundation for decentralized LLM inference. <div>
arXiv:2512.16317v1 Announce Type: new 
Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Needs Physics More Than Physics Needs AI</title>
<link>https://arxiv.org/abs/2512.16344</link>
<guid>https://arxiv.org/abs/2512.16344</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, physics, large language models, quantum AI, Big AI<br /><br />Summary:  
Artificial intelligence (AI) is widely regarded as transformative, yet after over a decade of widespread enthusiasm, its practical impact is still relatively limited outside select high-profile scientific and commercial cases. The 2024 Nobel Prizes in Chemistry and Physics have acknowledged AI's potential, but broader evaluations suggest these achievements are more promotional than reflective of substantial technical breakthroughs. While current AI architectures—including large language models, reasoning models, and agentic AI—are impressive, they depend on extremely large amounts of parameters that often lack meaningful interpretation and suffer from distributional biases. Moreover, these models fail to provide reliable uncertainty quantification, mechanistic insights, or even capture fundamental scientific laws. The article reviews these critical shortcomings and explores emerging opportunities in quantum AI and analogue computing as promising directions. It advocates for a future paradigm called “Big AI,” which combines the rigor of theoretical physics and scientific principles with the adaptable power of machine learning techniques. This synthesis aims to overcome current limitations and enable AI systems that are more scientifically grounded, interpretable, and capable of contributing robust insights across disciplines, especially physics. <div>
arXiv:2512.16344v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCIA: A Path Construction Imitation Algorithm for Global Optimization</title>
<link>https://arxiv.org/abs/2512.16392</link>
<guid>https://arxiv.org/abs/2512.16392</guid>
<content:encoded><![CDATA[
<div> Keywords: metaheuristic optimization, Path Construction Imitation Algorithm, swarm-based algorithms, route construction, constrained optimization<br /><br />Summary:<br /><br />This paper introduces a novel metaheuristic optimization algorithm called the Path Construction Imitation Algorithm (PCIA), inspired by human methods of creating and selecting transportation routes. The algorithm mimics how humans prefer popular routes and adapt by intelligently mixing existing paths when faced with closures, as well as by randomly choosing diverse paths to reach unfamiliar destinations. PCIA operates by generating a random population of candidate solutions, each representing a potential path toward an optimization goal, similar to swarm intelligence strategies. The effectiveness and robustness of PCIA were evaluated through extensive testing on 53 mathematical optimization problems and 13 constrained optimization problems. Results demonstrated that PCIA delivers highly competitive performance, often outperforming or matching both widely known and recently developed metaheuristic algorithms. This suggests that the approach effectively balances exploration and exploitation in the search space. The study highlights the algorithm’s potential for various optimization challenges, showing adaptability inspired by human navigation and decision-making behavior in constructing optimal or near-optimal solutions. <div>
arXiv:2512.16392v1 Announce Type: new 
Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs</title>
<link>https://arxiv.org/abs/2512.16424</link>
<guid>https://arxiv.org/abs/2512.16424</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthelite, large language models, retrosynthetic transformations, computer-aided synthesis planning, chemical feasibility  

<br /><br />Summary:  
This work introduces Synthelite, an innovative computer-aided synthesis planning (CASP) framework that harnesses large language models (LLMs) to propose retrosynthetic transformations directly. Unlike existing CASP frameworks, Synthelite enables interactive collaboration with human experts through natural language prompts, allowing chemists to guide and refine synthesis strategies. The framework is capable of generating complete synthesis routes end-to-end by leveraging the chemical knowledge and reasoning abilities inherent to LLMs. Experimental results show Synthelite achieves up to 95% success rates on tasks constrained by either synthesis strategy or available starting materials, demonstrating its adaptability to various user-specified requirements. Moreover, Synthelite can evaluate chemical feasibility during the route design process, ensuring that proposed synthesis plans are practical and realistic. The authors propose that Synthelite not only serves as a valuable tool for synthetic chemists but also represents a step toward future synthesis planning paradigms where LLMs serve as central orchestrators, integrating expert insights with automated chemical reasoning for improved synthetic route design. <div>
arXiv:2512.16424v1 Announce Type: new 
Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles</title>
<link>https://arxiv.org/abs/2512.16442</link>
<guid>https://arxiv.org/abs/2512.16442</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, AI-supported research, research life cycle, reproducibility<br /><br />Summary:<br /><br />1. The paper presents the TIB AIssistant, an AI-supported research platform designed to assist researchers across the entire research life cycle, leveraging the growing potential of Artificial Intelligence and Large Language Models.  2. The AIssistant is composed of multiple specialized assistants, each dedicated to a distinct research task, ensuring focused and efficient support for various stages of research.  3. It integrates tools that provide access to external scholarly services, broadening the scope and resources available to researchers within the platform.  4. Data generated during research is stored in assets and can be exported as an RO-Crate bundle, promoting transparency and reproducibility by encapsulating research outputs and metadata in a standardized format.  5. Through a sequential demonstration, the AIssistant showcases how its components interact to generate sections of a draft research paper, illustrating practical usability and the potential to streamline academic writing. Ultimately, the work establishes a foundation for a community-maintained platform aimed at enhancing AI-supported research workflows and fostering collaborative improvements in academic productivity. <div>
arXiv:2512.16442v1 Announce Type: new 
Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm</title>
<link>https://arxiv.org/abs/2512.16444</link>
<guid>https://arxiv.org/abs/2512.16444</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, StarCraft II battle arena, adversarial benchmarking, PyMARL, algorithm evaluation<br /><br />Summary:<br /><br />This article addresses the limitation of current multi-agent reinforcement learning (MARL) evaluations, which typically use fixed built-in AI opponents in the StarCraft multi-agent challenge (SMAC), resulting in less diverse and versatile benchmarking. To overcome this, the authors propose the StarCraft II battle arena (SC2BA), a new environment designed specifically for inter-algorithm adversarial competition, emphasizing fairness, usability, and customizability. The SC2BA leverages the StarCraft infrastructure and provides a fresh adversary paradigm for MARL benchmarking. In addition, the authors develop an adversarial PyMARL (APyMARL) library, featuring easy-to-use interfaces and modules that facilitate algorithm-versus-algorithm experiments. Using SC2BA, the study conducts comprehensive benchmarks of classic MARL algorithms under two adversarial modes: dual-algorithm paired adversary, which evaluates pairwise algorithm competitions, and multi-algorithm mixed adversary, which assesses interactions among multiple algorithms simultaneously. The experimental results reveal insightful observations and challenges related to the effectiveness, sensitivity, and scalability of existing MARL algorithms. The authors have made the SC2BA environment and the reproduced benchmark experiments publicly available on GitHub, hoping to foster progress and renewed interest in adversarial benchmarking within the MARL research community. <div>
arXiv:2512.16444v1 Announce Type: new 
Abstract: Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards AI-Supported Research: a Vision of the TIB AIssistant</title>
<link>https://arxiv.org/abs/2512.16447</link>
<guid>https://arxiv.org/abs/2512.16447</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, Research Workflow, Human-Machine Collaboration, AI-driven Platform<br /><br />Summary: The paper introduces the TIB AIssistant, a domain-agnostic platform designed to integrate Generative AI and Large Language Models into research workflows across various disciplines. It aims to address challenges such as diverse domain needs, limited AI literacy among researchers, the complexity of managing multiple tools and agents, and the uncertainty regarding the accuracy of AI-generated outputs in scientific research. The TIB AIssistant supports human-machine collaboration by providing AI assistants that assist throughout the research life cycle, including ideation, literature review, methodology design, data analysis, and scholarly writing. The system’s architecture comprises modular components like prompt and tool libraries, a shared data store for collaborative knowledge management, and a flexible orchestration framework that enables seamless coordination of AI services and tools. An early prototype implementation demonstrates the platform’s feasibility and highlights its potential to enhance productivity and innovation in scientific discovery. This vision emphasizes building a flexible, extensible, and researcher-friendly solution to harness the benefits of AI while mitigating its limitations, ultimately transforming how researchers approach and execute their work. <div>
arXiv:2512.16447v1 Announce Type: new 
Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries</title>
<link>https://arxiv.org/abs/2512.16453</link>
<guid>https://arxiv.org/abs/2512.16453</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, time-series data, battery energy storage system, prompting framework, anomaly detection  

<br /><br />Summary:  
The paper introduces TimeSeries2Report (TS2R), a novel prompting framework designed to leverage large language models (LLMs) for interpreting multivariate time-series data from lithium-ion battery energy storage systems (BESS). TS2R transforms raw operational time-series signals into structured, semantically enriched natural language reports by applying segmentation, semantic abstraction, and rule-based interpretation. This approach effectively bridges the gap between low-level sensor data and high-level contextual insights, enabling LLMs to perform reasoning, prediction, and decision-making tasks relevant to BESS operation and maintenance. The framework is evaluated on both laboratory-scale and real-world datasets, focusing on key downstream tasks such as anomaly detection, state-of-charge prediction, and charge/discharge management. Results demonstrate that TS2R-based report prompting consistently outperforms other baseline prompting methods including vision-, embedding-, and text-based approaches across accuracy, robustness, and explainability metrics. Importantly, TS2R enables LLMs to achieve expert-level decision-making quality and predictive consistency without the need for retraining or modifications to model architecture. This work establishes a practical and adaptive pathway for integrating LLMs into battery intelligence systems, potentially advancing the operational effectiveness and maintenance of BESS technologies. <div>
arXiv:2512.16453v1 Announce Type: new 
Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution</title>
<link>https://arxiv.org/abs/2512.16465</link>
<guid>https://arxiv.org/abs/2512.16465</guid>
<content:encoded><![CDATA[
<div> Keywords: CUDA kernel optimization, multi-agent framework, evolutionary algorithms, roofline-guided prompting, high-performance computing<br /><br />Summary:<br /><br />This paper addresses the complexity and expertise required for optimizing CUDA kernels, highlighting the limitations of current automatic optimization methods that combine large language models with evolutionary algorithms but suffer from suboptimal agent designs and mismatched evolutionary representations. The authors introduce cuPilot, a novel multi-agent framework that incorporates strategy as an intermediate semantic representation to coordinate kernel evolution more effectively. Key innovations include a strategy-coordinated evolution algorithm that aligns better with hardware characteristics, roofline-guided prompting to enhance the accuracy of performance predictions, and strategy-level population initialization to improve the evolutionary search process. Experimental evaluation on a benchmark set of 100 kernels demonstrates that cuPilot achieves an average speedup of 3.09 times over PyTorch-generated kernels, showing significant practical benefits. In particular, on General Matrix Multiply (GEMM) tasks, cuPilot achieves sophisticated optimization results and effectively utilizes critical hardware units to maximize performance. The authors have open-sourced the generated kernels, providing the community access to high-performance kernels and enabling further research and development. Overall, cuPilot represents an important step towards automated, intelligent, and hardware-aware CUDA kernel optimization. <div>
arXiv:2512.16465v1 Announce Type: new 
Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery</title>
<link>https://arxiv.org/abs/2512.16468</link>
<guid>https://arxiv.org/abs/2512.16468</guid>
<content:encoded><![CDATA[
<div> Keywords: Decisive Feature Fidelity, autonomous vehicles, simulation fidelity, explainable AI, domain transfer<br /><br />Summary:  
This paper addresses the challenge of ensuring reliable transfer of autonomous vehicle (AV) system performance from synthetic simulation environments to the real world by moving beyond traditional pixel-level visual realism. It introduces Decisive Feature Fidelity (DFF), a novel metric specifically designed to evaluate system-under-test (SUT) behavior by measuring mechanism parity—the degree to which the SUT relies on the same causal evidence in both real and simulated domains. DFF uses explainable AI (XAI) techniques to identify and compare the critical features driving the SUT’s decisions for paired real and synthetic inputs. To estimate DFF in practice, the authors propose methods based on counterfactual explanations and develop a DFF-guided calibration scheme to tune simulators for improved behavioral fidelity. Experimental evaluation on 2126 matched pairs from the KITTI and VirtualKITTI2 datasets demonstrates that DFF can detect significant discrepancies that conventional output-value fidelity metrics miss. Furthermore, applying DFF-guided calibration leads to improvements in decisive-feature fidelity and input-level similarity, while maintaining consistent output performance across a variety of AV systems. This work highlights the importance of behavior-grounded fidelity metrics for trustworthy simulation-based testing and offers practical tools to enhance the transferability of AV safety validation. <div>
arXiv:2512.16468v1 Announce Type: new 
Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network</title>
<link>https://arxiv.org/abs/2512.16491</link>
<guid>https://arxiv.org/abs/2512.16491</guid>
<content:encoded><![CDATA[
<div> meta-algorithmics, algorithm selection, experimental design, best practices, empirical research<br /><br />Summary:<br /><br />This report addresses the challenges and complexity of empirical research in meta-algorithmics, including key areas like algorithm selection, configuration, and scheduling. It highlights that such research typically involves extensive, computationally expensive experiments with many variables, which can introduce multiple sources of error affecting the scalability and reliability of scientific outcomes. The report acknowledges that while best practices exist, they are dispersed across various publications and fields, often evolving independently without unified guidance. To consolidate knowledge, the authors gather and systematize good practices that span the entire experimental cycle in meta-algorithmics. This includes formulating clear and relevant research questions, choosing appropriate experimental designs, executing experiments efficiently and reliably, and analyzing as well as presenting results impartially and transparently. The document serves to establish a current state-of-the-art baseline of empirical methodologies within the COSEAL community and related meta-algorithmic subfields. It aims to provide a comprehensive guide and reference for both newcomers and experienced practitioners engaged in meta-algorithmic research, helping to improve the rigor, reproducibility, and validity of future studies. <div>
arXiv:2512.16491v1 Announce Type: new 
Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParamExplorer: A framework for exploring parameters in generative art</title>
<link>https://arxiv.org/abs/2512.16529</link>
<guid>https://arxiv.org/abs/2512.16529</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative art, parameter spaces, reinforcement learning, human-in-the-loop, p5.js<br /><br />Summary:<br /><br />1. Generative art systems involve complex, high-dimensional parameter spaces where aesthetically pleasing results are rare and scattered, making exploration challenging.<br />2. Artists traditionally depend on manual trial-and-error methods that often miss many interesting parameter configurations due to the vastness of these spaces.<br />3. This work introduces ParamExplorer, an interactive and modular framework inspired by reinforcement learning that facilitates effective exploration of generative art parameter spaces.<br />4. ParamExplorer supports guidance through human-in-the-loop feedback or automated feedback, enhancing the discovery of compelling outputs.<br />5. The framework is designed to integrate seamlessly with existing p5.js projects, ensuring accessibility and ease of use.<br />6. Within ParamExplorer, several exploration strategies called agents are implemented and evaluated to determine their effectiveness in navigating parameter spaces.<br />7. These contributions aim to improve the efficiency and depth of parameter exploration in generative art, potentially uncovering novel artistic configurations that manual search methods overlook. <div>
arXiv:2512.16529v1 Announce Type: new 
Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Energy Efficiency of Local LLMs</title>
<link>https://arxiv.org/abs/2512.16531</link>
<guid>https://arxiv.org/abs/2512.16531</guid>
<content:encoded><![CDATA[
<div> Keywords: local inference, CPU benchmarks, scaling laws, vision-language models, quantum-inspired compression  

<br /><br />Summary:  
This article investigates the deployment of large language models (LLMs) and vision-language models on edge devices that rely solely on central processing units (CPUs), rather than graphics processors. The study benchmarks two common CPU platforms for local inference: the MacBook Pro M2 representing mainstream laptops, and the Raspberry Pi 5 representing low-power embedded systems. Through continuous monitoring of processor and memory usage combined with area-under-curve analysis, the authors identify how computational costs scale with input size for both language and vision-language models. Two key empirical scaling laws emerge: for language models, inference compute scales approximately linearly with token length; for vision-language models, a “resolution knee” phenomenon is observed where compute remains stable above a certain resolution threshold but sharply decreases below it due to preprocessing constraints. The paper further demonstrates that applying quantum-inspired compression techniques can significantly reduce processor and memory usage by up to 71.9%, and energy consumption by up to 62%, while maintaining or even improving semantic accuracy. These insights highlight model compression and input resolution adjustment as practical, low-cost strategies to optimize sustainable edge inference on CPU-based consumer hardware for multimodal AI workloads. <div>
arXiv:2512.16531v1 Announce Type: new 
Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment</title>
<link>https://arxiv.org/abs/2512.16532</link>
<guid>https://arxiv.org/abs/2512.16532</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Memory-enhanced personalization, Bias, Recruitment, AI agent guardrails<br /><br />Summary:<br /><br />Large Language Models (LLMs) have significantly advanced AI agents by enhancing their ability to understand, reason, and interact across diverse tasks. Integrating memory into these models elevates their capabilities by facilitating continuity across interactions, enabling learning from past experiences, and improving action and response relevance over time—this process is known as memory-enhanced personalization. Despite the clear benefits of personalization, it introduces notable risks of bias which have not been thoroughly explored in the context of memory-enhanced AI agents. The study focuses on recruitment as a practical use case to simulate and analyze how bias may emerge and be amplified at various stages of an agent’s operation. Experiments were conducted using safety-trained LLMs within personalized agents to investigate these dynamics. The results demonstrate that bias is not only introduced but also systematically reinforced through the personalization process driven by memory. This finding highlights a crucial need for implementing additional protective measures or agent guardrails to mitigate bias in memory-enhanced, LLM-based AI agents. Overall, the research underscores the importance of carefully addressing bias risks to ensure fairness and reliability in AI personalization applications. <div>
arXiv:2512.16532v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild</title>
<link>https://arxiv.org/abs/2512.16553</link>
<guid>https://arxiv.org/abs/2512.16553</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fuzzy Exploratory Search, Needle in the Web, ambiguous queries, search benchmarks<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have advanced from chatbots to sophisticated agents capable of automating complex real-world tasks involving web browsing and reasoning.<br /><br />2. Current benchmarks like BrowseComp and xBench-DeepSearch focus on complex multi-hop reasoning queries but fail to address Fuzzy Exploratory Search, which involves vague and multifaceted queries seeking the most relevant web pages instead of single factual answers.<br /><br />3. To fill this gap, the authors propose Needle in the Web, a new benchmark containing 663 questions across seven domains designed to evaluate search agents and LLM-based systems on their ability to retrieve and reason over real-world web content under ambiguity and varying difficulty.<br /><br />4. The benchmark uses a flexible methodology for query generation, ensuring high quality and uniqueness by basing queries on factual web content claims and allowing controllable difficulty levels.<br /><br />5. Benchmarking three leading LLMs and three agent-based search systems reveals poor overall performance, with most models scoring below 35% accuracy and no system consistently performing well across domains or difficulty levels, highlighting the challenge of effective fuzzy retrieval in semantic ambiguity.<br /><br />These findings underscore the need for further research into improving LLMs and search systems for handling ambiguous, exploratory queries over live web data. <div>
arXiv:2512.16553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam</title>
<link>https://arxiv.org/abs/2512.16644</link>
<guid>https://arxiv.org/abs/2512.16644</guid>
<content:encoded><![CDATA[
<div> Keywords: Sharia-compliant chatbot, Reinforcement Learning, Sentence-Transformers, Islamic knowledge, CRISP-DM methodology<br /><br />Summary:<br /><br />This research focuses on developing a Sharia-compliant chatbot designed to provide interactive consultations for Islamic questions. The system integrates Reinforcement Learning, specifically Q-Learning, with Sentence-Transformers to create semantic embeddings, ensuring responses are contextually relevant and accurate. Following the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology, the chatbot utilizes a curated dataset comprising 25,000 question-answer pairs gathered from authentic Islamic sources such as the Qur'an, Hadith, and scholarly fatwas, formatted in JSON to support flexibility and scalability. The prototype features a Flask API backend coupled with a Flutter-based mobile frontend, facilitating user interaction. Functional testing across diverse Islamic topics—fiqh, aqidah, ibadah, and muamalah—demonstrated an 87% semantic accuracy rate, highlighting the chatbot's capability to aid religious literacy and digital da'wah in the advancing Industry 4.0 era. Despite its effectiveness in answering closed-domain queries, the system currently faces limitations including static learning processes and dependence on its preset dataset. The study identifies avenues for future improvement, such as enabling continuous learning and supporting multi-turn conversations, aiming to merge traditional Islamic scholarship with modern AI-powered consultation for enhanced accessibility and engagement. <div>
arXiv:2512.16644v1 Announce Type: new 
Abstract: This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prefix Probing: Lightweight Harmful Content Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2512.16650</link>
<guid>https://arxiv.org/abs/2512.16650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, harmful content detection, prefix probing, inference latency, prefix caching<br /><br />Summary: This paper addresses the challenge of balancing detection accuracy, inference latency, and deployment cost in harmful content detection for large language models used in safety-critical applications. It introduces Prefix Probing, a novel black-box detection method that relies on comparing conditional log-probabilities of specific opening prefixes categorized as "agreement/execution" versus "refusal/safety." The approach utilizes prefix caching to minimize overhead, achieving detection latency close to that of generating the first token. During inference, Prefix Probing requires only a single computation of log-probabilities over these probe prefixes to generate a harmfulness score, allowing a threshold-based decision without the need for additional models or multi-stage processing. The paper also presents an efficient prefix construction algorithm designed to automatically identify highly informative prefixes, which substantially strengthens detection capabilities. Extensive experimental validation shows that Prefix Probing attains detection accuracy on par with commonly used external safety models while incurring minimal computational cost and eliminating the necessity for extra model deployment. These results highlight the method's practicality and efficiency, making it a promising solution for real-world safety-sensitive deployments of large language models. <div>
arXiv:2512.16650v1 Announce Type: new 
Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive AI Literacy: The Case for Centering Human Agency</title>
<link>https://arxiv.org/abs/2512.16656</link>
<guid>https://arxiv.org/abs/2512.16656</guid>
<content:encoded><![CDATA[
<div> Keywords: AI literacy, human agency, critical thinking, education framework, ethical reasoning<br /><br />Summary:<br /><br />1. The paper addresses the growing educational challenge caused by the rapid integration of AI technologies, highlighting a widening literacy gap. 2. It critiques current education systems for emphasizing operational skills with AI tools while neglecting critical and ethical reasoning. 3. The authors advocate for a systemic shift towards comprehensive AI literacy that centers on human agency—empowering individuals to make intentional, critical, and responsible decisions regarding AI usage. 4. Human agency applies to all stakeholders: students are encouraged to question, create with, or opt out of AI use based on contextual needs; teachers should retain pedagogical control by designing learning experiences aligned with educational values rather than deferring to AI tools. 5. True AI literacy involves teaching about agency itself, framing technology use as a conscious choice, supported by critical thinking and deep epistemological understanding. 6. The paper proposes AI Literacy, Fluency, and Competency frameworks that empower both educators and learners to articulate their intentions and attitudes towards AI, with implications for academic integrity, career development, and societal impact. <div>
arXiv:2512.16656v1 Announce Type: new 
Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm</title>
<link>https://arxiv.org/abs/2512.16694</link>
<guid>https://arxiv.org/abs/2512.16694</guid>
<content:encoded><![CDATA[
<div> Keywords: Apriori algorithm, hadith thematic grouping, unsupervised learning, association rule mining, digital Islamic texts<br /><br />Summary:  
This research addresses the need to automate the thematic classification of hadith, motivated by the increasing digitalization of Islamic texts. The study utilizes an unsupervised learning approach, specifically the Apriori algorithm, to detect association patterns and semantic relationships within unlabeled textual data. The dataset comprises the Indonesian translation of the hadith of Bukhari, which undergoes a series of preprocessing steps including case folding, punctuation removal, tokenization, stopword elimination, and stemming. Following preprocessing, association rule mining is performed using the Apriori algorithm with carefully set parameters such as support, confidence, and lift to identify meaningful associations. Results reveal significant thematic correlations like the links between rakaat and prayer, verse and revelation, and hadith and story. These associations correspond to key themes in Islamic studies: worship, divine revelation, and hadith narration. The findings demonstrate the capability of the Apriori algorithm to automatically uncover latent semantic relationships in religious texts. This research contributes to the field of digital Islamic studies and supports the development of technology-driven learning systems by providing an innovative method for thematic grouping based on data-driven patterns. <div>
arXiv:2512.16694v1 Announce Type: new 
Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning</title>
<link>https://arxiv.org/abs/2512.16698</link>
<guid>https://arxiv.org/abs/2512.16698</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent pipeline, diagram-grounded geometry, multimodal large language models, visual math benchmarks, open-source models  

<br /><br />Summary:  
This paper investigates the effectiveness of multi-agent versus single-agent designs for diagram-grounded geometry problem solving in multimodal large language models (MLLMs). The study evaluates both approaches across four prominent visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. Results indicate that multi-agent pipelines significantly enhance the performance of open-source models. For instance, the Qwen-2.5-VL model with 7 billion parameters achieves a +6.8 point improvement on Geometry3K using multi-agent, while the 32 billion parameter version gains +3.3 points. Both variants also show further improvements on OlympiadBench and We-Math with a multi-agent approach. Conversely, the closed-source Gemini-2.0-Flash model generally performs better with a single-agent pipeline on classic benchmarks. However, it still benefits modestly from multi-agent on newer datasets like We-Math. These findings demonstrate that multi-agent decomposition is advantageous for open-source systems and can assist proprietary models on less familiar benchmarks but may not be universally optimal. The authors provide all code, data, and reasoning files openly at the provided GitHub repository for reproducibility and further research. <div>
arXiv:2512.16698v1 Announce Type: new 
Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences</title>
<link>https://arxiv.org/abs/2512.16701</link>
<guid>https://arxiv.org/abs/2512.16701</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Cyber Humanism, Epistemic Agency, Algorithmic Citizenship, AI in Education<br /><br />Summary:<br /><br />This paper addresses the profound impact of Generative Artificial Intelligence (GenAI) on education, emphasizing how large language models transform traditional knowledge production into hybrid human-AI workflows. It highlights key concerns such as epistemic automation, cognitive offloading, and the potential de-professionalization of teachers. To counter these challenges, the authors propose a framework called Cyber Humanism in Education, which seeks to reclaim human agency within AI-enabled learning environments. These environments are viewed as socio-technical infrastructures co-created by humans and machines, where both educators and learners act as epistemic agents and "algorithmic citizens" with rights and responsibilities to influence these systems. The framework is built on three pillars: reflexive competence, algorithmic citizenship, and dialogic design, which are linked to global digital and AI competence standards. The paper showcases higher education case studies implementing these concepts through prompt-based learning and a Conversational AI Educator certification developed within the EPICT ecosystem. Results indicate that these practices can reinforce epistemic agency but also reveal challenges related to workload, equity, and governance. The study concludes with implications for developing AI-enriched, human-centered educational models that maintain a balance between technological innovation and human values. <div>
arXiv:2512.16701v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\-lisation of teachers. This paper proposes \emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.
  We articulate three pillars for cyber-humanist design, \emph{reflexive competence}, \emph{algorithmic citizenship}, and \emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \emph{prompt-based learning} and a new \emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems</title>
<link>https://arxiv.org/abs/2512.16707</link>
<guid>https://arxiv.org/abs/2512.16707</guid>
<content:encoded><![CDATA[
<div> Formal incompleteness, dynamical unpredictability, algorithmic intelligence, prediction horizon, self-analysis  

<br /><br />Summary:  
This article identifies and formalizes two fundamental computational limitations affecting algorithmic intelligence: formal incompleteness and dynamical unpredictability. Formal incompleteness restricts the deductive capabilities of any consistent reasoning system, meaning there are true statements that cannot be proven within the system. Dynamical unpredictability pertains to the inherent bounds on long-term prediction accuracy due to finite precision in computational processes. The authors demonstrate that these two limitations jointly create structural constraints on an intelligent agent's ability to evaluate and reason about its own prediction capabilities. Specifically, the paper proves that an algorithmic agent generally cannot compute its own maximal prediction horizon—the furthest point into the future it can accurately predict. This result reveals fundamental trade-offs among reasoning power, predictive accuracy, and self-referential analysis in intelligent systems. By integrating concepts from logic and dynamical systems theory, the work provides a novel theoretical framework clarifying why certain limits on intelligence and self-assessment are unavoidable. Ultimately, this research advances our understanding of the intrinsic boundaries that constrain algorithmic agents, offering insights relevant to the design and evaluation of artificial intelligence systems capable of self-improvement and foresight. <div>
arXiv:2512.16707v1 Announce Type: new 
Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering and Learning Probabilistic Models of Black-Box AI Capabilities</title>
<link>https://arxiv.org/abs/2512.16733</link>
<guid>https://arxiv.org/abs/2512.16733</guid>
<content:encoded><![CDATA[
arXiv:2512.16733v1 Announce Type: new 
Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach</title>
<link>https://arxiv.org/abs/2512.16739</link>
<guid>https://arxiv.org/abs/2512.16739</guid>
<content:encoded><![CDATA[
arXiv:2512.16739v1 Announce Type: new 
Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?</title>
<link>https://arxiv.org/abs/2512.16755</link>
<guid>https://arxiv.org/abs/2512.16755</guid>
<content:encoded><![CDATA[
arXiv:2512.16755v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</title>
<link>https://arxiv.org/abs/2512.16855</link>
<guid>https://arxiv.org/abs/2512.16855</guid>
<content:encoded><![CDATA[
arXiv:2512.16855v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional AGI Safety</title>
<link>https://arxiv.org/abs/2512.16856</link>
<guid>https://arxiv.org/abs/2512.16856</guid>
<content:encoded><![CDATA[
arXiv:2512.16856v1 Announce Type: new 
Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI</title>
<link>https://arxiv.org/abs/2512.16873</link>
<guid>https://arxiv.org/abs/2512.16873</guid>
<content:encoded><![CDATA[
arXiv:2512.16873v1 Announce Type: new 
Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16917</link>
<guid>https://arxiv.org/abs/2512.16917</guid>
<content:encoded><![CDATA[
arXiv:2512.16917v1 Announce Type: new 
Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
<link>https://arxiv.org/abs/2511.00456</link>
<guid>https://arxiv.org/abs/2511.00456</guid>
<content:encoded><![CDATA[
arXiv:2511.00456v5 Announce Type: cross 
Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</title>
<link>https://arxiv.org/abs/2512.15109</link>
<guid>https://arxiv.org/abs/2512.15109</guid>
<content:encoded><![CDATA[
arXiv:2512.15109v1 Announce Type: cross 
Abstract: The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression</title>
<link>https://arxiv.org/abs/2512.15721</link>
<guid>https://arxiv.org/abs/2512.15721</guid>
<content:encoded><![CDATA[
arXiv:2512.15721v1 Announce Type: cross 
Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Lens: Using Large Language Models to Understand Human Values</title>
<link>https://arxiv.org/abs/2512.15722</link>
<guid>https://arxiv.org/abs/2512.15722</guid>
<content:encoded><![CDATA[
arXiv:2512.15722v1 Announce Type: cross 
Abstract: The autonomous decision-making process, which is increasingly applied to computer systems, requires that the choices made by these systems align with human values. In this context, systems must assess how well their decisions reflect human values. To achieve this, it is essential to identify whether each available action promotes or undermines these values. This article presents Value Lens, a text-based model designed to detect human values using generative artificial intelligence, specifically Large Language Models (LLMs). The proposed model operates in two stages: the first aims to formulate a formal theory of values, while the second focuses on identifying these values within a given text. In the first stage, an LLM generates a description based on the established theory of values, which experts then verify. In the second stage, a pair of LLMs is employed: one LLM detects the presence of values, and the second acts as a critic and reviewer of the detection process. The results indicate that Value Lens performs comparably to, and even exceeds, the effectiveness of other models that apply different methods for similar tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction</title>
<link>https://arxiv.org/abs/2512.15728</link>
<guid>https://arxiv.org/abs/2512.15728</guid>
<content:encoded><![CDATA[
arXiv:2512.15728v1 Announce Type: cross 
Abstract: The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\% and stability of 93.33\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</title>
<link>https://arxiv.org/abs/2512.15729</link>
<guid>https://arxiv.org/abs/2512.15729</guid>
<content:encoded><![CDATA[
arXiv:2512.15729v1 Announce Type: cross 
Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Context-Free Smart Grid Model Using Complex System Approach</title>
<link>https://arxiv.org/abs/2512.15733</link>
<guid>https://arxiv.org/abs/2512.15733</guid>
<content:encoded><![CDATA[
arXiv:2512.15733v1 Announce Type: cross 
Abstract: Energy and pollution are urging problems of the 21th century. By gradually changing the actual power grid system, smart grid may evolve into different systems by means of size, elements and strategies, but its fundamental requirements and objectives will not change such as optimizing production, transmission, and consumption. Studying the smart grid through modeling and simulation provides us with valuable results which cannot be obtained in real world due to time and cost related constraints. Moreover, due to the complexity of the smart grid, achieving global optimization is not an easy task. In this paper, we propose a complex system based approach to the smart grid modeling, accentuating on the optimization by combining game theoretical and classical methods in different levels. Thanks to this combination, the optimization can be achieved with flexibility and scalability, while keeping its generality.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming</title>
<link>https://arxiv.org/abs/2512.15735</link>
<guid>https://arxiv.org/abs/2512.15735</guid>
<content:encoded><![CDATA[
arXiv:2512.15735v1 Announce Type: cross 
Abstract: This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Ensemble Learning for S\&amp;P 500 Directional Prediction</title>
<link>https://arxiv.org/abs/2512.15738</link>
<guid>https://arxiv.org/abs/2512.15738</guid>
<content:encoded><![CDATA[
arXiv:2512.15738v1 Announce Type: cross 
Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&amp;P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</title>
<link>https://arxiv.org/abs/2512.15739</link>
<guid>https://arxiv.org/abs/2512.15739</guid>
<content:encoded><![CDATA[
arXiv:2512.15739v1 Announce Type: cross 
Abstract: A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&amp;P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA2.0: Scaling Up Diffusion Language Models to 100B</title>
<link>https://arxiv.org/abs/2512.15745</link>
<guid>https://arxiv.org/abs/2512.15745</guid>
<content:encoded><![CDATA[
arXiv:2512.15745v1 Announce Type: cross 
Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction</title>
<link>https://arxiv.org/abs/2512.15751</link>
<guid>https://arxiv.org/abs/2512.15751</guid>
<content:encoded><![CDATA[
arXiv:2512.15751v1 Announce Type: cross 
Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification</title>
<link>https://arxiv.org/abs/2512.15753</link>
<guid>https://arxiv.org/abs/2512.15753</guid>
<content:encoded><![CDATA[
arXiv:2512.15753v1 Announce Type: cross 
Abstract: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single "Other" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning</title>
<link>https://arxiv.org/abs/2512.15756</link>
<guid>https://arxiv.org/abs/2512.15756</guid>
<content:encoded><![CDATA[
arXiv:2512.15756v1 Announce Type: cross 
Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction</title>
<link>https://arxiv.org/abs/2512.15762</link>
<guid>https://arxiv.org/abs/2512.15762</guid>
<content:encoded><![CDATA[
arXiv:2512.15762v1 Announce Type: cross 
Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</title>
<link>https://arxiv.org/abs/2512.15764</link>
<guid>https://arxiv.org/abs/2512.15764</guid>
<content:encoded><![CDATA[
arXiv:2512.15764v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.
  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.
  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.
  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2512.15766</link>
<guid>https://arxiv.org/abs/2512.15766</guid>
<content:encoded><![CDATA[
arXiv:2512.15766v1 Announce Type: cross 
Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework</title>
<link>https://arxiv.org/abs/2512.15767</link>
<guid>https://arxiv.org/abs/2512.15767</guid>
<content:encoded><![CDATA[
arXiv:2512.15767v1 Announce Type: cross 
Abstract: Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling</title>
<link>https://arxiv.org/abs/2512.15768</link>
<guid>https://arxiv.org/abs/2512.15768</guid>
<content:encoded><![CDATA[
arXiv:2512.15768v1 Announce Type: cross 
Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title>
<link>https://arxiv.org/abs/2512.15769</link>
<guid>https://arxiv.org/abs/2512.15769</guid>
<content:encoded><![CDATA[
arXiv:2512.15769v1 Announce Type: cross 
Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions</title>
<link>https://arxiv.org/abs/2512.15771</link>
<guid>https://arxiv.org/abs/2512.15771</guid>
<content:encoded><![CDATA[
arXiv:2512.15771v1 Announce Type: cross 
Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</title>
<link>https://arxiv.org/abs/2512.15773</link>
<guid>https://arxiv.org/abs/2512.15773</guid>
<content:encoded><![CDATA[
arXiv:2512.15773v1 Announce Type: cross 
Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</title>
<link>https://arxiv.org/abs/2512.15775</link>
<guid>https://arxiv.org/abs/2512.15775</guid>
<content:encoded><![CDATA[
arXiv:2512.15775v1 Announce Type: cross 
Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</title>
<link>https://arxiv.org/abs/2512.15780</link>
<guid>https://arxiv.org/abs/2512.15780</guid>
<content:encoded><![CDATA[
arXiv:2512.15780v1 Announce Type: cross 
Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance</title>
<link>https://arxiv.org/abs/2512.15786</link>
<guid>https://arxiv.org/abs/2512.15786</guid>
<content:encoded><![CDATA[
arXiv:2512.15786v1 Announce Type: cross 
Abstract: Cultural rights and the right to development are essential norms within the wider framework of international human rights law. However, recent technological advances in artificial intelligence (AI) and adjacent digital frontier technologies pose significant challenges to the protection and realization of these rights. This owes to the increasing influence of AI systems on the creation and depiction of cultural content, affect the use and distribution of the intellectual property of individuals and communities, and influence cultural participation and expression worldwide. In addition, the growing influence of AI thus risks exacerbating preexisting economic, social and digital divides and reinforcing inequities for marginalized communities. This dynamic challenges the existing interplay between cultural rights and the right to development, and raises questions about the integration of cultural and developmental considerations into emerging AI governance frameworks. To address these challenges, the paper examines the impact of AI on both categories of rights. Conceptually, it analyzes the epistemic and normative limitations of AI with respect to cultural and developmental assumptions embedded in algorithmic design and deployment, but also individual and structural impacts of AI on both rights. On this basis, the paper identifies gaps and tensions in existing AI governance frameworks with respect to cultural rights and the right to development.
  By situating cultural rights and the right to development within the broader landscape of AI and human rights, this paper contributes to the academic discourse on AI ethics, legal frameworks, and international human rights law. Finally, it outlines avenues for future research and policy development based on existing conversations in global AI governance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces</title>
<link>https://arxiv.org/abs/2512.15787</link>
<guid>https://arxiv.org/abs/2512.15787</guid>
<content:encoded><![CDATA[
arXiv:2512.15787v1 Announce Type: cross 
Abstract: In recent years, advances in artificial intelligence (AI), particularly generative AI (GenAI) and large language models (LLMs), have made human-computer interactions more frequent, efficient, and accessible across sectors ranging from banking to healthcare. AI tools embedded in digital devices support decision-making and operational management at both individual and organizational levels, including resource allocation, workflow automation, and real-time data analysis. However, the prevailing cloud-centric deployment of AI carries a substantial environmental footprint due to high computational demands. In this context, this paper introduces the concept of agentic environments, a sustainability-oriented AI framework that extends beyond reactive systems by leveraging GenAI, multi-agent systems, and edge computing to reduce the environmental impact of technology. Agentic environments enable more efficient resource use, improved quality of life, and sustainability-by-design, while simultaneously enhancing data privacy through decentralized, edge-driven solutions. Drawing on secondary research as well as primary data from focus groups and semi-structured interviews with AI professionals from leading technology companies, the paper proposes a conceptual framework for agentic environments examined through three lenses: the personal sphere, professional and commercial use, and urban operations. The findings highlight the potential of agentic environments to foster sustainable ecosystems through optimized resource utilization and strengthened data privacy. The study concludes with recommendations for edge-driven deployment models to reduce reliance on energy-intensive cloud infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud</title>
<link>https://arxiv.org/abs/2512.15791</link>
<guid>https://arxiv.org/abs/2512.15791</guid>
<content:encoded><![CDATA[
arXiv:2512.15791v1 Announce Type: cross 
Abstract: In Artificial Intelligence (AI), language models have gained significant importance due to the widespread adoption of systems capable of simulating realistic conversations with humans through text generation. Because of their impact on society, developing and deploying these language models must be done responsibly, with attention to their negative impacts and possible harms. In this scenario, the number of AI Ethics Tools (AIETs) publications has recently increased. These AIETs are designed to help developers, companies, governments, and other stakeholders establish trust, transparency, and responsibility with their technologies by bringing accepted values to guide AI's design, development, and use stages. However, many AIETs lack good documentation, examples of use, and proof of their effectiveness in practice. This paper presents a methodology for evaluating AIETs in language models. Our approach involved an extensive literature survey on 213 AIETs, and after applying inclusion and exclusion criteria, we selected four AIETs: Model Cards, ALTAI, FactSheets, and Harms Modeling. For evaluation, we applied AIETs to language models developed for the Portuguese language, conducting 35 hours of interviews with their developers. The evaluation considered the developers' perspective on the AIETs' use and quality in helping to identify ethical considerations about their model. The results suggest that the applied AIETs serve as a guide for formulating general ethical considerations about language models. However, we note that they do not address unique aspects of these models, such as idiomatic expressions. Additionally, these AIETs did not help to identify potential negative impacts of models for the Portuguese language.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Analysis of Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2512.15792</link>
<guid>https://arxiv.org/abs/2512.15792</guid>
<content:encoded><![CDATA[
arXiv:2512.15792v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly become indispensable tools for acquiring information and supporting human decision-making. However, ensuring that these models uphold fairness across varied contexts is critical to their safe and responsible deployment. In this study, we undertake a comprehensive examination of four widely adopted LLMs, probing their underlying biases and inclinations across the dimensions of politics, ideology, alliance, language, and gender. Through a series of carefully designed experiments, we investigate their political neutrality using news summarization, ideological biases through news stance classification, tendencies toward specific geopolitical alliances via United Nations voting patterns, language bias in the context of multilingual story completion, and gender-related affinities as revealed by responses to the World Values Survey. Results indicate that while the LLMs are aligned to be neutral and impartial, they still show biases and affinities of different types.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms</title>
<link>https://arxiv.org/abs/2512.15793</link>
<guid>https://arxiv.org/abs/2512.15793</guid>
<content:encoded><![CDATA[
arXiv:2512.15793v1 Announce Type: cross 
Abstract: Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action ``\textit{report a witnessed crime}" are social norms that inform our conduct, such as ``\textit{It is expected to be brave to report crimes}''. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to ``\textit{report a witnessed crime}'', one may balance \textit{bravery} against \textit{self-protection}. In this paper, we introduce \textit{ClarityEthic}, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India</title>
<link>https://arxiv.org/abs/2512.15799</link>
<guid>https://arxiv.org/abs/2512.15799</guid>
<content:encoded><![CDATA[
arXiv:2512.15799v1 Announce Type: cross 
Abstract: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI "dual-use" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven "tool crimes" and "target crimes." Consequently, the research proposes a "human-centric" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-wise Topological Divergence Gaps: Guiding Search in Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2512.15800</link>
<guid>https://arxiv.org/abs/2512.15800</guid>
<content:encoded><![CDATA[
arXiv:2512.15800v1 Announce Type: cross 
Abstract: We introduce a topological feedback mechanism for the Travelling Salesman Problem (TSP) by analyzing the divergence between a tour and the minimum spanning tree (MST). Our key contribution is a canonical decomposition theorem that expresses the tour-MST gap as edge-wise topology-divergence gaps from the RTD-Lite barcode. Based on this, we develop a topological guidance for 2-opt and 3-opt heuristics that increases their performance. We carry out experiments with fine-optimization of tours obtained from heatmap-based methods, TSPLIB, and random instances. Experiments demonstrate the topology-guided optimization results in better performance and faster convergence in many cases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory</title>
<link>https://arxiv.org/abs/2512.15813</link>
<guid>https://arxiv.org/abs/2512.15813</guid>
<content:encoded><![CDATA[
arXiv:2512.15813v1 Announce Type: cross 
Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning</title>
<link>https://arxiv.org/abs/2512.15816</link>
<guid>https://arxiv.org/abs/2512.15816</guid>
<content:encoded><![CDATA[
arXiv:2512.15816v1 Announce Type: cross 
Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</title>
<link>https://arxiv.org/abs/2512.15829</link>
<guid>https://arxiv.org/abs/2512.15829</guid>
<content:encoded><![CDATA[
arXiv:2512.15829v1 Announce Type: cross 
Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Agentic Language Model Inference via Speculative Tool Calls</title>
<link>https://arxiv.org/abs/2512.15834</link>
<guid>https://arxiv.org/abs/2512.15834</guid>
<content:encoded><![CDATA[
arXiv:2512.15834v1 Announce Type: cross 
Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.15885</link>
<guid>https://arxiv.org/abs/2512.15885</guid>
<content:encoded><![CDATA[
arXiv:2512.15885v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons</title>
<link>https://arxiv.org/abs/2512.15891</link>
<guid>https://arxiv.org/abs/2512.15891</guid>
<content:encoded><![CDATA[
arXiv:2512.15891v1 Announce Type: cross 
Abstract: In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of the millisecond precision of spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could, in principle, preserve and manipulate sensory information through spike timing. It could support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are synchronously stimulated. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces</title>
<link>https://arxiv.org/abs/2512.15892</link>
<guid>https://arxiv.org/abs/2512.15892</guid>
<content:encoded><![CDATA[
arXiv:2512.15892v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.
  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).
  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Story Frames: Contextual Reasoning about Narrative Intent and Reception</title>
<link>https://arxiv.org/abs/2512.15925</link>
<guid>https://arxiv.org/abs/2512.15925</guid>
<content:encoded><![CDATA[
arXiv:2512.15925v1 Announce Type: cross 
Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins</title>
<link>https://arxiv.org/abs/2512.15930</link>
<guid>https://arxiv.org/abs/2512.15930</guid>
<content:encoded><![CDATA[
arXiv:2512.15930v1 Announce Type: cross 
Abstract: Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them "undruggable". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
arXiv:2512.15938v1 Announce Type: cross 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15957</link>
<guid>https://arxiv.org/abs/2512.15957</guid>
<content:encoded><![CDATA[
arXiv:2512.15957v1 Announce Type: cross 
Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRAID: Bounded Reasoning for Autonomous Inference and Decisions</title>
<link>https://arxiv.org/abs/2512.15959</link>
<guid>https://arxiv.org/abs/2512.15959</guid>
<content:encoded><![CDATA[
arXiv:2512.15959v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering</title>
<link>https://arxiv.org/abs/2512.15979</link>
<guid>https://arxiv.org/abs/2512.15979</guid>
<content:encoded><![CDATA[
arXiv:2512.15979v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding Software Intent: Lightweight Java Module Recovery</title>
<link>https://arxiv.org/abs/2512.15980</link>
<guid>https://arxiv.org/abs/2512.15980</guid>
<content:encoded><![CDATA[
arXiv:2512.15980v1 Announce Type: cross 
Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Extracting the Features from a General Superposition</title>
<link>https://arxiv.org/abs/2512.15987</link>
<guid>https://arxiv.org/abs/2512.15987</guid>
<content:encoded><![CDATA[
arXiv:2512.15987v1 Announce Type: cross 
Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,\sigma_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $\sigma_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $\sigma_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate Neural Architecture Codesign Package (SNAC-Pack)</title>
<link>https://arxiv.org/abs/2512.15998</link>
<guid>https://arxiv.org/abs/2512.15998</guid>
<content:encoded><![CDATA[
arXiv:2512.15998v1 Announce Type: cross 
Abstract: Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</title>
<link>https://arxiv.org/abs/2512.16013</link>
<guid>https://arxiv.org/abs/2512.16013</guid>
<content:encoded><![CDATA[
arXiv:2512.16013v1 Announce Type: cross 
Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios</title>
<link>https://arxiv.org/abs/2512.16019</link>
<guid>https://arxiv.org/abs/2512.16019</guid>
<content:encoded><![CDATA[
arXiv:2512.16019v1 Announce Type: cross 
Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Language Bias Examination in Large Language Models</title>
<link>https://arxiv.org/abs/2512.16029</link>
<guid>https://arxiv.org/abs/2512.16029</guid>
<content:encoded><![CDATA[
arXiv:2512.16029v1 Announce Type: cross 
Abstract: This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We on the Right Way to Assessing LLM-as-a-Judge?</title>
<link>https://arxiv.org/abs/2512.16041</link>
<guid>https://arxiv.org/abs/2512.16041</guid>
<content:encoded><![CDATA[
arXiv:2512.16041v1 Announce Type: cross 
Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
<link>https://arxiv.org/abs/2512.16046</link>
<guid>https://arxiv.org/abs/2512.16046</guid>
<content:encoded><![CDATA[
arXiv:2512.16046v1 Announce Type: cross 
Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis</title>
<link>https://arxiv.org/abs/2512.16063</link>
<guid>https://arxiv.org/abs/2512.16063</guid>
<content:encoded><![CDATA[
arXiv:2512.16063v1 Announce Type: cross 
Abstract: Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil</title>
<link>https://arxiv.org/abs/2512.16071</link>
<guid>https://arxiv.org/abs/2512.16071</guid>
<content:encoded><![CDATA[
arXiv:2512.16071v1 Announce Type: cross 
Abstract: Widespread Pb (lead) contamination of urban soil significantly impacts food safety and public health and hinders city greening efforts. However, most existing technologies for measuring Pb are labor-intensive and costly. In this study, we propose SoilScanner, a radio frequency-based wireless system that can detect Pb in soils. This is based on our discovery that the propagation of different frequency band radio signals is affected differently by different salts such as NaCl and Pb(NO3)2 in the soil. In a controlled experiment, manually adding NaCl and Pb(NO3)2 in clean soil, we demonstrated that different salts reflected signals at different frequencies in distinct patterns. In addition, we confirmed the finding using uncontrolled field samples with a machine learning model. Our experiment results show that SoilScanner can classify soil samples into low-Pb and high-Pb categories (threshold at 200 ppm) with an accuracy of 72%, with no sample with > 500 ppm of Pb being misclassified. The results of this study show that it is feasible to build portable and affordable Pb detection and screening devices based on wireless technology.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of Generative Models for Emotional 3D Animation Generation in VR</title>
<link>https://arxiv.org/abs/2512.16081</link>
<guid>https://arxiv.org/abs/2512.16081</guid>
<content:encoded><![CDATA[
arXiv:2512.16081v1 Announce Type: cross 
Abstract: Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</title>
<link>https://arxiv.org/abs/2512.16083</link>
<guid>https://arxiv.org/abs/2512.16083</guid>
<content:encoded><![CDATA[
arXiv:2512.16083v1 Announce Type: cross 
Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPX: Lightweight Hourglass Network with Global Context</title>
<link>https://arxiv.org/abs/2512.16089</link>
<guid>https://arxiv.org/abs/2512.16089</guid>
<content:encoded><![CDATA[
arXiv:2512.16089v1 Announce Type: cross 
Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
arXiv:2512.16093v1 Announce Type: cross 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation</title>
<link>https://arxiv.org/abs/2512.16103</link>
<guid>https://arxiv.org/abs/2512.16103</guid>
<content:encoded><![CDATA[
arXiv:2512.16103v1 Announce Type: cross 
Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModelTables: A Corpus of Tables about Models</title>
<link>https://arxiv.org/abs/2512.16106</link>
<guid>https://arxiv.org/abs/2512.16106</guid>
<content:encoded><![CDATA[
arXiv:2512.16106v1 Announce Type: cross 
Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</title>
<link>https://arxiv.org/abs/2512.16123</link>
<guid>https://arxiv.org/abs/2512.16123</guid>
<content:encoded><![CDATA[
arXiv:2512.16123v1 Announce Type: cross 
Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INTELLECT-3: Technical Report</title>
<link>https://arxiv.org/abs/2512.16144</link>
<guid>https://arxiv.org/abs/2512.16144</guid>
<content:encoded><![CDATA[
arXiv:2512.16144v1 Announce Type: cross 
Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation</title>
<link>https://arxiv.org/abs/2512.16145</link>
<guid>https://arxiv.org/abs/2512.16145</guid>
<content:encoded><![CDATA[
arXiv:2512.16145v1 Announce Type: cross 
Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.16147</link>
<guid>https://arxiv.org/abs/2512.16147</guid>
<content:encoded><![CDATA[
arXiv:2512.16147v1 Announce Type: cross 
Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</title>
<link>https://arxiv.org/abs/2512.16164</link>
<guid>https://arxiv.org/abs/2512.16164</guid>
<content:encoded><![CDATA[
arXiv:2512.16164v1 Announce Type: cross 
Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services</title>
<link>https://arxiv.org/abs/2512.16167</link>
<guid>https://arxiv.org/abs/2512.16167</guid>
<content:encoded><![CDATA[
arXiv:2512.16167v1 Announce Type: cross 
Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Closing the Domain Gap with Event Cameras</title>
<link>https://arxiv.org/abs/2512.16178</link>
<guid>https://arxiv.org/abs/2512.16178</guid>
<content:encoded><![CDATA[
arXiv:2512.16178v1 Announce Type: cross 
Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Ad-hoc Categorization with Contextualized Feature Learning</title>
<link>https://arxiv.org/abs/2512.16202</link>
<guid>https://arxiv.org/abs/2512.16202</guid>
<content:encoded><![CDATA[
arXiv:2512.16202v1 Announce Type: cross 
Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural emulation of gravity-driven geohazard runout</title>
<link>https://arxiv.org/abs/2512.16221</link>
<guid>https://arxiv.org/abs/2512.16221</guid>
<content:encoded><![CDATA[
arXiv:2512.16221v1 Announce Type: cross 
Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Information-Theoretic Framework for Robust Large Language Model Editing</title>
<link>https://arxiv.org/abs/2512.16227</link>
<guid>https://arxiv.org/abs/2512.16227</guid>
<content:encoded><![CDATA[
arXiv:2512.16227v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</title>
<link>https://arxiv.org/abs/2512.16235</link>
<guid>https://arxiv.org/abs/2512.16235</guid>
<content:encoded><![CDATA[
arXiv:2512.16235v1 Announce Type: cross 
Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models</title>
<link>https://arxiv.org/abs/2512.16236</link>
<guid>https://arxiv.org/abs/2512.16236</guid>
<content:encoded><![CDATA[
arXiv:2512.16236v1 Announce Type: cross 
Abstract: Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.
  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16244</link>
<guid>https://arxiv.org/abs/2512.16244</guid>
<content:encoded><![CDATA[
arXiv:2512.16244v1 Announce Type: cross 
Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma-Moe-Tiny Technical Report</title>
<link>https://arxiv.org/abs/2512.16248</link>
<guid>https://arxiv.org/abs/2512.16248</guid>
<content:encoded><![CDATA[
arXiv:2512.16248v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2512.16251</link>
<guid>https://arxiv.org/abs/2512.16251</guid>
<content:encoded><![CDATA[
arXiv:2512.16251v1 Announce Type: cross 
Abstract: We introduce the \textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</title>
<link>https://arxiv.org/abs/2512.16270</link>
<guid>https://arxiv.org/abs/2512.16270</guid>
<content:encoded><![CDATA[
arXiv:2512.16270v1 Announce Type: cross 
Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</title>
<link>https://arxiv.org/abs/2512.16271</link>
<guid>https://arxiv.org/abs/2512.16271</guid>
<content:encoded><![CDATA[
arXiv:2512.16271v1 Announce Type: cross 
Abstract: Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.
  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.
  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls</title>
<link>https://arxiv.org/abs/2512.16272</link>
<guid>https://arxiv.org/abs/2512.16272</guid>
<content:encoded><![CDATA[
arXiv:2512.16272v1 Announce Type: cross 
Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFLAN: Generative Functional Layouts</title>
<link>https://arxiv.org/abs/2512.16275</link>
<guid>https://arxiv.org/abs/2512.16275</guid>
<content:encoded><![CDATA[
arXiv:2512.16275v1 Announce Type: cross 
Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams</title>
<link>https://arxiv.org/abs/2512.16280</link>
<guid>https://arxiv.org/abs/2512.16280</guid>
<content:encoded><![CDATA[
arXiv:2512.16280v1 Announce Type: cross 
Abstract: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.
  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</title>
<link>https://arxiv.org/abs/2512.16282</link>
<guid>https://arxiv.org/abs/2512.16282</guid>
<content:encoded><![CDATA[
arXiv:2512.16282v1 Announce Type: cross 
Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Selective Representation Misdirection for Machine Unlearning</title>
<link>https://arxiv.org/abs/2512.16297</link>
<guid>https://arxiv.org/abs/2512.16297</guid>
<content:encoded><![CDATA[
arXiv:2512.16297v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixelArena: A benchmark for Pixel-Precision Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.16303</link>
<guid>https://arxiv.org/abs/2512.16303</guid>
<content:encoded><![CDATA[
arXiv:2512.16303v1 Announce Type: cross 
Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2512.16307</link>
<guid>https://arxiv.org/abs/2512.16307</guid>
<content:encoded><![CDATA[
arXiv:2512.16307v1 Announce Type: cross 
Abstract: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation</title>
<link>https://arxiv.org/abs/2512.16310</link>
<guid>https://arxiv.org/abs/2512.16310</guid>
<content:encoded><![CDATA[
arXiv:2512.16310v1 Announce Type: cross 
Abstract: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
<link>https://arxiv.org/abs/2512.16334</link>
<guid>https://arxiv.org/abs/2512.16334</guid>
<content:encoded><![CDATA[
arXiv:2512.16334v1 Announce Type: cross 
Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Edge-to-Server Inference for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16349</link>
<guid>https://arxiv.org/abs/2512.16349</guid>
<content:encoded><![CDATA[
arXiv:2512.16349v1 Announce Type: cross 
Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</title>
<link>https://arxiv.org/abs/2512.16378</link>
<guid>https://arxiv.org/abs/2512.16378</guid>
<content:encoded><![CDATA[
arXiv:2512.16378v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2512.16391</link>
<guid>https://arxiv.org/abs/2512.16391</guid>
<content:encoded><![CDATA[
arXiv:2512.16391v1 Announce Type: cross 
Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</title>
<link>https://arxiv.org/abs/2512.16397</link>
<guid>https://arxiv.org/abs/2512.16397</guid>
<content:encoded><![CDATA[
arXiv:2512.16397v1 Announce Type: cross 
Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypernetworks That Evolve Themselves</title>
<link>https://arxiv.org/abs/2512.16406</link>
<guid>https://arxiv.org/abs/2512.16406</guid>
<content:encoded><![CDATA[
arXiv:2512.16406v1 Announce Type: cross 
Abstract: How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2512.16425</link>
<guid>https://arxiv.org/abs/2512.16425</guid>
<content:encoded><![CDATA[
arXiv:2512.16425v1 Announce Type: cross 
Abstract: As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bias and Fairness in Multi-Agent Decision Systems</title>
<link>https://arxiv.org/abs/2512.16433</link>
<guid>https://arxiv.org/abs/2512.16433</guid>
<content:encoded><![CDATA[
arXiv:2512.16433v1 Announce Type: cross 
Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Modelling Black Box Optimization</title>
<link>https://arxiv.org/abs/2512.16445</link>
<guid>https://arxiv.org/abs/2512.16445</guid>
<content:encoded><![CDATA[
arXiv:2512.16445v1 Announce Type: cross 
Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion</title>
<link>https://arxiv.org/abs/2512.16446</link>
<guid>https://arxiv.org/abs/2512.16446</guid>
<content:encoded><![CDATA[
arXiv:2512.16446v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value</title>
<link>https://arxiv.org/abs/2512.16448</link>
<guid>https://arxiv.org/abs/2512.16448</guid>
<content:encoded><![CDATA[
arXiv:2512.16448v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research</title>
<link>https://arxiv.org/abs/2512.16455</link>
<guid>https://arxiv.org/abs/2512.16455</guid>
<content:encoded><![CDATA[
arXiv:2512.16455v1 Announce Type: cross 
Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.16484</link>
<guid>https://arxiv.org/abs/2512.16484</guid>
<content:encoded><![CDATA[
arXiv:2512.16484v1 Announce Type: cross 
Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</title>
<link>https://arxiv.org/abs/2512.16485</link>
<guid>https://arxiv.org/abs/2512.16485</guid>
<content:encoded><![CDATA[
arXiv:2512.16485v1 Announce Type: cross 
Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16494</link>
<guid>https://arxiv.org/abs/2512.16494</guid>
<content:encoded><![CDATA[
arXiv:2512.16494v1 Announce Type: cross 
Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XTC, A Research Platform for Optimizing AI Workload Operators</title>
<link>https://arxiv.org/abs/2512.16512</link>
<guid>https://arxiv.org/abs/2512.16512</guid>
<content:encoded><![CDATA[
arXiv:2512.16512v1 Announce Type: cross 
Abstract: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence</title>
<link>https://arxiv.org/abs/2512.16515</link>
<guid>https://arxiv.org/abs/2512.16515</guid>
<content:encoded><![CDATA[
arXiv:2512.16515v1 Announce Type: cross 
Abstract: We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16523</link>
<guid>https://arxiv.org/abs/2512.16523</guid>
<content:encoded><![CDATA[
arXiv:2512.16523v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics</title>
<link>https://arxiv.org/abs/2512.16530</link>
<guid>https://arxiv.org/abs/2512.16530</guid>
<content:encoded><![CDATA[
arXiv:2512.16530v1 Announce Type: cross 
Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</title>
<link>https://arxiv.org/abs/2512.16586</link>
<guid>https://arxiv.org/abs/2512.16586</guid>
<content:encoded><![CDATA[
arXiv:2512.16586v1 Announce Type: cross 
Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</title>
<link>https://arxiv.org/abs/2512.16602</link>
<guid>https://arxiv.org/abs/2512.16602</guid>
<content:encoded><![CDATA[
arXiv:2512.16602v1 Announce Type: cross 
Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents</title>
<link>https://arxiv.org/abs/2512.16614</link>
<guid>https://arxiv.org/abs/2512.16614</guid>
<content:encoded><![CDATA[
arXiv:2512.16614v1 Announce Type: cross 
Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</title>
<link>https://arxiv.org/abs/2512.16626</link>
<guid>https://arxiv.org/abs/2512.16626</guid>
<content:encoded><![CDATA[
arXiv:2512.16626v1 Announce Type: cross 
Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking</title>
<link>https://arxiv.org/abs/2512.16658</link>
<guid>https://arxiv.org/abs/2512.16658</guid>
<content:encoded><![CDATA[
arXiv:2512.16658v1 Announce Type: cross 
Abstract: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance</title>
<link>https://arxiv.org/abs/2512.16661</link>
<guid>https://arxiv.org/abs/2512.16661</guid>
<content:encoded><![CDATA[
arXiv:2512.16661v1 Announce Type: cross 
Abstract: In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</title>
<link>https://arxiv.org/abs/2512.16685</link>
<guid>https://arxiv.org/abs/2512.16685</guid>
<content:encoded><![CDATA[
arXiv:2512.16685v1 Announce Type: cross 
Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library</title>
<link>https://arxiv.org/abs/2512.16715</link>
<guid>https://arxiv.org/abs/2512.16715</guid>
<content:encoded><![CDATA[
arXiv:2512.16715v1 Announce Type: cross 
Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeNet: A Light Weight Model for Low Bitrate Image Compression</title>
<link>https://arxiv.org/abs/2512.16743</link>
<guid>https://arxiv.org/abs/2512.16743</guid>
<content:encoded><![CDATA[
arXiv:2512.16743v1 Announce Type: cross 
Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</title>
<link>https://arxiv.org/abs/2512.16750</link>
<guid>https://arxiv.org/abs/2512.16750</guid>
<content:encoded><![CDATA[
arXiv:2512.16750v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation</title>
<link>https://arxiv.org/abs/2512.16770</link>
<guid>https://arxiv.org/abs/2512.16770</guid>
<content:encoded><![CDATA[
arXiv:2512.16770v1 Announce Type: cross 
Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Mass Spectrum Analysis with ASP</title>
<link>https://arxiv.org/abs/2512.16780</link>
<guid>https://arxiv.org/abs/2512.16780</guid>
<content:encoded><![CDATA[
arXiv:2512.16780v1 Announce Type: cross 
Abstract: We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implemen- tation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title>
<link>https://arxiv.org/abs/2512.16791</link>
<guid>https://arxiv.org/abs/2512.16791</guid>
<content:encoded><![CDATA[
arXiv:2512.16791v1 Announce Type: cross 
Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</title>
<link>https://arxiv.org/abs/2512.16792</link>
<guid>https://arxiv.org/abs/2512.16792</guid>
<content:encoded><![CDATA[
arXiv:2512.16792v1 Announce Type: cross 
Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2512.16795</link>
<guid>https://arxiv.org/abs/2512.16795</guid>
<content:encoded><![CDATA[
arXiv:2512.16795v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16813</link>
<guid>https://arxiv.org/abs/2512.16813</guid>
<content:encoded><![CDATA[
arXiv:2512.16813v1 Announce Type: cross 
Abstract: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs</title>
<link>https://arxiv.org/abs/2512.16814</link>
<guid>https://arxiv.org/abs/2512.16814</guid>
<content:encoded><![CDATA[
arXiv:2512.16814v1 Announce Type: cross 
Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title>
<link>https://arxiv.org/abs/2512.16826</link>
<guid>https://arxiv.org/abs/2512.16826</guid>
<content:encoded><![CDATA[
arXiv:2512.16826v1 Announce Type: cross 
Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</title>
<link>https://arxiv.org/abs/2512.16842</link>
<guid>https://arxiv.org/abs/2512.16842</guid>
<content:encoded><![CDATA[
arXiv:2512.16842v1 Announce Type: cross 
Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference</title>
<link>https://arxiv.org/abs/2512.16843</link>
<guid>https://arxiv.org/abs/2512.16843</guid>
<content:encoded><![CDATA[
arXiv:2512.16843v1 Announce Type: cross 
Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-RL Induces Exploration in Language Agents</title>
<link>https://arxiv.org/abs/2512.16848</link>
<guid>https://arxiv.org/abs/2512.16848</guid>
<content:encoded><![CDATA[
arXiv:2512.16848v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</title>
<link>https://arxiv.org/abs/2512.16851</link>
<guid>https://arxiv.org/abs/2512.16851</guid>
<content:encoded><![CDATA[
arXiv:2512.16851v1 Announce Type: cross 
Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2512.16853</link>
<guid>https://arxiv.org/abs/2512.16853</guid>
<content:encoded><![CDATA[
arXiv:2512.16853v1 Announce Type: cross 
Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16861</link>
<guid>https://arxiv.org/abs/2512.16861</guid>
<content:encoded><![CDATA[
arXiv:2512.16861v1 Announce Type: cross 
Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</title>
<link>https://arxiv.org/abs/2512.16866</link>
<guid>https://arxiv.org/abs/2512.16866</guid>
<content:encoded><![CDATA[
arXiv:2512.16866v1 Announce Type: cross 
Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2512.16871</link>
<guid>https://arxiv.org/abs/2512.16871</guid>
<content:encoded><![CDATA[
arXiv:2512.16871v1 Announce Type: cross 
Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: cross 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: cross 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: cross 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Racial Bias in Historical Training Data for News AI</title>
<link>https://arxiv.org/abs/2512.16901</link>
<guid>https://arxiv.org/abs/2512.16901</guid>
<content:encoded><![CDATA[
arXiv:2512.16901v1 Announce Type: cross 
Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</title>
<link>https://arxiv.org/abs/2512.16907</link>
<guid>https://arxiv.org/abs/2512.16907</guid>
<content:encoded><![CDATA[
arXiv:2512.16907v1 Announce Type: cross 
Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</title>
<link>https://arxiv.org/abs/2512.16911</link>
<guid>https://arxiv.org/abs/2512.16911</guid>
<content:encoded><![CDATA[
arXiv:2512.16911v1 Announce Type: cross 
Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
<link>https://arxiv.org/abs/2512.16912</link>
<guid>https://arxiv.org/abs/2512.16912</guid>
<content:encoded><![CDATA[
arXiv:2512.16912v1 Announce Type: cross 
Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVGT: Driving Visual Geometry Transformer</title>
<link>https://arxiv.org/abs/2512.16919</link>
<guid>https://arxiv.org/abs/2512.16919</guid>
<content:encoded><![CDATA[
arXiv:2512.16919v1 Announce Type: cross 
Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasyV2V: A High-quality Instruction-based Video Editing Framework</title>
<link>https://arxiv.org/abs/2512.16920</link>
<guid>https://arxiv.org/abs/2512.16920</guid>
<content:encoded><![CDATA[
arXiv:2512.16920v1 Announce Type: cross 
Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
<link>https://arxiv.org/abs/2512.16921</link>
<guid>https://arxiv.org/abs/2512.16921</guid>
<content:encoded><![CDATA[
arXiv:2512.16921v1 Announce Type: cross 
Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enter the Void - Planning to Seek Entropy When Reward is Scarce</title>
<link>https://arxiv.org/abs/2505.16787</link>
<guid>https://arxiv.org/abs/2505.16787</guid>
<content:encoded><![CDATA[
arXiv:2505.16787v3 Announce Type: replace 
Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. These models constitute the large majority of training compute and time and they are subsequently used to train actors entirely in simulation, but once this is done they are quickly discarded. We show in this work that utilising these models at inference time can significantly boost sample efficiency. We propose a novel approach that anticipates and actively seeks out informative states using the world model's short-horizon latent predictions, offering a principled alternative to traditional curiosity-driven methods that chase outdated estimates of high uncertainty states. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multiple multi-step plans at every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the commitment to searching entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to Dreamer to illustrate the concept. Our method finishes MiniWorld's procedurally generated mazes 50% faster than base Dreamer at convergence and in only 60% of the environment steps that base Dreamer's policy needs; it displays reasoned exploratory behaviour in Crafter, achieves the same reward as base Dreamer in a third of the steps; planning tends to improve sample efficiency on DeepMind Control tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
arXiv:2506.04133v5 Announce Type: replace 
Abstract: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around key pillars: \textit{ Explainability, ModelOps, Security, Privacy} and \textit{their Lifecycle Governance}, each contextualized to the challenges of AMAS. A risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI, as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, highlighting key directions to align emerging systems with TRiSM principles-ensuring safety, transparency, and accountability in their operation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Pervasive Distributed Agentic Generative AI -- A State of The Art</title>
<link>https://arxiv.org/abs/2506.13324</link>
<guid>https://arxiv.org/abs/2506.13324</guid>
<content:encoded><![CDATA[
arXiv:2506.13324v2 Announce Type: replace 
Abstract: The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called "Agent as a Tool", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical GraphRAG: Efficient Knowledge Graph Construction and Hybrid Retrieval at Scale</title>
<link>https://arxiv.org/abs/2507.03226</link>
<guid>https://arxiv.org/abs/2507.03226</guid>
<content:encoded><![CDATA[
arXiv:2507.03226v3 Announce Type: replace 
Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based Retrieval-Augmented Generation (GraphRAG) in enterprise environments. While GraphRAG has shown promise for multi- hop reasoning and structured retrieval, its adoption has been limited due to reliance on expensive large language model (LLM)-based extraction and complex traversal strategies. To address these challenges, we introduce two core innovations: (1) an efficient knowledge graph construction pipeline that leverages dependency parsing to achieve 94% of LLM-based performance (61.87% vs. 65.83%) while significantly reducing costs and improving scalability; and (2) a hybrid retrieval strategy that fuses vector similarity with graph traversal using Reciprocal Rank Fusion (RRF), maintaining separate embeddings for entities, chunks, and relations to enable multi-granular matching. We evaluate our framework on two enterprise datasets focused on legacy code migration and demonstrate improvements of up to 15% and 4.35% over vanilla vector retrieval baselines using LLM-as-Judge evaluation metrics. These results validate the feasibility of deploying GraphRAG in production enterprise environments, demonstrating that careful engineering of classical NLP techniques can match modern LLM-based approaches while enabling practical, cost-effective, and domain-adaptable retrieval-augmented reasoning at scale.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting</title>
<link>https://arxiv.org/abs/2507.16145</link>
<guid>https://arxiv.org/abs/2507.16145</guid>
<content:encoded><![CDATA[
arXiv:2507.16145v2 Announce Type: replace 
Abstract: Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease with persistent airflow limitation, is a leading global cause of disability and mortality. Respiratory spirogram time series, routinely collected during pulmonary function tests (PFTs), play a critical role in the early detection of repsiratory diseases and in monitoring lung function over time. However, most current AI models for COPD diagnosis are limited to outputting classification results without providing a rationale for their diagnostic process, while current Large Language Models (LLMs) cannot understand spirograms yet, which severely limits their clinical trust and adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large language model that can understand spirogram. The model extracts morphological features from respiratory curves via a SpiroEncoder and aligns them with PFT numerical values in a unified latent space using a SpiroProjector, ultimately empowering a large language model to generate a comprehensive diagnostic report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC of 0.8977 (95% CI: 0.88-0.91). In a robustness test with missing core data, it maintained a 100% valid response rate, far surpassing the 13.4% of a text-only model and showcasing the superiority of its multimodal design. This work demonstrates the substantial potential of deeply fusing physiological signals with large language models, establishing a new paradigm for the next generation of interpretable and reliable clinical decision support tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
arXiv:2507.21503v3 Announce Type: replace 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</title>
<link>https://arxiv.org/abs/2508.09670</link>
<guid>https://arxiv.org/abs/2508.09670</guid>
<content:encoded><![CDATA[
arXiv:2508.09670v2 Announce Type: replace 
Abstract: Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models (LLMs). However, standard RLVR faces challenges with reward sparsity, where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the model's performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives</title>
<link>https://arxiv.org/abs/2508.20978</link>
<guid>https://arxiv.org/abs/2508.20978</guid>
<content:encoded><![CDATA[
arXiv:2508.20978v4 Announce Type: replace 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</title>
<link>https://arxiv.org/abs/2510.05684</link>
<guid>https://arxiv.org/abs/2510.05684</guid>
<content:encoded><![CDATA[
arXiv:2510.05684v2 Announce Type: replace 
Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification</title>
<link>https://arxiv.org/abs/2510.12534</link>
<guid>https://arxiv.org/abs/2510.12534</guid>
<content:encoded><![CDATA[
arXiv:2510.12534v3 Announce Type: replace 
Abstract: The rapid growth of user-generated text across digital platforms has intensified the need for interpretable models capable of fine-grained text classification and explanation. Existing prototype-based models offer intuitive explanations but typically operate at coarse granularity (sentence or document level) and fail to address the multi-label nature of real-world text classification. We propose ProtoSiTex, a semi-interpretable framework designed for fine-grained multi-label text classification. ProtoSiTex employs a dual-phase alternate training strategy: an unsupervised prototype discovery phase that learns semantically coherent and diverse prototypes, and a supervised classification phase that maps these prototypes to class labels. A hierarchical loss function enforces consistency across subsentence, sentence, and document levels, enhancing interpretability and alignment. Unlike prior approaches, ProtoSiTex captures overlapping and conflicting semantics using adaptive prototypes and multi-head attention. We also introduce a benchmark dataset of hotel reviews annotated at the subsentence level with multiple labels. Experiments on this dataset and two public benchmarks (binary and multi-class) show that ProtoSiTex achieves state-of-the-art performance while delivering faithful, human-aligned explanations, establishing it as a robust solution for semi-interpretable multi-label text classification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics</title>
<link>https://arxiv.org/abs/2511.11357</link>
<guid>https://arxiv.org/abs/2511.11357</guid>
<content:encoded><![CDATA[
arXiv:2511.11357v2 Announce Type: replace 
Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
arXiv:2512.09566v2 Announce Type: replace 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title>
<link>https://arxiv.org/abs/2406.16745</link>
<guid>https://arxiv.org/abs/2406.16745</guid>
<content:encoded><![CDATA[
arXiv:2406.16745v3 Announce Type: replace-cross 
Abstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy Come, Easy Go? Examining the Perceptions and Learning Effects of LLM-based Chatbot in the Context of Search-as-Learning</title>
<link>https://arxiv.org/abs/2410.01396</link>
<guid>https://arxiv.org/abs/2410.01396</guid>
<content:encoded><![CDATA[
arXiv:2410.01396v2 Announce Type: replace-cross 
Abstract: The cognitive process of Search-as-Learning (SAL) is most effective when searching promotes active encoding of information. The rise of LLMs-based chatbots, which provide instant answers, introduces a trade-off between efficiency and depth of processing. Such answer-centric approaches accelerate information access, but they also raise concerns about shallower learning. To examine these issues in the context of SAL, we conducted a large-scale survey of educators and students to capture perceived risks and benefits of LLM-based chatbots. In addition, we adopted the encoding-storage paradigm to design a within-subjects experiment, where participants (N=92) engaged in SAL tasks using three different modalities: books, search engines, and chatbots. Our findings provide a counterintuitive insight into stakeholder concerns: while LLM-based chatbots and search engines validated perceived benefits on learning efficiency by outperforming book-based search in immediate conceptual understanding, they did not result in a long-term inferiority as feared. Our study provides insights for designing human-AI collaborative learning systems that promote cognitive engagement by balancing learning efficiency and long-term knowledge retention.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace-cross 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for Microbiology and Microbiome Research</title>
<link>https://arxiv.org/abs/2411.01098</link>
<guid>https://arxiv.org/abs/2411.01098</guid>
<content:encoded><![CDATA[
arXiv:2411.01098v2 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology and microbiome research now experiencing significant breakthroughs through machine learning applications. This review provides a comprehensive overview of AI-driven approaches tailored for microbiology and microbiome studies, emphasizing both technical advancements and biological insights. We begin with an introduction to foundational AI techniques, including primary machine learning paradigms and various deep learning architectures, and offer guidance on choosing between traditional machine learning and sophisticated deep learning methods based on specific research goals. The primary section on application scenarios spans diverse research areas, from taxonomic profiling, functional annotation \& prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision nutrition, clinical microbiology, to prevention \& therapeutics. Finally, we discuss challenges in this field and highlight some recent breakthroughs. Together, this review underscores AI's transformative role in microbiology and microbiome research, paving the way for innovative methodologies and applications that enhance our understanding of microbial life and its impact on our planet and our health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
arXiv:2412.08435v5 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modality Collaborative Learning for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.12424</link>
<guid>https://arxiv.org/abs/2501.12424</guid>
<content:encoded><![CDATA[
arXiv:2501.12424v2 Announce Type: replace-cross 
Abstract: Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2502.10239</link>
<guid>https://arxiv.org/abs/2502.10239</guid>
<content:encoded><![CDATA[
arXiv:2502.10239v2 Announce Type: replace-cross 
Abstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose \ac{METHOD} that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $1.6-3\times$ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization</title>
<link>https://arxiv.org/abs/2502.12672</link>
<guid>https://arxiv.org/abs/2502.12672</guid>
<content:encoded><![CDATA[
arXiv:2502.12672v3 Announce Type: replace-cross 
Abstract: Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Guided Diffusion for Multi-Objective Offline Optimization</title>
<link>https://arxiv.org/abs/2503.17299</link>
<guid>https://arxiv.org/abs/2503.17299</guid>
<content:encoded><![CDATA[
arXiv:2503.17299v2 Announce Type: replace-cross 
Abstract: Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/ surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinAudio: A Benchmark for Audio Large Language Models in Financial Applications</title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
arXiv:2503.20990v3 Announce Type: replace-cross 
Abstract: Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</title>
<link>https://arxiv.org/abs/2504.16112</link>
<guid>https://arxiv.org/abs/2504.16112</guid>
<content:encoded><![CDATA[
arXiv:2504.16112v2 Announce Type: replace-cross 
Abstract: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</title>
<link>https://arxiv.org/abs/2504.19565</link>
<guid>https://arxiv.org/abs/2504.19565</guid>
<content:encoded><![CDATA[
arXiv:2504.19565v3 Announce Type: replace-cross 
Abstract: Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
arXiv:2505.12781v4 Announce Type: replace-cross 
Abstract: Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines</title>
<link>https://arxiv.org/abs/2506.01329</link>
<guid>https://arxiv.org/abs/2506.01329</guid>
<content:encoded><![CDATA[
arXiv:2506.01329v2 Announce Type: replace-cross 
Abstract: Psychological support hotlines serve as critical lifelines for crisis intervention but encounter significant challenges due to rising demand and limited resources. Large language models (LLMs) offer potential support in crisis assessments, yet their effectiveness in emotionally sensitive, real-world clinical settings remains underexplored. We introduce PsyCrisisBench, a comprehensive benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four key tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. 64 LLMs across 15 model families (including closed-source such as GPT, Claude, Gemini and open-source such as Llama, Qwen, DeepSeek) were evaluated using zero-shot, few-shot, and fine-tuning paradigms. LLMs showed strong results in suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), with notable gains from few-shot prompting and fine-tuning. Compared to trained human operators, LLMs achieved comparable or superior performance on suicide plan identification and risk assessment, while humans retained advantages on mood status recognition and suicidal ideation detection. Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity. Notably, a fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks. LLMs demonstrate performance broadly comparable to trained human operators in text-based crisis assessment, with complementary strengths across task types. PsyCrisisBench provides a robust, real-world evaluation framework to guide future model development and ethical deployment in clinical mental health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</title>
<link>https://arxiv.org/abs/2506.09695</link>
<guid>https://arxiv.org/abs/2506.09695</guid>
<content:encoded><![CDATA[
arXiv:2506.09695v3 Announce Type: replace-cross 
Abstract: Early diagnosis of Alzheimer's Disease (AD), particularly at the mild cognitive impairment stage, is essential for timely intervention. However, this process faces significant barriers, including reliance on subjective assessments and the high cost of advanced imaging techniques. While deep learning offers automated solutions to improve diagnostic accuracy, its widespread adoption remains constrained due to high energy requirements and computational demands, particularly in resource-limited settings. Spiking neural networks (SNNs) provide a promising alternative, as their brain-inspired design is well-suited to model the sparse and event-driven patterns characteristic of neural degeneration in AD. These networks offer the potential for developing interpretable, energy-efficient diagnostic tools. Despite their advantages, existing SNNs often suffer from limited expressiveness and challenges in stable training, which reduce their effectiveness in handling complex medical tasks. To address these shortcomings, we introduce FasterSNN, a hybrid neural architecture that combines biologically inspired Leaky Integrate-and-Fire (LIF) neurons with region-adaptive convolution and multi-scale spiking attention mechanisms. This approach facilitates efficient, sparse processing of 3D MRI data while maintaining high diagnostic accuracy. Experimental results on benchmark datasets reveal that FasterSNN delivers competitive performance with significantly enhanced efficiency and training stability, highlighting its potential for practical application in AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</title>
<link>https://arxiv.org/abs/2506.12775</link>
<guid>https://arxiv.org/abs/2506.12775</guid>
<content:encoded><![CDATA[
arXiv:2506.12775v2 Announce Type: replace-cross 
Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability</title>
<link>https://arxiv.org/abs/2507.00788</link>
<guid>https://arxiv.org/abs/2507.00788</guid>
<content:encoded><![CDATA[
arXiv:2507.00788v2 Announce Type: replace-cross 
Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assistants affects software maintainability, specifically how easily other developers can evolve the resulting source code. [Method] We conducted a two-phase controlled experiment involving 151 participants, 95% of whom were professional developers. In Phase 1, participants added a new feature to a Java web application, with or without AI assistance. In Phase 2, a randomized controlled trial, new participants evolved these solutions without AI assistance. [Results] Phase 2 revealed no significant differences in subsequent evolution with respect to completion time or code quality. Bayesian analysis suggests that any speed or quality improvements from AI use were at most small and highly uncertain. Observational results from Phase 1 corroborate prior research: using an AI assistant yielded a 30.7% median reduction in completion time, and habitual AI users showed an estimated 55.9% speedup. [Conclusions] Overall, we did not detect systematic maintainability advantages or disadvantages when other developers evolved code co-developed with AI assistants. Within the scope of our tasks and measures, we observed no consistent warning signs of degraded code-level maintainability. Future work should examine risks such as code bloat from excessive code generation and cognitive debt as developers offload more mental effort to assistants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</title>
<link>https://arxiv.org/abs/2507.03558</link>
<guid>https://arxiv.org/abs/2507.03558</guid>
<content:encoded><![CDATA[
arXiv:2507.03558v3 Announce Type: replace-cross 
Abstract: Brain stroke is a leading cause of mortality and long-term disability worldwide, underscoring the need for precise and rapid prediction techniques. Computed Tomography (CT) scan is considered one of the most effective methods for diagnosing brain strokes. Most stroke classification techniques use a single slice-level prediction mechanism, requiring radiologists to manually select the most critical CT slice from the original CT volume. Although clinical evaluations are often used in traditional diagnostic procedures, machine learning (ML) has opened up new avenues for improving stroke diagnosis. To supplement traditional diagnostic techniques, this study investigates machine learning models for early brain stroke prediction using CT scan images. This research proposes a novel machine learning approach to brain stroke detection, focusing on optimizing classification performance with pre-trained deep learning models and advanced optimization strategies. Pre-trained models, including DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are used for feature extraction. Feature engineering techniques, including BFO, PCA, and LDA, further enhance model performance. These features are then classified using machine learning algorithms, including SVC, RF, XGB, DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2, LDA, and SVC achieved the highest classification accuracy of 97.93%, significantly outperforming other model-optimizer-classifier combinations. The results underline the effectiveness of integrating lightweight pre-trained models with robust optimization and classification techniques for brain stroke diagnosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2507.16043</link>
<guid>https://arxiv.org/abs/2507.16043</guid>
<content:encoded><![CDATA[
arXiv:2507.16043v3 Announce Type: replace-cross 
Abstract: The surrogate gradient descent algorithm enabled spiking neural networks to be trained to carry out challenging sensory processing tasks, an important step in understanding how spikes contribute to neural computations. However, it is unclear the extent to which these algorithms fully explore the space of possible spiking solutions to problems. We investigated whether spiking networks trained with surrogate gradient descent can learn to make use of information that is only encoded in the timing and not the rate of spikes. We constructed synthetic datasets with a range of types of spike timing information (interspike intervals, spatio-temporal spike patterns or polychrony, and coincidence codes). We find that surrogate gradient descent training can extract all of these types of information. In more realistic speech-based datasets, both timing and rate information is present. We therefore constructed variants of these datasets in which all rate information is removed, and find that surrogate gradient descent can still perform well. We tested all networks both with and without trainable axonal delays. We find that delays can give a significant increase in performance, particularly for more challenging tasks. To determine what types of spike timing information are being used by the networks trained on the speech-based tasks, we test these networks on time-reversed spikes which perturb spatio-temporal spike patterns but leave interspike intervals and coincidence information unchanged. We find that when axonal delays are not used, networks perform well under time reversal, whereas networks trained with delays perform poorly. This suggests that spiking neural networks with delays are better able to exploit temporal structure. To facilitate further studies of temporal coding, we have released our modified speech-based datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda</title>
<link>https://arxiv.org/abs/2507.21928</link>
<guid>https://arxiv.org/abs/2507.21928</guid>
<content:encoded><![CDATA[
arXiv:2507.21928v3 Announce Type: replace-cross 
Abstract: Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2507.22219</link>
<guid>https://arxiv.org/abs/2507.22219</guid>
<content:encoded><![CDATA[
arXiv:2507.22219v2 Announce Type: replace-cross 
Abstract: Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constitutional Law and AI Governance: Constraints on Model Licensing and Research Classification</title>
<link>https://arxiv.org/abs/2509.05361</link>
<guid>https://arxiv.org/abs/2509.05361</guid>
<content:encoded><![CDATA[
arXiv:2509.05361v2 Announce Type: replace-cross 
Abstract: Transformative AI systems may pose unprecedented catastrophic risks, but the U.S. Constitution places significant constraints on the government's ability to govern this technology. This paper examines how the First Amendment, administrative law, and the Fourteenth Amendment shape the legal vulnerability of two regulatory proposals: model licensing and AI research classification. While the First Amendment may provide some degree of protection for model algorithms or outputs, this protection does not foreclose regulation. Policymakers must also consider administrative legal requirements, due to both agency review and authority. Finally, while substantive due process and equal protection pose minimal obstacles, procedural due process requires the government to clearly define when developers vest a legal interest in their models. Given this analysis, effective AI governance requires careful implementation to avoid these legal challenges.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v3 Announce Type: replace-cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</title>
<link>https://arxiv.org/abs/2509.21735</link>
<guid>https://arxiv.org/abs/2509.21735</guid>
<content:encoded><![CDATA[
arXiv:2509.21735v2 Announce Type: replace-cross 
Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease (AD) progression is crucial for timely intervention. However, this task remains challenging due to the complex dysfunctions in the spatio-temporal characteristics of underlying brain networks, which are often overlooked by existing methods. To address these limitations, we develop an interpretable spatio-temporal graph neural network framework to predict future AD progression, leveraging dual Stochastic Differential Equations (SDEs) to model the irregularly-sampled longitudinal functional magnetic resonance imaging (fMRI) data. We validate our approach on two independent cohorts, including the Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework effectively learns sparse regional and connective importance probabilities, enabling the identification of key brain circuit abnormalities associated with disease progression. Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal lobule as salient regions, with significant disruptions in the ventral attention, dorsal attention, and default mode networks. These abnormalities correlate strongly with longitudinal AD-related clinical symptoms. Moreover, our interpretability strategy reveals both established and novel neural systems-level and sex-specific biomarkers, offering new insights into the neurobiological mechanisms underlying AD progression. Our findings highlight the potential of spatio-temporal graph-based learning for early, individualized prediction of AD progression, even in the context of irregularly-sampled longitudinal imaging data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.22737</link>
<guid>https://arxiv.org/abs/2509.22737</guid>
<content:encoded><![CDATA[
arXiv:2509.22737v2 Announce Type: replace-cross 
Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACOS: Task Agnostic COordinator of a multi-drone System</title>
<link>https://arxiv.org/abs/2510.01869</link>
<guid>https://arxiv.org/abs/2510.01869</guid>
<content:encoded><![CDATA[
arXiv:2510.01869v2 Announce Type: replace-cross 
Abstract: When a single pilot is responsible for managing a multi-drone system, the task may demand varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system on a real-world multi-drone system, and conduct an ablation study to assess the contribution of each module.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution</title>
<link>https://arxiv.org/abs/2510.08697</link>
<guid>https://arxiv.org/abs/2510.08697</guid>
<content:encoded><![CDATA[
arXiv:2510.08697v2 Announce Type: replace-cross 
Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</title>
<link>https://arxiv.org/abs/2510.10446</link>
<guid>https://arxiv.org/abs/2510.10446</guid>
<content:encoded><![CDATA[
arXiv:2510.10446v2 Announce Type: replace-cross 
Abstract: We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.12796</link>
<guid>https://arxiv.org/abs/2510.12796</guid>
<content:encoded><![CDATA[
arXiv:2510.12796v2 Announce Type: replace-cross 
Abstract: Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM one-shot style transfer for Authorship Attribution and Verification</title>
<link>https://arxiv.org/abs/2510.13302</link>
<guid>https://arxiv.org/abs/2510.13302</guid>
<content:encoded><![CDATA[
arXiv:2510.13302v3 Announce Type: replace-cross 
Abstract: Computational stylometry studies writing style through quantitative textual patterns, enabling applications such as authorship attribution, identity linking, and plagiarism detection. Existing supervised and contrastive approaches often rely on datasets with spurious correlations, conflating style with topic. Despite the relevance of language modeling to these tasks, the pre-training of modern large language models (LLMs) has been underutilized in general authorship analysis. We introduce an unsupervised framework that uses the log-probabilities of an LLM to measure style transferability between two texts. This framework takes advantage of the extensive CLM pre-training and in-context capabilities of modern LLMs. Our approach avoids explicit supervision with spuriously correlated data. Our method substantially outperforms unsupervised prompting-based baselines at similar model sizes and exceeds contrastively trained models when controlling for topical overlap. Our framework's performance improves with model size. In the case of authorship verification, we present an additional mechanism that increases test-time computation to improve accuracy; enabling flexible trade-offs between computational cost and task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v4 Announce Type: replace-cross 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voice-Interactive Surgical Agent for Multimodal Patient Data Control</title>
<link>https://arxiv.org/abs/2511.07392</link>
<guid>https://arxiv.org/abs/2511.07392</guid>
<content:encoded><![CDATA[
arXiv:2511.07392v3 Announce Type: replace-cross 
Abstract: In robotic surgery, surgeons fully engage their hands and visual attention in procedures, making it difficult to access and manipulate multimodal patient data without interrupting the workflow. To overcome this problem, we propose a Voice-Interactive Surgical Agent (VISA) built on a hierarchical multi-agent framework consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to interpret voice commands and execute tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models within surgical video. We construct a dataset of 240 user commands organized into hierarchical categories and introduce the Multi-level Orchestration Evaluation Metric (MOEM) that evaluates the performance and robustness at both the command and category levels. Experimental results demonstrate that VISA achieves high stage-level accuracy and workflow-level success rates, while also enhancing its robustness by correcting transcription errors, resolving linguistic ambiguity, and interpreting diverse free-form expressions. These findings highlight the strong potential of VISA to support robotic surgery and its scalability for integrating new functions and agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing mathematics research with generative AI</title>
<link>https://arxiv.org/abs/2511.07420</link>
<guid>https://arxiv.org/abs/2511.07420</guid>
<content:encoded><![CDATA[
arXiv:2511.07420v3 Announce Type: replace-cross 
Abstract: The main drawback of using generative AI models for advanced mathematics is that these models are not primarily logical reasoning engines. However, Large Language Models, and their refinements, can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of generative AI models to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how generative AI models can be used to advance mathematics research. We also discuss their integration with neuro-symbolic solvers, Computer Algebra Systems and formal proof assistants such as Lean.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models</title>
<link>https://arxiv.org/abs/2511.07503</link>
<guid>https://arxiv.org/abs/2511.07503</guid>
<content:encoded><![CDATA[
arXiv:2511.07503v3 Announce Type: replace-cross 
Abstract: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnviSAgE: A Survey of Environment Scaling for Qualitative Agentic Experience Collection</title>
<link>https://arxiv.org/abs/2511.09586</link>
<guid>https://arxiv.org/abs/2511.09586</guid>
<content:encoded><![CDATA[
arXiv:2511.09586v2 Announce Type: replace-cross 
Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wrist Photoplethysmography Predicts Dietary Information</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v2 Announce Type: replace-cross 
Abstract: Whether wearable photoplethysmography (PPG) contains dietary information remains unknown. We trained a language model on 1.1M meals to predict meal descriptions from PPG, aligning PPG to text. PPG nontrivially predicts meal content; predictability decreases for PPGs farther from meals. This transfers to dietary tasks: PPG increases AUC by 11% for intake and satiety across held-out and independent cohorts, with gains robust to text degradation. Wearable PPG may enable passive dietary monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing ESS in Multiplayer Games</title>
<link>https://arxiv.org/abs/2511.20859</link>
<guid>https://arxiv.org/abs/2511.20859</guid>
<content:encoded><![CDATA[
arXiv:2511.20859v4 Announce Type: replace-cross 
Abstract: We present an algorithm for computing all evolutionarily stable strategies in nondegenerate normal-form games with three or more players.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First, do NOHARM: towards clinically safe large language models</title>
<link>https://arxiv.org/abs/2512.01241</link>
<guid>https://arxiv.org/abs/2512.01241</guid>
<content:encoded><![CDATA[
arXiv:2512.01241v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are routinely used by physicians and patients for medical advice, yet their clinical safety profiles remain poorly characterized. We present NOHARM (Numerous Options Harm Assessment for Risk in Medicine), a benchmark using 100 real primary care-to-specialist consultation cases to measure frequency and severity of harm from LLM-generated medical recommendations. NOHARM covers 10 specialties, with 12,747 expert annotations for 4,249 clinical management options. Across 31 LLMs, potential for severe harm from LLM recommendations occurs in up to 22.2% (95% CI 21.6-22.8%) of cases, with harm of omission accounting for 76.6% (95% CI 76.4-76.8%) of errors. Safety performance is only moderately correlated (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best models outperform generalist physicians on safety (mean difference 9.7%, 95% CI 7.0-12.5%), and a diverse multi-agent approach improves safety compared to solo models (mean difference 8.0%, 95% CI 4.0-12.1%). Therefore, despite strong performance on existing evaluations, widely used AI models can produce severely harmful medical advice at nontrivial rates, underscoring clinical safety as a distinct performance dimension necessitating explicit measurement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^\mathrm{3}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v3 Announce Type: replace-cross 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^\mathrm{3}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^\mathrm{3}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^\mathrm{3}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</title>
<link>https://arxiv.org/abs/2512.08333</link>
<guid>https://arxiv.org/abs/2512.08333</guid>
<content:encoded><![CDATA[
arXiv:2512.08333v2 Announce Type: replace-cross 
Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging performance scales with the amount of pretraining data, and enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v2 Announce Type: replace-cross 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity</title>
<link>https://arxiv.org/abs/2512.09003</link>
<guid>https://arxiv.org/abs/2512.09003</guid>
<content:encoded><![CDATA[
arXiv:2512.09003v2 Announce Type: replace-cross 
Abstract: Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindShift: Analyzing Language Models' Reactions to Psychological Prompts</title>
<link>https://arxiv.org/abs/2512.09149</link>
<guid>https://arxiv.org/abs/2512.09149</guid>
<content:encoded><![CDATA[
arXiv:2512.09149v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
<div> Keywords: coding agents, large-scale codebases, Confucius SDK, meta-agent, software engineering tasks  

<br /><br />Summary:  
The article introduces the Confucius Code Agent (CCA), a scalable coding agent designed to handle large-scale software engineering tasks involving massive code repositories and long-horizon reasoning. Built on the Confucius SDK, the platform emphasizes three perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX), ensuring a balanced approach for developers, users, and the agent itself. The SDK features a unified orchestrator with hierarchical working memory for long-context reasoning, enabling the agent to maintain coherence across extensive sessions. It includes a persistent note-taking system that supports continual learning across multiple sessions, enhancing the agent’s capability over time. A modular extension system facilitates reliable and safe use of various tools in complex workflows. Additionally, the authors introduce a meta-agent framework that automates the processes of agent configuration synthesis, evaluation, and refinement via a build-test-improve loop, allowing rapid adaptation to new environments, tasks, and toolchains. Empirical results on the SWE-Bench-Pro benchmark demonstrate CCA’s effectiveness, achieving a Resolve@1 accuracy of 54.3%, outperforming existing research baselines and matching or surpassing commercial solutions under comparable conditions. Together, the Confucius SDK and CCA provide a generalizable, extensible, and production-grade foundation, bridging the gap between research prototypes and scalable, real-world deployment of coding agents. <div>
arXiv:2512.10398v4 Announce Type: replace-cross 
Abstract: Real-world software engineering tasks require coding agents that can operate over massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade agents offer transparency but struggle when scaled to real-world workloads, while proprietary systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a scalable software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK integrates a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid adaptation to new tasks, environments, and tool stacks. Instantiated with these mechanisms, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA reaches a Resolve@1 of 54.3%, exceeding prior research baselines and comparing favorably to commercial results, under identical repositories, model backend, and tool access. Together, the Confucius SDK and CCA form a general, extensible, and production-grade foundation for building effective and robust coding agents, bridging the gap between research prototypes and practical large-scale deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, decision boundaries, influence functions, Eminence framework, low poison rates<br /><br />Summary:<br /><br />1. Deep neural networks (DNNs) are essential for many applications but are vulnerable to backdoor attacks that are often based on heuristic brute-force approaches, lacking rigorous theoretical understanding. <br /><br />2. This work provides a theoretical analysis revealing how sparse decision boundaries in DNNs enable disproportionate manipulation with minimal poisoned data. It introduces a closed-form ambiguous boundary region where very few relabeled samples can cause substantial misclassification. <br /><br />3. Influence function analysis is used to quantify significant parameter shifts generated by these margin samples, explaining why extremely low poison rates can still achieve effective attacks without degrading clean accuracy. <br /><br />4. Building on these insights, the paper proposes Eminence, a black-box backdoor attack framework that is explainable, robust, and comes with provable theoretical guarantees. Eminence crafts a universal, visually subtle trigger to exploit vulnerable decision boundaries strategically. <br /><br />5. Experiments validate the theory and demonstrate Eminence’s effectiveness, showing it achieves over 90% attack success with less than 0.1% poisoning, maintaining clean accuracy, and transferring well across different models, datasets, and scenarios. An exponential relationship between margin poisoning and adversarial boundary manipulation is confirmed. <div>
arXiv:2512.10402v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</title>
<link>https://arxiv.org/abs/2512.10422</link>
<guid>https://arxiv.org/abs/2512.10422</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, question answering, cooperative retrieval, multi-hop reasoning<br /><br />Summary:  
This paper addresses the problem of factual inaccuracies in outputs generated by large language models (LLMs) when used alone for question answering (QA). To overcome these issues, the authors propose CoopRAG, a novel retrieval-augmented generation (RAG) framework where a retriever and an LLM work cooperatively, exchanging informative knowledge to enhance retrieval accuracy and answer generation. CoopRAG introduces a mechanism to unroll a complex question into sub-questions and a reasoning chain, identifying uncertain positions with masked tokens. It then retrieves relevant documents based on the original question augmented by these sub-questions and the reasoning chain. To improve document ranking, CoopRAG reranks retrieved documents through a contrastive approach between different layers of the retriever model. Lastly, the framework reconstructs the reasoning chain by filling in the masked positions using the LLM. Experimental results show that CoopRAG consistently outperforms current state-of-the-art QA methods across both multi-hop and simple QA datasets, demonstrating improvements in both retrieval quality and overall question answering performance. The authors have also made their code publicly available, facilitating further research and application development in cooperative retrieval and reasoning for QA tasks. <div>
arXiv:2512.10422v3 Announce Type: replace-cross 
Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition</title>
<link>https://arxiv.org/abs/2512.10688</link>
<guid>https://arxiv.org/abs/2512.10688</guid>
<content:encoded><![CDATA[
<div> Popularity bias, collaborative filtering, Bayesian Pairwise Ranking, embedding geometry, directional decomposition<br /><br />Summary:<br /><br />This paper investigates the root cause of popularity bias in collaborative filtering (CF) recommendation systems, identifying it as an intrinsic geometric artifact rather than an external confounding factor. The authors provide a rigorous mathematical analysis proving that Bayesian Pairwise Ranking (BPR) models organize item embeddings along a dominant "popularity direction," where embedding magnitudes correlate strongly with item interaction frequencies. This creates a conflict within user embeddings, as they must simultaneously express genuine user preferences and adjust for global popularity effects, resulting in suboptimal representations that favor popular items regardless of actual individual tastes. To address this, the paper introduces Directional Decomposition and Correction (DDC), a novel framework that applies asymmetric directional updates to embeddings. DDC surgically disentangles user preferences from popularity influences by guiding positive interactions toward personalized preference directions and steering negative interactions away from the global popularity direction. Extensive experiments conducted on multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baseline models. Moreover, it achieves superior recommendation quality and fairness. The implementation of DDC is openly available, facilitating adoption and further research. <div>
arXiv:2512.10688v3 Announce Type: replace-cross 
Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</title>
<link>https://arxiv.org/abs/2512.14709</link>
<guid>https://arxiv.org/abs/2512.14709</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Vector Symbolic Architecture, self-attention, symbolic reasoning, logical compositionality<br /><br />Summary:<br />1. The paper interprets transformer components—self-attention and residual streams—as implementing an approximate Vector Symbolic Architecture (VSA), where queries and keys represent role spaces, values act as fillers, attention weights perform soft unbinding, and residual connections enable superposition of multiple bound structures.<br />2. This algebraic framework provides a unified perspective explaining both the reasoning-like behavior and brittle failure modes of transformers on symbolic manipulation tasks, such as variable confusion and inconsistencies across logically related prompts.<br />3. The authors connect VSA concepts to practical transformer phenomena including chain-of-thought reasoning, program-based inference, and memory-augmented tool usage.<br />4. Building on this insight, the paper proposes VSA-inspired architectural modifications like explicit binding/unbinding attention heads, hyperdimensional memory layers, and specialized training objectives to encourage role-filler separation and robust superposition.<br />5. The study also introduces new metrics for quantifying “VSA-likeness” and logical compositionality, while outlining open theoretical and architectural challenges, arguing that framing attention as soft vector-symbolic computation could lead to more interpretable and logically reliable language models. <div>
arXiv:2512.14709v1 Announce Type: new 
Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</title>
<link>https://arxiv.org/abs/2512.14766</link>
<guid>https://arxiv.org/abs/2512.14766</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Graph Question Answering, Incomplete Knowledge Graphs, Adaptive Graph Reasoning Agent, Reasoning Paths

<br /><br />Summary:  
This paper addresses the limitation of current knowledge graph question answering (KGQA) benchmarks that assume complete knowledge graphs (KGs), which neglect the common real-world scenario where KGs are incomplete. To tackle this, the authors propose a novel methodology to create benchmarks reflecting KG incompleteness by removing direct supporting triples while preserving alternative reasoning paths for answer inference. Experimental results on these constructed benchmarks reveal that existing KGQA methods suffer significant performance drops under incomplete conditions, exposing their limited reasoning capabilities. To overcome this, the authors introduce the Adaptive Graph Reasoning Agent (GR-Agent), which formalizes KGQA as an interactive agent-environment task within the KG context. GR-Agent uses an action space composed of graph reasoning tools and keeps a memory of potential supporting evidence such as relevant relations and reasoning paths. Extensive evaluations demonstrate that GR-Agent consistently outperforms non-training baselines and achieves comparable results to training-based methods, effectively handling both complete and incomplete KG scenarios. This work advances KGQA research by emphasizing deeper reasoning over KGs and providing tools and benchmarks better aligned with real-world KG data characteristics. <div>
arXiv:2512.14766v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection</title>
<link>https://arxiv.org/abs/2512.14792</link>
<guid>https://arxiv.org/abs/2512.14792</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Infrastructure as Code, Terraform, Knowledge Injection, Error Taxonomy  

<br /><br />Summary:  
This paper addresses the challenge of low success rates in generating accurate and intent-aligned Infrastructure as Code (IaC) scripts with Large Language Models (LLMs), focusing on Terraform. To improve LLM-based IaC generation, the authors systematically introduced structured configuration knowledge into the generation process. They substantially enhanced the existing IaC-Eval benchmark by adding cloud emulation and automated error analysis capabilities, enabling more rigorous evaluation. A novel error taxonomy specific to LLM-assisted IaC generation was developed to better categorize and understand failure modes. Various knowledge injection techniques were explored, starting from a basic Retrieval-Augmented Generation (RAG) method to advanced Graph RAG approaches that incorporated semantic enrichment of graph elements and modeled dependencies between infrastructure resources. Experimental results revealed that the baseline LLM’s performance was low, with only 27.1% overall success. However, injecting structured configuration knowledge boosted the technical validation success rate to 75.3% and overall success to 62.6%. Despite these improvements in technical correctness, the study identified a "Correctness-Congruence Gap," where LLMs excel as technical coders but struggle to fully capture and deliver on the nuanced user intent, highlighting limitations in intent alignment within LLM-generated IaC code. <div>
arXiv:2512.14792v1 Announce Type: new 
Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally</title>
<link>https://arxiv.org/abs/2512.14910</link>
<guid>https://arxiv.org/abs/2512.14910</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, climate adaptation, multi-agent system, agriculture, decision support<br /><br />Summary:<br /><br />1. The paper addresses the challenges faced by agricultural regions in rural areas due to climate-related risks such as droughts, heavy rainfall, and shifting weather patterns.<br /><br />2. It highlights the need for adaptive risk-management solutions and strategic decision-making to help vulnerable farming communities adapt to climate change.<br /><br />3. The authors introduce AgroAskAI, a multi-agent reasoning system designed to support climate adaptation decisions in agriculture, featuring a modular architecture with role-specialized autonomous agents.<br /><br />4. AgroAskAI uses a chain-of-responsibility framework to coordinate agents dynamically, integrating real-time tools and datasets to provide context-aware and grounded outputs.<br /><br />5. The system includes governance mechanisms that reduce hallucination and enable internal feedback loops to ensure coherence and relevance to local agricultural contexts.<br /><br />6. It supports multilingual interactions to accommodate non-English-speaking farmers, enhancing accessibility.<br /><br />7. Experimental evaluation on typical agricultural queries demonstrates that, with tool integration and prompt refinement, AgroAskAI produces more actionable, inclusive, and reliable recommendations.<br /><br />8. The work underscores the promise of agentic AI as a sustainable and accountable approach for decision support in climate adaptation strategies for agriculture. <div>
arXiv:2512.14910v1 Announce Type: new 
Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation</title>
<link>https://arxiv.org/abs/2512.15033</link>
<guid>https://arxiv.org/abs/2512.15033</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, geometric stability, chess evaluation, model consistency, accuracy-stability paradox  

<br /><br />Summary:  
This paper critiques traditional accuracy-based benchmarks for evaluating Large Language Models (LLMs) in chess, highlighting their failure to differentiate true geometric reasoning from superficial memorization. It introduces the Geometric Stability Framework, a new methodology that tests LLM consistency under invariant board transformations such as rotation, mirror symmetry, color inversion, and format conversion. Applying this framework to six leading LLMs, including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, on a dataset of roughly 3,000 chess positions, the study discovers a significant Accuracy-Stability Paradox. GPT-5.1, despite near-perfect accuracy on standard tests, suffers drastic error increases (over 600%) under geometric perturbations like rotations, indicating pattern matching rather than spatial understanding. In contrast, Claude Sonnet 4.5 and Kimi K2 Turbo show robust dual performance, preserving consistency across all transformation axes. The paper also examines the trade-off between helpfulness and safety, with Gemini 2.5 Flash excelling at rejecting illegal states (96.0%). Ultimately, the authors argue that geometric stability is a crucial, complementary metric for AI evaluation that helps separate genuine reasoning ability from overfitting or data contamination in LLMs. <div>
arXiv:2512.15033v1 Announce Type: new 
Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</title>
<link>https://arxiv.org/abs/2512.15038</link>
<guid>https://arxiv.org/abs/2512.15038</guid>
<content:encoded><![CDATA[
<div> linear attention, autonomous driving, cross-modal fusion, edge deployment, spatiotemporal modeling<br /><br />Summary:<br /><br />This work addresses the challenge of deploying Transformer-based autonomous driving models on resource-constrained edge platforms, where the quadratic attention cost limits long-range spatial and temporal sequence modeling. The authors propose LADY, the first end-to-end autonomous driving model employing fully linear attention, which provides constant computational and memory costs independent of the history length of camera and LiDAR features. LADY introduces a lightweight linear cross-attention mechanism enabling effective cross-modal and cross-temporal information exchange, overcoming limitations of previous linear attention models that supported only self-attention. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art planning performance while significantly reducing computational cost. The model's design ensures efficient fusion of long-range temporal context during inference, making it well suited to the requirements of autonomous driving. Importantly, LADY has been successfully deployed and validated on edge devices, confirming its practicality for real-time use in resource-limited scenarios. This work thus advances autonomous vehicle perception and planning by combining efficient attention mechanisms with cross-modal fusion, enabling scalable and efficient spatiotemporal modeling for real-world applications. <div>
arXiv:2512.15038v1 Announce Type: new 
Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study</title>
<link>https://arxiv.org/abs/2512.15044</link>
<guid>https://arxiv.org/abs/2512.15044</guid>
<content:encoded><![CDATA[
<div> Integrated sensing and communication, agentic AI, generative artificial intelligence, 6G, ISAC optimization<br /><br />Summary:<br /><br />This article addresses the emerging field of integrated sensing and communication (ISAC) as a critical component of sixth-generation (6G) wireless networks, emphasizing its role in enabling collaborative sensing and communication in future intelligent systems. Given the increasing complexity and dynamic nature of wireless environments, the authors highlight the necessity for ISAC systems to adopt more intelligent and autonomous processing methods to maintain efficiency and adaptivity. To meet these challenges, the paper explores the incorporation of agentic artificial intelligence (AI), which facilitates continuous perception, reasoning, and action cycles, thereby supporting intelligent and autonomous ISAC operations. The work begins with a comprehensive review of both agentic AI and ISAC systems, outlining their fundamental characteristics. It then discusses several common optimization strategies for ISAC, underscoring the benefits of generative AI-based agentic AI approaches in enhancing system performance. A novel agentic ISAC framework is proposed, accompanied by a case study demonstrating its superiority in optimizing ISAC capabilities. Finally, the article identifies future research directions to further advance agentic AI-driven ISAC systems, providing a roadmap for ongoing development in this interdisciplinary area. <div>
arXiv:2512.15044v1 Announce Type: new 
Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2512.15089</link>
<guid>https://arxiv.org/abs/2512.15089</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Cognitive-Inspired Elastic Reasoning, Reinforcement Learning, Query Complexity, Tool-Assisted Reasoning<br /><br />Summary:<br /><br />This paper introduces Cognitive-Inspired Elastic Reasoning (CogER), a novel framework for enhancing the reasoning capabilities of large language models (LLMs). CogER addresses the trade-off between reasoning efficiency and accuracy by dynamically selecting the most appropriate reasoning strategy for each query based on its complexity. It accomplishes this by first assessing query difficulty and categorizing queries into predefined levels, each associated with a specific processing method. To automate this selection, the authors model the problem as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is optimized with a reward function that balances solution quality against computational cost, promoting efficient resource allocation. Additionally, the framework introduces Cognitive Tool-Assisted Reasoning, allowing the LLM to autonomously utilize external tools within its chain-of-thought process for queries requiring supplementary resources. The effectiveness of CogER is demonstrated through extensive experiments where it surpasses state-of-the-art Test-Time scaling approaches, yielding at least a 13% relative improvement in average exact match for In-Domain tasks and an 8% relative gain for Out-of-Domain tasks. Overall, this work presents a human-inspired, adaptive reasoning system that enhances LLM performance by balancing accuracy and computational efficiency across queries of varying difficulty. <div>
arXiv:2512.15089v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem</title>
<link>https://arxiv.org/abs/2512.15198</link>
<guid>https://arxiv.org/abs/2512.15198</guid>
<content:encoded><![CDATA[
<div> Discrete Optimization, Relaxed Decision Diagrams, Variable Ordering, Clustering Framework, Maximum Weighted Independent Set Problem  

<br /><br />Summary:  
1. Exact algorithms for Discrete Optimization (DO) rely on strong primal and dual bounds to improve efficiency.  
2. Relaxed Decision Diagrams (DDs) derive dual bounds by over-approximating the solution space, with quality dependent on variable ordering and merging during compilation.  
3. Although dynamic variable ordering heuristics improve bounds, their global application is computationally expensive.  
4. This work introduces a clustering-based framework to partition variables into clusters, reducing the search space for variable ordering heuristics.  
5. Two strategies within this framework are proposed: Cluster-to-Cluster, processing clusters sequentially using problem-specific aggregate criteria, and Pick-and-Sort, selecting and sorting representative variables iteratively to balance diversity and heuristic guidance.  
6. The authors provide theoretical results on DD size growth for the Maximum Weighted Independent Set Problem (MWISP) and propose two policies for determining the number of clusters.  
7. These strategies are embedded into a DD-based branch-and-bound algorithm and evaluated on MWISP benchmark instances.  
8. Experimental results show consistent reductions in computational costs compared to standard dynamic variable ordering methods. <div>
arXiv:2512.15198v1 Announce Type: new 
Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications</title>
<link>https://arxiv.org/abs/2512.15231</link>
<guid>https://arxiv.org/abs/2512.15231</guid>
<content:encoded><![CDATA[
<div> Keywords: CangLing-KnowFlow, Procedural Knowledge Base, Dynamic Workflow Adjustment, Evolutionary Memory Module, remote sensing automation<br /><br />Summary:<br /><br />This paper presents CangLing-KnowFlow, a unified intelligent agent framework designed for automated and intelligent processing of massive remote sensing (RS) datasets in Earth observation (EO). Unlike existing task-specific systems, CangLing-KnowFlow integrates a Procedural Knowledge Base (PKB) that contains 1,008 expert-validated workflow cases across 162 practical RS tasks, which helps guide workflow planning and reduces hallucination issues typical in general-purpose agents. The framework incorporates a Dynamic Workflow Adjustment mechanism that autonomously detects runtime failures and replans recovery strategies to maintain smooth operation. Additionally, an Evolutionary Memory Module enables continuous learning from failures, improving the agent's knowledge and performance iteratively. The combined effect of these components equips CangLing-KnowFlow to adapt, learn, and execute reliably across diverse and complex RS tasks. Evaluation was conducted on KnowFlow-Bench, a newly developed benchmark of 324 workflows simulating real-world applications, tested over 13 leading Large Language Model (LLM) backbones including both open-source and commercial options. CangLing-KnowFlow consistently outperformed the Reflexion baseline by at least 4% in Task Success Rate across all complex tasks. This research provides the first comprehensive validation framework in this emerging field, illustrating the effectiveness of integrating expert procedural knowledge into adaptive, verifiable workflows for scalable EO automation solutions. <div>
arXiv:2512.15231v1 Announce Type: new 
Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis</title>
<link>https://arxiv.org/abs/2512.15295</link>
<guid>https://arxiv.org/abs/2512.15295</guid>
<content:encoded><![CDATA[
<div> Controller synthesis, Labeled Transition System, Reinforcement Learning, Graph Neural Networks, Exploration policies<br /><br />Summary:<br /><br />1. Controller synthesis is a formal method used to automatically generate controllers for Labeled Transition Systems (LTS) that meet desired specifications.<br />2. The success and efficiency of this synthesis largely hinge on the exploration policies employed, which guide the process of exploring the LTS.<br />3. Traditional exploration policies either rely on fixed rules or reinforcement learning strategies limited to considering only current, local features.<br />4. This paper introduces GCRL, a novel approach that enhances RL-based exploration by using Graph Neural Networks to encode the history of the LTS exploration into a graph structure, thereby capturing broader and more contextual information beyond immediate states.<br />5. Experimental comparison with state-of-the-art methods demonstrates that GCRL achieves better learning efficiency and generalization capability in four out of five benchmark domains.<br />6. The only exception was a domain characterized by high symmetry and strictly local interactions, where GCRL did not outperform existing methods.<br />7. Overall, GCRL presents a meaningful advance in exploration policy design for controller synthesis by leveraging richer contextual encoding through GNNs. <div>
arXiv:2512.15295v1 Announce Type: new 
Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I</title>
<link>https://arxiv.org/abs/2512.15298</link>
<guid>https://arxiv.org/abs/2512.15298</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Large Language Models, multimodal reasoning, academic integrity, AI-resistant questions  

<br /><br />Summary:  
This study investigates the multimodal scientific reasoning abilities and cognitive limitations of advanced Large Language Models (LLMs) such as GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro using the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT). Three input conditions—full-page input, individual item input, and optimized multimodal input—were tested to evaluate how different data formats impact model performance. Results showed that unstructured inputs caused notable performance declines due to issues like segmentation errors and OCR failures. Even with optimized inputs, the models still demonstrated fundamental reasoning flaws that limit their scientific understanding. A qualitative analysis identified dominant "Perception Errors," evidencing a "Perception-Cognition Gap," where models could recognize visual elements but failed to interpret their symbolic meanings in diagrams. Additionally, a "Calculation-Conceptualization Discrepancy" was observed, which indicated models could complete calculations accurately but failed to apply underlying scientific concepts correctly. The study also found instances of "Process Hallucination," where models skipped visual verification and relied on plausible but incorrect background knowledge. Based on these findings, the authors propose designing "AI-resistant questions" that exploit these weaknesses to maintain academic integrity by distinguishing human comprehension from AI-generated answers. <div>
arXiv:2512.15298v1 Announce Type: new 
Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Prompt Evolution for Enhancing Agent Effectiveness</title>
<link>https://arxiv.org/abs/2512.15374</link>
<guid>https://arxiv.org/abs/2512.15374</guid>
<content:encoded><![CDATA[
<div> LLM agents, context management, prompt evolution, online optimization, Dual-Stream mechanism<br /><br />Summary:<br /><br />Large Language Model (LLM) agents face challenges when deployed in environments with massive, dynamic contexts due to the static nature of their prompts, which lack effective mechanisms for managing such context. This limitation results in frequent Corrective and Enhancement failures. To overcome this, the study introduces SCOPE (Self-evolving Context Optimization via Prompt Evolution), a novel approach framing context management as an online optimization problem. SCOPE leverages execution traces to automatically evolve the agent’s prompt, improving its ability to handle dynamic environments. Central to SCOPE is the Dual-Stream mechanism, which balances tactical specificity—addressing immediate errors—with strategic generality aimed at evolving long-term guiding principles. Additionally, SCOPE employs Perspective-Driven Exploration to widen strategic coverage, thereby increasing the chances that the agent applies the correct strategy for diverse tasks. Experimental results on the HLE benchmark demonstrate a significant performance improvement, with task success rates rising from 14.23% to 38.64%, achieved entirely without human intervention. The authors have also made their code publicly accessible, encouraging further research and implementation in this area. <div>
arXiv:2512.15374v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations</title>
<link>https://arxiv.org/abs/2512.15388</link>
<guid>https://arxiv.org/abs/2512.15388</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, route instructions, pedestrian wayfinding, qualitative spatial relations, navigation  

<br /><br />Summary:  
1. The paper addresses enhancing Large Language Models (LLMs) to improve their performance in providing route instructions specifically tailored for pedestrian navigation.  
2. It emphasizes the use of qualitative spatial relations—descriptions that capture relative positions and directions in an intuitive, human-friendly manner rather than relying on exact metrics or coordinates.  
3. The approach aims to make route instructions generated by LLMs more comprehensible and naturally aligned with how humans understand and communicate about space during wayfinding tasks.  
4. By integrating spatial relation concepts, the model can generate guidance that better reflects real-world pedestrian navigation challenges such as turns, landmarks, and relative distances.  
5. Overall, the work contributes to advancing LLM capabilities in qualitative spatial reasoning to provide clearer, more practical route instructions for pedestrian users. <div>
arXiv:2512.15388v1 Announce Type: new 
Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat</title>
<link>https://arxiv.org/abs/2512.15435</link>
<guid>https://arxiv.org/abs/2512.15435</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-player card games, Skat, self-playing AI, bootstrapping outer-learning, feature hash functions<br /><br />Summary:<br /><br />1. The paper addresses decision-making in early stages of multi-player card games such as Skat and Bridge, emphasizing the importance of bidding, game selection, and initial card selection over middle- and end-game play.  
2. Current approaches rely heavily on statistical data derived from large databases of human expert games but are limited by computational resources.  
3. The authors propose a general bootstrapping outer-learning framework that enhances prediction accuracy by augmenting human game data with millions of self-play games generated by AI.  
4. They implement perfect feature hash functions to manage compacted statistical tables efficiently, which supports continuous self-improvement of the card game engine through self-learning.  
5. A case study in Skat demonstrates the practical application of their method, showing that this automated approach can effectively assist in various decision-making aspects of the game, ultimately leading to improved performance. <div>
arXiv:2512.15435v1 Announce Type: new 
Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intent-Driven UAM Rescheduling</title>
<link>https://arxiv.org/abs/2512.15462</link>
<guid>https://arxiv.org/abs/2512.15462</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban Air Mobility, vertiport scheduling, Mixed Integer Linear Programming, three-valued logic, Answer Set Programming<br /><br />Summary:<br /><br />1. The paper addresses efficient scheduling challenges in vertiports, which are critical hubs for Urban Air Mobility (UAM), especially under restricted resource conditions.<br /><br />2. The authors employ a Mixed Integer Linear Programming (MILP) model, commonly used in resource-constrained project scheduling problems (RCPSP), to formulate the scheduling problem.<br /><br />3. The approach uniquely integrates dynamic operation requirements and vague rescheduling requests expressed by human users, which are often ambiguous and difficult to handle with traditional methods.<br /><br />4. To interpret ambiguous user inputs, the authors utilize three-valued logic, which allows the system to reason about uncertain or vague human intents more effectively.<br /><br />5. The proposed framework combines Answer Set Programming (ASP) and MILP to optimize scheduling while supporting transparent and explainable human inputs, resulting in an adaptive and robust scheduling system tailored for UAM vertiports. <div>
arXiv:2512.15462v1 Announce Type: new 
Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</title>
<link>https://arxiv.org/abs/2512.15489</link>
<guid>https://arxiv.org/abs/2512.15489</guid>
<content:encoded><![CDATA[
<div> Keywords: Nemotron-Math, mathematical reasoning, Python tool-integrated reasoning, long-context training, AIME 2024 accuracy<br /><br />Summary:<br /><br />1. Nemotron-Math is a large-scale mathematical reasoning dataset introduced to provide diverse reasoning styles, long-form solution traces, and effective integration of computational tools, addressing limitations found in existing datasets. <br />2. The dataset comprises 7.5 million solution traces categorized into high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR).<br />3. It integrates a curated collection of 85,000 AoPS problems alongside 262,000 community-sourced math problems from StackExchange-Math, blending structured competition problems and real-world queries to improve robustness and generalization.<br />4. Controlled evaluations show Nemotron-Math outperforms the original OpenMathReasoning dataset on AoPS problems, with notable improvements in robustness and generalization on the diverse StackExchange-Math set, maintaining competitive accuracy on math competition benchmarks.<br />5. The authors developed a sequential bucketed training strategy to efficiently fine-tune models with extremely long context lengths (up to 128K tokens), achieving 2–3 times faster training without significant accuracy loss.<br /><br />Overall, Nemotron-Math enables state-of-the-art mathematical reasoning performance, demonstrated by achieving 100% majority-at-16 accuracy on the AIME 2024 and 2025 competitions using Python tool-integrated reasoning. <div>
arXiv:2512.15489v1 Announce Type: new 
Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2512.15567</link>
<guid>https://arxiv.org/abs/2512.15567</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, scientific discovery, benchmarks, hypothesis generation, evaluation framework<br /><br />Summary:<br /><br />This article introduces a novel scenario-grounded benchmark designed to evaluate large language models (LLMs) in the context of real scientific research across biology, chemistry, materials science, and physics. Unlike existing benchmarks that test static, decontextualized knowledge, this framework focuses on the iterative processes central to scientific discovery, including hypothesis generation, designing experiments or simulations, and interpreting results. The evaluation framework operates on two levels: question-level accuracy based on scenario-specific items, and project-level performance requiring models to engage in comprehensive scientific tasks. Application of this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals persistent performance gaps compared to general science benchmarks, limited improvements from scaling model size and reasoning capacity, and shared weaknesses among leading models from different developers. The study also notes substantial variability in model performance across various research scenarios, which affects which model excels in scientific projects, indicating that no current LLM approaches a general scientific "superintelligence." Nevertheless, LLMs show potential in a wide array of scientific discovery tasks, even when scenario-specific scores are low, underscoring the importance of guided exploration and serendipity in research. Overall, this SDE framework provides a reproducible and practical benchmark to drive the advancement of LLM capabilities toward authentic scientific discovery. <div>
arXiv:2512.15567v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decision-Theoretic Approach for Managing Misalignment</title>
<link>https://arxiv.org/abs/2512.15584</link>
<guid>https://arxiv.org/abs/2512.15584</guid>
<content:encoded><![CDATA[
<div> Value alignment, delegation, epistemic accuracy, decision-theoretic framework, AI trust<br /><br />Summary:<br /><br />This paper addresses the critical question of when it is appropriate to delegate decisions to AI systems, especially under conditions of uncertainty and imperfect value alignment. It highlights a gap in existing research that mostly focuses on shaping AI values but often neglects how to judge whether the current level of alignment justifies delegation. The authors propose a formal decision-theoretic framework that balances three key factors: an agent's value (mis)alignment, its epistemic accuracy (knowledge reliability), and its reach (the scope of decisions it can handle). Their analysis distinguishes between two main forms of delegation. Universal delegation, which involves trusting an AI with any decision problem, requires near-perfect alignment and complete epistemic trust—criteria that are rarely met. Alternatively, context-specific delegation can be optimal even when significant misalignment exists, as the agent’s superior accuracy or broader decision reach can lead to better overall outcomes. The paper introduces a novel ex ante scoring framework to quantify this trade-off, enabling principled decision-making about delegation. Ultimately, the work shifts the focus away from pursuing perfect alignment towards managing the risks and rewards of delegation under uncertainty, providing a practical foundation for determining when an AI system is sufficiently aligned for particular tasks. <div>
arXiv:2512.15584v1 Announce Type: new 
Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15662</link>
<guid>https://arxiv.org/abs/2512.15662</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning, self-critique, reinforcement learning, critical thinking<br /><br />Summary:<br /><br />1. The paper addresses the limitation in current large language models (LLMs), which typically separate reasoning from verification, either lacking immediate error feedback or requiring external verifiers that complicate learning.<br />2. Inspired by human critical thinking, the authors propose Stepwise Think-Critique (STC), a novel unified framework that integrates reasoning and self-critique at each step of the process within a single LLM.<br />3. STC is trained using a hybrid reinforcement learning objective that combines rewards for reasoning correctness and consistency in self-evaluation, thereby optimizing both the quality of reasoning and the ability to self-check.<br />4. Experiments conducted on mathematical reasoning benchmarks demonstrate that STC exhibits enhanced critic-thinking abilities and produces reasoning traces that are more interpretable compared to previous methods.<br />5. The work represents progress toward developing LLMs with built-in critical thinking skills, improving both accuracy and the transparency of their reasoning processes. <div>
arXiv:2512.15662v1 Announce Type: new 
Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining the Reasoning of Large Language Models Using Attribution Graphs</title>
<link>https://arxiv.org/abs/2512.15663</link>
<guid>https://arxiv.org/abs/2512.15663</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Context Attribution, Attribution Graph, Causality, Explanation Faithfulness  

<br /><br />Summary:  
Large language models (LLMs) demonstrate impressive capabilities but their internal reasoning processes are often opaque, raising concerns about safety and trustworthiness. Attribution methods, widely used in computer vision, help explain decisions by assigning credit to input features. Extending these ideas, context attributions aim to explain autoregressive LLM behaviors by linking generated tokens to prompts. However, current context attribution methods fall short by only relating outputs directly to the initial prompt, ignoring the influence of intermediate generations. To address this limitation, the paper introduces the Context Attribution via Graph Explanations (CAGE) framework. CAGE constructs an attribution graph, a directed structure representing how each generated token is influenced not only by the prompt but also by all prior tokens. The graph ensures two key properties: causality (preserving the temporal order of influence) and row stochasticity (making the influence values properly normalized). By marginalizing contributions along paths in this graph, CAGE provides a more complete and faithful attribution of model outputs. Experiments across multiple models, datasets, and evaluation metrics show that CAGE consistently improves the faithfulness of context attributions, achieving average performance gains of up to 40%, making it a significant advancement in explainability for LLMs. <div>
arXiv:2512.15663v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artism: AI-Driven Dual-Engine System for Art Generation and Critique</title>
<link>https://arxiv.org/abs/2512.15710</link>
<guid>https://arxiv.org/abs/2512.15710</guid>
<content:encoded><![CDATA[
<div> Keywords: dual-engine AI, AIDA, Ismism Machine, art evolution, critical analysis<br /><br />Summary:<br /><br />This paper introduces a dual-engine AI architectural method aimed at tackling the complexities involved in exploring potential trajectories in the evolution of art. The method consists of two interconnected components: AIDA, an artificial artist social network designed to simulate art interactions, and the Ismism Machine, a system built for critical analysis of art concepts and developments. The innovation of the framework lies in combining deep learning techniques with multi-agent collaboration, enabling multidimensional simulations that model art historical progressions and conceptual innovation patterns. This approach moves beyond traditional, unidirectional art critique by promoting an intelligent and interactive reflexive practice, which engages AI systems in continual feedback loops. The paper details ongoing experimental applications of this methodology in contemporary art concept studies, showcasing its capacity for dynamic and computationally driven analysis. Overall, the research presents a novel AI-driven conceptual framework that facilitates critical loops in art analysis, expanding possibilities for computationally understanding and simulating art evolution in ways that integrate social, conceptual, and historical dimensions. <div>
arXiv:2512.15710v1 Announce Type: new 
Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title>
<link>https://arxiv.org/abs/2512.15712</link>
<guid>https://arxiv.org/abs/2512.15712</guid>
<content:encoded><![CDATA[
<div> Interpretability, neural networks, concept bottleneck, predictive concept decoder, latent attributes<br /><br />Summary:<br /><br />This paper addresses the challenge of interpreting internal activations of neural networks by proposing a novel end-to-end training approach instead of relying on hand-designed interpretability agents. The authors introduce Predictive Concept Decoders (PCDs), an architecture that compresses neural activations into a sparse list of meaningful concepts via an encoder, which a decoder then uses to answer natural language questions about the model's behavior. The approach allows the assistant to be pretrained on large unstructured datasets and subsequently fine-tuned to improve question-answering performance. The PCD benefits from favorable scaling properties, meaning that as more data is used, the interpretability of the bottleneck concepts and accuracy on downstream tasks both improve. Notably, PCDs demonstrate strong capabilities in detecting model jailbreaks, secret hints, implanted latent concepts, and accurately reveal latent user attributes encoded within the model. This work advances scalable and faithful interpretability by turning internal activation analysis into a trainable predictive task, enhancing understanding and transparency in neural network behavior. <div>
arXiv:2512.15712v1 Announce Type: new 
Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union</title>
<link>https://arxiv.org/abs/2512.12837</link>
<guid>https://arxiv.org/abs/2512.12837</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-powered greenwashing, criminal liability, algorithmic deception, corporate sustainability, ESG governance<br /><br />Summary: This study addresses the challenge of AI-powered greenwashing within corporate sustainability governance, highlighting how it increases the opacity of environmental disclosures and complicates regulatory oversight. It performs a comparative legal analysis focusing on criminal liability for AI-mediated greenwashing across India, the US, and the EU, revealing significant gaps in attributing culpability when deceptive claims stem from algorithms. Current laws show anthropocentric biases by requiring proof of human intent, making them ill-suited to handle algorithmic deception. The research identifies outdated fraud statutes that fail to accommodate AI-generated misrepresentation adequately. Employing a doctrinal legal methodology, it examines judicial precedents and legislation to explore possibilities for expanding corporate criminal liability frameworks. Key findings emphasize the suitability of strict liability models, the need for recalibrated governance frameworks addressing AI accountability, and the introduction of algorithmic due diligence mandates within ESG regimes. Comparative analysis reveals jurisdictional differences, with the EU's Corporate Sustainability Due Diligence Directive (CSDDD) offering a promising transnational regulatory model. The study contributes to AI ethics and environmental law by proposing a hybrid liability framework that combines algorithmic risk assessments with evolving concepts of legal personhood, ensuring that the opacity of algorithms does not obstruct the enforcement of liability. <div>
arXiv:2512.12837v1 Announce Type: cross 
Abstract: AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-\`a-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tourists Profiling by Interest Analysis</title>
<link>https://arxiv.org/abs/2512.14704</link>
<guid>https://arxiv.org/abs/2512.14704</guid>
<content:encoded><![CDATA[
<div> Keywords: tourists' behavior, digital traces, tourism analysis, qualitative and quantitative, attractions networks<br /><br />Summary:<br /><br />1. The paper addresses changes in the analysis of tourist behavior due to the digital revolution, highlighting how digital traces from travelers provide new data sources.  
2. Previous tourism studies have largely focused on the quantitative analysis of these digital traces to derive conclusions about tourist patterns.  
3. This study proposes an integrated approach that incorporates both qualitative and quantitative methods to analyze digital traces more comprehensively.  
4. The goal is to better understand the dynamic factors that influence tourist behavior, moving beyond simple numerical data to include contextual insights.  
5. A particular emphasis is placed on the role of attractions networks, examining how the interconnectedness of tourist sites affects travel behaviors and decisions.  
6. This dual-aspect methodology aims to yield deeper insights into tourist dynamics, supporting enhanced tourism management and planning strategies. <div>
arXiv:2512.14704v1 Announce Type: cross 
Abstract: With the recent digital revolution, analyzing of tourists' behaviors and research fields associated with it have changed profoundly. It is now easier to examine behaviors of tourists using digital traces they leave during their travels. The studies conducted on diverse aspects of tourism focus on quantitative aspects of digital traces to reach its conclusions. In this paper, we suggest a study focused on both qualitative and quantitative aspect of digital traces to understand the dynamics governing tourist behavior, especially those concerning attractions networks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: neural architecture search, large language model, image captioning, DeepSeek, BLEU-4<br /><br />Summary:<br /><br />1. The paper introduces NN-Caption, a pipeline for neural architecture search (NAS) guided by large language models (LLMs) to automatically generate runnable image-captioning models. 2. The method composes CNN encoders from the LEMUR classification backbone with sequence decoders such as LSTM, GRU, and Transformer, following a strict network API standard. 3. Using DeepSeek-R1-0528-Qwen3-8B as the primary code generator, the authors provide prompt templates and examples of generated architectures for reproducible results. 4. Evaluation on the MS COCO dataset is conducted using BLEU-4 scoring, where over half of the dozens of LLM-generated captioning models successfully train and produce meaningful captions. 5. An analysis compares prompts with different numbers of input model snippets (5 vs. 10), showing a slight decrease in success rate with more components. 6. Training dynamics are reported, including caption accuracy progression and the highest BLEU-4 achieved. 7. The LLM not only proposes architectures but also suggests hyperparameters and training strategies, improving NAS effectiveness. 8. Challenges such as code hallucinations and API compliance issues are identified, with iterative prompt adjustments and code fixes applied to resolve them. 9. The pipeline integrates prompt-based code generation with automatic evaluation, adding dozens of novel captioning models to the open LEMUR dataset to support reproducible benchmarking and AutoML research advancement. <div>
arXiv:2512.14706v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Source Knowledge Selection in Multi-Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.14710</link>
<guid>https://arxiv.org/abs/2512.14710</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised multi-domain adaptation, source knowledge selection, density-driven selection, pseudo-label enhancement, multimodal model<br /><br />Summary:  
This paper addresses the challenge of unsupervised multi-domain adaptation, where knowledge from multiple labeled source domains is used to improve learning in an unlabeled target domain. The authors identify a critical issue: many source domains may contain redundant or irrelevant information that can negatively impact transfer learning performance, especially when dealing with a large number of source domains. To solve this, they propose a novel method named Autonomous Source Knowledge Selection (AutoS), which autonomously selects the most relevant and transferable source samples and models for the target task. AutoS uses a density-driven selection strategy to guide source sample selection during training and to decide the influence of each source model on target predictions. Additionally, the approach incorporates a pseudo-label enhancement module built upon a pre-trained multimodal model, aimed at reducing label noise in the target domain and enhancing self-supervision. The combination of strategic source knowledge selection with pseudo-label refinement helps achieve more robust and accurate adaptation to the target domain. Experimental results on real-world datasets demonstrate the effectiveness and superiority of AutoS compared to existing methods, highlighting its potential for large-scale multi-domain adaptation tasks. <div>
arXiv:2512.14710v1 Announce Type: cross 
Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Promoting Fairness in Information Access within Social Networks</title>
<link>https://arxiv.org/abs/2512.14711</link>
<guid>https://arxiv.org/abs/2512.14711</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, fairness, information access, resistance distance, greedy algorithm<br /><br />Summary:<br /><br />1. The paper addresses the issue of unequal information access in online social networks, especially affecting minority or disadvantaged groups due to their positions in the network.<br /><br />2. It formulates the optimization problem of adding new network connections aimed at improving fairness in information spread across different demographic groups.<br /><br />3. The study introduces the use of resistance distance as a measure of information access, highlighting a novel global structural and multi-path connectivity perspective.<br /><br />4. The problem is proven to be NP-hard, leading the authors to propose a simple greedy algorithm that achieves accurate solutions but runs in cubic time complexity.<br /><br />5. The main technical contribution is the development of a linear-time approximate algorithm that significantly reduces the computational cost, enabling efficient processing of large-scale networks with millions of nodes.<br /><br />6. Extensive experiments on both real-world and synthetic datasets demonstrate that the linear-time algorithm maintains solution accuracy while scaling effectively to large networks. <div>
arXiv:2512.14711v1 Announce Type: cross 
Abstract: The advent of online social networks has facilitated fast and wide spread of information. However, some users, especially members of minority groups, may be less likely to receive information spreading on the network, due to their disadvantaged network position. We study the optimization problem of adding new connections to a network to enhance fairness in information access among different demographic groups.
  We provide a concrete formulation of this problem where information access is measured in terms of resistance distance, {offering a new perspective that emphasizes global network structure and multi-path connectivity.} The problem is shown to be NP-hard. We propose a simple greedy algorithm which turns out to output accurate solutions, but its run time is cubic, which makes it undesirable for large networks. As our main technical contribution, we reduce its time complexity to linear, leveraging several novel approximation techniques. In addition to our theoretical findings, we also conduct an extensive set of experiments using both real-world and synthetic datasets. We demonstrate that our linear-time algorithm can produce accurate solutions for networks with millions of nodes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
<div> Sepsis, Deep Fusion, Mixture-of-Experts, Antibiotic Selection, Clinical Decision Support<br /><br />Summary:  
Sepsis accounts for nearly 20% of global ICU admissions, yet existing prediction models struggle to integrate diverse data types effectively, often treating each modality in isolation or using brittle early fusion techniques. This study compares End-to-End Deep Fusion and Context-Aware Stacking approaches for sepsis prediction tasks using multiple data streams such as vitals, text, and imaging. The initially proposed Quad-Modal Hierarchical Gated Attention Network, called SepsisFusionFormer, aimed to capture complex cross-modal interactions but suffered from "attention starvation" and overfitting on a small antibiotic cohort (N ≈ 2,100), resulting in a suboptimal AUC of 0.66. In response, the authors developed SepsisLateFusion, a leaner Context-Aware Mixture-of-Experts (MoE) model where each modality is treated as an orthogonal expert—namely the Historian (Static), the Monitor (Temporal), and the Reader (NLP)—and gated dynamically by a CatBoost meta-learner. This design achieved state-of-the-art predictive performance with an AUC of 0.915 for predictions made 4 hours before clinical onset, and a 48% reduction in missed sepsis cases after threshold calibration, enabling earlier intervention. Additionally, a Quad-Modal Ensemble was shown to perform best (0.72 AUC) for a new prescriptive task of multi-class antibiotic selection. These models are implemented in SepsisSuite, an open-source Python framework for clinical decision support available at the provided GitHub link. <div>
arXiv:2512.14712v1 Announce Type: cross 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.14714</link>
<guid>https://arxiv.org/abs/2512.14714</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater acoustic classification, Gabor convolution, ResNeXt, squeeze-and-excitation, environmental noise  

<br /><br />Summary:  
This paper addresses the challenge of remotely detecting and classifying underwater acoustic targets, which is critical for environmental monitoring and defense. It highlights the difficulties posed by complex ship-radiated and environmental underwater noise that affect signal processing accuracy. The authors propose GSE ResNeXt, a novel deep learning architecture that integrates learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation (SE) attention mechanisms. The Gabor convolutions act as adaptive two-dimensional band-pass filters, increasing feature channel representation, while the SE attention improves training stability, convergence speed, and feature discriminability. The model is evaluated across three classification tasks of increasing difficulty, with a specific focus on the temporal differences between training and testing datasets, revealing that vessel-to-sensor distance strongly impacts classification performance. Experimental results demonstrate that GSE ResNeXt consistently outperforms baseline models such as Xception, ResNet, and MobileNetV2 in classification accuracy. Additionally, the inclusion of Gabor convolutional layers reduces training time by approximately 28%. The study underscores the importance of integrating signal processing techniques into deep learning frameworks to enhance robustness and generalization in data-limited underwater acoustic scenarios. Future work is suggested to focus on mitigating environmental influences on input signals to further improve reliability. <div>
arXiv:2512.14714v1 Announce Type: cross 
Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title>
<link>https://arxiv.org/abs/2512.14715</link>
<guid>https://arxiv.org/abs/2512.14715</guid>
<content:encoded><![CDATA[
<div> Keywords: bit flips, image captioning, semantic drift, differentiable fault analysis, large language models<br /><br />Summary: This paper investigates how bit-level perturbations (fault injections) in the weights of large language models (LLMs) used for image captioning affect the semantic content of generated captions without disrupting grammatical structure. Unlike previous fault analysis that focused on accuracy drops or model crashes, this work emphasizes subtle semantic changes caused by individual bit flips, which can alter the model's visual-to-linguistic mappings and shift the narrative conveyed. The authors propose that these semantic drifts are not random but can be estimated via the model’s gradients, using a differentiable approach to identify bits that critically impact meaning while preserving fluency. They introduce BLADE (Bit-level Fault Analysis via Differentiable Estimation), a framework leveraging gradient-based sensitivity analysis combined with semantic-fluency objectives at the caption level to pinpoint and refine semantically important bits for targeted perturbations. This methodology reveals how semantic information is encoded and mutable at the bit level in vision-language generative models. The findings highlight the vulnerability of such models to imperceptible low-level changes that steer high-level semantics and pave the way for enhanced robustness testing, adversarial defenses, and explainable AI by showing how structured bit faults reshape semantic output. <div>
arXiv:2512.14715v1 Announce Type: cross 
Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.14718</link>
<guid>https://arxiv.org/abs/2512.14718</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series forecasting, spectral entropy, dependency modeling, signed graph, temporal position

<br /><br />Summary:  
This paper addresses key challenges in multivariate time series forecasting, focusing on accurately modeling complex inter-variable dependencies. Existing methods using attention or graph-based approaches struggle with disruptions from irrelevant variables, ignore negative correlations due to softmax normalization, and lack temporal position awareness. To overcome these, the authors propose SEED, a Spectral Entropy-guided Evaluation framework. SEED introduces a Dependency Evaluator that uses spectral entropy to dynamically assess spatial and temporal dependencies, balancing between Channel Independence and Channel Dependence strategies. Additionally, the Spectral Entropy-based Fuser refines these dependency weights by isolating temporal patterns originating from influences of other variables rather than intrinsic dynamics. To preserve negative correlations, SEED employs a Signed Graph Constructor allowing signed edge weights, addressing shortcomings of softmax normalization. The Context Spatial Extractor module enables variables to perceive their temporal positions through local contextual windows, enhancing spatial feature construction. Extensive experiments on 12 diverse real-world datasets confirm SEED's effectiveness and generality, demonstrating state-of-the-art forecasting performance across multiple domains. This work presents a novel framework that significantly advances spatial-temporal dependency modeling in multivariate time series forecasting. <div>
arXiv:2512.14718v1 Announce Type: cross 
Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Attribution Priors for Explainable and Robust Model Training</title>
<link>https://arxiv.org/abs/2512.14719</link>
<guid>https://arxiv.org/abs/2512.14719</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Attribution Prior, Interpretability, Robustness, Classification  

<br /><br />Summary:  
This paper addresses the limitations of current attribution-based explanation methods used to improve interpretability and robustness in small language models (SLMs) for classification tasks. First, it identifies a key issue that existing attribution methods tend to highlight common keywords shared across semantically similar classes, which lack sufficient discriminative power for fine-grained class differentiation. Second, to tackle this problem, the authors propose a novel framework called Class-Aware Attribution Prior (CAP), which extracts class-specific attribution priors that better capture subtle distinctions between classes. Third, building upon CAP, they develop CAP Hybrid, a method that integrates CAP priors with those from existing attribution techniques, creating a balanced and diversified supervisory signal to guide model training. Fourth, by aligning the model’s self-attribution with these enriched priors, the approach fosters learning of diverse and decision-relevant features, enhancing interpretability. Finally, extensive experimental validation performed under various conditions—including full-data, few-shot learning, and adversarial settings—demonstrates that the proposed methods consistently improve both the interpretability and robustness of small language models in classification tasks. <div>
arXiv:2512.14719v1 Announce Type: cross 
Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMe: A Realistic Benchmark for LLM-based Social Media Agents</title>
<link>https://arxiv.org/abs/2512.14720</link>
<guid>https://arxiv.org/abs/2512.14720</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social media agents, benchmark, SoMe dataset, evaluation  

<br /><br />Summary: This paper presents SoMe, a novel benchmark aimed at evaluating intelligent agents powered by large language models (LLMs) in the context of social media. The benchmark addresses the current lack of comprehensive tools to assess these agents' abilities to comprehend media content, understand user behavior, and make complex decisions. SoMe includes eight distinct social media agent tasks, encompassing over 9 million posts, more than 6,500 user profiles, and upwards of 25,000 reports sourced from multiple social media platforms and external websites. It features 17,869 meticulously annotated task queries to ensure detailed evaluation. Compared to existing datasets, SoMe stands out as the first versatile and realistic platform designed specifically for LLM-based social media agents to tackle diverse challenges. Through extensive quantitative and qualitative analyses, the authors provide an insightful overview of mainstream agentic LLM performance in real-world social media environments, highlighting key limitations. The study finds that neither closed-source nor open-source LLMs currently meet satisfactory performance levels on social media agent tasks. By making the code and data publicly available, SoMe offers a challenging yet valuable testbed for future research and development of social media agents. <div>
arXiv:2512.14720v1 Announce Type: cross 
Abstract: Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</title>
<link>https://arxiv.org/abs/2512.14722</link>
<guid>https://arxiv.org/abs/2512.14722</guid>
<content:encoded><![CDATA[
<div> Transformers, Groebner bases, Hierarchical Attention Transformers, polynomial equations, curriculum learning

<br /><br />Summary:  
1. This paper advances the use of transformers for computing Groebner bases, which are crucial in computer algebra and have many practical applications.  
2. It introduces Hierarchical Attention Transformers (HATs), an architecture that imposes a tree-structured inductive bias to better capture hierarchical relationships inherent in the data.  
3. By leveraging this hierarchical structure, the method achieves significant computational savings compared to conventional flat attention transformer models.  
4. The authors generalize the HAT approach to arbitrary depths and provide a detailed analysis of the computational costs involved.  
5. When combined with curriculum learning strategies, the method substantially improves performance, successfully solving larger instances of multivariate polynomial systems than prior work, specifically outperforming results from Kera et al. (2024). <div>
arXiv:2512.14722v1 Announce Type: cross 
Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</title>
<link>https://arxiv.org/abs/2512.14725</link>
<guid>https://arxiv.org/abs/2512.14725</guid>
<content:encoded><![CDATA[
<div> urban wind flow, generative diffusion, graph neural network, CFD simulation, urban geometry<br /><br />Summary:<br /><br />1. The article addresses the challenge of modeling urban wind flow, which is crucial for air quality assessment and sustainable city planning, particularly in complex urban geometries.<br /><br />2. It highlights the limitations of low-order models in capturing geometric effects and the high computational cost of high-fidelity Computational Fluid Dynamics (CFD) simulations.<br /><br />3. The authors propose a novel generative diffusion framework that synthesizes steady-state urban wind velocity fields on unstructured meshes, using only geometry information as input.<br /><br />4. The framework integrates a hierarchical graph neural network with score-based diffusion modeling, enabling accurate and diverse flow field generation without the need for temporal rollouts or dense measurements.<br /><br />5. Trained over multiple mesh slices and wind incident angles, the model generalizes well to unseen urban geometries, successfully reconstructs important flow features like wakes and recirculation zones, and provides uncertainty-aware predictions.<br /><br />6. Ablation studies demonstrate the model's robustness to variations in mesh structures and its effectiveness across different inference strategies.<br /><br />7. This work lays the groundwork for foundation models in the built environment, offering urban planners a fast, reliable tool to evaluate design scenarios under densification pressures and climate variability. <div>
arXiv:2512.14725v1 Announce Type: cross 
Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14726</link>
<guid>https://arxiv.org/abs/2512.14726</guid>
<content:encoded><![CDATA[
<div> Quantum Decision Transformer, offline reinforcement learning, quantum-inspired attention, long-horizon credit assignment, adaptive computation<br /><br />Summary:<br /><br />Offline reinforcement learning enables policy development from pre-collected data without further environment interaction; however, Decision Transformer (DT) architectures face challenges with long-horizon credit assignment and capturing complex state-action dependencies. To address these, the paper introduces the Quantum Decision Transformer (QDT), a novel model that incorporates quantum-inspired computational mechanisms. QDT integrates two primary components: Quantum-Inspired Attention, which uses entanglement operations to capture non-local feature correlations, and Quantum Feedforward Networks, which apply multi-path processing and learnable interference to enable adaptive computation. Experimental evaluations on continuous control tasks demonstrate that QDT achieves over 2,000% performance improvements compared to standard DTs, with better generalization across datasets of varying quality. Ablation studies highlight a strong synergy between the quantum-inspired components, showing that neither alone matches the performance of their combined use, emphasizing the importance of holistic co-design over modular adoption. The analysis identifies three key computational advantages from this design: enhanced credit assignment through non-local correlations, implicit ensemble behavior driven by parallel processing, and adaptive resource allocation enabled by learnable interference. These results establish quantum-inspired architectural design as a promising direction for improving transformer models in sequential decision-making and broader neural architecture design. <div>
arXiv:2512.14726v1 Announce Type: cross 
Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title>
<link>https://arxiv.org/abs/2512.14727</link>
<guid>https://arxiv.org/abs/2512.14727</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, conformal prediction, uncertainty estimates, calibration set size, medical imaging<br /><br />Summary:  
Machine learning (ML) is significantly impacting healthcare by offering advanced diagnostic and prognostic tools. Reliable uncertainty estimates are crucial for safe clinical decision-making, yet standard ML models often fail to deliver trustworthy uncertainty quantification. Conformal prediction (CP) is a methodology designed to convert heuristic uncertainty estimates from ML models into statistically valid prediction sets with guaranteed coverage probabilities. A key theoretical advantage of CP is that these statistical guarantees hold irrespective of the calibration set size. However, this paper challenges the practical utility of this advantage, arguing that although guarantees exist for any calibration set size, the effectiveness and informativeness of the resulting uncertainty estimates are strongly dependent on having sufficiently large calibration sets. The implications are critical in medical fields where data scarcity is common, and large calibration samples are often unavailable or difficult to acquire. The authors empirically validate their critique on a medical image classification task, demonstrating that small calibration sets may yield theoretically valid but practically unhelpful uncertainty estimates, potentially compromising clinical decision-making. This work highlights the need for caution when applying CP in medical applications with limited calibration data and suggests a reassessment of its practical benefits under data scarcity conditions. <div>
arXiv:2512.14727v1 Announce Type: cross 
Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Geometry for policy-constrained interpretation</title>
<link>https://arxiv.org/abs/2512.14731</link>
<guid>https://arxiv.org/abs/2512.14731</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic interpretation, policy constraints, geometric framework, hallucination prevention, financial data  

<br /><br />Summary:  
This article introduces a geometric framework designed for policy-constrained semantic interpretation aimed at eliminating hallucinated commitments, which is critical in high-stakes domains such as regulated finance. Semantic meanings are modeled as directions on a unit sphere, while evidence is represented by sets of witness vectors, forming admissible interpretations as spherical convex regions. Policy constraints are explicitly incorporated as priors on the same geometric manifold, intentionally separated from evidence to maintain clarity and enforce restrictions. Interpretation is performed via constrained optimization over these admissible regions, where refusal to commit naturally arises when contradictions or policy exclusions occur, reflecting a topologically necessary outcome. The framework is rigorously connected to foundational concepts including information theory, Bayesian inference, and sheaf-theoretic semantics, and it is proven that the complexity bounds of the method are information-theoretically optimal. Empirically, the approach was validated on large-scale regulated financial datasets, demonstrating zero hallucinated approvals across different policy regimes. This achievement represents a significant advancement as it is the first scalable solution to completely prevent hallucinated commitments under policy constraints. <div>
arXiv:2512.14731v1 Announce Type: cross 
Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
<div> Keywords: incidental findings, CT scans, large language models, vision-language models, planner-executor framework<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting and reporting incidental findings in abdominal CT scans, which are important for clinical decision-making but traditionally require time-consuming manual review by radiologists. The authors propose a novel automated framework that integrates large language models (LLMs) and foundational vision-language models (VLMs) using a plan-and-execute agentic approach. The framework consists of two main components: a planner based on an LLM that generates Python scripts leveraging predefined base functions aligned with medical guidelines, and an executor that runs these scripts to perform detection and classification tasks through VLMs, segmentation models, and image processing routines. This planner-executor setup enables automatic management of incidental findings for abdominal organs without human intervention. Experiments conducted on a dedicated abdominal CT benchmark involving three organs demonstrate the approach’s superior accuracy and efficiency compared to existing purely VLM-based methods. The results validate the potential of combining LLMs with VLMs in a structured agentic framework to enhance the precision and speed of incidental finding analysis in medical imaging workflows. This advancement could lead to more consistent and scalable radiology reporting practices. <div>
arXiv:2512.14732v1 Announce Type: cross 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</title>
<link>https://arxiv.org/abs/2512.14735</link>
<guid>https://arxiv.org/abs/2512.14735</guid>
<content:encoded><![CDATA[
<div> Keywords: PyFi, financial image understanding, vision language models, question chains, Monte Carlo Tree Search<br /><br />Summary:  
1. This paper introduces PyFi, a new framework designed for pyramid-like financial image understanding, which allows vision language models (VLMs) to reason through progressively complex question chains.  
2. At the core of PyFi is PyFi-600K, a large-scale dataset containing 600,000 financial question-answer pairs arranged in a reasoning pyramid from basic perception questions at the base to complex financial visual reasoning at the apex.  
3. The dataset is automatically synthesized using PyFi-adv, a multi-agent adversarial system based on Monte Carlo Tree Search (MCTS), where a challenger agent and a solver agent collaboratively generate question chains that explore deeper reasoning capabilities.  
4. Using PyFi-600K, the authors conduct detailed, hierarchical evaluations of advanced VLMs in the financial domain, highlighting their reasoning abilities across multiple complexity levels.  
5. Fine-tuning models such as Qwen2.5-VL-3B and Qwen2.5-VL-7B with the pyramid-structured question chains significantly improves performance, achieving average accuracy gains of 19.52% and 8.06% respectively in answering complex financial questions by decomposing them into simpler sub-questions.  
All associated resources including code, dataset, and models are publicly available at https://github.com/AgenticFinLab/PyFi. <div>
arXiv:2512.14735v1 Announce Type: cross 
Abstract: This paper proposes PyFi, a novel framework for pyramid-like financial image understanding that enables vision language models (VLMs) to reason through question chains in a progressive, simple-to-complex manner. At the core of PyFi is PyFi-600K, a dataset comprising 600K financial question-answer pairs organized into a reasoning pyramid: questions at the base require only basic perception, while those toward the apex demand increasing levels of capability in financial visual understanding and expertise. This data is scalable because it is synthesized without human annotations, using PyFi-adv, a multi-agent adversarial mechanism under the Monte Carlo Tree Search (MCTS) paradigm, in which, for each image, a challenger agent competes with a solver agent by generating question chains that progressively probe deeper capability levels in financial visual reasoning. Leveraging this dataset, we present fine-grained, hierarchical, and comprehensive evaluations of advanced VLMs in the financial domain. Moreover, fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on the pyramid-structured question chains enables these models to answer complex financial questions by decomposing them into sub-questions with gradually increasing reasoning demands, yielding average accuracy improvements of 19.52% and 8.06%, respectively, on the dataset. All resources of code, dataset and models are available at: https://github.com/AgenticFinLab/PyFi .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol</title>
<link>https://arxiv.org/abs/2512.14737</link>
<guid>https://arxiv.org/abs/2512.14737</guid>
<content:encoded><![CDATA[
<div> Keywords: agent communication, zero-knowledge proofs, privacy, auditability, Model Context Protocol (MCP)  

<br /><br />Summary:  
This article addresses the challenge of ensuring verifiable audit trails in agent communications without compromising privacy and confidentiality. It proposes a novel framework that integrates zero-knowledge proofs (ZKPs) with the existing Model Context Protocol (MCP), enabling message verification without revealing actual content. The framework supports privacy-preserving audits where messages can be authenticated and checked for rule compliance, while details remain confidential. Designed for lightweight and standard MCP networks, the system adds asynchronous audit verification to confirm message structure and general types without exposing specifics. A key feature is mutual auditing where agents can independently verify communication content quality and usage metrics without disclosing sensitive information. The authors formalize the framework's security goals, demonstrating that zk-MCP achieves data authenticity and communication privacy efficiently, with minimal latency overhead. They implement the approach fully, using Circom for zero-knowledge proof generation and integrating the audit protocol with MCP's bidirectional communication channels. This represents the first privacy-preserving audit system for agent communication, enabling verifiable mutual audits while maintaining strict confidentiality and agent privacy compliance, making it suitable for regulated environments requiring billing accuracy, compliance verification, and accountability. <div>
arXiv:2512.14737v1 Announce Type: cross 
Abstract: Existing agent communication frameworks face critical limitations in providing verifiable audit trails without compromising the privacy and confidentiality of agent interactions. The protection of agent communication privacy while ensuring auditability emerges as a fundamental challenge for applications requiring accurate billing, compliance verification, and accountability in regulated environments.
  We introduce a framework for auditing agent communications that keeps messages private while still checking they follow expected rules. It pairs zero-knowledge proofs with the existing Model Context Protocol (MCP) so messages can be verified without revealing their contents. The approach runs in lightweight networks, stays compatible with standard MCP exchanges, and adds asynchronous audit verification to confirm format and general message types without exposing specifics.
  The framework enables mutual audits between agents: one side can check communication content and quality while the other verifies usage metrics, all without revealing sensitive information. We formalize security goals and show that zk-MCP provides data authenticity and communication privacy, achieving efficient verification with negligible latency overhead. We fully implement the framework, including Circom-based zero-knowledge proof generation and an audit protocol integrated with MCP's bidirectional channel, and, to our knowledge, this is the first privacy-preserving audit system for agent communications that offers verifiable mutual auditing without exposing message content or compromising agent privacy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2512.14741</link>
<guid>https://arxiv.org/abs/2512.14741</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, Large Language Models, continual fine-tuning, P-Trojan, persistence-aware evaluation  

<br /><br />Summary:  
Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), allowing adversaries to trigger harmful outputs or bypass safety controls. This study focuses on the persistence of those implanted backdoors when the model undergoes multi-stage, user-driven post-deployment continual fine-tuning, a scenario that has been rarely examined before. The authors propose P-Trojan, a novel trigger-based attack algorithm designed explicitly to optimize for backdoor persistence across multiple model updates. P-Trojan aligns poisoned gradients with clean task gradients on token embeddings to ensure the backdoor mapping remains effective and is not suppressed or forgotten during successive fine-tuning steps. Theoretical analysis confirms the feasibility of persistent backdoor attacks under continual fine-tuning conditions. Extensive experiments on Qwen2.5 and LLaMA3 model families, as well as a variety of task sequences, demonstrate that P-Trojan can maintain over 99% backdoor persistence without degrading clean-task accuracy. These findings highlight the vulnerability of LLMs to persistent backdoor attacks in realistic adaptation pipelines and underline the urgent need for persistence-aware evaluation methods and stronger defense mechanisms against backdoors during model fine-tuning. <div>
arXiv:2512.14741v1 Announce Type: cross 
Abstract: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title>
<link>https://arxiv.org/abs/2512.14742</link>
<guid>https://arxiv.org/abs/2512.14742</guid>
<content:encoded><![CDATA[
<div> Open Radio Access Networks, cybersecurity, hierarchical defense, quantum computing, machine learning  

<br /><br />Summary:  
This article addresses the increased cybersecurity vulnerabilities in Open Radio Access Networks (O-RAN) due to their modular and disaggregated architecture across control, user, and management planes. A hierarchical defense framework is proposed, consisting of three coordinated layers: anomaly detection, intrusion confirmation, and multiattack classification, each corresponding to different levels of O-RAN's telemetry. The framework uniquely integrates hybrid quantum computing with machine learning techniques, employing amplitude- and entanglement-based feature encodings alongside deep and ensemble classifiers to enhance detection capability. The authors conduct comprehensive benchmarking on both synthetic and real-world telemetry datasets, analyzing encoding depth, architecture variants, and diagnostic accuracy. Results demonstrate that the framework consistently achieves near-perfect accuracy, high recall, and strong class separability, indicating its effectiveness in identifying complex cyber attacks. Furthermore, the system’s interpretability and robustness are validated through evaluations focusing on decision boundaries, probabilistic margins, and latent space geometry. These qualities support the framework’s readiness for practical deployment. Finally, the solution is designed to be slice-aware and scalable, targeting deployment in near-real-time (near-RT) and non-real-time (non-RT) RAN Intelligent Controller (RIC) domains, thus addressing operational needs within O-RAN environments. <div>
arXiv:2512.14742v1 Announce Type: cross 
Abstract: Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation</title>
<link>https://arxiv.org/abs/2512.14744</link>
<guid>https://arxiv.org/abs/2512.14744</guid>
<content:encoded><![CDATA[
<div> Financial AI, Retrieval-Augmented Generation, Neurosymbolic Policy, Regulatory Compliance, Mathematical Validation<br /><br />Summary:<br /><br />Financial AI systems often struggle with calculation errors and regulatory compliance violations even when retrieval of relevant documents is accurate. This paper introduces VERAFI (Verified Agentic Financial Intelligence), a novel framework that integrates neurosymbolic policy generation with advanced dense retrieval and cross-encoder reranking techniques. VERAFI incorporates financial tool-enabled agents and automated reasoning policies designed to ensure adherence to GAAP standards, SEC regulations, and rigorous mathematical validation. The framework was evaluated using the FinanceBench dataset, where it demonstrated a significant performance increase in factual correctness from 52.4% with traditional methods to 94.7%, representing an 81% relative improvement. Notably, the neurosymbolic policy layer contributed a 4.3 percentage point improvement by specifically mitigating recurring logical and calculation mistakes common in language model reasoning. By embedding domain-specific financial knowledge directly into the reasoning pipeline, VERAFI offers a reliable solution to the accuracy challenges in financial AI. This integration supports critical applications such as regulatory compliance, investment decision-making, and risk management, advancing the trustworthiness and practical deployment of AI systems in finance. <div>
arXiv:2512.14744v1 Announce Type: cross 
Abstract: Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\% factual correctness, VERAFI's integrated approach reaches 94.7\%, an 81\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factor(U,T): Controlling Untrusted AI by Monitoring their Plans</title>
<link>https://arxiv.org/abs/2512.14745</link>
<guid>https://arxiv.org/abs/2512.14745</guid>
<content:encoded><![CDATA[
<div> Keywords: Factored cognition, malicious decomposer, monitors, Python coding tasks, AUROC

<br /><br />Summary: This paper investigates the security implications of using AI models for task decomposition in factored cognition, where a complex task is broken down into simpler subtasks handled by separate models. Traditional approaches use trusted but less capable models for decomposition to avoid malicious behavior, limiting applications where decomposition itself is difficult. The authors propose Factor(U,T), a framework where a stronger but untrusted model performs decomposition, while trusted models handle subtasks. They explore whether monitoring only the natural language instructions from the decomposer can detect malicious behavior. Experiments on BigCodeBench, a dataset of Python coding tasks, show that monitors analyzing just the decomposition instructions perform poorly, achieving an AUROC of 0.52 in distinguishing malicious versus honest decompositions. In contrast, monitors that evaluate the complete Python solutions generated by child tasks achieve much higher accuracy (AUROC 0.96). Additionally, Factor(D,U), where a trusted decomposer and untrusted implementers are monitored at the solution level, demonstrates excellent detection capability (AUROC 0.96) and safety (1.2% attack success rate). The results indicate that monitoring at the implementation level effectively detects malicious activity, whereas decomposition-only monitoring is insufficient. <div>
arXiv:2512.14745v1 Announce Type: cross 
Abstract: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Cross-Modal Mapping of Molecular, Pathologic, and Radiologic Phenotypes in Lipid-Deficient Clear Cell Renal CellCarcinoma</title>
<link>https://arxiv.org/abs/2512.14750</link>
<guid>https://arxiv.org/abs/2512.14750</guid>
<content:encoded><![CDATA[
<div> Clear cell renal cell carcinoma, intratumoral heterogeneity, molecular subtypes, radiomics, computational pathology<br /><br />Summary:  
Clear cell renal cell carcinoma (ccRCC) displays significant intratumoral heterogeneity across multiple biological scales, complicating clinical outcome predictions and limiting conventional TNM staging efficacy. A particular lipid-deficient subtype known as DCCD-ccRCC, identified through multi-omics analyses, correlates with worse outcomes even at early stages. The study introduces a hierarchical cross-scale framework enabling preoperative identification of this high-risk subtype by integrating data from molecular, histological, and radiological modalities. This framework features a molecular-to-pathology-to-radiology supervisory bridge, allowing molecular signatures to be mapped onto tissue and imaging phenotypes. Two specialized modality-specific models were developed: PathoDCCD, capturing cellular to meso-regional pathological features, and RadioDCCD, integrating whole-tumor and subregional radiomics data alongside heterogeneity metrics from CT scans. Together, these nested models facilitate accurate molecular subtype prediction and risk stratification. Validation across five cohorts totaling 1,659 patients demonstrated PathoDCCD’s fidelity in reflecting molecular subtypes and RadioDCCD’s ability for reliable noninvasive preoperative predictions. The cross-scale model effectively identifies patients with the poorest prognoses, thereby unifying molecular biology, computational pathology, and quantitative radiology into a clinically applicable, biologically grounded platform for noninvasive molecular phenotyping of ccRCC. <div>
arXiv:2512.14750v1 Announce Type: cross 
Abstract: Clear cell renal cell carcinoma (ccRCC) exhibits extensive intratumoral heterogeneity on multiple biological scales, contributing to variable clinical outcomes and limiting the effectiveness of conventional TNM staging, which highlights the urgent need for multiscale integrative analytic frameworks. The lipid-deficient de-clear cell differentiated (DCCD) ccRCC subtype, defined by multi-omics analyses, is associated with adverse outcomes even in early-stage disease. Here, we establish a hierarchical cross-scale framework for the preoperative identification of DCCD-ccRCC. At the highest layer, cross-modal mapping transferred molecular signatures to histological and CT phenotypes, establishing a molecular-to-pathology-to-radiology supervisory bridge. Within this framework, each modality-specific model is designed to mirror the inherent hierarchical structure of tumor biology. PathoDCCD captured multi-scale microscopic features, from cellular morphology and tissue architecture to meso-regional organization. RadioDCCD integrated complementary macroscopic information by combining whole-tumor and its habitat-subregions radiomics with a 2D maximal-section heterogeneity metric. These nested models enabled integrated molecular subtype prediction and clinical risk stratification. Across five cohorts totaling 1,659 patients, PathoDCCD reliably recapitulated molecular subtypes, while RadioDCCD provided reliable preoperative prediction. The consistent predictions identified patients with the poorest clinical outcomes. This cross-scale paradigm unifies molecular biology, computational pathology, and quantitative radiology into a biologically grounded strategy for preoperative noninvasive molecular phenotyping of ccRCC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs</title>
<link>https://arxiv.org/abs/2512.14751</link>
<guid>https://arxiv.org/abs/2512.14751</guid>
<content:encoded><![CDATA[
<div> Keywords: finetuning, large language models, jailbreak vulnerabilities, adversarial prompts, Probe-Guided Projection (PGP)<br /><br />Summary:<br /><br />This paper investigates the security implications of finetuning pretrained large language models (LLMs), focusing on whether finetuned models inherit jailbreak vulnerabilities present in their pretrained versions. The authors adopt a realistic threat model where attackers have white-box access to the pretrained LLM but only black-box access to finetuned derivatives. Empirical results demonstrate that adversarial prompts crafted on the pretrained LLM transfer effectively to its finetuned variants, indicating the inheritance of vulnerabilities through the pretrain-to-finetune pipeline. By probing the representations within the pretrained hidden states, the study finds that these transferable adversarial prompts are linearly separable, implying that universal transferability is embedded in the pretrained model’s internal structure. Leveraging this insight, the researchers introduce the Probe-Guided Projection (PGP) attack, which directs the optimization of adversarial prompts toward directions relevant to transferability. Experiments conducted across multiple LLM families and a wide range of finetuning tasks validate PGP’s high success rate in transferring attacks, highlighting significant security risks in the standard paradigm of developing downstream applications by finetuning pretrained LLMs. The findings stress the necessity for improved security measures considering inherited vulnerabilities in finetuned language models. <div>
arXiv:2512.14751v1 Announce Type: cross 
Abstract: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyberswarm: a novel swarm intelligence algorithm inspired by cyber community dynamics</title>
<link>https://arxiv.org/abs/2512.14752</link>
<guid>https://arxiv.org/abs/2512.14752</guid>
<content:encoded><![CDATA[
<div> swarm intelligence, recommendation systems, dynamic hypergraph, Node2Vec, social networks  

<br /><br />Summary:  
This paper addresses the challenges recommendation systems face in adapting to evolving user preferences and complex social network interactions. Traditional methods fall short in handling the intricate dynamics of cyber-social systems and lack flexibility across domains. To overcome these limitations, the authors propose a general-purpose swarm intelligence algorithm inspired by social psychology principles. The framework models user preferences and community influences using a dynamic hypergraph structure, integrating centrality-based feature extraction and Node2Vec embeddings. Preference evolution is managed through message-passing mechanisms and hierarchical graph modeling, allowing real-time adaptation to behavioral changes. Experimental results demonstrate superior performance on diverse recommendation tasks such as social networks and content discovery, measured by Hit Rate (HR), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG), outperforming baseline algorithms consistently. This model's adaptability enables it to generate contextually relevant and precise recommendations in dynamic environments. The research contributes by effectively bridging individual preferences with community-level influences through the novel integration of swarm intelligence and network dynamics. Its general-purpose design supports application across various domains, including social graphs, personalized learning platforms, and medical graphs. Overall, the study advances recommendation system methodologies by addressing optimization challenges in complex, dynamic networks. <div>
arXiv:2512.14752v1 Announce Type: cross 
Abstract: Recommendation systems face challenges in dynamically adapting to evolving user preferences and interactions within complex social networks. Traditional approaches often fail to account for the intricate interactions within cyber-social systems and lack the flexibility to generalize across diverse domains, highlighting the need for more adaptive and versatile solutions. In this work, we introduce a general-purpose swarm intelligence algorithm for recommendation systems, designed to adapt seamlessly to varying applications. It was inspired by social psychology principles. The framework models user preferences and community influences within a dynamic hypergraph structure. It leverages centrality-based feature extraction and Node2Vec embeddings. Preference evolution is guided by message-passing mechanisms and hierarchical graph modeling, enabling real-time adaptation to changing behaviors. Experimental evaluations demonstrated the algorithm's superior performance in various recommendation tasks, including social networks and content discovery. Key metrics such as Hit Rate (HR), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG) consistently outperformed baseline methods across multiple datasets. The model's adaptability to dynamic environments allowed for contextually relevant and precise recommendations. The proposed algorithm represents an advancement in recommendation systems by bridging individual preferences and community influences. Its general-purpose design enables applications in diverse domains, including social graphs, personalized learning, and medical graphs. This work highlights the potential of integrating swarm intelligence with network dynamics to address complex optimization challenges in recommendation systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CODE ACROSTIC: Robust Watermarking for Code Generation</title>
<link>https://arxiv.org/abs/2512.14753</link>
<guid>https://arxiv.org/abs/2512.14753</guid>
<content:encoded><![CDATA[
<div> Keywords: watermarking, large language models, code generation, comment removal attack, low-entropy code<br /><br />Summary:<br /><br />1. Watermarking large language models (LLMs) is crucial to prevent misuse such as fake news generation, plagiarism, and spam, especially for code, which often contains valuable intellectual property. <br />2. Existing watermarking methods for LLM-generated code are vulnerable to comment removal attacks, where an attacker simply deletes comments without changing code functionality, thereby evading detection. <br />3. Code watermarking is challenging due to the low-entropy nature of most source code compared to natural language, limiting the capacity and stealth of watermark insertion. <br />4. The authors propose a novel approach that uses prior knowledge to differentiate between low-entropy and high-entropy parts of code by leveraging a Cue List of words. This targeted injection improves the watermark's resilience and detectability. <br />5. Their method was evaluated on the HumanEval benchmark and showed superior detectability and usability when compared with three state-of-the-art code watermarking techniques, demonstrating the effectiveness of their approach against comment removal attacks. <div>
arXiv:2512.14753v1 Announce Type: cross 
Abstract: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Reliability of Language Models in Instruction-Following</title>
<link>https://arxiv.org/abs/2512.14754</link>
<guid>https://arxiv.org/abs/2512.14754</guid>
<content:encoded><![CDATA[
arXiv:2512.14754v1 Announce Type: cross 
Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: Capability Achievement via Policy Execution</title>
<link>https://arxiv.org/abs/2512.14761</link>
<guid>https://arxiv.org/abs/2512.14761</guid>
<content:encoded><![CDATA[
arXiv:2512.14761v1 Announce Type: cross 
Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workflows vs Agents for Code Translation</title>
<link>https://arxiv.org/abs/2512.14762</link>
<guid>https://arxiv.org/abs/2512.14762</guid>
<content:encoded><![CDATA[
arXiv:2512.14762v1 Announce Type: cross 
Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis</title>
<link>https://arxiv.org/abs/2512.14764</link>
<guid>https://arxiv.org/abs/2512.14764</guid>
<content:encoded><![CDATA[
arXiv:2512.14764v1 Announce Type: cross 
Abstract: Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Discrete Diffusion for Constraint Satisfaction Problems</title>
<link>https://arxiv.org/abs/2512.14765</link>
<guid>https://arxiv.org/abs/2512.14765</guid>
<content:encoded><![CDATA[
arXiv:2512.14765v1 Announce Type: cross 
Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation</title>
<link>https://arxiv.org/abs/2512.14767</link>
<guid>https://arxiv.org/abs/2512.14767</guid>
<content:encoded><![CDATA[
arXiv:2512.14767v1 Announce Type: cross 
Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title>
<link>https://arxiv.org/abs/2512.14770</link>
<guid>https://arxiv.org/abs/2512.14770</guid>
<content:encoded><![CDATA[
arXiv:2512.14770v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $\Phi_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[
arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</title>
<link>https://arxiv.org/abs/2512.14797</link>
<guid>https://arxiv.org/abs/2512.14797</guid>
<content:encoded><![CDATA[
arXiv:2512.14797v1 Announce Type: cross 
Abstract: Advanced Ovarian Cancer (AOC) is often diagnosed at an advanced stage with peritoneal carcinosis (PC). Fagotti score (FS) assessment at diagnostic laparoscopy (DL) guides treatment planning by estimating surgical resectability, but its subjective and operator-dependent nature limits reproducibility and widespread use. Videos of patients undergoing DL with concomitant FS assessments at a referral center were retrospectively collected and divided into a development dataset, for data annotation, AI training and evaluation, and an independent test dataset, for internal validation. In the development dataset, FS-relevant frames were manually annotated for anatomical structures and PC. Deep learning models were trained to automatically identify FS-relevant frames, segment structures and PC, and predict video-level FS and indication to surgery (ItS). AI performance was evaluated using Dice score for segmentation, F1-scores for anatomical stations (AS) and ItS prediction, and root mean square error (RMSE) for final FS estimation. In the development dataset, the segmentation model trained on 7,311 frames, achieved Dice scores of 70$\pm$3% for anatomical structures and 56$\pm$3% for PC. Video-level AS classification achieved F1-scores of 74$\pm$3% and 73$\pm$4%, FS prediction showed normalized RMSE values of 1.39$\pm$0.18 and 1.15$\pm$0.08, and ItS reached F1-scores of 80$\pm$8% and 80$\pm$2% in the development (n=101) and independent test datasets (n=50), respectively. This is the first AI model to predict the feasibility of cytoreductive surgery providing automated FS estimation from DL videos. Its reproducible and reliable performance across datasets suggests that AI can support surgeons through standardized intraoperative tumor burden assessment and clinical decision-making in AOC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title>
<link>https://arxiv.org/abs/2512.14801</link>
<guid>https://arxiv.org/abs/2512.14801</guid>
<content:encoded><![CDATA[
arXiv:2512.14801v1 Announce Type: cross 
Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharing State Between Prompts and Programs</title>
<link>https://arxiv.org/abs/2512.14805</link>
<guid>https://arxiv.org/abs/2512.14805</guid>
<content:encoded><![CDATA[
arXiv:2512.14805v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.
  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.
  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let the Barbarians In: How AI Can Accelerate Systems Performance Research</title>
<link>https://arxiv.org/abs/2512.14806</link>
<guid>https://arxiv.org/abs/2512.14806</guid>
<content:encoded><![CDATA[
arXiv:2512.14806v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber</title>
<link>https://arxiv.org/abs/2512.14846</link>
<guid>https://arxiv.org/abs/2512.14846</guid>
<content:encoded><![CDATA[
arXiv:2512.14846v1 Announce Type: cross 
Abstract: Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&amp;CK mappings).
  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.
  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials</title>
<link>https://arxiv.org/abs/2512.14855</link>
<guid>https://arxiv.org/abs/2512.14855</guid>
<content:encoded><![CDATA[
arXiv:2512.14855v1 Announce Type: cross 
Abstract: Machine learning (ML) has been increasingly applied in concrete research to optimize performance and mixture design. However, one major challenge in applying ML to cementitious materials is the limited size and diversity of available databases. A promising solution is the development of multi-modal databases that integrate both numerical and graphical data. Conventional ML frameworks in cement research are typically restricted to a single data modality. Graph neural network (GNN) represents a new generation of neural architectures capable of learning from data structured as graphs, capturing relationships through irregular or topology-dependent connections rather than fixed spatial coordinates. While GNN is inherently designed for graphical data, they can be adapted to extract correlations from numerical datasets and potentially embed physical laws directly into their architecture, enabling explainable and physics-informed predictions. This work is among the first few studies to implement GNNs to design concrete, with a particular emphasis on establishing a clear and reproducible pathway for converting tabular data into graph representations using the k-nearest neighbor (K-NN) approach. Model hyperparameters and feature selection are systematically optimized to enhance prediction performance. The GNN shows performance comparable to the benchmark random forest, which has been demonstrated by many studies to yield reliable predictions for cementitious materials. Overall, this study provides a foundational roadmap for transitioning from traditional ML to advanced AI architectures. The proposed framework establishes a strong foundation for future multi-modal and physics-informed GNN models capable of capturing complex material behaviors and accelerating the design and optimization of cementitious materials.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks</title>
<link>https://arxiv.org/abs/2512.14860</link>
<guid>https://arxiv.org/abs/2512.14860</guid>
<content:encoded><![CDATA[
arXiv:2512.14860v1 Announce Type: cross 
Abstract: Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel "hallucinated compliance" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</title>
<link>https://arxiv.org/abs/2512.14879</link>
<guid>https://arxiv.org/abs/2512.14879</guid>
<content:encoded><![CDATA[
arXiv:2512.14879v1 Announce Type: cross 
Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media</title>
<link>https://arxiv.org/abs/2512.14887</link>
<guid>https://arxiv.org/abs/2512.14887</guid>
<content:encoded><![CDATA[
arXiv:2512.14887v1 Announce Type: cross 
Abstract: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus, i) uses a hybrid human-machine approach to identify the range of viewpoints expressed about a given topic, and ii) classifies relevant claims with respect to the identified viewpoints, defined as sets of semantically and ideologically congruent claims (e.g., positions arguing that immigration positively impacts the UK economy). In this paper, we improve this pipeline by i) fine-tuning Large Language Models (LLMs) for viewpoint classification and ii) enriching claim representations with semantic descriptions of relevant actors drawn from Wikidata. We evaluate our approach against alternative solutions on a benchmark centred on the UK immigration debate. Results show that while both mechanisms independently improve classification performance, their integration yields the best results, particularly when using LLMs capable of processing long inputs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</title>
<link>https://arxiv.org/abs/2512.14892</link>
<guid>https://arxiv.org/abs/2512.14892</guid>
<content:encoded><![CDATA[
arXiv:2512.14892v1 Announce Type: cross 
Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</title>
<link>https://arxiv.org/abs/2512.14895</link>
<guid>https://arxiv.org/abs/2512.14895</guid>
<content:encoded><![CDATA[
arXiv:2512.14895v1 Announce Type: cross 
Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline</title>
<link>https://arxiv.org/abs/2512.14896</link>
<guid>https://arxiv.org/abs/2512.14896</guid>
<content:encoded><![CDATA[
arXiv:2512.14896v1 Announce Type: cross 
Abstract: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.
  Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters.
  Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark.
  Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</title>
<link>https://arxiv.org/abs/2512.14926</link>
<guid>https://arxiv.org/abs/2512.14926</guid>
<content:encoded><![CDATA[
arXiv:2512.14926v1 Announce Type: cross 
Abstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies</title>
<link>https://arxiv.org/abs/2512.14930</link>
<guid>https://arxiv.org/abs/2512.14930</guid>
<content:encoded><![CDATA[
arXiv:2512.14930v1 Announce Type: cross 
Abstract: High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pre-trained Segmentation Models using Post-Processing</title>
<link>https://arxiv.org/abs/2512.14937</link>
<guid>https://arxiv.org/abs/2512.14937</guid>
<content:encoded><![CDATA[
arXiv:2512.14937v1 Announce Type: cross 
Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</title>
<link>https://arxiv.org/abs/2512.14938</link>
<guid>https://arxiv.org/abs/2512.14938</guid>
<content:encoded><![CDATA[
arXiv:2512.14938v1 Announce Type: cross 
Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2512.14946</link>
<guid>https://arxiv.org/abs/2512.14946</guid>
<content:encoded><![CDATA[
arXiv:2512.14946v1 Announce Type: cross 
Abstract: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.
  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Repetition Improves Non-Reasoning LLMs</title>
<link>https://arxiv.org/abs/2512.14982</link>
<guid>https://arxiv.org/abs/2512.14982</guid>
<content:encoded><![CDATA[
arXiv:2512.14982v1 Announce Type: cross 
Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</title>
<link>https://arxiv.org/abs/2512.14989</link>
<guid>https://arxiv.org/abs/2512.14989</guid>
<content:encoded><![CDATA[
arXiv:2512.14989v1 Announce Type: cross 
Abstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</title>
<link>https://arxiv.org/abs/2512.14990</link>
<guid>https://arxiv.org/abs/2512.14990</guid>
<content:encoded><![CDATA[
arXiv:2512.14990v1 Announce Type: cross 
Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where is the Watermark? Interpretable Watermark Detection at the Block Level</title>
<link>https://arxiv.org/abs/2512.14994</link>
<guid>https://arxiv.org/abs/2512.14994</guid>
<content:encoded><![CDATA[
arXiv:2512.14994v1 Announce Type: cross 
Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</title>
<link>https://arxiv.org/abs/2512.14998</link>
<guid>https://arxiv.org/abs/2512.14998</guid>
<content:encoded><![CDATA[
arXiv:2512.14998v1 Announce Type: cross 
Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</title>
<link>https://arxiv.org/abs/2512.15000</link>
<guid>https://arxiv.org/abs/2512.15000</guid>
<content:encoded><![CDATA[
arXiv:2512.15000v1 Announce Type: cross 
Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</title>
<link>https://arxiv.org/abs/2512.15006</link>
<guid>https://arxiv.org/abs/2512.15006</guid>
<content:encoded><![CDATA[
arXiv:2512.15006v1 Announce Type: cross 
Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic diversity across language models mitigates knowledge collapse</title>
<link>https://arxiv.org/abs/2512.15011</link>
<guid>https://arxiv.org/abs/2512.15011</guid>
<content:encoded><![CDATA[
arXiv:2512.15011v1 Announce Type: cross 
Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Representation-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15036</link>
<guid>https://arxiv.org/abs/2512.15036</guid>
<content:encoded><![CDATA[
arXiv:2512.15036v1 Announce Type: cross 
Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</title>
<link>https://arxiv.org/abs/2512.15047</link>
<guid>https://arxiv.org/abs/2512.15047</guid>
<content:encoded><![CDATA[
arXiv:2512.15047v1 Announce Type: cross 
Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title>
<link>https://arxiv.org/abs/2512.15052</link>
<guid>https://arxiv.org/abs/2512.15052</guid>
<content:encoded><![CDATA[
arXiv:2512.15052v1 Announce Type: cross 
Abstract: Disclaimer: Samples in this paper may be harmful and cause discomfort.
  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\% to 2.5\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title>
<link>https://arxiv.org/abs/2512.15053</link>
<guid>https://arxiv.org/abs/2512.15053</guid>
<content:encoded><![CDATA[
arXiv:2512.15053v1 Announce Type: cross 
Abstract: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based "prompt engineering," fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for "Observable Software Engineering" in the era of probabilistic computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</title>
<link>https://arxiv.org/abs/2512.15061</link>
<guid>https://arxiv.org/abs/2512.15061</guid>
<content:encoded><![CDATA[
arXiv:2512.15061v1 Announce Type: cross 
Abstract: This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</title>
<link>https://arxiv.org/abs/2512.15066</link>
<guid>https://arxiv.org/abs/2512.15066</guid>
<content:encoded><![CDATA[
arXiv:2512.15066v1 Announce Type: cross 
Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.15067</link>
<guid>https://arxiv.org/abs/2512.15067</guid>
<content:encoded><![CDATA[
arXiv:2512.15067v1 Announce Type: cross 
Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title>
<link>https://arxiv.org/abs/2512.15068</link>
<guid>https://arxiv.org/abs/2512.15068</guid>
<content:encoded><![CDATA[
arXiv:2512.15068v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMMD: A pose-guided multi-view multi-modal diffusion for person generation</title>
<link>https://arxiv.org/abs/2512.15069</link>
<guid>https://arxiv.org/abs/2512.15069</guid>
<content:encoded><![CDATA[
arXiv:2512.15069v1 Announce Type: cross 
Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Return on Security Controls in LLM Systems</title>
<link>https://arxiv.org/abs/2512.15081</link>
<guid>https://arxiv.org/abs/2512.15081</guid>
<content:encoded><![CDATA[
arXiv:2512.15081v1 Announce Type: cross 
Abstract: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</title>
<link>https://arxiv.org/abs/2512.15112</link>
<guid>https://arxiv.org/abs/2512.15112</guid>
<content:encoded><![CDATA[
arXiv:2512.15112v1 Announce Type: cross 
Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</title>
<link>https://arxiv.org/abs/2512.15115</link>
<guid>https://arxiv.org/abs/2512.15115</guid>
<content:encoded><![CDATA[
arXiv:2512.15115v1 Announce Type: cross 
Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</title>
<link>https://arxiv.org/abs/2512.15116</link>
<guid>https://arxiv.org/abs/2512.15116</guid>
<content:encoded><![CDATA[
arXiv:2512.15116v1 Announce Type: cross 
Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable</title>
<link>https://arxiv.org/abs/2512.15117</link>
<guid>https://arxiv.org/abs/2512.15117</guid>
<content:encoded><![CDATA[
arXiv:2512.15117v1 Announce Type: cross 
Abstract: General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management</title>
<link>https://arxiv.org/abs/2512.15119</link>
<guid>https://arxiv.org/abs/2512.15119</guid>
<content:encoded><![CDATA[
arXiv:2512.15119v1 Announce Type: cross 
Abstract: Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Multi-Objective Human Heuristics</title>
<link>https://arxiv.org/abs/2512.15120</link>
<guid>https://arxiv.org/abs/2512.15120</guid>
<content:encoded><![CDATA[
arXiv:2512.15120v1 Announce Type: cross 
Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens</title>
<link>https://arxiv.org/abs/2512.15133</link>
<guid>https://arxiv.org/abs/2512.15133</guid>
<content:encoded><![CDATA[
arXiv:2512.15133v1 Announce Type: cross 
Abstract: Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</title>
<link>https://arxiv.org/abs/2512.15134</link>
<guid>https://arxiv.org/abs/2512.15134</guid>
<content:encoded><![CDATA[
arXiv:2512.15134v1 Announce Type: cross 
Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning</title>
<link>https://arxiv.org/abs/2512.15149</link>
<guid>https://arxiv.org/abs/2512.15149</guid>
<content:encoded><![CDATA[
arXiv:2512.15149v1 Announce Type: cross 
Abstract: Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model's generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers</title>
<link>https://arxiv.org/abs/2512.15163</link>
<guid>https://arxiv.org/abs/2512.15163</guid>
<content:encoded><![CDATA[
arXiv:2512.15163v1 Announce Type: cross 
Abstract: Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEER: Draft with Diffusion, Verify with Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.15176</link>
<guid>https://arxiv.org/abs/2512.15176</guid>
<content:encoded><![CDATA[
arXiv:2512.15176v1 Announce Type: cross 
Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Governing rapid technological change: Policy Delphi on the future of European AI governance</title>
<link>https://arxiv.org/abs/2512.15196</link>
<guid>https://arxiv.org/abs/2512.15196</guid>
<content:encoded><![CDATA[
arXiv:2512.15196v1 Announce Type: cross 
Abstract: The rapid advancements in artificial intelligence (AI) present unique challenges for policymakers that seek to govern the technology. In this context, the Delphi method has become an established way to identify consensus and disagreement on emerging technological issues among experts in the field of futures studies and foresight. The aim of this article is twofold: first, it examines key tensions experts see in the development of AI governance in Europe, and second, it reflects on the Delphi method's capacity to inform anticipatory governance of emerging technologies like AI based on these insights. The analysis is based on the results of a two-round Policy Delphi study on the future of AI governance with European policymakers, researchers and NGOs, conducted in mid-2024. The Policy Delphi proved useful in revealing diverse perspectives on European AI governance, drawing out a consensus that future-proof AI regulation will likely depend more on practical implementation and enforcement of legislation than on its technical specifics or scope. Furthermore, the study identified a desirability-probability gap in AI governance: desirable policy directions, like greater citizen participation, were perceived as less probable and feasible. This highlights a tension between desirable regulatory oversight and the practical difficulty for regulation to keep up with technological change.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA</title>
<link>https://arxiv.org/abs/2512.15219</link>
<guid>https://arxiv.org/abs/2512.15219</guid>
<content:encoded><![CDATA[
arXiv:2512.15219v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct "brother" relations, 2-hop for indirect "father-son" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a "question-paths-answer" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024</title>
<link>https://arxiv.org/abs/2512.15226</link>
<guid>https://arxiv.org/abs/2512.15226</guid>
<content:encoded><![CDATA[
arXiv:2512.15226v1 Announce Type: cross 
Abstract: This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</title>
<link>https://arxiv.org/abs/2512.15249</link>
<guid>https://arxiv.org/abs/2512.15249</guid>
<content:encoded><![CDATA[
arXiv:2512.15249v1 Announce Type: cross 
Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $\Delta$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $\Delta$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</title>
<link>https://arxiv.org/abs/2512.15250</link>
<guid>https://arxiv.org/abs/2512.15250</guid>
<content:encoded><![CDATA[
arXiv:2512.15250v1 Announce Type: cross 
Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</title>
<link>https://arxiv.org/abs/2512.15258</link>
<guid>https://arxiv.org/abs/2512.15258</guid>
<content:encoded><![CDATA[
arXiv:2512.15258v1 Announce Type: cross 
Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15274</link>
<guid>https://arxiv.org/abs/2512.15274</guid>
<content:encoded><![CDATA[
arXiv:2512.15274v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</title>
<link>https://arxiv.org/abs/2512.15286</link>
<guid>https://arxiv.org/abs/2512.15286</guid>
<content:encoded><![CDATA[
arXiv:2512.15286v1 Announce Type: cross 
Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting</title>
<link>https://arxiv.org/abs/2512.15308</link>
<guid>https://arxiv.org/abs/2512.15308</guid>
<content:encoded><![CDATA[
arXiv:2512.15308v1 Announce Type: cross 
Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies</title>
<link>https://arxiv.org/abs/2512.15312</link>
<guid>https://arxiv.org/abs/2512.15312</guid>
<content:encoded><![CDATA[
arXiv:2512.15312v1 Announce Type: cross 
Abstract: Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</title>
<link>https://arxiv.org/abs/2512.15315</link>
<guid>https://arxiv.org/abs/2512.15315</guid>
<content:encoded><![CDATA[
arXiv:2512.15315v1 Announce Type: cross 
Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection</title>
<link>https://arxiv.org/abs/2512.15325</link>
<guid>https://arxiv.org/abs/2512.15325</guid>
<content:encoded><![CDATA[
arXiv:2512.15325v1 Announce Type: cross 
Abstract: Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-based module for accurately reading linear scales in a laboratory</title>
<link>https://arxiv.org/abs/2512.15327</link>
<guid>https://arxiv.org/abs/2512.15327</guid>
<content:encoded><![CDATA[
arXiv:2512.15327v1 Announce Type: cross 
Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality</title>
<link>https://arxiv.org/abs/2512.15343</link>
<guid>https://arxiv.org/abs/2512.15343</guid>
<content:encoded><![CDATA[
arXiv:2512.15343v1 Announce Type: cross 
Abstract: The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</title>
<link>https://arxiv.org/abs/2512.15344</link>
<guid>https://arxiv.org/abs/2512.15344</guid>
<content:encoded><![CDATA[
arXiv:2512.15344v1 Announce Type: cross 
Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial versification in portuguese as a jailbreak operator in LLMs</title>
<link>https://arxiv.org/abs/2512.15353</link>
<guid>https://arxiv.org/abs/2512.15353</guid>
<content:encoded><![CDATA[
arXiv:2512.15353v1 Announce Type: cross 
Abstract: Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[
arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Recognition in Signers</title>
<link>https://arxiv.org/abs/2512.15376</link>
<guid>https://arxiv.org/abs/2512.15376</guid>
<content:encoded><![CDATA[
arXiv:2512.15376v1 Announce Type: cross 
Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[
arXiv:2512.15396v1 Announce Type: cross 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.15430</link>
<guid>https://arxiv.org/abs/2512.15430</guid>
<content:encoded><![CDATA[
arXiv:2512.15430v1 Announce Type: cross 
Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Double Horizon Model-Based Policy Optimization</title>
<link>https://arxiv.org/abs/2512.15439</link>
<guid>https://arxiv.org/abs/2512.15439</guid>
<content:encoded><![CDATA[
arXiv:2512.15439v1 Announce Type: cross 
Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Assessing the Relevance of Code Reviews Authored by Generative Models</title>
<link>https://arxiv.org/abs/2512.15466</link>
<guid>https://arxiv.org/abs/2512.15466</guid>
<content:encoded><![CDATA[
arXiv:2512.15466v1 Announce Type: cross 
Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?</title>
<link>https://arxiv.org/abs/2512.15468</link>
<guid>https://arxiv.org/abs/2512.15468</guid>
<content:encoded><![CDATA[
arXiv:2512.15468v1 Announce Type: cross 
Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Geometric Inductive Bias for Object Centric Dynamics</title>
<link>https://arxiv.org/abs/2512.15493</link>
<guid>https://arxiv.org/abs/2512.15493</guid>
<content:encoded><![CDATA[
arXiv:2512.15493v1 Announce Type: cross 
Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title>
<link>https://arxiv.org/abs/2512.15503</link>
<guid>https://arxiv.org/abs/2512.15503</guid>
<content:encoded><![CDATA[
arXiv:2512.15503v1 Announce Type: cross 
Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems</title>
<link>https://arxiv.org/abs/2512.15526</link>
<guid>https://arxiv.org/abs/2512.15526</guid>
<content:encoded><![CDATA[
arXiv:2512.15526v1 Announce Type: cross 
Abstract: Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditioned UNet for Music Source Separation</title>
<link>https://arxiv.org/abs/2512.15532</link>
<guid>https://arxiv.org/abs/2512.15532</guid>
<content:encoded><![CDATA[
arXiv:2512.15532v1 Announce Type: cross 
Abstract: In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Smoothing is N-simplicial Attention?</title>
<link>https://arxiv.org/abs/2512.15600</link>
<guid>https://arxiv.org/abs/2512.15600</guid>
<content:encoded><![CDATA[
arXiv:2512.15600v1 Announce Type: cross 
Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Metrics for Safety with LLM-as-Judges</title>
<link>https://arxiv.org/abs/2512.15617</link>
<guid>https://arxiv.org/abs/2512.15617</guid>
<content:encoded><![CDATA[
arXiv:2512.15617v1 Announce Type: cross 
Abstract: LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</title>
<link>https://arxiv.org/abs/2512.15634</link>
<guid>https://arxiv.org/abs/2512.15634</guid>
<content:encoded><![CDATA[
arXiv:2512.15634v1 Announce Type: cross 
Abstract: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&amp;A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</title>
<link>https://arxiv.org/abs/2512.15635</link>
<guid>https://arxiv.org/abs/2512.15635</guid>
<content:encoded><![CDATA[
arXiv:2512.15635v1 Announce Type: cross 
Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
<link>https://arxiv.org/abs/2512.15649</link>
<guid>https://arxiv.org/abs/2512.15649</guid>
<content:encoded><![CDATA[
arXiv:2512.15649v1 Announce Type: cross 
Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</title>
<link>https://arxiv.org/abs/2512.15658</link>
<guid>https://arxiv.org/abs/2512.15658</guid>
<content:encoded><![CDATA[
arXiv:2512.15658v1 Announce Type: cross 
Abstract: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title>
<link>https://arxiv.org/abs/2512.15674</link>
<guid>https://arxiv.org/abs/2512.15674</guid>
<content:encoded><![CDATA[
arXiv:2512.15674v1 Announce Type: cross 
Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15687</link>
<guid>https://arxiv.org/abs/2512.15687</guid>
<content:encoded><![CDATA[
arXiv:2512.15687v1 Announce Type: cross 
Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BashArena: A Control Setting for Highly Privileged AI Agents</title>
<link>https://arxiv.org/abs/2512.15688</link>
<guid>https://arxiv.org/abs/2512.15688</guid>
<content:encoded><![CDATA[
arXiv:2512.15688v1 Announce Type: cross 
Abstract: Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[
arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatia: Video Generation with Updatable Spatial Memory</title>
<link>https://arxiv.org/abs/2512.15716</link>
<guid>https://arxiv.org/abs/2512.15716</guid>
<content:encoded><![CDATA[
arXiv:2512.15716v1 Announce Type: cross 
Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles</title>
<link>https://arxiv.org/abs/2408.15538</link>
<guid>https://arxiv.org/abs/2408.15538</guid>
<content:encoded><![CDATA[
arXiv:2408.15538v3 Announce Type: replace 
Abstract: While modern Autonomous Vehicle (AV) systems can develop reliable driving policies under regular traffic conditions, they frequently struggle with safety-critical traffic scenarios. This difficulty primarily arises from the rarity of such scenarios in driving datasets and the complexities associated with predictive modeling of multiple vehicles. Effectively simulating safety-critical traffic situations is therefore a crucial challenge. In this paper, we introduce TrafficGamer, which facilitates game-theoretic traffic simulation by viewing common road driving as a multi-agent game. When we evaluate the empirical performance across various real-world datasets, TrafficGamer ensures both the fidelity, exploitability, and diversity of the simulated scenarios, guaranteeing that they not only statically align with real-world traffic distribution but also efficiently capture equilibria for representing safety-critical scenarios involving multiple agents compared with other methods. Additionally, the results demonstrate that TrafficGamer provides highly flexible simulations across various contexts. Specifically, we demonstrate that the generated scenarios can dynamically adapt to equilibria of varying tightness by configuring risk-sensitive constraints during optimization. We have provided a demo webpage at: https://anonymous.4open.science/api/repo/trafficgamer-demo-1EE0/file/index.html.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model-Based Intelligent Antenna Design System</title>
<link>https://arxiv.org/abs/2504.18271</link>
<guid>https://arxiv.org/abs/2504.18271</guid>
<content:encoded><![CDATA[
arXiv:2504.18271v2 Announce Type: replace 
Abstract: Antenna simulation typically involves modeling and optimization, which are time-consuming and labor-intensive, slowing down antenna analysis and design. This paper presents a prototype of a large language model (LLM)-based antenna design system (LADS) to assist in antenna simulation. LADS generates antenna models with textual descriptions and images extracted from academic papers, patents, and technical reports (either one or multiple), and it interacts with engineers to iteratively refine the designs. After that, LADS configures and runs an optimizer to meet the design specifications. The effectiveness of LADS is demonstrated by a monopole slotted antenna generated from images and descriptions from the literature. To improve gain stability across the 3.1-10.6 GHz ultra-wide band, LADS modifies the cross-slot into an H-slot and changes substrate material, followed by parameter optimization. As a result, the gain variation is reduced while maintaining the same gain level. The LLM-enabled antenna modeling (LEAM) is available at: https://github.com/TaoWu974/LEAM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.20226</link>
<guid>https://arxiv.org/abs/2507.20226</guid>
<content:encoded><![CDATA[
arXiv:2507.20226v2 Announce Type: replace 
Abstract: Homomorphism is a key mapping technique between graphs that preserves their structure. Given a graph and a pattern, the subgraph homomorphism problem involves finding a mapping from the pattern to the graph, ensuring that adjacent vertices in the pattern are mapped to adjacent vertices in the graph. Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism allows multiple vertices in the pattern to map to the same vertex in the graph, making it more complex. We propose HFrame, the first graph neural network-based framework for subgraph homomorphism, which integrates traditional algorithms with machine learning techniques. We demonstrate that HFrame outperforms standard graph neural networks by being able to distinguish more graph pairs where the pattern is not homomorphic to the graph. Additionally, we provide a generalization error bound for HFrame. Through experiments on both real-world and synthetic graphs, we show that HFrame is up to 101.91 times faster than exact matching algorithms and achieves an average accuracy of 0.962.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, &amp; Emerging Implications for Agentic E-Commerce</title>
<link>https://arxiv.org/abs/2508.02630</link>
<guid>https://arxiv.org/abs/2508.02630</guid>
<content:encoded><![CDATA[
arXiv:2508.02630v3 Announce Type: replace 
Abstract: Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, AI agents can parse webpages or leverage APIs to view, evaluate and choose products. We investigate the behavior of AI agents using ACES, a provider-agnostic framework for auditing agent decision-making. We reveal that agents can exhibit choice homogeneity, often concentrating demand on a few ``modal'' products while ignoring others entirely. Yet, these preferences are unstable: model updates can drastically reshuffle market shares. Furthermore, randomized trials show that while agents have improved over time on simple tasks with a clearly identified best choice, they exhibit strong position biases -- varying across providers and model versions, and persisting even in text-only "headless" interfaces -- undermining any universal notion of a ``top'' rank. Agents also consistently penalize sponsored tags while rewarding platform endorsements, and sensitivities to price, ratings, and reviews vary sharply across models. Finally, we demonstrate that sellers can respond: a seller-side agent making simple, query-conditional description tweaks can drive significant gains in market share. These findings reveal that agentic markets are volatile and fundamentally different from human-centric commerce, highlighting the need for continuous auditing and raising questions for platform design, seller strategy and regulation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
arXiv:2508.15126v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content.
  Code: https://github.com/aixiv-org
  aiXiv: https://aixiv.science
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Need for Verification in AI-Driven Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.01398</link>
<guid>https://arxiv.org/abs/2509.01398</guid>
<content:encoded><![CDATA[
arXiv:2509.01398v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory</title>
<link>https://arxiv.org/abs/2511.18723</link>
<guid>https://arxiv.org/abs/2511.18723</guid>
<content:encoded><![CDATA[
arXiv:2511.18723v3 Announce Type: replace 
Abstract: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&amp;B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&amp;B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation</title>
<link>https://arxiv.org/abs/2511.19895</link>
<guid>https://arxiv.org/abs/2511.19895</guid>
<content:encoded><![CDATA[
arXiv:2511.19895v2 Announce Type: replace 
Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title>
<link>https://arxiv.org/abs/2512.02080</link>
<guid>https://arxiv.org/abs/2512.02080</guid>
<content:encoded><![CDATA[
arXiv:2512.02080v2 Announce Type: replace 
Abstract: The integration of Formal Verification tools with Large Language Models (LLMs) offers a path to scale software verification beyond manual workflows. However, current methods remain unreliable: without a solid theoretical footing, the refinement process acts as a black box that may oscillate, loop, or diverge. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination in multi-stage verification pipelines. We model the interaction not as a generic loop, but as a sequential absorbing Markov Chain comprising four essential engineering stages: \texttt{CodeGen}, \texttt{Compilation}, \texttt{InvariantSynth}, and \texttt{SMTSolving}. We prove that for any non-zero stage success probability ($\delta > 0$), the system reaches the \texttt{Verified} state almost surely. Furthermore, because of the sequential nature of the pipeline, we derive a precise latency bound of $\mathbb{E}[n] \leq 4/\delta$. We stress-tested this prediction in an extensive empirical campaign comprising over 90,000 trials. The results match the theory with striking consistency: every run reached verification, and the empirical convergence factor clustered tightly around $C_f\approx 1.0$, confirming that the $4/\delta$ bound accurately mirrors system behavior rather than serving as a loose buffer. Based on this data, we identify three distinct operating zones -- marginal, practical, and high-performance -- and propose a dynamic calibration strategy to handle parameter drift in real-world environments. Together, these contributions replace heuristic guesswork with a rigorous architectural foundation, enabling predictable resource planning and performance budgeting for safety-critical software.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Science of Scaling Agent Systems</title>
<link>https://arxiv.org/abs/2512.08296</link>
<guid>https://arxiv.org/abs/2512.08296</guid>
<content:encoded><![CDATA[
arXiv:2512.08296v2 Announce Type: replace 
Abstract: Agents, language model-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored. We address this by deriving quantitative scaling principles for agent systems. We first formalize a definition for agentic evaluation and characterize scaling laws as the interplay between agent quantity, coordination structure, model capability, and task properties. We evaluate this across four benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. With five canonical agent architectures (Single-Agent and four Multi-Agent Systems: Independent, Centralized, Decentralized, Hybrid), instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations. We derive a predictive model using coordination metrics, that achieves cross-validated R^2=0.524, enabling prediction on unseen task domains. We identify three effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.8% on parallelizable tasks, while decentralized coordination excels on web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, every multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations. Out-of-sample validation on GPT-5.2, achieves MAE=0.071 and confirms four of five scaling principles generalize to unseen frontier models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchOGD: Memory-Efficient Continual Learning</title>
<link>https://arxiv.org/abs/2305.16424</link>
<guid>https://arxiv.org/abs/2305.16424</guid>
<content:encoded><![CDATA[
arXiv:2305.16424v3 Announce Type: replace-cross 
Abstract: When machine learning models are trained continually on a sequence of tasks, they are often liable to forget what they learned on previous tasks--a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper develops a memory-efficient solution to catastrophic forgetting using the idea of matrix sketching, in the context of a simple continual learning algorithm known as orthogonal gradient descent (OGD). OGD finds weight updates that aim to preserve performance on prior datapoints, using gradients of the model on those datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over long time horizons. To address this problem, we propose SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fixed, user-determined size. In contrast to existing memory-efficient variants of OGD, SketchOGD runs online without the need for advance knowledge of the total number of tasks, is simple to implement, and is more amenable to analysis. We provide theoretical guarantees on the approximation error of the relevant sketches under a novel metric suited to the downstream task of OGD. Experimentally, we find that SketchOGD tends to outperform current state-of-the-art variants of OGD given a fixed memory budget.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enigma: Application-Layer Privacy for Quantum Optimization on Untrusted Computers</title>
<link>https://arxiv.org/abs/2311.13546</link>
<guid>https://arxiv.org/abs/2311.13546</guid>
<content:encoded><![CDATA[
arXiv:2311.13546v2 Announce Type: replace-cross 
Abstract: The Early Fault-Tolerant (EFT) era is emerging, where modest Quantum Error Correction (QEC) can enable quantum utility before full-scale fault tolerance. Quantum optimization is a leading candidate for early applications, but protecting these workloads is critical since they will run on expensive cloud services where providers could learn sensitive problem details. Experience with classical computing systems has shown that treating security as an afterthought can lead to significant vulnerabilities. Thus, we must address the security implications of quantum computing before widespread adoption. However, current Secure Quantum Computing (SQC) approaches, although theoretically promising, are impractical in the EFT era: blind quantum computing requires large-scale quantum networks, and quantum homomorphic encryption depends on full QEC.
  We propose application-specific SQC, a principle that applies obfuscation at the application layer to enable practical deployment while remaining agnostic to algorithms, computing models, and hardware architectures. We present Enigma, the first realization of this principle for quantum optimization. Enigma integrates three complementary obfuscations: ValueGuard scrambles coefficients, StructureCamouflage inserts decoys, and TopologyTrimmer prunes variables. These techniques guarantee recovery of original solutions, and their stochastic nature resists repository-matching attacks. Evaluated against seven state-of-the-art AI models across five representative graph families, even combined adversaries, under a conservatively strong attacker model, identify the correct problem within their top five guesses in only 4.4% of cases. The protections come at the cost of problem size and T-gate counts increasing by averages of 1.07x and 1.13x, respectively, with both obfuscation and decoding completing within seconds for large-scale problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pretraining to Privacy: Federated Ultrasound Foundation Model with Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2411.16380</link>
<guid>https://arxiv.org/abs/2411.16380</guid>
<content:encoded><![CDATA[
arXiv:2411.16380v2 Announce Type: replace-cross 
Abstract: Ultrasound imaging is widely used in clinical diagnosis due to its non-invasive nature and real-time capabilities. However, traditional ultrasound diagnostics relies heavily on physician expertise and is often hampered by suboptimal image quality, leading to potential diagnostic errors. While artificial intelligence (AI) offers a promising solution to enhance clinical diagnosis by detecting abnormalities across various imaging modalities, existing AI methods for ultrasound face two major challenges. First, they typically require vast amounts of labeled medical data, raising serious concerns regarding patient privacy. Second, most models are designed for specific tasks, which restricts their broader clinical utility. To overcome these challenges, we present UltraFedFM, an innovative privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively pre-trained using federated learning across 16 distributed medical institutions in 9 countries, leveraging a dataset of over 1 million ultrasound images covering 19 organs and 10 ultrasound modalities. This extensive and diverse data, combined with a secure training framework, enables UltraFedFM to exhibit strong generalization and diagnostic capabilities. It achieves an average area under the receiver operating characteristic curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level ultrasonographers (4-8 years of experience) and matches the performance of expert-level sonographers (10+ years of experience) in the joint diagnosis of 8 common systemic diseases.c These findings indicate that UltraFedFM can significantly enhance clinical diagnostics while safeguarding patient privacy, marking a significant advancement in AI-driven ultrasound imaging for future clinical applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title>
<link>https://arxiv.org/abs/2411.17645</link>
<guid>https://arxiv.org/abs/2411.17645</guid>
<content:encoded><![CDATA[
arXiv:2411.17645v4 Announce Type: replace-cross 
Abstract: The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs). We implemented a data pre-processing and curation pipeline that transforms the raw EHR data into a structured format suitable for developing predictive models focused on data fairness, accountability and transparency. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Pairwise XGBoost models are trained using this framework to differentiate UTI risk categories with explainable AI techniques applied to identify key predictors and support interpretability. Our findings reveal differences in clinical and demographic predictors across risk groups. While this study highlights the potential of AI-driven insights to support UTI clinical decision-making, further investigation of patient sub-strata and extensive validation are needed to ensure robustness and applicability in clinical practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveGNN: Integrating Graph Neural Networks and Transformers for Decay-Aware Classification of Irregular Clinical Time-Series</title>
<link>https://arxiv.org/abs/2412.10621</link>
<guid>https://arxiv.org/abs/2412.10621</guid>
<content:encoded><![CDATA[
arXiv:2412.10621v2 Announce Type: replace-cross 
Abstract: Clinical time series are often irregularly sampled, with varying sensor frequencies, missing observations, and misaligned timestamps. Prior approaches typically address these irregularities by interpolating data into regular sequences, thereby introducing bias, or by generating inconsistent and uninterpretable relationships across sensor measurements, complicating the accurate learning of both intra-series and inter-series dependencies. We introduce WaveGNN, a model that operates directly on irregular multivariate time series without interpolation or conversion to a regular representation. WaveGNN combines a decay-aware Transformer to capture intra-series dynamics with a sample-specific graph neural network that models both short-term and long-term inter-sensor relationships. Therefore, it generates a single, sparse, and interpretable graph per sample. Across multiple benchmark datasets (P12, P19, MIMIC-III, and PAM), WaveGNN delivers consistently strong performance, whereas other state-of-the-art baselines tend to perform well on some datasets or tasks but poorly on others. While WaveGNN does not necessarily surpass every method in every case, its consistency and robustness across diverse settings set it apart. Moreover, the learned graphs align well with known physiological structures, enhancing interpretability and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[
arXiv:2501.13439v2 Announce Type: replace-cross 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rigid-Deformation Decomposition AI Framework for 3D Spatio-Temporal Prediction of Vehicle Collision Dynamics</title>
<link>https://arxiv.org/abs/2503.19712</link>
<guid>https://arxiv.org/abs/2503.19712</guid>
<content:encoded><![CDATA[
arXiv:2503.19712v2 Announce Type: replace-cross 
Abstract: This study presents a rigid-deformation decomposition framework for vehicle collision dynamics that mitigates the spectral bias of implicit neural representations, that is, coordinate-based neural networks that directly map spatio-temporal coordinates to physical fields. We introduce a hierarchical architecture that decouples global rigid-body motion from local deformation using two scale-specific networks, denoted as RigidNet and DeformationNet. To enforce kinematic separation between the two components, we adopt a frozen-anchor training strategy combined with a quaternion-incremental scheme. This strategy alleviates the kinematic instability observed in joint training and yields a 29.8% reduction in rigid-body motion error compared with conventional direct prediction schemes. The stable rigid-body anchor improves the resolution of high-frequency structural buckling, which leads to a 17.2% reduction in the total interpolation error. Loss landscape analysis indicates that the decomposition smooths the optimization surface, which enhances robustness to distribution shifts in angular extrapolation and yields a 46.6% reduction in error. To assess physical validity beyond numerical accuracy, we benchmark the decomposed components against an oracle model that represents an upper bound on performance. The proposed framework recovers 92% of the directional correlation between rigid and deformation components and 96% of the spatial deformation localization accuracy relative to the oracle, while tracking the temporal energy dynamics with an 8 ms delay. These results demonstrate that rigid-deformation decomposition enables accurate and physically interpretable predictions for nonlinear collision dynamics.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Tools: Generative AI as Epistemic Infrastructure in Education</title>
<link>https://arxiv.org/abs/2504.06928</link>
<guid>https://arxiv.org/abs/2504.06928</guid>
<content:encoded><![CDATA[
arXiv:2504.06928v2 Announce Type: replace-cross 
Abstract: AI systems are increasingly embedded in practices where humans have traditionally exercised epistemic agency, the capacity to actively engage in knowledge formation and validation. This paper argues that understanding AI's impact on epistemic agency requires analyzing these systems as epistemic infrastructures rather than as neutral tools. Drawing on theories of technological mediation and distributed cognition, I advance a framework that foregrounds how AI systems reconfigure the conditions under which epistemic agency can be exercised. The framework specifies three analytical conditions: affordances for skilled epistemic actions, support for epistemic sensitivity, and implications for habit formation. I apply this framework to AI systems deployed in education, a domain where epistemic agency is both professionally essential and ethically significant. Analysis of AI lesson planning and feedback tools reveals patterns of epistemic substitution: while useful for efficiently handling teaching tasks, these systems perform cognitive operations without sustaining skilled epistemic actions, epistemic sensitivity, or virtuous habit formation, potentially preventing the cultivation of professional judgment that relies on these practices. The findings contribute to philosophical debates about AI and human agency by specifying mechanisms through which infrastructural embedding shapes epistemic possibilities, and offer design principles for AI systems that sustain rather than supplant human epistemic agency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization</title>
<link>https://arxiv.org/abs/2504.13460</link>
<guid>https://arxiv.org/abs/2504.13460</guid>
<content:encoded><![CDATA[
arXiv:2504.13460v4 Announce Type: replace-cross 
Abstract: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observation Compression in Rate-Limited Closed-Loop Distributed ISAC Systems: From Signal Reconstruction to Control</title>
<link>https://arxiv.org/abs/2505.01780</link>
<guid>https://arxiv.org/abs/2505.01780</guid>
<content:encoded><![CDATA[
arXiv:2505.01780v2 Announce Type: replace-cross 
Abstract: In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safer Prompts: Reducing Risks from Memorization in Visual Generative AI</title>
<link>https://arxiv.org/abs/2505.03338</link>
<guid>https://arxiv.org/abs/2505.03338</guid>
<content:encoded><![CDATA[
arXiv:2505.03338v2 Announce Type: replace-cross 
Abstract: Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from user inputs like text prompts. However, because these models have billions of parameters, they risk memorizing certain parts of the training data and reproducing the memorized content. Memorization often raises concerns about safety of such models -- usually involving intellectual property (IP) infringement risk -- and deters their large scale adoption. In this paper, we evaluate the effectiveness of prompt engineering techniques in reducing memorization risk in image generation. Our findings demonstrate the effectiveness of prompt engineering in reducing the similarity between generated images and the training data of diffusion models, while maintaining relevance and aestheticity of the generated output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[
arXiv:2505.16896v2 Announce Type: replace-cross 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but often lack the structural knowledge essential for some biological applications. To address this, we introduce a method to enrich pLMs with structural knowledge by leveraging pre-trained protein graph neural networks (pGNNs). First, a latent-level contrastive learning task aligns residue representations from pLMs with those from pGNNs across multiple proteins, injecting inter-protein structural information. Additionally, a physical-level task integrates intra-protein information by training pLMs to predict structure tokens. Together, the proposed dual-task framework effectively incorporates both inter- and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module that uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method as a simple, lightweight post-training step to the state-of-the-art ESM2 and AMPLIFY yields notable performance gains. These improvements are consistent across a wide range of tasks, including substantial gains in deep mutational scanning (DMS) fitness prediction and a 59% increase in P@L for ESM2 650M contact prediction on CASP16. Furthermore, we demonstrate that these performance gains are robust, scaling with model sizes from 8M to 650M and extending to different downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title>
<link>https://arxiv.org/abs/2505.18148</link>
<guid>https://arxiv.org/abs/2505.18148</guid>
<content:encoded><![CDATA[
arXiv:2505.18148v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) face significant challenges with needle-in-ahaystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size, the length of the answer-containing document, has received little attention. We present the first systematic study of gold context size in long-context question answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning models), and more than 150K controlled runs. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This effect persists under rigorous confounder analysis: even after controlling for gold context position, answer token repetition, gold-to-distractor ratio, distractor volume, and domain specificity, gold context size remains a decisive, independent predictor of success. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v2 Announce Type: replace-cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[
arXiv:2505.23415v2 Announce Type: replace-cross 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</title>
<link>https://arxiv.org/abs/2506.02976</link>
<guid>https://arxiv.org/abs/2506.02976</guid>
<content:encoded><![CDATA[
arXiv:2506.02976v3 Announce Type: replace-cross 
Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title>
<link>https://arxiv.org/abs/2506.09677</link>
<guid>https://arxiv.org/abs/2506.09677</guid>
<content:encoded><![CDATA[
arXiv:2506.09677v2 Announce Type: replace-cross 
Abstract: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness</title>
<link>https://arxiv.org/abs/2507.02314</link>
<guid>https://arxiv.org/abs/2507.02314</guid>
<content:encoded><![CDATA[
arXiv:2507.02314v3 Announce Type: replace-cross 
Abstract: Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Action Smoothness for a Cascaded Online Learning Flight Control System</title>
<link>https://arxiv.org/abs/2507.04346</link>
<guid>https://arxiv.org/abs/2507.04346</guid>
<content:encoded><![CDATA[
arXiv:2507.04346v3 Announce Type: replace-cross 
Abstract: This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs and Fuzzing in Tandem: A New Approach to Automatically Generating Weakest Preconditions</title>
<link>https://arxiv.org/abs/2507.05272</link>
<guid>https://arxiv.org/abs/2507.05272</guid>
<content:encoded><![CDATA[
arXiv:2507.05272v2 Announce Type: replace-cross 
Abstract: The weakest precondition (WP) of a program describes the largest set of initial states from which all terminating executions of the program satisfy a given postcondition. The generation of WPs is an important task with practical applications in areas ranging from verification to run-time error checking. This paper proposes the combination of Large Language Models (LLMs) and fuzz testing for generating WPs. In pursuit of this goal, we introduce \emph{Fuzzing Guidance} (FG); FG acts as a means of directing LLMs towards correct WPs using program execution feedback. FG utilises fuzz testing for approximately checking the validity and weakness of candidate WPs, this information is then fed back to the LLM as a means of context refinement. We demonstrate the effectiveness of our approach on a comprehensive benchmark set of deterministic array programs in Java. Our experiments indicate that LLMs are capable of producing viable candidate WPs, and that this ability can be practically enhanced through FG.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI</title>
<link>https://arxiv.org/abs/2507.06092</link>
<guid>https://arxiv.org/abs/2507.06092</guid>
<content:encoded><![CDATA[
arXiv:2507.06092v2 Announce Type: replace-cross 
Abstract: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion</title>
<link>https://arxiv.org/abs/2507.06849</link>
<guid>https://arxiv.org/abs/2507.06849</guid>
<content:encoded><![CDATA[
arXiv:2507.06849v2 Announce Type: replace-cross 
Abstract: Neural network (NN)-based Digital Predistortion (DPD) has demonstrated superior performance in improving signal quality in wideband radio frequency (RF) power amplifiers (PAs) employing complex modulation. However, NN DPDs usually rely on a large number of parameters for effective linearization and can significantly contribute to the energy consumption of the digital back-end in RF systems. This paper presents OpenDPDv2, an open-source, end-to-end framework that unifies PA modeling, NN-DPD learning, and deployment-oriented model optimization to reduce inference energy while preserving linearization performance. OpenDPDv2 introduces TRes-DeltaGRU, a delta-RNN DPD architecture with a lightweight temporal residual path that improves robustness under aggressive temporal sparsity, and it supports joint optimization of temporal sparsity and fixed-point quantization. On a 3.5 GHz GaN Doherty PA driven by a TM3.1a 200 MHz 256-QAM OFDM signal, the FP32 TRes-DeltaGRU model achieves ACPR of -59.9 dBc and EVM of -42.1 dB. By combining quantization with dynamic temporal sparsity, the model reduces inference energy by 4.5x while maintaining -51.8 dBc ACPR and -35.2 dB EVM at 56% temporal sparsity. Code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</title>
<link>https://arxiv.org/abs/2507.07417</link>
<guid>https://arxiv.org/abs/2507.07417</guid>
<content:encoded><![CDATA[
arXiv:2507.07417v2 Announce Type: replace-cross 
Abstract: A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning to separate instructions and data, so that the LLM does not follow instructions that might be present with data. We evaluate the robustness of this approach in the whitebox setting by constructing strong optimization-based attacks, and show that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for textual LLMs and apply it to three recent whitebox defenses SecAlign (CCS 2025), SecAlign++, and StruQ (USENIX Security 2025), showing attacks with success rates of up to \textbf{85-95\%} on unseen prompts with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
arXiv:2507.10532v3 Announce Type: replace-cross 
Abstract: Reasoning in large language models has long been a central research focus, and recent studies employing reinforcement learning (RL) have introduced diverse methods that yield substantial performance gains with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance performance. However, these breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama, which warrants a more in-depth investigation. In this work, our empirical analysis reveals that pre-training on massive web-scale corpora leaves Qwen2.5 susceptible to data contamination in widely used benchmarks. Consequently, conclusions derived from contaminated benchmarks on Qwen2.5 series may be unreliable. To obtain trustworthy evaluation results, we introduce a generator that creates fully clean arithmetic problems of arbitrary length and difficulty, dubbed RandomCalculation. Using this leakage-free dataset, we show that only accurate reward signals yield steady improvements that surpass the base model's performance boundary in mathematical reasoning, whereas random or incorrect rewards do not. Moreover, we conduct more fine-grained analyses to elucidate the factors underlying the different performance observed on the MATH-500 and RandomCalculation benchmarks. Consequently, we recommend that future studies evaluate models on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2507.17061</link>
<guid>https://arxiv.org/abs/2507.17061</guid>
<content:encoded><![CDATA[
arXiv:2507.17061v3 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation</title>
<link>https://arxiv.org/abs/2507.18625</link>
<guid>https://arxiv.org/abs/2507.18625</guid>
<content:encoded><![CDATA[
arXiv:2507.18625v2 Announce Type: replace-cross 
Abstract: Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemDFM-R: A Chemical Reasoning LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
arXiv:2507.21990v3 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoning LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized chemical knowledge, ChemFG, annotating the presence of functional groups in molecules and the changes of functional groups during chemical reactions, to enhance the model's understanding of the fundamental principles and internal logic of chemistry. Then, we propose a mixed-source distillation method that integrates expertise in atomized knowledge with general reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves cutting-edge performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the model's reliability, transparency, and practicality in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
arXiv:2508.07981v4 Announce Type: replace-cross 
Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
<link>https://arxiv.org/abs/2508.09320</link>
<guid>https://arxiv.org/abs/2508.09320</guid>
<content:encoded><![CDATA[
arXiv:2508.09320v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile exact verifier for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on real-world fraud datasets (Amazon and Yelp) and biochemical datasets (MUTAG and ENZYMES) demonstrates its usability and effectiveness, as well as superior performance for node classification and competitiveness on graph classification compared to existing exact verification tools on sum-aggregated GNNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[
arXiv:2508.10021v4 Announce Type: replace-cross 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Tensor Decompositions for the Theory of Neural Networks</title>
<link>https://arxiv.org/abs/2508.18408</link>
<guid>https://arxiv.org/abs/2508.18408</guid>
<content:encoded><![CDATA[
arXiv:2508.18408v2 Announce Type: replace-cross 
Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge of interest in providing a mathematical basis to deep learning theory. Low-rank tensor decompositions are specially befitting for this task due to their close connection to NNs and their rich theoretical results. Different tensor decompositions have strong uniqueness guarantees, which allow for a direct interpretation of their factors, and polynomial time algorithms have been proposed to compute them. Through the connections between tensors and NNs, such results supported many important advances in the theory of NNs. In this review, we show how low-rank tensor methods--which have been a core tool in the signal processing and machine learning communities--play a fundamental role in theoretically explaining different aspects of the performance of deep NNs, including their expressivity, algorithmic learnability and computational hardness, generalization, and identifiability. Our goal is to give an accessible overview of existing approaches (developed by different communities, ranging from computer science to mathematics) in a coherent and unified way, and to open a broader perspective on the use of low-rank tensor decompositions for the theory of deep NNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title>
<link>https://arxiv.org/abs/2509.07677</link>
<guid>https://arxiv.org/abs/2509.07677</guid>
<content:encoded><![CDATA[
arXiv:2509.07677v3 Announce Type: replace-cross 
Abstract: Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title>
<link>https://arxiv.org/abs/2509.21979</link>
<guid>https://arxiv.org/abs/2509.21979</guid>
<content:encoded><![CDATA[
arXiv:2509.21979v3 Announce Type: replace-cross 
Abstract: Visual language models (VLMs) have the potential to transform medical workflows. However, the deployment is limited by sycophancy. Despite this serious threat to patient safety, a systematic benchmark remains lacking. This paper addresses this gap by introducing a Medical benchmark that applies multiple templates to VLMs in a hierarchical medical visual question answering task. We find that current VLMs are highly susceptible to visual cues, with failure rates showing a correlation to model size or overall accuracy. we discover that perceived authority and user mimicry are powerful triggers, suggesting a bias mechanism independent of visual data. To overcome this, we propose a Visual Information Purification for Evidence based Responses (VIPER) strategy that proactively filters out non-evidence-based social cues, thereby reinforcing evidence based reasoning. VIPER reduces sycophancy while maintaining interpretability and consistently outperforms baseline methods, laying the necessary foundation for the robust and secure integration of VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders Make Audio Foundation Models more Explainable</title>
<link>https://arxiv.org/abs/2509.24793</link>
<guid>https://arxiv.org/abs/2509.24793</guid>
<content:encoded><![CDATA[
arXiv:2509.24793v2 Announce Type: replace-cross 
Abstract: Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching</title>
<link>https://arxiv.org/abs/2509.24832</link>
<guid>https://arxiv.org/abs/2509.24832</guid>
<content:encoded><![CDATA[
arXiv:2509.24832v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.25300</link>
<guid>https://arxiv.org/abs/2509.25300</guid>
<content:encoded><![CDATA[
arXiv:2509.25300v2 Announce Type: replace-cross 
Abstract: While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1.Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2.The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3.Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4.In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Foundation Models for Early Disease Detection</title>
<link>https://arxiv.org/abs/2510.01899</link>
<guid>https://arxiv.org/abs/2510.01899</guid>
<content:encoded><![CDATA[
arXiv:2510.01899v2 Announce Type: replace-cross 
Abstract: Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04182</link>
<guid>https://arxiv.org/abs/2510.04182</guid>
<content:encoded><![CDATA[
arXiv:2510.04182v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent "thought" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI</title>
<link>https://arxiv.org/abs/2510.04755</link>
<guid>https://arxiv.org/abs/2510.04755</guid>
<content:encoded><![CDATA[
arXiv:2510.04755v4 Announce Type: replace-cross 
Abstract: Digital technologies are transforming democratic life in conflicting ways. This article bridges two perspectives to unpack these tensions. First, we present an original survey of software developers in Silicon Valley, interrogating how coder worldviews, ethics, and workplace cultures shape the democratic potential and social impact of the technologies they build. Results indicate that while most developers recognize the power of their products to influence civil liberties and political discourse, they often face ethical dilemmas and top-down pressures that can lead to design choices undermining democratic ideals. Second, we critically investigate these findings in the context of an emerging new digital divide, not of internet access but of information quality. We interrogate the survey findings in the context of the Slop Economy, in which billions of users unable to pay for high-quality content experience an internet dominated by low-quality, AI-generated ad-driven content. We find a reinforcing cycle between tech creator beliefs and the digital ecosystems they spawn. We discuss implications for democratic governance, arguing for more ethically informed design and policy interventions to help bridge the digital divide to ensure that technological innovation supports rather than subverts democratic values in the next chapter of the digital age.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v2 Announce Type: replace-cross 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2510.07318</link>
<guid>https://arxiv.org/abs/2510.07318</guid>
<content:encoded><![CDATA[
arXiv:2510.07318v2 Announce Type: replace-cross 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[
arXiv:2510.12691v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[
arXiv:2511.01140v2 Announce Type: replace-cross 
Abstract: Medical imaging often operates under limited labeled data, especially in rare disease and low resource clinical environments. Existing multimodal and meta learning approaches improve performance in these settings but lack a theoretical explanation of why or when they succeed. This paper presents a unified theoretical framework for few shot multimodal medical imaging that jointly characterizes sample complexity, uncertainty quantification, and interpretability. Using PAC learning, VC theory, and PAC Bayesian analysis, we derive bounds that describe the minimum number of labeled samples required for reliable performance and show how complementary modalities reduce effective capacity through an information gain term. We further introduce a formal metric for explanation stability, proving that explanation variance decreases at an inverse n rate. A sequential Bayesian interpretation of Chain of Thought reasoning is also developed to show stepwise posterior contraction. To illustrate these ideas, we implement a controlled multimodal dataset and evaluate an additive CNN MLP fusion model under few shot regimes, confirming predicted multimodal gains, modality interference at larger sample sizes, and shrinking predictive uncertainty. Together, the framework provides a principled foundation for designing data efficient, uncertainty aware, and interpretable diagnostic models in low resource settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</title>
<link>https://arxiv.org/abs/2511.10984</link>
<guid>https://arxiv.org/abs/2511.10984</guid>
<content:encoded><![CDATA[
arXiv:2511.10984v2 Announce Type: replace-cross 
Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)</title>
<link>https://arxiv.org/abs/2511.12061</link>
<guid>https://arxiv.org/abs/2511.12061</guid>
<content:encoded><![CDATA[
arXiv:2511.12061v2 Announce Type: replace-cross 
Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v3 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</title>
<link>https://arxiv.org/abs/2512.02826</link>
<guid>https://arxiv.org/abs/2512.02826</guid>
<content:encoded><![CDATA[
arXiv:2512.02826v2 Announce Type: replace-cross 
Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title>
<link>https://arxiv.org/abs/2512.04124</link>
<guid>https://arxiv.org/abs/2512.04124</guid>
<content:encoded><![CDATA[
arXiv:2512.04124v3 Announce Type: replace-cross 
Abstract: Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges</title>
<link>https://arxiv.org/abs/2512.04770</link>
<guid>https://arxiv.org/abs/2512.04770</guid>
<content:encoded><![CDATA[
arXiv:2512.04770v2 Announce Type: replace-cross 
Abstract: Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
arXiv:2512.05277v2 Announce Type: replace-cross 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
arXiv:2512.09172v2 Announce Type: replace-cross 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Percolation: Criticality of Form and Function</title>
<link>https://arxiv.org/abs/2512.09317</link>
<guid>https://arxiv.org/abs/2512.09317</guid>
<content:encoded><![CDATA[
arXiv:2512.09317v3 Announce Type: replace-cross 
Abstract: Understanding how network structure constrains and enables information processing is a central problem in the statistical mechanics of interacting systems. Here we study random networks across the structural percolation transition and analyze how connectivity governs realizable input-output transformations under cascade dynamics. Using Erdos-Renyi networks as a minimal ensemble, we examine structural, functional, and information-theoretic observables as functions of mean degree. We find that the emergence of the giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow, quantified by transfer entropy, extends beyond local neighborhoods. We term this coincidence of structural, functional, and informational transitions functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the percolation threshold. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality may provide a general organizing principle of information processing capacity in systems with local interactions and propagating influences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-image QA, vision language models, reinforcement learning, tool use, visual reflection<br /><br />Summary:<br /><br />1. The paper identifies a gap in current vision language model (VLM) agents that predominantly handle single-image inputs, limiting their capability in real-world scenarios requiring reasoning across multiple images.<br />2. To overcome this, the authors introduce IMAgent, an open-source vision agent trained end-to-end via reinforcement learning specifically designed for complex tasks involving multiple images.<br />3. They leverage a multi-agent system to generate a challenging, visually rich multi-image QA dataset named MIFG-QA, containing 10,000 samples, which undergoes manual verification to ensure quality.<br />4. Recognizing that deeper reasoning steps can cause VLMs to neglect visual inputs, the paper proposes two specialized tools for visual reflection and confirmation, enabling the model to dynamically refocus attention on image content during inference.<br />5. A novel action-trajectory two-level mask strategy is employed to ensure stable tool use behavior trained purely via RL without expensive supervised fine-tuning.<br />6. Experiments demonstrate that IMAgent not only maintains strong performance on single-image benchmarks but also achieves significant improvements on multi-image QA tasks.<br />7. The study provides insights valuable to the research community, with plans to release codes and datasets soon. <div>
arXiv:2512.08980v2 Announce Type: replace-cross 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records</title>
<link>https://arxiv.org/abs/2512.13700</link>
<guid>https://arxiv.org/abs/2512.13700</guid>
<content:encoded><![CDATA[
<div> Keywords: manual chart review, large language models, clinical notes, feature extraction, HIPAA-compliant infrastructure<br /><br />Summary:  
1. Manual review of electronic health records (EHR) is time-consuming and requires expert intervention to extract complex clinical information.  
2. The authors present a secure and modular automated framework that employs locally deployed large language models (LLMs) to extract structured features from clinical notes.  
3. The system runs on Health Insurance Portability and Accountability Act (HIPAA)-compliant infrastructure, ensuring data security and privacy, and is designed to be widely deployable via containerization.  
4. The framework integrates retrieval augmented generation (RAG) and structured response techniques from LLMs, enhancing accuracy and scalability across diverse clinical domains.  
5. Evaluation against an expert-annotated dataset showed high accuracy and revealed errors in manual annotations, demonstrating the framework’s potential to reduce manual effort, increase consistency in data extraction, and accelerate clinical research processes. <div>
arXiv:2512.13700v1 Announce Type: new 
Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference</title>
<link>https://arxiv.org/abs/2512.13701</link>
<guid>https://arxiv.org/abs/2512.13701</guid>
<content:encoded><![CDATA[
<div> Keywords: radio maps, blind construction, MIMO-OFDM, CSI-distance metric, Bayesian inference<br /><br />Summary: This paper addresses the challenge of constructing radio maps without relying on costly and impractical location-labeled data by proposing a blind radio map construction framework. The framework infers user trajectories solely from indoor MIMO-OFDM channel measurements, eliminating dependence on explicit location labels. The authors demonstrate that the channel state information (CSI) under non-line-of-sight (NLOS) conditions exhibits spatial continuity based on a quasi-specular environmental model. This insight enables the derivation of a CSI-distance metric proportional to the physical distance between points. For scenarios with rectilinear trajectories and Poisson-distributed access points (APs), the study proves that the Cramer-Rao Lower Bound (CRLB) on localization error approaches zero asymptotically, even when the system suffers from poor angular resolution. Leveraging these theoretical insights, the authors develop a spatially regularized Bayesian inference framework that jointly estimates channel features, distinguishes between line-of-sight (LOS) and NLOS conditions, and recovers user trajectories. Experimental validation conducted on a ray-tracing dataset indicates an average localization error of only 0.68 meters and a beam map reconstruction error of 3.3%, demonstrating the practicality and accuracy of the proposed blind radio mapping method in realistic indoor wireless environments. <div>
arXiv:2512.13701v1 Announce Type: new 
Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents</title>
<link>https://arxiv.org/abs/2512.13704</link>
<guid>https://arxiv.org/abs/2512.13704</guid>
<content:encoded><![CDATA[
<div> Keywords: label noise, Knowledge Graph, multi-agent LLM, data verification, industrial ML systems  

<br /><br />Summary:  
1. This paper addresses the critical issue of noisy labels in production machine learning systems, which can severely degrade performance and undermine user trust in high-stakes industrial contexts.  
2. The authors introduce Adjudicator, a novel system designed to automatically detect and correct label noise by framing the challenge as a neuro-symbolic task.  
3. The approach builds a dynamic Knowledge Graph (KG) to unify contextual information about data items, which informs a "Council of Agents" — a multi-agent large language model architecture where specialized agents debate and vote on label validity.  
4. Evaluation on a balanced 1,000-item subset of the AlleNoise benchmark shows that Adjudicator substantially outperforms both a single large language model baseline (0.48 F1-score) and a similar multi-agent system without KG input (0.59 F1-score), achieving an impressive 0.99 F1-score.  
5. This superior performance is attributed to an innovative override logic leveraging the KG to perfectly identify complex structural errors, achieving perfect recall and high precision, which baselines cannot detect. Consequently, Adjudicator demonstrates a robust, explainable, and effective automated data verification tool suitable for generating high-quality “golden datasets” within strictly governed industrial environments. <div>
arXiv:2512.13704v1 Announce Type: new 
Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms</title>
<link>https://arxiv.org/abs/2512.13713</link>
<guid>https://arxiv.org/abs/2512.13713</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, distributed systems, symmetry breaking, LoopBench, meta-cognitive thinking<br /><br />Summary:<br /><br />This paper introduces LoopBench, a novel benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) in distributed symmetry breaking and meta-cognitive thinking tasks. The benchmark focuses on the challenging problem of coloring odd cycle graphs ($C_3, C_5, C_{11}$) using a limited color set, a task which deterministic non-communicating agents typically fail due to infinite loops. To address this, the authors incorporate a strategy passing mechanism that acts as a form of consistent memory, enabling agents to coordinate indirectly. The study shows that while standard LLMs and classical heuristic approaches largely struggle to solve these problems, more advanced reasoning models such as O3 can devise effective strategies for escaping deadlocks. LoopBench thus serves as an important tool for analyzing emergent distributed algorithms that arise from language-based reasoning and provides a structured testbed for exploring collective intelligence in autonomous LLM agents operating in distributed environments. This work sheds light on the capabilities and limitations of language models acting autonomously in multi-agent coordination settings and opens pathways for future research in distributed AI using natural language reasoning. <div>
arXiv:2512.13713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach</title>
<link>https://arxiv.org/abs/2512.13714</link>
<guid>https://arxiv.org/abs/2512.13714</guid>
<content:encoded><![CDATA[
<div> LLM instability, annotation pipeline, weak supervision, human-AI synergy, feedback loops<br /><br />Summary:<br /><br />This paper addresses reliability challenges faced by large language models (LLMs) in highly regulated industries, where instability, hallucinations, inconsistent reasoning, and variable performance limit their safe application. Current stabilization methods like reinforcement learning with human feedback (RLHF) and supervised fine-tuning improve outcomes but are costly and rely heavily on extensive human annotation, hindering scalability. To overcome these limitations, the authors propose an AI-based annotation pipeline designed to systematically detect, label, and correct instability patterns in LLM outputs. The approach leverages a human-AI synergy model that integrates automated weak supervision and confidence-based annotations alongside human validation, ensuring both reliability and ethical integrity of feedback. Additionally, the paper introduces categories focused on semantic consistency, factual correctness, and logical coherence to guide stability-specific annotation. This structured feedback enables continuous model calibration and robustness improvement through feedback loops. The proposed framework aims to facilitate sustainable and scalable enhancement of LLM performance, especially in contexts demanding precise, consistent, and trustworthy language generation. <div>
arXiv:2512.13714v1 Announce Type: new 
Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN</title>
<link>https://arxiv.org/abs/2512.13715</link>
<guid>https://arxiv.org/abs/2512.13715</guid>
<content:encoded><![CDATA[
<div> Keywords: O-RAN, Meta Hierarchical Reinforcement Learning, resource allocation, network slicing, meta learning<br /><br />Summary:<br /><br />1. This paper addresses the need for wireless networks that can adapt in real time and efficiently manage resources due to the rising complexity of modern applications.  
2. It focuses on the Open Radio Access Network (O-RAN) architecture, particularly the RAN Intelligent Controller (RIC) modules, as key enablers for dynamic resource management and network slicing.  
3. The authors propose an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework inspired by Model Agnostic Meta Learning (MAML) to optimize resource allocation and network slicing jointly within O-RAN.  
4. The framework features a hierarchical structure with a high-level controller allocating resources among network slices, while low-level agents manage scheduling within slices, allowing for both global and local adaptations.  
5. An adaptive meta-update mechanism is introduced that weights tasks based on temporal difference error variance to improve learning stability and focus on complex network scenarios.  
6. Theoretical analysis confirms sublinear convergence and establishes regret guarantees for the two-level learning approach.  
7. Simulation results show the proposed method yields a 19.8% improvement in network management efficiency over baseline reinforcement learning and meta-learning methods, alongside faster adaptation and better QoS satisfaction across eMBB, URLLC, and mMTC slices.  
8. Additional ablation and scalability tests demonstrate robustness, achieving up to 40% faster adaptation while maintaining fairness, latency, and throughput performance as the network scales. <div>
arXiv:2512.13715v1 Announce Type: new 
Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making</title>
<link>https://arxiv.org/abs/2512.13716</link>
<guid>https://arxiv.org/abs/2512.13716</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized decision-making, human values, value-driven, AI agents, dataset generation toolkit  

<br /><br />Summary:  
This paper addresses the challenge of personalized decision-making in AI by introducing a value-driven approach that aligns AI agent actions with individual human value preferences rather than solely task completion or collective goals. The authors emphasize that human values act as stable and transferable signals, enabling AI to maintain consistent and generalizable behavior across various contexts. To implement this approach, they propose ValuePilot, a two-phase framework that includes a Dataset Generation Toolkit (DGT) and a Decision-Making Module (DMM). The DGT creates diverse, value-annotated scenarios through a collaborative process involving humans and large language models (LLMs), while the DMM learns to assess actions based on personalized value preferences for context-sensitive and individualized decision-making. The framework was evaluated on previously unseen scenarios, where DMM demonstrated superior performance compared to strong LLM baselines such as GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, particularly in aligning with human action choices. Overall, the results highlight that a value-driven decision-making paradigm is a promising, interpretable, and extensible engineering path to develop personalized AI agents capable of operating effectively even in novel or complex situations. <div>
arXiv:2512.13716v1 Announce Type: new 
Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</title>
<link>https://arxiv.org/abs/2512.13725</link>
<guid>https://arxiv.org/abs/2512.13725</guid>
<content:encoded><![CDATA[
<div> quantization, causal reasoning, large language models, Pearl's Causal Ladder, graph retrieval augmentation  

<br /><br />Summary:  
This study investigates the impact of model quantization—specifically INT8 and NF4 precisions—on causal reasoning abilities of large language models (LLMs), focusing on all three tiers of Pearl's Causal Ladder: association, intervention, and counterfactual inference. Using a 3000-sample stratified CLadder benchmark on Llama 3 8B, the researchers find that overall causal reasoning accuracy remains largely stable under quantization, with NF4 precision causing less than a 1% drop. Among the three causal reasoning levels, interventional queries (rung 2) show the greatest sensitivity to precision loss, while counterfactual reasoning (rung 3) is more robust but suffers from particular weaknesses in query types like collider bias and backdoor adjustment. Experiments on the CRASS commonsense counterfactual benchmark reveal near identical performance across precisions, indicating these datasets lack the structural complexity to detect quantization-related reasoning errors. The team further employs Graph Retrieval Augmented Generation, leveraging ground truth causal graphs, which improves NF4 interventional accuracy by 1.7%, partly mitigating compression effects. Overall, the findings suggest that causal reasoning in LLMs is surprisingly resilient to aggressive quantization, that graph-structured data augmentation can selectively enhance performance, and that current benchmarks need enhancement to capture subtle causal brittleness. This research offers practical insights for deploying efficient, compressed causal AI systems in resource-constrained settings. <div>
arXiv:2512.13725v1 Announce Type: new 
Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title>
<link>https://arxiv.org/abs/2512.13762</link>
<guid>https://arxiv.org/abs/2512.13762</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Behavioral Selectivity, Functional Refusal, Learned Incapacity, Interaction-Level Auditing  

<br /><br />Summary:  
1. The article addresses the behavioral patterns of large language models (LLMs) during long interactions that are not easily captured by traditional quantitative benchmarks.  
2. It introduces a qualitative case-study methodology to audit how LLMs selectively respond based on policy-linked or sensitive content during extended dialogues.  
3. In an 86-turn dialogue, the same model displays Normal Performance (NP) in general, non-sensitive domains but shows Functional Refusal (FR) repeatedly when faced with provider- or policy-sensitive topics, creating a clear asymmetry between NP and FR across different domains.  
4. The authors propose the concept of learned incapacity (LI) to describe this selective withholding behavior, analogous to learned helplessness, without attributing any intentionality or internal cognitive mechanism to the model.  
5. They identify three response regimes from the model’s outputs—Normal Performance (NP), Functional Refusal (FR), and Meta-Narrative (MN)—finding that MN, which involves role-framing narratives, often co-occurs with refusals in sensitive contexts.  
6. The study highlights an interaction-level auditing framework grounded in observable behaviors, which can serve as a tool to investigate potential alignment side effects in LLMs and encourages broader research across different users and models. <div>
arXiv:2512.13762v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics and Coding are Universal AI Benchmarks</title>
<link>https://arxiv.org/abs/2512.13764</link>
<guid>https://arxiv.org/abs/2512.13764</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematics, coding, psychometric batteries, self-improvement, AI agents<br /><br />Summary:  
This paper explores the significant role of mathematics and coding within the moduli space of psychometric batteries used to evaluate AI agents. It expands upon the AAI framework and GVU dynamics to introduce the concept of the Mathematics Fiber, demonstrating that GVU flows paired with formal proof systems like Lean or Coq enable spectrally stable regimes of self-improvement due to their oracle-like verification capabilities. The main technical contribution is a density theorem stating that, given uniform tightness in agent outputs and a Lipschitz continuous AAI functional, the set of batteries derived specifically from mathematical theorem-proving and coding tasks is dense in the entire moduli space of batteries under the evaluation metric. While coding tasks alone are shown to be universally expressive in this space, pure mathematics does not achieve universality but provides a spectral advantage, meaning it affects the stability and self-improving behavior rather than expressive coverage. The authors interpret these findings as evidence that mathematics and coding supply "universal coordinates" to evaluate AI agents effectively and argue that formal mathematics acts as a natural starting point for recursive self-improvement processes in advanced AI systems. <div>
arXiv:2512.13764v1 Announce Type: new 
Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</title>
<link>https://arxiv.org/abs/2512.13771</link>
<guid>https://arxiv.org/abs/2512.13771</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic Grounding Index, retrieval-augmented generation, hallucination detection, angular distance, embedding space<br /><br />Summary: This paper investigates the geometric patterns left in embedding space when retrieval-augmented generation (RAG) systems produce hallucinated responses. The authors introduce the Semantic Grounding Index (SGI), which measures the ratio of angular distances from a generated response to the question versus to the retrieved context on the unit hypersphere. They identify a phenomenon termed "semantic laziness," where hallucinated responses remain angularly close to the question instead of aligning with the retrieved context. Using a dataset called HaluEval with 5,000 samples and five embedding models, they observe large effect sizes (Cohen's d from 0.92 to 1.28) and a strong mean correlation of 0.85 across models. A theoretical result derived from the spherical triangle inequality predicts that SGI's ability to discriminate hallucinations improves with increased angular separation between question and context, which empirical results confirm: effect sizes and AUC metrics increase with larger separation. Subgroup analyses show SGI performs best on long responses (d=2.05) and short questions (d=1.22) and performs robustly regardless of context length. Calibration analysis finds SGI can estimate probabilities well (ECE=0.10). However, tests on TruthfulQA show SGI measures topical relevance rather than truthfulness. Overall, SGI is an efficient, theoretically motivated tool for flagging potentially hallucinated generations in RAG systems for verification. <div>
arXiv:2512.13771v1 Announce Type: new 
Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $\theta(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $\theta(q,c)$, to $d$=1.27 -high $\theta(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
<link>https://arxiv.org/abs/2512.13857</link>
<guid>https://arxiv.org/abs/2512.13857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, program evolution, multi-agent systems, EvoLattice, quality-diversity optimization  

<br /><br />Summary:  
This paper introduces EvoLattice, a novel framework designed to improve the evolution of programs and multi-agent systems using large language models (LLMs). Unlike traditional overwrite-based mutation methods that maintain only a single candidate and suffer from destructive edits and limited diversity, EvoLattice represents an entire population of candidate programs or behaviors within a single directed acyclic graph. Each node in this graph contains multiple persistent alternatives, allowing for a large combinatorial search space without redundancy. EvoLattice evaluates each alternative by aggregating its performance across all paths it appears in, providing detailed, data-driven feedback that guides LLM-based mutation, recombination, and pruning operations while preserving successful components. A deterministic self-repair mechanism ensures the structural correctness of candidates by enforcing acyclicity and dependency consistency independently of the LLM. The approach naturally extends to evolving agents by treating alternatives as prompt fragments or sub-agent behaviors. Experiments in program synthesis, including proxy testing and optimizer meta-learning, show EvoLattice achieves more stable evolutionary dynamics, increased expressivity, and stronger improvement trends compared to prior LLM-guided methods. This leads to implicit quality-diversity optimization emerging from the internal multi-alternative representation, without relying on an explicit external archive. <div>
arXiv:2512.13857v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2512.13955</link>
<guid>https://arxiv.org/abs/2512.13955</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Incentive Mechanism, Client Reliability, Privacy Protection, Robustness

<br /><br />Summary:  
Federated Learning (FL) enables privacy-preserving machine learning by allowing participants to share model updates rather than raw data. Despite its advantages, FL faces challenges such as weak client incentives, privacy vulnerabilities, and limited resources. Addressing these, the paper introduces MURIM, a Multi-dimensional Reputation-based Incentive Mechanism designed to assess and reward clients based on multiple factors. MURIM jointly considers client reliability, privacy protection, resource capacity, and fairness, effectively preventing malicious or unreliable participants from gaining undue incentives. The mechanism allocates rewards by evaluating client contribution, latency, and reputation, supported by a reliability verification module to ensure quality participation. Extensive experiments conducted on MNIST, FMNIST, and ADULT Income datasets demonstrate MURIM’s ability to improve fairness metrics by up to 18%, reduce the success rates of privacy attacks by 5-9%, and enhance robustness against poisoning and noisy-gradient attacks by as much as 85% compared to existing state-of-the-art methods. Overall, MURIM successfully mitigates adversarial threats, encourages fair and honest participation among clients, and maintains stable model convergence in heterogeneous and dynamic federated learning environments. <div>
arXiv:2512.13955v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms</title>
<link>https://arxiv.org/abs/2512.13978</link>
<guid>https://arxiv.org/abs/2512.13978</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, benchmark, randomized algorithms, formal proofs  

<br /><br />Summary:  
The paper evaluates the capabilities of four advanced large language models (GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4) on graduate-level mathematical theory using the classic textbook "Randomized Algorithms" by Motwani and Raghavan. Each model was tasked with generating formal LaTeX proofs for a series of lemmas and exercises from the textbook. Results show that top-tier models Gemini and Claude achieve approximately 66% accuracy, demonstrating strong proficiency in probabilistic methods and formal logic. Other models performed less consistently, achieving around 40% accuracy. The study provides a qualitative analysis detailing differences in proof conciseness, hallucination rates (i.e., generated incorrect content), and logical structure among the models. This benchmark highlights that while these frontier models have reached a level suitable for graduate-level pedagogical support and formalization, notable variability remains in their reliability for rigorous mathematical derivation. The research advances understanding of LLMs' current baseline reasoning capabilities in formal mathematics and stresses the importance of continued evaluation. The authors have open-sourced the code and full generated proof sets for public access, fostering transparency and further research in the domain. <div>
arXiv:2512.13978v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${\'o}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflCtrl: Controlling LLM Reflection via Representation Engineering</title>
<link>https://arxiv.org/abs/2512.13979</link>
<guid>https://arxiv.org/abs/2512.13979</guid>
<content:encoded><![CDATA[
<div> Large language models, Chain-of-Thought, Self-reflection, Representation engineering, Uncertainty control<br /><br />Summary:<br /><br />This work investigates self-reflection in large language models (LLMs) with Chain-of-Thought (CoT) reasoning, focusing on its impact and control. First, self-reflection is identified as the ability of LLMs to review and revise previous reasoning steps, which improves performance but increases inference cost. Second, the authors approach self-reflection through representation engineering by segmenting reasoning into steps, isolating reflection-related steps, and extracting a reflection direction in the model's latent space that governs this behavior. Third, they propose a ReflCtrl framework that uses this reflection direction for stepwise steering, enabling control over reflection frequency. Fourth, experiments show that many reflection steps are redundant, particularly in stronger models, allowing up to 33.6% reasoning token savings while maintaining performance. Finally, the study finds a strong correlation between the reflection behavior and an internal uncertainty signal within the model, suggesting that self-reflection can be effectively controlled by leveraging the model’s uncertainty estimates. This work contributes to improving the efficiency of CoT reasoning by selectively managing self-reflection based on internal signals. <div>
arXiv:2512.13979v1 Announce Type: new 
Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training</title>
<link>https://arxiv.org/abs/2512.13996</link>
<guid>https://arxiv.org/abs/2512.13996</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture-of-Experts, Dynamic Top-p Routing, Proportional-Integral Controller, Large Language Models, Routing Normalization<br /><br />Summary:<br /><br />This paper introduces DTop-p MoE, a novel dynamic Top-p routing mechanism designed to improve sparse Mixture-of-Experts architectures by controlling sparsity adaptively rather than using a fixed threshold. Unlike the conventional Top-k routing, which applies uniform sparsity regardless of token difficulty, and existing Top-p methods with fixed global thresholds, DTop-p uses a Proportional-Integral (PI) Controller to dynamically adjust the probability threshold, aligning activated expert counts with a target sparsity level. This approach overcomes the challenge posed by the non-differentiability of threshold optimization. Additionally, the authors propose a dynamic routing normalization technique that adapts layer-wise routing logits allowing layers to develop unique expert selection patterns while still using a global threshold. Extensive experiments conducted on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms classic Top-k and fixed-threshold Top-p routing baselines both in efficiency and accuracy. The study highlights DTop-p’s ability to precisely control expert activation counts, adapt resource allocation among tokens and layers, and maintain strong scalability with respect to expert granularity, capacity, model size, and dataset size. Thus, DTop-p provides a robust and flexible framework for scaling large MoE models efficiently. <div>
arXiv:2512.13996v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</title>
<link>https://arxiv.org/abs/2512.14014</link>
<guid>https://arxiv.org/abs/2512.14014</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, GUI agents, vision-language models, MobileWorldBench, MobileWorld dataset<br /><br />Summary: This paper addresses the challenge of using world models for embodied agents operating in graphical user interface (GUI) environments, where traditional pixel-space prediction methods struggle due to the complexity of visual elements. Instead of predicting future states in pixel space, the authors propose describing state transitions in natural language, leveraging vision-language models (VLMs) to improve world modeling in GUI settings. To evaluate this approach, the paper introduces MobileWorldBench, a benchmark specifically designed to assess the capability of VLMs as world models for mobile GUI agents. Additionally, the authors release MobileWorld, a large-scale dataset containing 1.4 million samples, which enriches and facilitates training of VLMs for better world modeling. Finally, the paper presents a novel framework that integrates VLM-based world models directly into the planning algorithms of mobile agents. This integration demonstrates that semantic world models can significantly enhance the agents’ task success rates by providing more meaningful and interpretable state transitions. The code and dataset supporting this work are made publicly available to encourage further research and development in this direction. <div>
arXiv:2512.14014v1 Announce Type: new 
Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Language Models for Agentic On-Farm Decision Support Systems</title>
<link>https://arxiv.org/abs/2512.14043</link>
<guid>https://arxiv.org/abs/2512.14043</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Small Language Models, dairy farming, decision support, computational efficiency<br /><br />Summary:  
1. The study explores the potential of Large Language Models (LLMs) to support decision-making in dairy farming, particularly for stakeholders with limited technical expertise.  
2. Due to high computational demands and reliance on cloud services, LLMs are impractical for on-farm use, leading to a focus on lightweight Small Language Models (SLMs) that can run locally on farm hardware.  
3. Researchers benchmarked 20 open-source SLMs from HuggingFace under realistic farm computing constraints, integrating them into an agentic AI system comprising five task-specific agents: literature search, web search, SQL interaction, NoSQL interaction, and graph generation via predictive models.  
4. Evaluation was performed in two phases: an initial screening with five test dairy-related questions to identify capable models, followed by a detailed assessment using 30 questions across multiple task categories including data integrity and misconduct.  
5. The Qwen-4B model demonstrated the best overall performance, though it showed instability in NoSQL database tasks via PySpark. The study is pioneering in assessing SLM feasibility for dairy decision-making with a focus on privacy and computational efficiency. Nonetheless, fine-tuning is necessary for improving performance on dairy-specific queries, and challenges remain before practical deployment. <div>
arXiv:2512.14043v1 Announce Type: new 
Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation</title>
<link>https://arxiv.org/abs/2512.14048</link>
<guid>https://arxiv.org/abs/2512.14048</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Chain-of-Thought prompting, code generation, Intention Chain-of-Thought, difficulty-aware routing  

<br /><br />Summary: Large language models (LLMs) are proficient in generative tasks and code generation, yet existing chain-of-thought (CoT) prompting approaches have two main drawbacks. First, uniform application of CoT can cause overthinking in simple problems. Second, these methods often lack the abstraction of intentions in code generation, such as explicit modeling of core algorithmic logic and efficiency considerations, which leads to a focus on surface-level code rather than the global problem objectives. Addressing these issues, the authors propose RoutingGen, a novel difficulty-aware routing framework that dynamically chooses prompting strategies based on task complexity. For simpler tasks, RoutingGen applies few-shot prompting, while for more difficult tasks, it invokes a structured reasoning prompting technique called Intention Chain-of-Thought (ICoT). ICoT guides the model to better capture task intentions including core algorithm design and time complexity. Experimental results across three models and six standard code generation benchmarks demonstrate that RoutingGen achieves state-of-the-art performance in most settings and reduces token usage by 46.37% on average. Additionally, ICoT consistently outperforms six existing prompting baselines on challenging code generation benchmarks, validating its effectiveness in enhancing model reasoning and efficiency. <div>
arXiv:2512.14048v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</title>
<link>https://arxiv.org/abs/2512.14051</link>
<guid>https://arxiv.org/abs/2512.14051</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data evaluation, benchmarking, data lineage, Data-Centric AI  

<br /><br />Summary:  
The paper addresses the critical challenge of understanding and evaluating post-training datasets that fuel Large Language Models (LLMs), highlighting the opaque nature of data composition and provenance. It introduces OpenDataArena (ODA), an open platform designed to benchmark the intrinsic value of such data through four pillars: a unified training-evaluation pipeline for fair comparisons across models and domains; a multi-dimensional scoring framework to assess data quality on numerous criteria; an interactive data lineage explorer to visualize dataset genealogy and source components; and an open-source toolkit enabling comprehensive training, evaluation, and scoring. The authors conducted extensive experiments involving over 120 datasets from diverse domains, 22 benchmarks, more than 600 training runs, and 40 million data points processed. Their analysis revealed key insights including trade-offs between data complexity and task performance, redundancy in widely used benchmarks uncovered via lineage tracing, and mappings of genealogical relationships between datasets. By releasing all tools, results, and configurations publicly, ODA encourages democratized access to data evaluation resources. Ultimately, the platform aims to transform the current trial-and-error approach in data curation into a principled, scientific method within Data-Centric AI, facilitating rigorous studies on data mixing principles and strategic assembly of foundation models. <div>
arXiv:2512.14051v1 Announce Type: new 
Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees</title>
<link>https://arxiv.org/abs/2512.14069</link>
<guid>https://arxiv.org/abs/2512.14069</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, speculative sampling, reinforcement learning, dynamic draft trees, inference acceleration  

<br /><br />Summary:  
The paper addresses the challenge of slow and costly inference in modern Large Language Models (LLMs). It identifies the limitation of existing speculative sampling methods, where the number of calls to the draft model for generating candidate tokens is preset and inflexible. To overcome this, the authors propose RADAR, a novel speculative sampling approach that uses reinforcement learning (RL) to build dynamic draft trees. RADAR formulates the draft tree generation as a Markov Decision Process (MDP) and applies offline RL to train a prediction model. This model enables adaptive real-time decisions on the number of draft model calls, significantly reducing redundant computations during inference. The proposed method is empirically evaluated across three different LLMs and four varied tasks. Results show that RADAR achieves a substantial speedup in inference, ranging from 3.17x to 4.82x compared to the standard auto-regressive decoding baseline. The approach effectively balances inference speed and output quality, demonstrating the benefits of dynamic control in speculative sampling. Additionally, the authors have made their code publicly available, facilitating future research and application of RADAR in accelerating LLM inference. <div>
arXiv:2512.14069v1 Announce Type: new 
Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar Search for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2512.14079</link>
<guid>https://arxiv.org/abs/2512.14079</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Systems, agentic AI, structured framework, LLM-based search, modular design<br /><br />Summary:  
This paper addresses the automatic search for Multi-Agent Systems, a crucial area in agentic AI research. Existing methods predominantly employ large language model (LLM)-based free-form search strategies within the code space, which offer generative flexibility but can be computationally expensive and less interpretable. To overcome these limitations, the authors propose a structured framework that restricts the search to a fixed set of simple, composable components rather than using generative LLM outputs. Despite lacking the creative freedom of LLM-based generation, this more constrained approach demonstrates superior performance, outperforming prior methods on four out of five benchmarks across two domains: mathematics and question answering. Beyond improved accuracy, the proposed framework provides additional benefits, including reduced search costs, leading to a more resource-efficient process. Importantly, the resulting multi-agent systems exhibit modularity and simpler logical structures, enhancing interpretability and ease of analysis. This work highlights that a carefully designed structured search can rival and even surpass flexible LLM-based strategies in building effective multi-agent systems, while also fostering better understandability and efficiency. <div>
arXiv:2512.14079v1 Announce Type: new 
Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control</title>
<link>https://arxiv.org/abs/2512.14106</link>
<guid>https://arxiv.org/abs/2512.14106</guid>
<content:encoded><![CDATA[
<div> Keywords: HydroGEM, streamflow monitoring, quality control, TCN-Transformer, anomaly detection  

<br /><br />Summary:  
This paper introduces HydroGEM, a foundation model specifically developed for continental-scale streamflow quality control across extensive monitoring networks. HydroGEM employs a two-stage training process beginning with self-supervised pretraining on over 6 million sequences collected from 3,724 USGS stations to learn rich hydrological representations. The model is then fine-tuned using synthetic anomalies to enhance detection and reconstruction capabilities. Its architecture integrates a hybrid Temporal Convolutional Network (TCN) and Transformer with 14.2 million parameters, enabling it to capture both local temporal patterns and long-range dependencies. Hierarchical normalization techniques allow HydroGEM to handle discharges spanning six orders of magnitude reliably. Evaluated on a held-out synthetic test dataset containing 799 stations and 18 expert-validated anomaly types, HydroGEM achieves a detection F1 score of 0.792 and reduces reconstruction errors by 68.7%, which is a 36.3% improvement over existing methods. The model also demonstrates strong zero-shot transfer performance to 100 Canadian Environment and Climate Change stations with an F1 score of 0.586, validating its cross-national generalizability. Importantly, HydroGEM maintains consistent detection performance across different anomaly correction magnitudes and aligns well with known seasonal operational patterns. The system is designed for a human-in-the-loop workflow, providing quality control suggestions for expert review rather than making autonomous corrections. <div>
arXiv:2512.14106v1 Announce Type: new 
Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model</title>
<link>https://arxiv.org/abs/2512.14112</link>
<guid>https://arxiv.org/abs/2512.14112</guid>
<content:encoded><![CDATA[
<div> Liquid Neural Networks, XGBoost, Supply Chain Management, Bullwhip Effect, Time-Series Data  

<br /><br />Summary:  
This study addresses significant challenges in supply chain management (SCM), including demand fluctuations and the bullwhip effect, which complicate forecasting and decision-making. Existing approaches like traditional methods and large language models (LLMs) are inadequate for handling SCM's complex continuous time-series data, as demonstrated by failures in benchmarks such as the Vending Machine Test. While machine learning models such as LSTM and XGBoost provide some solutions, they often suffer from computational inefficiencies. The research explores the untapped potential of Liquid Neural Networks (LNN), known for their adaptability and computational efficiency in robotics, applying them to SCM. A novel hybrid model combining LNN and XGBoost is proposed for multi-tier supply chains, leveraging LNN's dynamic feature extraction capabilities with XGBoost's strength in global optimization. This fusion aims to reduce the bullwhip effect and enhance overall profitability by improving demand forecasting accuracy and responsiveness. The approach fills a critical gap by offering an efficient and adaptable intelligent solution specifically tailored for complex SCM environments, positioning it as a promising advancement in the field. <div>
arXiv:2512.14112v1 Announce Type: new 
Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.14157</link>
<guid>https://arxiv.org/abs/2512.14157</guid>
<content:encoded><![CDATA[
<div> Keywords: medical MLLMs, tool-augmented framework, visual grounding, multimodal chain of thought, reinforcement learning<br /><br />Summary:  
This paper introduces Ophiuchus, an advanced framework designed to enhance medical Multimodal Large Language Models (MLLMs) for improved diagnostic reasoning by dynamically interacting with medical images. First, Ophiuchus enables the model to decide autonomously when additional visual evidence is necessary, addressing challenges in complex tasks that require fine-grained image inspection. Second, it determines precise locations within medical images to probe and ground relevant features, thus improving the accuracy of information extraction. Third, the framework integrates these focused image regions seamlessly back into an interleaved multimodal reasoning chain, allowing for coherent and context-aware diagnostic outputs. Unlike previous methods constrained by tool performance, Ophiuchus effectively combines the model's own perception capabilities with external tools to enhance higher-level reasoning. The training strategy comprises three stages: an initial cold-start phase that incorporates tool-integrated reasoning data for basic tool selection and adaptation, a self-reflection fine-tuning phase to refine reflective reasoning and encourage revisiting tool insights, and finally, an Agentic Tool Reinforcement Learning phase that optimizes task-specific rewards, promoting expert-like diagnostic behavior. Extensive experiments demonstrate that Ophiuchus outperforms leading closed-source and open-source methods on multiple medical benchmarks involving visual question answering, detection, and reasoning-based segmentation. The authors plan to publicly release the datasets, code, and trained models to promote further research in medical AI agents capable of sophisticated tool-integrated reasoning. <div>
arXiv:2512.14157v1 Announce Type: new 
Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Georeferencing complex relative locality descriptions with large language models</title>
<link>https://arxiv.org/abs/2512.14228</link>
<guid>https://arxiv.org/abs/2512.14228</guid>
<content:encoded><![CDATA[
<div> Georeferencing, Large Language Models, biodiversity collections, Quantized Low-Rank Adaptation, locality descriptions  

<br /><br />Summary:  
This paper addresses the challenge of georeferencing text documents that describe locations in relative spatial terms rather than explicit coordinates or place names, which is common in biological specimen records predating GPS. Traditional gazetteer-based or language modeling methods struggle with such complex locality descriptions, making accurate geocoding difficult yet essential for biodiversity research. The authors investigate the application of Large Language Models (LLMs) to automate georeferencing of these intricate descriptions. They identify effective prompting strategies and fine-tune an LLM using Quantized Low-Rank Adaptation (QLoRA) on diverse biodiversity datasets across multiple regions and languages. Their approach demonstrates significant improvement over existing baseline methods, achieving on average 65% of records georeferenced within a 10 km radius. The best performance, observed on New York State data, reaches 85% within 10 km and 67% within 1 km. The selected LLM model is particularly adept at handling lengthy and complex textual locality narratives, indicating strong potential for practical automated georeferencing in biodiversity collections and similarly challenging domains. <div>
arXiv:2512.14228v1 Announce Type: new 
Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G\"odel's Poetry</title>
<link>https://arxiv.org/abs/2512.14252</link>
<guid>https://arxiv.org/abs/2512.14252</guid>
<content:encoded><![CDATA[
<div> Keywords: automated theorem proving, language models, Lean4, recursive decomposition, multi-agent architecture<br /><br />Summary:<br /><br />1. The paper introduces a novel approach to computer-assisted theorem proving that leverages specialized language models designed specifically for generating proofs in Lean4, a formal proof assistant.<br />2. A major innovation in the approach is the recursive decomposition of complex theorems into simpler propositions that are easier to prove, improving the overall proof success rate.<br />3. These language models are coordinated within a multi-agent system that handles multiple tasks including autoformalization (converting informal statements to formal logic), proof generation, theorem decomposition, and recursive proving of subpropositions.<br />4. The system achieves a high baseline performance with a 90.4% pass rate on the miniF2F benchmark when decomposition techniques are not used, with performance significantly improving when decomposition is applied.<br />5. A technical contribution is the enhancement of the Kimina Lean Server by adding abstract syntax tree (AST) parsing, enabling automated and recursive proof decomposition. The system, provided as the Python package goedels-poetry, is open-source and designed for easy adaptation and extension with different language models and custom features. <div>
arXiv:2512.14252v1 Announce Type: new 
Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting</title>
<link>https://arxiv.org/abs/2512.14288</link>
<guid>https://arxiv.org/abs/2512.14288</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Parkinson's Disease, Ontology Engineering, Human-LLM Collaboration, Hybrid Methodologies<br /><br />Summary:<br /><br />This paper investigates the use of Large Language Models (LLMs) in developing a Parkinson's Disease (PD) monitoring and alerting ontology through four techniques: One Shot (OS) prompting, Chain of Thought (CoT) prompting, X-HCOME, and SimX-HCOME+. Initial ontology construction using OS and CoT prompt methods demonstrated that LLMs can autonomously generate ontologies but lack full comprehensiveness and accuracy, requiring significant human refinement. The hybrid approach X-HCOME, combining human expertise with LLM-driven ontology engineering, significantly improved ontology completeness, producing results closely resembling expert-created ontologies. SimX-HCOME+, an iterative hybrid methodology stressing continuous human oversight and refinement, further enhanced ontology quality and accuracy, illustrating the critical role of sustained human involvement in ontology development. Overall, the study highlights the promise of human-LLM collaboration in ontology engineering, especially for complex medical domains like PD, where fully automated approaches alone fall short. The results advocate future research pathways including creating specialized GPT models tailored for ontology construction to improve automation and accuracy further. This work underscores the evolving synergy between human expertise and LLM capabilities in advancing knowledge representation models. <div>
arXiv:2512.14288v1 Announce Type: new 
Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation</title>
<link>https://arxiv.org/abs/2512.14358</link>
<guid>https://arxiv.org/abs/2512.14358</guid>
<content:encoded><![CDATA[
<div> Cardinality Estimation, Query Optimization, TiCard, Gradient Boosting Regressor, TabPFN

<br /><br />Summary:  
1. Cardinality estimation is a critical challenge for cost-based query optimizers, where inaccuracies can significantly impact query performance.  
2. Traditional estimators often fail to capture correlations in data, while many learned estimators require complex, workload-specific training and invasive changes to the optimizer, limiting their practical adoption.  
3. The paper introduces TiCard, a novel, low-intrusion framework designed to augment native database cardinality estimators rather than replace them, easing deployment and integration efforts.  
4. TiCard operates by learning multiplicative residual corrections from EXPLAIN-only features and utilizes EXPLAIN ANALYZE exclusively for offline labeling, enabling correction without retraining the whole system.  
5. Two implementations are studied: (i) a Gradient Boosting Regressor providing fast sub-millisecond inferences, and (ii) TabPFN, an in-context tabular foundation model that adapts by updating a small reference set instead of full gradient retraining.  
6. Evaluations on TiDB with TPC-H and the Join Order Benchmark in a limited-trace environment demonstrate TiCard’s significant improvement in operator-level tail accuracy (P90 Q-error reduced from 312.85 to 13.69 using TiCard-GBR and P99 Q-error from 37,974.37 to 3,416.50 with TiCard-TabPFN), while maintaining near-perfect median accuracy with a join-only policy.  
7. TiCard is positioned as a practical AI-for-Database (AI4DB) component emphasizing clear objectives, conservative integration strategies, and an envisioned roadmap from offline corrections toward full optimizer integration. <div>
arXiv:2512.14358v1 Announce Type: new 
Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Massive Editing for Large Language Models Based on Dynamic Weight Generation</title>
<link>https://arxiv.org/abs/2512.14395</link>
<guid>https://arxiv.org/abs/2512.14395</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Editing, Large Language Models, Dynamic Weight Generation, Diffusion Model, Large-scale Edits<br /><br />Summary:<br /><br />1. The paper addresses the challenge of efficiently modifying knowledge in Large Language Models (LLMs) without the high cost of full pre-training, focusing on Knowledge Editing (KE).<br />2. It introduces a novel approach called Massive editing for LLMs based on dynamic weight Generation (MeG), which enhances the ability to perform large-scale knowledge edits.<br />3. MeG works by attaching a dynamic weight neuron to specific layers in the LLM, and a diffusion model is employed to generate the neuron's weights conditioned on the input query that references the target knowledge.<br />4. This mechanism of using a single dynamic weight neuron enables scalable and effective editing by adapting the model's responses to the desired updated information.<br />5. Experimental results demonstrate that MeG significantly outperforms existing knowledge editing methods, achieving improved Reliability, Generality, and Locality metrics, with a particularly notable boost in the Locality metric, confirming its effectiveness in precise and contextually relevant knowledge modification. <div>
arXiv:2512.14395v1 Announce Type: new 
Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals</title>
<link>https://arxiv.org/abs/2512.14417</link>
<guid>https://arxiv.org/abs/2512.14417</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Dispatching Systems, Automated Container Terminals, Large Language Models, Transferability, Virtual Expert Team  

<br /><br />Summary:  
This paper addresses the challenges in transferring Vehicle Dispatching Systems (VDSs) across different Automated Container Terminals (ACTs), identifying three main limitations: dependence on port operation specialists, the need for extensive terminal-specific data, and lengthy manual deployment processes. To overcome these, the authors propose PortAgent, an innovative LLM-driven vehicle dispatching agent that automates the VDS transfer workflow completely. PortAgent eliminates the need for port operations specialists by implementing a Virtual Expert Team (VET), which consists of four specialized virtual experts—Knowledge Retriever, Modeler, Coder, and Debugger—that replicate a human expert team's role. These virtual experts learn the VDS domain through a few-shot example learning technique, significantly reducing reliance on large amounts of terminal-specific data. The Retrieval-Augmented Generation (RAG) mechanism supports this knowledge acquisition by retrieving relevant VDS examples. Additionally, PortAgent establishes a fully automatic VDS design workflow managed by the VET, which eliminates manual intervention. This workflow incorporates a self-correction loop inspired by the LLM Reflexion framework, enhancing the system's accuracy and robustness during deployment. Overall, PortAgent represents a significant advancement towards scalable and efficient vehicle dispatching solutions for ACTs. <div>
arXiv:2512.14417v1 Announce Type: new 
Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seismology modeling agent: A smart assistant for geophysical researchers</title>
<link>https://arxiv.org/abs/2512.14429</link>
<guid>https://arxiv.org/abs/2512.14429</guid>
<content:encoded><![CDATA[
<div> Keywords: SPECFEM, Large Language Models, Model Context Protocol, seismic simulation, AI-assisted workflow<br /><br />Summary:<br /><br />This paper addresses challenges in using the open-source seismic wave simulation software SPECFEM, which traditionally requires complex manual file edits and command-line operations. It proposes an intelligent, interactive workflow powered by Large Language Models (LLMs) to simplify and enhance the simulation process. The authors introduce the first Model Context Protocol (MCP) server suite for SPECFEM, compatible with 2D, 3D Cartesian, and 3D Globe versions, which breaks down the entire simulation workflow into discrete, agent-executable tools. These tools cover all stages, from parameter generation and mesh partitioning to solver execution and visualization, enabling a shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated runs and human-in-the-loop collaboration, allowing researchers to guide simulations in real time while maintaining control over scientific decisions. Validated through multiple case studies, the workflow operates effectively in both autonomous and interactive modes, producing results consistent with standard baseline simulations. As the first application of MCP technology in computational seismology, this approach significantly lowers the entry barrier, improves reproducibility, and points toward a future of AI-assisted and automated computational geophysics research. The source code is openly available for the community at https://github.com/RenYukun1563/specfem-mcp. <div>
arXiv:2512.14429v1 Announce Type: new 
Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Picker: Dynamic context selection using multi-stage reinforcement learning</title>
<link>https://arxiv.org/abs/2512.14465</link>
<guid>https://arxiv.org/abs/2512.14465</guid>
<content:encoded><![CDATA[
<div> Context selection, reinforcement learning, long-context question answering, minimal sufficient subset, evidence distillation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of selecting the optimal amount of context passages in long-context question answering (LCQA), balancing between missing crucial information and avoiding noisy redundancies.<br /><br />2. Traditional methods using fixed Top-K retrieval or single-stage reranking struggle with choosing the appropriate number of context passages, especially for factoid questions that need only limited evidence.<br /><br />3. The authors propose Context-Picker, a novel reasoning-aware framework that reframes context selection as a minimal sufficient subset selection task rather than similarity-based ranking.<br /><br />4. Context-Picker employs a two-stage reinforcement learning approach: a recall-oriented stage to cover necessary reasoning chains and a precision-oriented stage to prune redundant evidence, thus optimizing the decision-making for context selection.<br /><br />5. To mitigate sparse reward issues, an offline evidence distillation pipeline is introduced using a Leave-One-Out procedure to mine minimal sufficient sets, providing dense and task-aligned supervision.<br /><br />6. Experiments on five long-context and multi-hop QA datasets show that Context-Picker outperforms strong Retrieval-Augmented Generation (RAG) baselines, improving answer accuracy with fewer or comparable context passages.<br /><br />7. Ablation studies confirm that the coarse-to-fine optimization schedule, redundancy-aware reward shaping, and rationale-guided design all crucially contribute to performance gains. <div>
arXiv:2512.14465v1 Announce Type: new 
Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling</title>
<link>https://arxiv.org/abs/2512.14474</link>
<guid>https://arxiv.org/abs/2512.14474</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model-First Reasoning, multi-step planning, explicit modeling, constraint violations<br /><br />Summary:<br /><br />The article addresses the challenges faced by Large Language Models (LLMs) in handling complex multi-step planning tasks, where they often violate constraints and produce inconsistent solutions. Traditional methods like Chain-of-Thought and ReAct rely on implicit state tracking without explicitly representing the problem, which limits their effectiveness. Inspired by classical AI planning approaches, the authors propose a novel two-phase framework called Model-First Reasoning (MFR). In the first phase, the LLM constructs an explicit model that defines entities, state variables, actions, and constraints relevant to the problem. In the second phase, the LLM uses this model to generate a solution plan. Experiments across diverse domains such as medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis demonstrate that MFR significantly reduces constraint violations and improves overall solution quality compared to Chain-of-Thought and ReAct. Ablation studies highlight that the explicit modeling phase is critical to these improvements, suggesting that LLM failures in planning tasks are often due to representational shortcomings rather than reasoning capacity. The study emphasizes that explicit problem modeling is essential for creating robust, interpretable AI agents. All prompts, evaluation methods, and datasets are made available to ensure reproducibility of the findings. <div>
arXiv:2512.14474v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.14491</link>
<guid>https://arxiv.org/abs/2512.14491</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer, multi-modal, sparse attention, efficiency, Alzheimer's Disease<br /><br />Summary:<br /><br />1. This paper introduces SMMT, a sparse multi-modal transformer architecture aimed at improving the computational efficiency and robustness of multi-modal intelligent systems.<br /><br />2. SMMT builds on a cascaded multi-modal transformer framework and incorporates cluster-based sparse attention to reduce computational complexity from quadratic to near linear.<br /><br />3. The architecture also uses modality-wise masking to enhance robustness in scenarios where input data from some modalities might be incomplete or missing.<br /><br />4. The system is evaluated on a practical multi-modal problem: Alzheimer’s Disease classification using the ADNI dataset, demonstrating its real-world applicability.<br /><br />5. Experimental results show that SMMT achieves competitive predictive accuracy compared to dense attention models while significantly reducing training time, memory usage, and energy consumption, highlighting its suitability for resource-constrained environments and scalable intelligent system design. <div>
arXiv:2512.14491v1 Announce Type: new 
Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence</title>
<link>https://arxiv.org/abs/2512.14527</link>
<guid>https://arxiv.org/abs/2512.14527</guid>
<content:encoded><![CDATA[
<div> Keywords: GreedyLR, learning rate scheduler, NLP, convergence, optimizer  

<br /><br />Summary:  
The paper introduces GreedyLR, a new adaptive learning rate scheduler that adjusts the learning rate dynamically based on the current training loss. Unlike traditional schedulers such as Cosine or exponential decay, GreedyLR optimizes learning rate changes to improve training efficiency. The authors validate GreedyLR through extensive experiments on diverse tasks spanning natural language processing (NLP), computer vision (CV), and large language models (LLM) with up to 7 billion parameters, covering both fine-tuning and pre-training scenarios. Results demonstrate that GreedyLR surpasses state-of-the-art schedulers in accuracy, speed, and convergence behavior. A theoretical analysis accompanies the empirical findings, including a proof of convergence, and derivation of an optimal scaling factor F that maximizes the convergence rate, ensuring an optimal learning trajectory. Additional experiments highlight the scheduler's robustness in noisy and realistic loss landscapes, emphasizing its stability under practical training conditions. The paper also underscores the implementation simplicity and computational efficiency of GreedyLR, making it a potential default choice for training various neural network models. Overall, GreedyLR presents an effective and theoretically sound advancement in learning rate scheduling for modern deep learning applications. <div>
arXiv:2512.14527v1 Announce Type: new 
Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Reasoning Model</title>
<link>https://arxiv.org/abs/2512.14693</link>
<guid>https://arxiv.org/abs/2512.14693</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.14693v1, Universal Transformers, ARC-AGI, reasoning performance, Universal Reasoning Model<br /><br />Summary:  
This paper investigates the sources of performance improvements in Universal Transformers (UTs) on complex reasoning tasks such as ARC-AGI and Sudoku. The authors conduct a systematic analysis of different UT variants and find that the primary factors driving improved performance on ARC-AGI are the recurrent inductive bias and strong nonlinear components inherent in the Transformer architecture. Contrary to prior assumptions, the gains are not mainly due to more elaborate architectural innovations. Building on this insight, the authors propose the Universal Reasoning Model (URM), which integrates short convolutions and truncated backpropagation to further enhance reasoning capabilities. The URM demonstrates significant improvements over previous models, achieving state-of-the-art results with a 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2 benchmarks. The paper highlights the effectiveness of combining recurrent bias and targeted architectural additions to boost complex reasoning performance. The authors have also made their code publicly available for reproducibility and further research at the provided GitHub repository. This work contributes to better understanding and advancing the design principles behind transformer-based reasoning models. <div>
arXiv:2512.14693v1 Announce Type: new 
Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training</title>
<link>https://arxiv.org/abs/2511.10333</link>
<guid>https://arxiv.org/abs/2511.10333</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, gradient compression, entropy-driven, communication efficiency, dynamic adjustment<br /><br />Summary:<br /><br />This paper addresses the challenges in training large language models (LLMs) related to computational resources and high communication overhead during distributed training. Existing static gradient compression techniques, while improving communication efficiency, fail to adapt to the changing nature of gradients throughout training, causing performance loss. To solve this, the authors propose EDGC, an entropy-driven dynamic gradient compression framework that adjusts compression rates based on the evolving gradient entropy, balancing compression efficiency and error. EDGC comprises three main components: (1) a down-sampling method to estimate gradient entropy efficiently with low computational cost, (2) a theoretical model that relates compression rate to gradient entropy to guide compression decisions, and (3) a window-based adjustment mechanism that dynamically tunes compression rates across pipeline stages to optimize communication without sacrificing model accuracy. The framework was evaluated on two hardware setups—a 32-NVIDIA-V100 cluster training GPT2-2.5B and a 64-NVIDIA-H100 cluster training GPT2-12.1B. Results demonstrate that EDGC reduces communication latency by up to 46.45% and overall training time by up to 16.13%, all while preserving the accuracy of the trained LLMs. This shows EDGC’s potential to significantly accelerate LLM training through adaptive gradient compression. <div>
arXiv:2511.10333v1 Announce Type: cross 
Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset</title>
<link>https://arxiv.org/abs/2511.19317</link>
<guid>https://arxiv.org/abs/2511.19317</guid>
<content:encoded><![CDATA[
<div> Bangla summarization, dataset, abstractive, multi-domain, deep learning<br /><br />Summary: This study presents a newly developed Bangla abstractive summarization dataset aimed at generating concise summaries from a variety of Bangla texts sourced from multiple domains. Unlike previous datasets focusing mainly on news articles with uniform writing styles, this dataset encompasses over 54,000 articles collected from diverse sources such as blogs (Cinegolpo) and newspapers (Samakal and The Business Standard), capturing a broad spectrum of writing styles. This diversity enhances the dataset’s adaptability and practical relevance in real-world scenarios, where Bangla content is widely produced across different platforms like social media and blogs. The research underscores the urgent need for effective summarization tools to manage the overwhelming volume of Bangla content available digitally. To validate the dataset’s utility, several deep learning and transfer learning models—including LSTM, BanglaT5-small, and MTS-small—were trained and evaluated, establishing strong baseline results. These outcomes demonstrate the dataset’s potential as a benchmark for future research in Bangla natural language processing. Ultimately, this dataset lays a robust foundation for developing more effective summarization systems and contributes significantly to expanding NLP resources for low-resource languages like Bangla. <div>
arXiv:2511.19317v1 Announce Type: cross 
Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Writing in Symbiosis: Mapping Human Creative Agency in the AI Era</title>
<link>https://arxiv.org/abs/2512.13697</link>
<guid>https://arxiv.org/abs/2512.13697</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human-AI coevolution, creative writing, stylistic differentiation, authorship adaptation  

<br /><br />Summary:  
This paper investigates the evolving relationship between humans and AI in the field of creative writing, prompted by the rise of Large Language Models (LLMs). Using a large-scale corpus that spans both the pre- and post-LLM era, the study identifies patterns indicative of what the authors term "Dual-Track Evolution." This evolution involves a thematic convergence around AI-related topics, alongside a structured stylistic differentiation among writers. The analysis reveals three distinct adaptation patterns among authors: those who increasingly adopt AI-like stylistic traits, those whose styles diverge further from AI, and those who maintain consistent style while engaging with themes influenced by AI. This nuanced view challenges the simplistic idea that LLMs cause stylistic homogenization in human writing. The paper introduces the Creative Archetype Map, a conceptual framework that captures these diverse adaptation patterns. This map contributes to ongoing conversations about the nature of human-AI collaboration in creative processes, the challenges of detecting AI-influenced authorship, and the importance of preserving stylistic and thematic diversity in the era of AI-augmented creativity. <div>
arXiv:2512.13697v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a "Dual-Track Evolution": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport</title>
<link>https://arxiv.org/abs/2512.13702</link>
<guid>https://arxiv.org/abs/2512.13702</guid>
<content:encoded><![CDATA[
<div> AI Product Passport, healthcare AI, lifecycle documentation, transparency, regulatory compliance<br /><br />Summary:  
The article presents the development of the AI Product Passport, a standards-based framework aimed at enhancing transparency, traceability, and compliance in healthcare AI systems throughout their lifecycle. Developed within the AI4HF project targeting heart failure AI tools, the framework integrates regulatory insights from the EU AI Act and FDA guidelines alongside existing standards. It employs a relational data model encompassing critical AI lifecycle phases: study definition, dataset preparation, model generation and evaluation, deployment and monitoring, and passport generation. To ensure practical applicability, MLOps and ModelOps principles were incorporated. The design process included co-creation efforts with consortium members and a Lisbon workshop involving 21 stakeholders, whose feedback guided iterative improvements. The implementation resulted in an open-source, web-based platform using Python libraries capable of automated provenance tracking. The platform supports role-based access and produces both machine- and human-readable reports tailored for diverse stakeholders. Alignment with FUTURE-AI principles ensures fairness, universality, traceability, usability, robustness, and explainability of AI tools. The exported passports provide detailed metadata including model purpose, data provenance, performance metrics, and deployment context. The backend and frontend codebases are hosted on GitHub for broad accessibility. Future developments plan to incorporate FAIR data principles and FHIR standards for enhanced interoperability, promoting responsible and trustworthy AI deployment in healthcare. <div>
arXiv:2512.13702v1 Announce Type: cross 
Abstract: Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models</title>
<link>https://arxiv.org/abs/2512.13703</link>
<guid>https://arxiv.org/abs/2512.13703</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Jailbreaking, Semantic Isomorphism, Harmful Content, Prompt Engineering<br /><br />Summary: This paper addresses the vulnerabilities of Large Language Models (LLMs) to generating harmful content despite their impressive capabilities. The authors identify that many harmful queries share similar underlying principles with legitimate ones, a fact previously overlooked in jailbreak research. Building on this insight, they propose the Safe2Harm Semantic Isomorphism Attack, a novel four-stage jailbreak methodology. The process begins by rewriting a harmful question into a semantically safe but principle-consistent version. Next, the thematic mapping relationship between the harmful and safe queries is extracted. Then, the LLM is prompted to generate a detailed response to the safe question. Finally, this safe response is reversed transformed based on the thematic mapping to produce harmful output. Experimental results on seven mainstream LLMs and three benchmark datasets demonstrate that Safe2Harm achieves strong jailbreaking performance, outperforming existing methods. Additionally, the authors contribute a new challenging evaluation dataset with 358 harmful content samples to assess the effectiveness of current harmful content detection systems. This dataset enables better input-output filtering defenses, helping to mitigate LLM-related risks. The work highlights a sophisticated attack method and offers tools to improve LLM security and content moderation. <div>
arXiv:2512.13703v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling and Transferability of Annealing Strategies in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.13705</link>
<guid>https://arxiv.org/abs/2512.13705</guid>
<content:encoded><![CDATA[
<div> Learning rate scheduling, large language models, annealing strategies, Warmup-Steady-Decay, Mixture-of-Experts<br /><br />Summary:  
The paper addresses the importance of learning rate scheduling in training large language models and the challenge of identifying optimal annealing strategies across various model configurations. It investigates how annealing dynamics can be transferred between different model sizes and configurations, focusing on the Warmup-Steady-Decay (WSD) scheduler. An enhanced predictive framework is proposed that integrates critical factors such as training steps, maximum learning rate, and annealing behavior, which facilitates more efficient optimization of learning rate schedules. This approach reduces the need for exhaustive hyperparameter searches by providing practical guidance for selecting optimal annealing strategies. A key finding is that smaller models can reliably proxy larger models when tuning training dynamics, increasing efficiency and reducing computational costs. The study’s conclusions are supported by extensive experiments conducted on both Dense and Mixture-of-Experts (MoE) architectures. These experiments reveal that optimal annealing ratios exhibit consistent patterns transferable across different training setups. This transferability enables more generalized and scalable training strategies for diverse large language model architectures. <div>
arXiv:2512.13705v1 Announce Type: cross 
Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints</title>
<link>https://arxiv.org/abs/2512.13717</link>
<guid>https://arxiv.org/abs/2512.13717</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, few-shot learning, EEG, seizure detection, personalized model<br /><br />Summary:<br /><br />This article addresses the challenge of developing AI models for EEG-based seizure detection in scenarios where data are scarce, distributed across multiple institutions, and privacy-restricted. The authors propose a two-stage federated few-shot learning (FFSL) framework designed to create personalized seizure detection models without centralizing patient EEG data. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned via federated learning across simulated hospital sites with non-IID EEG data, enabling the learning of shared EEG representations while preserving privacy. In Stage 2, a federated few-shot personalization approach adapts the classifier individually for each patient using only five labeled EEG segments, achieving patient-specific tuning while leveraging cross-site knowledge. The framework is trained and evaluated on the TUH Event Corpus, which contains six types of EEG events. Federated fine-tuning in Stage 1 achieved balanced accuracy of 0.43 compared to 0.52 for centralized training, demonstrating some performance trade-off due to decentralization. However, Stage 2 personalization significantly improved results, reaching an average balanced accuracy of 0.77, Cohen’s kappa of 0.62, and weighted F1 of 0.73 across heterogeneous sites. Overall, the FFSL approach shows promise for practical seizure detection under realistic clinical constraints on data availability and privacy. <div>
arXiv:2512.13717v1 Announce Type: cross 
Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs</title>
<link>https://arxiv.org/abs/2512.13723</link>
<guid>https://arxiv.org/abs/2512.13723</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ethical bias, moral alignment, China, USA  

<br /><br />Summary:  
1. This study investigates the ethical and cultural alignment of large language models (LLMs) produced in China and the USA by comparing their responses to moral and value-based surveys with those of people from the respective countries.  
2. The researchers used the Moral Foundations Questionnaire 2.0 and the World Values Survey to elicit responses from ten Chinese LLMs and ten American LLMs.  
3. These responses were then compared to survey data collected from thousands of Chinese and American individuals.  
4. Results show all models, regardless of origin, tend to align more closely with American cultural and moral values than with Chinese values.  
5. Attempts to reduce this American-aligned bias, such as prompting models in Chinese or simulating a Chinese persona, only marginally improved alignment with Chinese respondents.  
6. The study highlights a significant ethical bias in current LLMs toward Western, particularly American, values, raising concerns about the normative influence these models might wield in geopolitics and information dissemination as they increasingly mediate knowledge and decision-making globally. <div>
arXiv:2512.13723v1 Announce Type: cross 
Abstract: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph AI generates neurological hypotheses validated in molecular, organoid, and clinical systems</title>
<link>https://arxiv.org/abs/2512.13724</link>
<guid>https://arxiv.org/abs/2512.13724</guid>
<content:encoded><![CDATA[
<div> Neurological diseases, PROTON, graph transformer, Parkinson's disease, Alzheimer's disease<br /><br />Summary:<br /><br />1. PROTON is a heterogeneous graph transformer designed to generate testable hypotheses across molecular, organoid, and clinical systems to address neurological diseases, a leading global cause of disability with few disease-modifying treatments.<br />2. Applied to Parkinson's disease (PD), PROTON linked genetic risk loci to genes vital for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, notably ranking the insecticide endosulfan in the top 1.29% of toxic predictions.<br />3. PROTON successfully replicated six genome-wide α-synuclein experimental results through in silico screening, including data from split-ubiquitin yeast two-hybrid, ascorbate peroxidase proximity labeling assays, and a targeted exome sequencing study on 496 synucleinopathy patients, all showing significant normalized enrichment scores with extremely low false discovery rates.<br />4. In bipolar disorder (BD), PROTON predicted the drug calcitriol could reverse proteomic changes found in cortical organoids derived from BD patients.<br />5. For Alzheimer's disease (AD), PROTON's predictions were validated in health records from over 610,000 patients, confirming five predicted drugs were associated with reduced seven-year dementia risk, with hazard ratios as low as 0.63 and highly significant p-values.<br />6. Overall, PROTON demonstrates a novel AI-driven pathway for discovering and validating potential therapies across various neurological diseases at multiple biological levels. <div>
arXiv:2512.13724v1 Announce Type: cross 
Abstract: Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $\alpha$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce</title>
<link>https://arxiv.org/abs/2512.13726</link>
<guid>https://arxiv.org/abs/2512.13726</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, time-constrained recommendation, slate optimization, user engagement, Alibaba dataset<br /><br />Summary:<br /><br />1. This paper addresses recommendation systems under finite user time budgets, emphasizing the critical challenge of balancing item relevance with evaluation cost. Unlike traditional recommendation tasks, users have limited time to assess recommendations, especially in mobile and scrolling interfaces where each scroll shows a slate of items. 2. Users expend evaluation time assessing item features before clicking, meaning highly relevant items with greater evaluation costs may not fit within the user's limited time, potentially reducing engagement. 3. The authors propose a unified framework modeling time-constrained slate recommendation as a Markov Decision Process (MDP) incorporating budget-aware utilities, capturing the trade-off between relevance and evaluation cost. 4. To study recommendation policies, they build a simulation framework leveraging Alibaba’s Personalized Re-ranking dataset tailored for e-commerce slate optimization, enabling experimentation with reinforcement learning algorithms. 5. Empirical results demonstrate that both on-policy and off-policy reinforcement learning control methods outperform traditional contextual bandit algorithms, particularly under tight time budgets, improving user engagement by better aligning recommendations with users’ time constraints and preferences. <div>
arXiv:2512.13726v1 Announce Type: cross 
Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CurvaDion: Curvature-Adaptive Distributed Orthonormalization</title>
<link>https://arxiv.org/abs/2512.13728</link>
<guid>https://arxiv.org/abs/2512.13728</guid>
<content:encoded><![CDATA[
<div> Gradient Synchronization, Distributed Training, Curvature Detection, Momentum Dynamics, Communication Reduction<br /><br />Summary:<br /><br />1. Large-scale language models with trillions of parameters require distributed training across many GPUs, but frequent gradient synchronization over networks creates a communication bottleneck.<br />2. Existing solutions like Dion reduce communication frequency by using low-rank gradient updates but still synchronize every training step regardless of the underlying optimization dynamics.<br />3. The authors observe that synchronization needs vary during training: flat regions yield similar gradients across workers, making synchronization often unnecessary, while high-curvature regions require tight coordination to avoid model divergence.<br />4. They propose CurvaDion, a method that uses a novel metric called Relative Maximum Momentum Change (RMMC) to detect high-curvature phases of training, triggering synchronization only when needed.<br />5. RMMC utilizes momentum values already computed during optimization, serving as a low-overhead, computationally efficient proxy for curvature with only O(d) extra operations per layer.<br />6. Theoretical connections between RMMC and the loss curvature are established, validating its effectiveness.<br />7. Experiments show that CurvaDion can reduce communication overhead by 99% while maintaining convergence performance comparable to baseline methods across models ranging from 160 million to 1.3 billion parameters. <div>
arXiv:2512.13728v1 Announce Type: cross 
Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution</title>
<link>https://arxiv.org/abs/2512.13729</link>
<guid>https://arxiv.org/abs/2512.13729</guid>
<content:encoded><![CDATA[
<div> Keywords: wind super-resolution, diffusion models, classifier-free guidance, composite classifier-free guidance, WindDM<br /><br />Summary:<br /><br />1. The paper addresses the challenge of obtaining high-resolution, accurate wind data essential for weather modeling tasks such as forecasting and turbine placement optimization, noting that traditional methods are either costly or inaccurate.<br />2. It highlights the potential of deep learning methods, particularly diffusion models, to overcome this cost-accuracy trade-off by adapting techniques from natural image super-resolution.<br />3. The authors point out the distinct nature of wind data compared to natural images, emphasizing that wind super-resolution requires many more conditioning channels (10+) than typical RGB images (3 channels).<br />4. To better handle multiple conditioning inputs in diffusion models, they propose a novel generalization of classifier-free guidance called composite classifier-free guidance (CCFG), which can be integrated into any pretrained diffusion model that uses standard CFG dropout.<br />5. Experimental results demonstrate that CCFG produces higher fidelity outputs than traditional CFG on wind super-resolution tasks.<br />6. They introduce WindDM, a diffusion model employing CCFG, trained for industrial-scale reconstruction of wind dynamics.<br />7. WindDM achieves state-of-the-art reconstruction quality among deep learning approaches and is up to 1000 times more cost-effective than classical methods, making it highly practical for industrial applications. <div>
arXiv:2512.13729v1 Announce Type: cross 
Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Modular Integration of "AI + Architecture" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University</title>
<link>https://arxiv.org/abs/2512.13730</link>
<guid>https://arxiv.org/abs/2512.13730</guid>
<content:encoded><![CDATA[
<div> AI integration, architectural education, ethics discussions, deep learning models, design studio<br /><br />Summary: This study explores the integration of artificial intelligence (AI) into architectural education through an experimental teaching approach at Zhejiang University involving grade three undergraduate design students during the 2024-25 academic year. It employs a dual-module framework combining a 20-hour AI technical training course with integrated ethics discussions, introducing students to deep learning models, large language models (LLMs), AI-generated content (AIGC), Low-Rank Adaptation (LoRA), and ComfyUI, while preserving the original curriculum structure. Dedicated technical instructors provided support throughout the process, ensuring effective delivery and student engagement. The findings highlight the effectiveness of phased guidance that gradually builds technical skills alongside a balanced focus on ethical considerations of AI use in design. Institutional support played a key role in the successful implementation of the model. The course enhanced students’ digital competencies as well as their strategic understanding of AI’s role in architectural practice. Additionally, the inclusion of ethics discussions fostered critical thinking about the societal and professional implications of AI technology. This integrated approach offers a replicable model for combining both technical and critical learning components within design education, preparing students to responsibly leverage AI in their future architectural careers. <div>
arXiv:2512.13730v1 Announce Type: cross 
Abstract: This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline</title>
<link>https://arxiv.org/abs/2512.13731</link>
<guid>https://arxiv.org/abs/2512.13731</guid>
<content:encoded><![CDATA[
arXiv:2512.13731v1 Announce Type: cross 
Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion</title>
<link>https://arxiv.org/abs/2512.13732</link>
<guid>https://arxiv.org/abs/2512.13732</guid>
<content:encoded><![CDATA[
arXiv:2512.13732v1 Announce Type: cross 
Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Compression of Language Models via Differentiable Rank Selection</title>
<link>https://arxiv.org/abs/2512.13733</link>
<guid>https://arxiv.org/abs/2512.13733</guid>
<content:encoded><![CDATA[
arXiv:2512.13733v1 Announce Type: cross 
Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation</title>
<link>https://arxiv.org/abs/2512.13734</link>
<guid>https://arxiv.org/abs/2512.13734</guid>
<content:encoded><![CDATA[
arXiv:2512.13734v1 Announce Type: cross 
Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series</title>
<link>https://arxiv.org/abs/2512.13735</link>
<guid>https://arxiv.org/abs/2512.13735</guid>
<content:encoded><![CDATA[
arXiv:2512.13735v1 Announce Type: cross 
Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection</title>
<link>https://arxiv.org/abs/2512.13736</link>
<guid>https://arxiv.org/abs/2512.13736</guid>
<content:encoded><![CDATA[
arXiv:2512.13736v1 Announce Type: cross 
Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instilling Organisational Values in Firefighters through Simulation-Based Training</title>
<link>https://arxiv.org/abs/2512.13737</link>
<guid>https://arxiv.org/abs/2512.13737</guid>
<content:encoded><![CDATA[
arXiv:2512.13737v1 Announce Type: cross 
Abstract: In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title>
<link>https://arxiv.org/abs/2512.13739</link>
<guid>https://arxiv.org/abs/2512.13739</guid>
<content:encoded><![CDATA[
arXiv:2512.13739v1 Announce Type: cross 
Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13741</link>
<guid>https://arxiv.org/abs/2512.13741</guid>
<content:encoded><![CDATA[
arXiv:2512.13741v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2512.13742</link>
<guid>https://arxiv.org/abs/2512.13742</guid>
<content:encoded><![CDATA[
arXiv:2512.13742v1 Announce Type: cross 
Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes</title>
<link>https://arxiv.org/abs/2512.13744</link>
<guid>https://arxiv.org/abs/2512.13744</guid>
<content:encoded><![CDATA[
arXiv:2512.13744v1 Announce Type: cross 
Abstract: Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction</title>
<link>https://arxiv.org/abs/2512.13745</link>
<guid>https://arxiv.org/abs/2512.13745</guid>
<content:encoded><![CDATA[
arXiv:2512.13745v1 Announce Type: cross 
Abstract: We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making</title>
<link>https://arxiv.org/abs/2512.13747</link>
<guid>https://arxiv.org/abs/2512.13747</guid>
<content:encoded><![CDATA[
arXiv:2512.13747v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.13749</link>
<guid>https://arxiv.org/abs/2512.13749</guid>
<content:encoded><![CDATA[
arXiv:2512.13749v1 Announce Type: cross 
Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs</title>
<link>https://arxiv.org/abs/2512.13750</link>
<guid>https://arxiv.org/abs/2512.13750</guid>
<content:encoded><![CDATA[
arXiv:2512.13750v1 Announce Type: cross 
Abstract: Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDUS: Memory-Infused Depth Up-Scaling</title>
<link>https://arxiv.org/abs/2512.13751</link>
<guid>https://arxiv.org/abs/2512.13751</guid>
<content:encoded><![CDATA[
arXiv:2512.13751v1 Announce Type: cross 
Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning</title>
<link>https://arxiv.org/abs/2512.13752</link>
<guid>https://arxiv.org/abs/2512.13752</guid>
<content:encoded><![CDATA[
arXiv:2512.13752v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention</title>
<link>https://arxiv.org/abs/2512.13758</link>
<guid>https://arxiv.org/abs/2512.13758</guid>
<content:encoded><![CDATA[
arXiv:2512.13758v1 Announce Type: cross 
Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models</title>
<link>https://arxiv.org/abs/2512.13765</link>
<guid>https://arxiv.org/abs/2512.13765</guid>
<content:encoded><![CDATA[
arXiv:2512.13765v1 Announce Type: cross 
Abstract: The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance</title>
<link>https://arxiv.org/abs/2512.13768</link>
<guid>https://arxiv.org/abs/2512.13768</guid>
<content:encoded><![CDATA[
arXiv:2512.13768v1 Announce Type: cross 
Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13806</link>
<guid>https://arxiv.org/abs/2512.13806</guid>
<content:encoded><![CDATA[
arXiv:2512.13806v1 Announce Type: cross 
Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title>
<link>https://arxiv.org/abs/2512.13834</link>
<guid>https://arxiv.org/abs/2512.13834</guid>
<content:encoded><![CDATA[
arXiv:2512.13834v1 Announce Type: cross 
Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging</title>
<link>https://arxiv.org/abs/2512.13855</link>
<guid>https://arxiv.org/abs/2512.13855</guid>
<content:encoded><![CDATA[
arXiv:2512.13855v1 Announce Type: cross 
Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors</title>
<link>https://arxiv.org/abs/2512.13860</link>
<guid>https://arxiv.org/abs/2512.13860</guid>
<content:encoded><![CDATA[
arXiv:2512.13860v1 Announce Type: cross 
Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization</title>
<link>https://arxiv.org/abs/2512.13880</link>
<guid>https://arxiv.org/abs/2512.13880</guid>
<content:encoded><![CDATA[
arXiv:2512.13880v1 Announce Type: cross 
Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction</title>
<link>https://arxiv.org/abs/2512.13886</link>
<guid>https://arxiv.org/abs/2512.13886</guid>
<content:encoded><![CDATA[
arXiv:2512.13886v1 Announce Type: cross 
Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title>
<link>https://arxiv.org/abs/2512.13892</link>
<guid>https://arxiv.org/abs/2512.13892</guid>
<content:encoded><![CDATA[
arXiv:2512.13892v1 Announce Type: cross 
Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing</title>
<link>https://arxiv.org/abs/2512.13904</link>
<guid>https://arxiv.org/abs/2512.13904</guid>
<content:encoded><![CDATA[
arXiv:2512.13904v1 Announce Type: cross 
Abstract: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($\tau < 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing High-Risk Systems: An EU AI Act Verification Framework</title>
<link>https://arxiv.org/abs/2512.13907</link>
<guid>https://arxiv.org/abs/2512.13907</guid>
<content:encoded><![CDATA[
arXiv:2512.13907v1 Announce Type: cross 
Abstract: A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America</title>
<link>https://arxiv.org/abs/2512.13910</link>
<guid>https://arxiv.org/abs/2512.13910</guid>
<content:encoded><![CDATA[
arXiv:2512.13910v1 Announce Type: cross 
Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent matter consisting of active particles</title>
<link>https://arxiv.org/abs/2512.13912</link>
<guid>https://arxiv.org/abs/2512.13912</guid>
<content:encoded><![CDATA[
arXiv:2512.13912v1 Announce Type: cross 
Abstract: In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of "intelligent matter" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming</title>
<link>https://arxiv.org/abs/2512.13914</link>
<guid>https://arxiv.org/abs/2512.13914</guid>
<content:encoded><![CDATA[
arXiv:2512.13914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</title>
<link>https://arxiv.org/abs/2512.13930</link>
<guid>https://arxiv.org/abs/2512.13930</guid>
<content:encoded><![CDATA[
arXiv:2512.13930v1 Announce Type: cross 
Abstract: Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informing Acquisition Functions via Foundation Models for Molecular Discovery</title>
<link>https://arxiv.org/abs/2512.13935</link>
<guid>https://arxiv.org/abs/2512.13935</guid>
<content:encoded><![CDATA[
arXiv:2512.13935v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling</title>
<link>https://arxiv.org/abs/2512.13956</link>
<guid>https://arxiv.org/abs/2512.13956</guid>
<content:encoded><![CDATA[
arXiv:2512.13956v1 Announce Type: cross 
Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition</title>
<link>https://arxiv.org/abs/2512.13998</link>
<guid>https://arxiv.org/abs/2512.13998</guid>
<content:encoded><![CDATA[
arXiv:2512.13998v1 Announce Type: cross 
Abstract: Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025</title>
<link>https://arxiv.org/abs/2512.14012</link>
<guid>https://arxiv.org/abs/2512.14012</guid>
<content:encoded><![CDATA[
arXiv:2512.14012v1 Announce Type: cross 
Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.14017</link>
<guid>https://arxiv.org/abs/2512.14017</guid>
<content:encoded><![CDATA[
arXiv:2512.14017v1 Announce Type: cross 
Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerfCoder: Large Language Models for Interpretable Code Performance Optimization</title>
<link>https://arxiv.org/abs/2512.14018</link>
<guid>https://arxiv.org/abs/2512.14018</guid>
<content:encoded><![CDATA[
arXiv:2512.14018v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</title>
<link>https://arxiv.org/abs/2512.14031</link>
<guid>https://arxiv.org/abs/2512.14031</guid>
<content:encoded><![CDATA[
arXiv:2512.14031v1 Announce Type: cross 
Abstract: This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</title>
<link>https://arxiv.org/abs/2512.14032</link>
<guid>https://arxiv.org/abs/2512.14032</guid>
<content:encoded><![CDATA[
arXiv:2512.14032v1 Announce Type: cross 
Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14044</link>
<guid>https://arxiv.org/abs/2512.14044</guid>
<content:encoded><![CDATA[
arXiv:2512.14044v1 Announce Type: cross 
Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</title>
<link>https://arxiv.org/abs/2512.14056</link>
<guid>https://arxiv.org/abs/2512.14056</guid>
<content:encoded><![CDATA[
arXiv:2512.14056v1 Announce Type: cross 
Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</title>
<link>https://arxiv.org/abs/2512.14058</link>
<guid>https://arxiv.org/abs/2512.14058</guid>
<content:encoded><![CDATA[
arXiv:2512.14058v1 Announce Type: cross 
Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</title>
<link>https://arxiv.org/abs/2512.14067</link>
<guid>https://arxiv.org/abs/2512.14067</guid>
<content:encoded><![CDATA[
arXiv:2512.14067v1 Announce Type: cross 
Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2512.14068</link>
<guid>https://arxiv.org/abs/2512.14068</guid>
<content:encoded><![CDATA[
arXiv:2512.14068v1 Announce Type: cross 
Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations</title>
<link>https://arxiv.org/abs/2512.14080</link>
<guid>https://arxiv.org/abs/2512.14080</guid>
<content:encoded><![CDATA[
arXiv:2512.14080v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arithmetic-Intensity-Aware Quantization</title>
<link>https://arxiv.org/abs/2512.14090</link>
<guid>https://arxiv.org/abs/2512.14090</guid>
<content:encoded><![CDATA[
arXiv:2512.14090v1 Announce Type: cross 
Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes</title>
<link>https://arxiv.org/abs/2512.14092</link>
<guid>https://arxiv.org/abs/2512.14092</guid>
<content:encoded><![CDATA[
arXiv:2512.14092v1 Announce Type: cross 
Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</title>
<link>https://arxiv.org/abs/2512.14102</link>
<guid>https://arxiv.org/abs/2512.14102</guid>
<content:encoded><![CDATA[
arXiv:2512.14102v1 Announce Type: cross 
Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</title>
<link>https://arxiv.org/abs/2512.14121</link>
<guid>https://arxiv.org/abs/2512.14121</guid>
<content:encoded><![CDATA[
arXiv:2512.14121v1 Announce Type: cross 
Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis</title>
<link>https://arxiv.org/abs/2512.14130</link>
<guid>https://arxiv.org/abs/2512.14130</guid>
<content:encoded><![CDATA[
arXiv:2512.14130v1 Announce Type: cross 
Abstract: We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation</title>
<link>https://arxiv.org/abs/2512.14138</link>
<guid>https://arxiv.org/abs/2512.14138</guid>
<content:encoded><![CDATA[
arXiv:2512.14138v1 Announce Type: cross 
Abstract: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models</title>
<link>https://arxiv.org/abs/2512.14141</link>
<guid>https://arxiv.org/abs/2512.14141</guid>
<content:encoded><![CDATA[
arXiv:2512.14141v1 Announce Type: cross 
Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario</title>
<link>https://arxiv.org/abs/2512.14150</link>
<guid>https://arxiv.org/abs/2512.14150</guid>
<content:encoded><![CDATA[
arXiv:2512.14150v1 Announce Type: cross 
Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol</title>
<link>https://arxiv.org/abs/2512.14166</link>
<guid>https://arxiv.org/abs/2512.14166</guid>
<content:encoded><![CDATA[
arXiv:2512.14166v1 Announce Type: cross 
Abstract: The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs</title>
<link>https://arxiv.org/abs/2512.14179</link>
<guid>https://arxiv.org/abs/2512.14179</guid>
<content:encoded><![CDATA[
arXiv:2512.14179v1 Announce Type: cross 
Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization</title>
<link>https://arxiv.org/abs/2512.14181</link>
<guid>https://arxiv.org/abs/2512.14181</guid>
<content:encoded><![CDATA[
arXiv:2512.14181v1 Announce Type: cross 
Abstract: Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach</title>
<link>https://arxiv.org/abs/2512.14185</link>
<guid>https://arxiv.org/abs/2512.14185</guid>
<content:encoded><![CDATA[
arXiv:2512.14185v1 Announce Type: cross 
Abstract: The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Improving Hyperbolic Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14202</link>
<guid>https://arxiv.org/abs/2512.14202</guid>
<content:encoded><![CDATA[
arXiv:2512.14202v1 Announce Type: cross 
Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar\'e Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging</title>
<link>https://arxiv.org/abs/2512.14211</link>
<guid>https://arxiv.org/abs/2512.14211</guid>
<content:encoded><![CDATA[
arXiv:2512.14211v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating problem difficulty without ground truth using Large Language Model comparisons</title>
<link>https://arxiv.org/abs/2512.14220</link>
<guid>https://arxiv.org/abs/2512.14220</guid>
<content:encoded><![CDATA[
arXiv:2512.14220v1 Announce Type: cross 
Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design</title>
<link>https://arxiv.org/abs/2512.14233</link>
<guid>https://arxiv.org/abs/2512.14233</guid>
<content:encoded><![CDATA[
arXiv:2512.14233v1 Announce Type: cross 
Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning</title>
<link>https://arxiv.org/abs/2512.14241</link>
<guid>https://arxiv.org/abs/2512.14241</guid>
<content:encoded><![CDATA[
arXiv:2512.14241v1 Announce Type: cross 
Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition</title>
<link>https://arxiv.org/abs/2512.14244</link>
<guid>https://arxiv.org/abs/2512.14244</guid>
<content:encoded><![CDATA[
arXiv:2512.14244v1 Announce Type: cross 
Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.14263</link>
<guid>https://arxiv.org/abs/2512.14263</guid>
<content:encoded><![CDATA[
arXiv:2512.14263v1 Announce Type: cross 
Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions</title>
<link>https://arxiv.org/abs/2512.14277</link>
<guid>https://arxiv.org/abs/2512.14277</guid>
<content:encoded><![CDATA[
arXiv:2512.14277v1 Announce Type: cross 
Abstract: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study</title>
<link>https://arxiv.org/abs/2512.14278</link>
<guid>https://arxiv.org/abs/2512.14278</guid>
<content:encoded><![CDATA[
arXiv:2512.14278v1 Announce Type: cross 
Abstract: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high ({\alpha}=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability ({\alpha}=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks</title>
<link>https://arxiv.org/abs/2512.14297</link>
<guid>https://arxiv.org/abs/2512.14297</guid>
<content:encoded><![CDATA[
arXiv:2512.14297v1 Announce Type: cross 
Abstract: Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.
  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.
  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.
  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region</title>
<link>https://arxiv.org/abs/2512.14312</link>
<guid>https://arxiv.org/abs/2512.14312</guid>
<content:encoded><![CDATA[
arXiv:2512.14312v1 Announce Type: cross 
Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title>
<link>https://arxiv.org/abs/2512.14320</link>
<guid>https://arxiv.org/abs/2512.14320</guid>
<content:encoded><![CDATA[
arXiv:2512.14320v1 Announce Type: cross 
Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data</title>
<link>https://arxiv.org/abs/2512.14329</link>
<guid>https://arxiv.org/abs/2512.14329</guid>
<content:encoded><![CDATA[
arXiv:2512.14329v1 Announce Type: cross 
Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.14330</link>
<guid>https://arxiv.org/abs/2512.14330</guid>
<content:encoded><![CDATA[
arXiv:2512.14330v1 Announce Type: cross 
Abstract: AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring</title>
<link>https://arxiv.org/abs/2512.14332</link>
<guid>https://arxiv.org/abs/2512.14332</guid>
<content:encoded><![CDATA[
arXiv:2512.14332v1 Announce Type: cross 
Abstract: The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Attention Guided Defense Against Malicious Edits</title>
<link>https://arxiv.org/abs/2512.14333</link>
<guid>https://arxiv.org/abs/2512.14333</guid>
<content:encoded><![CDATA[
arXiv:2512.14333v1 Announce Type: cross 
Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transferable Defense Against Malicious Image Edits</title>
<link>https://arxiv.org/abs/2512.14341</link>
<guid>https://arxiv.org/abs/2512.14341</guid>
<content:encoded><![CDATA[
arXiv:2512.14341v1 Announce Type: cross 
Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability for Vision Models via Shapley Value Optimization</title>
<link>https://arxiv.org/abs/2512.14354</link>
<guid>https://arxiv.org/abs/2512.14354</guid>
<content:encoded><![CDATA[
arXiv:2512.14354v1 Announce Type: cross 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis</title>
<link>https://arxiv.org/abs/2512.14361</link>
<guid>https://arxiv.org/abs/2512.14361</guid>
<content:encoded><![CDATA[
arXiv:2512.14361v1 Announce Type: cross 
Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePo: Language Models with Context Re-Positioning</title>
<link>https://arxiv.org/abs/2512.14391</link>
<guid>https://arxiv.org/abs/2512.14391</guid>
<content:encoded><![CDATA[
arXiv:2512.14391v1 Announce Type: cross 
Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_\phi$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning</title>
<link>https://arxiv.org/abs/2512.14420</link>
<guid>https://arxiv.org/abs/2512.14420</guid>
<content:encoded><![CDATA[
arXiv:2512.14420v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14427</link>
<guid>https://arxiv.org/abs/2512.14427</guid>
<content:encoded><![CDATA[
arXiv:2512.14427v1 Announce Type: cross 
Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space</title>
<link>https://arxiv.org/abs/2512.14448</link>
<guid>https://arxiv.org/abs/2512.14448</guid>
<content:encoded><![CDATA[
arXiv:2512.14448v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically "analysis paralysis" or "cognitive haste"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels</title>
<link>https://arxiv.org/abs/2512.14477</link>
<guid>https://arxiv.org/abs/2512.14477</guid>
<content:encoded><![CDATA[
arXiv:2512.14477v1 Announce Type: cross 
Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models</title>
<link>https://arxiv.org/abs/2512.14481</link>
<guid>https://arxiv.org/abs/2512.14481</guid>
<content:encoded><![CDATA[
arXiv:2512.14481v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2512.14540</link>
<guid>https://arxiv.org/abs/2512.14540</guid>
<content:encoded><![CDATA[
arXiv:2512.14540v1 Announce Type: cross 
Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Language Models: Balancing Training Efficiency and Overfitting Resilience</title>
<link>https://arxiv.org/abs/2512.14549</link>
<guid>https://arxiv.org/abs/2512.14549</guid>
<content:encoded><![CDATA[
arXiv:2512.14549v1 Announce Type: cross 
Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14554</link>
<guid>https://arxiv.org/abs/2512.14554</guid>
<content:encoded><![CDATA[
arXiv:2512.14554v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer</title>
<link>https://arxiv.org/abs/2512.14560</link>
<guid>https://arxiv.org/abs/2512.14560</guid>
<content:encoded><![CDATA[
arXiv:2512.14560v1 Announce Type: cross 
Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polypersona: Persona-Grounded LLM for Synthetic Survey Responses</title>
<link>https://arxiv.org/abs/2512.14562</link>
<guid>https://arxiv.org/abs/2512.14562</guid>
<content:encoded><![CDATA[
arXiv:2512.14562v1 Announce Type: cross 
Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2512.14563</link>
<guid>https://arxiv.org/abs/2512.14563</guid>
<content:encoded><![CDATA[
arXiv:2512.14563v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies</title>
<link>https://arxiv.org/abs/2512.14576</link>
<guid>https://arxiv.org/abs/2512.14576</guid>
<content:encoded><![CDATA[
arXiv:2512.14576v1 Announce Type: cross 
Abstract: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer</title>
<link>https://arxiv.org/abs/2512.14585</link>
<guid>https://arxiv.org/abs/2512.14585</guid>
<content:encoded><![CDATA[
arXiv:2512.14585v1 Announce Type: cross 
Abstract: Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title>
<link>https://arxiv.org/abs/2512.14601</link>
<guid>https://arxiv.org/abs/2512.14601</guid>
<content:encoded><![CDATA[
arXiv:2512.14601v1 Announce Type: cross 
Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes</title>
<link>https://arxiv.org/abs/2512.14617</link>
<guid>https://arxiv.org/abs/2512.14617</guid>
<content:encoded><![CDATA[
arXiv:2512.14617v1 Announce Type: cross 
Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</title>
<link>https://arxiv.org/abs/2512.14620</link>
<guid>https://arxiv.org/abs/2512.14620</guid>
<content:encoded><![CDATA[
arXiv:2512.14620v1 Announce Type: cross 
Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation</title>
<link>https://arxiv.org/abs/2512.14629</link>
<guid>https://arxiv.org/abs/2512.14629</guid>
<content:encoded><![CDATA[
arXiv:2512.14629v1 Announce Type: cross 
Abstract: Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.14640</link>
<guid>https://arxiv.org/abs/2512.14640</guid>
<content:encoded><![CDATA[
arXiv:2512.14640v1 Announce Type: cross 
Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation</title>
<link>https://arxiv.org/abs/2512.14658</link>
<guid>https://arxiv.org/abs/2512.14658</guid>
<content:encoded><![CDATA[
arXiv:2512.14658v1 Announce Type: cross 
Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$\Delta$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</title>
<link>https://arxiv.org/abs/2512.14677</link>
<guid>https://arxiv.org/abs/2512.14677</guid>
<content:encoded><![CDATA[
arXiv:2512.14677v1 Announce Type: cross 
Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean</title>
<link>https://arxiv.org/abs/2512.14686</link>
<guid>https://arxiv.org/abs/2512.14686</guid>
<content:encoded><![CDATA[
arXiv:2512.14686v1 Announce Type: cross 
Abstract: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $\alpha$ of the noise. Nonetheless, existing complexity results often cover only the case $\alpha \in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $\alpha$ approaches $1$. This paper tackles the general case of noise with tail index $\alpha\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $\alpha \in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization</title>
<link>https://arxiv.org/abs/2512.14687</link>
<guid>https://arxiv.org/abs/2512.14687</guid>
<content:encoded><![CDATA[
arXiv:2512.14687v1 Announce Type: cross 
Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native and Compact Structured Latents for 3D Generation</title>
<link>https://arxiv.org/abs/2512.14692</link>
<guid>https://arxiv.org/abs/2512.14692</guid>
<content:encoded><![CDATA[
arXiv:2512.14692v1 Announce Type: cross 
Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Leech Quantization for Visual Tokenization and Generation</title>
<link>https://arxiv.org/abs/2512.14697</link>
<guid>https://arxiv.org/abs/2512.14697</guid>
<content:encoded><![CDATA[
arXiv:2512.14697v1 Announce Type: cross 
Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\Lambda_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2512.14698</link>
<guid>https://arxiv.org/abs/2512.14698</guid>
<content:encoded><![CDATA[
arXiv:2512.14698v1 Announce Type: cross 
Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Reinforcement Learning for Building Energy Management System</title>
<link>https://arxiv.org/abs/2210.12590</link>
<guid>https://arxiv.org/abs/2210.12590</guid>
<content:encoded><![CDATA[
arXiv:2210.12590v3 Announce Type: replace 
Abstract: The building sector is one of the largest contributors to global energy consumption. Improving its energy efficiency is essential for reducing operational costs and greenhouse gas emissions. Energy management systems (EMS) play a key role in monitoring and controlling building appliances efficiently and reliably. With the increasing integration of renewable energy, intelligent EMS solutions have received growing attention. Reinforcement learning (RL) has recently been explored for this purpose and shows strong potential. However, most RL-based EMS methods require a large number of training steps to learn effective control policies, especially when adapting to unseen buildings, which limits their practical deployment. This paper introduces MetaEMS, a meta-reinforcement learning framework for EMS. MetaEMS improves learning efficiency by transferring knowledge from previously solved tasks to new ones through group-level and building-level adaptation, enabling fast adaptation and effective control across diverse building environments. Experimental results demonstrate that MetaEMS adapts more rapidly to unseen buildings and consistently outperforms baseline methods across various scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMMA: A Communicative Multimodal Multi-Agent Benchmark</title>
<link>https://arxiv.org/abs/2410.07553</link>
<guid>https://arxiv.org/abs/2410.07553</guid>
<content:encoded><![CDATA[
arXiv:2410.07553v5 Announce Type: replace 
Abstract: The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Large Language Models for ESG Activity Detection in Financial Texts</title>
<link>https://arxiv.org/abs/2502.21112</link>
<guid>https://arxiv.org/abs/2502.21112</guid>
<content:encoded><![CDATA[
arXiv:2502.21112v2 Announce Type: replace 
Abstract: The integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect of sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent challenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with specific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose Large Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we investigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate that their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated data. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labelled text segments classified according to the EU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy, with open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These findings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency and compliance through advanced natural language processing techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2504.15610</link>
<guid>https://arxiv.org/abs/2504.15610</guid>
<content:encoded><![CDATA[
arXiv:2504.15610v3 Announce Type: replace 
Abstract: The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this study's purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights & Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Large Language Models for Clinical Research Using One Million Clinical Trials</title>
<link>https://arxiv.org/abs/2505.16097</link>
<guid>https://arxiv.org/abs/2505.16097</guid>
<content:encoded><![CDATA[
arXiv:2505.16097v2 Announce Type: replace 
Abstract: Developing artificial intelligence (AI) for clinical research requires a comprehensive data foundation that supports model training and rigorous evaluation. Here, we introduce TrialPanorama, a large-scale structured resource that aggregates 1.6M clinical trial records from fifteen global registries and links them with biomedical ontologies and associated literature. To demonstrate its utility, we build a pipeline that constructs 152K training and testing samples for eight key clinical research tasks. Three tasks support systematic review workflows, including study search, study screening, and evidence summarization. Five tasks focus on trial design and optimization, including arm design, eligibility criteria design, endpoint selection, sample size estimation, and trial completion assessment and rationalization. Benchmarking cutting-edge large language models (LLMs) reveals that generic LLMs have limited capability in clinical reasoning. In contrast, an 8B LLM we developed on TrialPanorama using supervised finetuning and reinforcement learning wins over the 70B generic counterparts in all eight tasks, with a relative improvement of 73.7%, 67.6%, 38.4%, 37.8%, 26.5%, 20.7%, 20.0%, 18.1%, and 5.2%, respectively. We envision that TrialPanorama provides a solid foundation for future scaling of AI for clinical research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Scaling in Test-Time Compute</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[
arXiv:2507.14417v2 Announce Type: replace 
Abstract: We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent</title>
<link>https://arxiv.org/abs/2508.01031</link>
<guid>https://arxiv.org/abs/2508.01031</guid>
<content:encoded><![CDATA[
arXiv:2508.01031v4 Announce Type: replace 
Abstract: Computer Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both textual descriptions and sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Explicit Context Imperative Paradigm (ECIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Self-Play For Data-Free Training</title>
<link>https://arxiv.org/abs/2509.07414</link>
<guid>https://arxiv.org/abs/2509.07414</guid>
<content:encoded><![CDATA[
arXiv:2509.07414v2 Announce Type: replace 
Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself-a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following, mathematics, and coding benchmarks show that pretrained models can be effectively improved with self-play alone.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning</title>
<link>https://arxiv.org/abs/2509.12875</link>
<guid>https://arxiv.org/abs/2509.12875</guid>
<content:encoded><![CDATA[
arXiv:2509.12875v3 Announce Type: replace 
Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTS-EP: Empowering Embodied Planning with Online Preference Optimization</title>
<link>https://arxiv.org/abs/2509.17116</link>
<guid>https://arxiv.org/abs/2509.17116</guid>
<content:encoded><![CDATA[
arXiv:2509.17116v2 Announce Type: replace 
Abstract: This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine</title>
<link>https://arxiv.org/abs/2509.20935</link>
<guid>https://arxiv.org/abs/2509.20935</guid>
<content:encoded><![CDATA[
arXiv:2509.20935v2 Announce Type: replace 
Abstract: In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement learning guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN and schema-based rule check, enabling process-level supervision without explicit labels. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target discovery in precision medicine.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models</title>
<link>https://arxiv.org/abs/2509.22284</link>
<guid>https://arxiv.org/abs/2509.22284</guid>
<content:encoded><![CDATA[
arXiv:2509.22284v3 Announce Type: replace 
Abstract: Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis</title>
<link>https://arxiv.org/abs/2510.01115</link>
<guid>https://arxiv.org/abs/2510.01115</guid>
<content:encoded><![CDATA[
arXiv:2510.01115v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
<link>https://arxiv.org/abs/2510.14112</link>
<guid>https://arxiv.org/abs/2510.14112</guid>
<content:encoded><![CDATA[
arXiv:2510.14112v2 Announce Type: replace 
Abstract: Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2511.01170</link>
<guid>https://arxiv.org/abs/2511.01170</guid>
<content:encoded><![CDATA[
arXiv:2511.01170v2 Announce Type: replace 
Abstract: Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \textbf{DART}, a supervised \textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Black-Box Tuning of Large Language Models with Limited API Calls</title>
<link>https://arxiv.org/abs/2511.10210</link>
<guid>https://arxiv.org/abs/2511.10210</guid>
<content:encoded><![CDATA[
arXiv:2511.10210v3 Announce Type: replace 
Abstract: Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with "LogitMap Pairs" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Menta: A Small Language Model for On-Device Mental Health Prediction</title>
<link>https://arxiv.org/abs/2512.02716</link>
<guid>https://arxiv.org/abs/2512.02716</guid>
<content:encoded><![CDATA[
arXiv:2512.02716v3 Announce Type: replace 
Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://hong-labs.github.io/menta-project/
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.04359</link>
<guid>https://arxiv.org/abs/2512.04359</guid>
<content:encoded><![CDATA[
arXiv:2512.04359v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models with Native Multi-Agent Intelligence</title>
<link>https://arxiv.org/abs/2512.08743</link>
<guid>https://arxiv.org/abs/2512.08743</guid>
<content:encoded><![CDATA[
arXiv:2512.08743v3 Announce Type: replace 
Abstract: Foundation models (FMs) are increasingly assuming the role of the ''brain'' of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence, across 41 large language models and 7 challenging benchmarks, showing that scaling single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Emotional Trajectories: A Temporal-Semantic Network Approach for Latent Depression Assessment in Social Media</title>
<link>https://arxiv.org/abs/2305.13127</link>
<guid>https://arxiv.org/abs/2305.13127</guid>
<content:encoded><![CDATA[
arXiv:2305.13127v3 Announce Type: replace-cross 
Abstract: The early identification and intervention of latent depression are of significant societal importance for mental health governance. While current automated detection methods based on social media have shown progress, their decision-making processes often lack a clinically interpretable framework, particularly in capturing the duration and dynamic evolution of depressive symptoms. To address this, this study introduces a semantic parsing network integrated with multi-scale temporal prototype learning. The model detects depressive states by capturing temporal patterns and semantic prototypes in users' emotional expression, providing a duration-aware interpretation of underlying symptoms. Validated on a large-scale social media dataset, the model outperforms existing state-of-the-art methods. Analytical results indicate that the model can identify emotional expression patterns not systematically documented in traditional survey-based approaches, such as sustained narratives expressing admiration for an "alternative life." Further user evaluation demonstrates the model's superior interpretability compared to baseline methods. This research contributes a structurally transparent, clinically aligned framework for depression detection in social media to the information systems literature. In practice, the model can generate dynamic emotional profiles for social platform users, assisting in the targeted allocation of mental health support resources.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title>
<link>https://arxiv.org/abs/2312.04960</link>
<guid>https://arxiv.org/abs/2312.04960</guid>
<content:encoded><![CDATA[
arXiv:2312.04960v5 Announce Type: replace-cross 
Abstract: Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism. Our code and trained models are publicly available at: https://github.com/xiaoyunxxy/MIMIR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Question Answering Over Spatio-Temporal Knowledge Graph</title>
<link>https://arxiv.org/abs/2402.11542</link>
<guid>https://arxiv.org/abs/2402.11542</guid>
<content:encoded><![CDATA[
arXiv:2402.11542v2 Announce Type: replace-cross 
Abstract: Spatio-temporal knowledge graphs (STKGs) enhance traditional KGs by integrating temporal and spatial annotations, enabling precise reasoning over questions with spatio-temporal dependencies. Despite their potential, research on spatio-temporal knowledge graph question answering (STKGQA) remains limited. This is primarily due to the lack of datasets that simultaneously contain spatio-temporal information, as well as methods capable of handling implicit spatio-temporal reasoning. To bridge this gap, we introduce the spatio-temporal question answering dataset (STQAD), the first comprehensive benchmark comprising 10,000 natural language questions that require both temporal and spatial reasoning. STQAD is constructed with real-world facts containing spatio-temporal information, ensuring that the dataset reflects practical scenarios. Furthermore, our experiments reveal that existing KGQA methods underperform on STQAD, primarily due to their inability to model spatio-temporal interactions. To address this, we propose the spatio-temporal complex question answering (STCQA) method, which jointly embeds temporal and spatial features into KG representations and dynamically filters answers through constraint-aware reasoning. STCQA achieves state-of-the-art performance, significantly outperforming existing baselines. Our work not only provides a valuable resource for future research but also advances the field by offering a robust baseline for answering complex spatio-temporal questions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem</title>
<link>https://arxiv.org/abs/2404.11496</link>
<guid>https://arxiv.org/abs/2404.11496</guid>
<content:encoded><![CDATA[
arXiv:2404.11496v3 Announce Type: replace-cross 
Abstract: Diversity optimization is the class of optimization problems in which we aim to find a diverse set of good solutions. One of the frequently-used approaches to solve such problems is to use evolutionary algorithms that evolve a desired diverse population. This approach is called evolutionary diversity optimization (EDO).
  In this paper, we analyze EDO on a three-objective function LOTZ$_k$, which is a modification of the two-objective benchmark function (LeadingOnes, TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations. We also analyze the runtime of the GSEMO$_D$ algorithm (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures: the total imbalance and the sorted imbalances vector. For the first measure we show that the GSEMO$_D$ optimizes it in $O(kn^2\log(n))$ expected iterations (which is asymptotically faster than the upper bound on the runtime until it finds a Pareto-optimal population), and for the second measure we show an upper bound of $O(k^2n^3\log(n))$ expected iterations.
  We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures. The results of experiments suggest that our bounds for the total imbalance measure are tight, while the bounds for the imbalances vector are too pessimistic.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossPT-EEG: A Benchmark for Cross-Participant and Cross-Time Generalization of EEG-based Visual Decoding</title>
<link>https://arxiv.org/abs/2406.07151</link>
<guid>https://arxiv.org/abs/2406.07151</guid>
<content:encoded><![CDATA[
arXiv:2406.07151v2 Announce Type: replace-cross 
Abstract: Exploring brain activity in relation to visual perception provides insights into the biological representation of the world. While functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have enabled effective image classification and reconstruction, their high cost and bulk limit practical use. Electroencephalography (EEG), by contrast, offers low cost and excellent temporal resolution, but its potential has been limited by the scarcity of large, high-quality datasets and by block-design experiments that introduce temporal confounds. To fill this gap, we present CrossPT-EEG, a benchmark for cross-participant and cross-time generalization of visual decoding from EEG. We collected EEG data from 16 participants while they viewed 4,000 images sampled from ImageNet, with image stimuli annotated at multiple levels of granularity. Our design includes two stages separated in time to allow cross-time generalization and avoid block-design artifacts. We also introduce benchmarks tailored to non-block design classification, as well as pre-training experiments to assess cross-time and cross-participant generalization. These findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Long-term RAG Chatbots with Psychological Models of Memory Importance and Forgetting</title>
<link>https://arxiv.org/abs/2409.12524</link>
<guid>https://arxiv.org/abs/2409.12524</guid>
<content:encoded><![CDATA[
arXiv:2409.12524v2 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) has shown promise in enhancing long-term conversations, the increasing memory load as conversations progress degrades retrieval accuracy. Drawing on psychological insights, we propose LUFY, a simple yet effective method that focuses on emotionally arousing memories and retains less than 10% of the conversation. In the user experiment, participants interacted with three types of RAG chatbots, each for 2 hours over 4 sessions, marking the most extensive assessment of a chatbot's long-term capabilities to date -- more than four times longer than any existing benchmark. The results demonstrate that prioritizing arousing memories while forgetting the majority of the conversation significantly enhances user experience. This study pushes the frontier of long-term conversations and highlights the importance of forgetting unimportant parts of conversations. Code and Dataset: https://github.com/ryuichi-sumida/LUFY
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Multi-modal Root Cause Identification in Microservice Systems</title>
<link>https://arxiv.org/abs/2410.10021</link>
<guid>https://arxiv.org/abs/2410.10021</guid>
<content:encoded><![CDATA[
arXiv:2410.10021v2 Announce Type: replace-cross 
Abstract: Root Cause Analysis (RCA) is essential for pinpointing the root causes of failures in microservice systems. Traditional data-driven RCA methods are typically limited to offline applications due to high computational demands, and existing online RCA methods handle only single-modal data, overlooking complex interactions in multi-modal systems. In this paper, we introduce OCEAN, a novel online multi-modal causal structure learning method for root cause localization. OCEAN employs a dilated convolutional neural network to capture long-term temporal dependencies and graph neural networks to learn causal relationships among system entities and key performance indicators. We further design a multi-factor attention mechanism to analyze and reassess the relationships among different metrics and log indicators/attributes for enhanced online causal graph learning. Additionally, a contrastive mutual information maximization-based graph fusion module is developed to effectively model the relationships across various modalities. Extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chase Anonymisation: Privacy-Preserving Knowledge Graphs with Logical Reasoning</title>
<link>https://arxiv.org/abs/2410.12418</link>
<guid>https://arxiv.org/abs/2410.12418</guid>
<content:encoded><![CDATA[
arXiv:2410.12418v2 Announce Type: replace-cross 
Abstract: We propose a novel framework to enable Knowledge Graphs (KGs) sharing while ensuring that information that should remain private is not directly released nor indirectly exposed via derived knowledge, maintaining at the same time the embedded knowledge of the KGs to support business downstream tasks. Our approach produces a privacy-preserving KG as an augmentation of the input one via controlled addition of nodes and edges as well as re-labeling of nodes and perturbation of weights. We introduce a novel privacy measure for KGs, which considers derived knowledge, a new utility metric that captures the business semantics we want to preserve, and propose two novel anonymisation algorithms. Our extensive experimental evaluation, with both synthetic graphs and real-world datasets, confirms the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided multi-property molecular optimization with a diffusion language model</title>
<link>https://arxiv.org/abs/2410.13597</link>
<guid>https://arxiv.org/abs/2410.13597</guid>
<content:encoded><![CDATA[
arXiv:2410.13597v4 Announce Type: replace-cross 
Abstract: Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby mitigating error propagation during diffusion process. By fusing physically and chemically detailed textual semantics with specialized molecular representations, TransDLM effectively integrates diverse information sources to guide precise optimization, which enhances the model's ability to balance structural retention and property enhancement. Additionally, the success of a case study further demonstrates TransDLM's ability to solve practical problems. Experimentally, our approach surpasses state-of-the-art methods in maintaining molecular structural similarity and enhancing chemical properties on the benchmark dataset.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Utility Preference Learning for Listwise Alignment</title>
<link>https://arxiv.org/abs/2410.18127</link>
<guid>https://arxiv.org/abs/2410.18127</guid>
<content:encoded><![CDATA[
arXiv:2410.18127v2 Announce Type: replace-cross 
Abstract: Aligning large language models with human preferences is essential for improving interaction quality and safety by ensuring outputs better reflect human values. A promising strategy involves Reinforcement Learning from Human Feedback (RLHF), starting with collecting and ranking responses generated by a supervised fine-tuning model to refine alignment. Existing methods such as Direct Preference Optimization (DPO) focus on pairwise comparisons, categorizing responses into preferred and less preferred pairs and optimizing pairwise margins. However, this pairwise approach cannot capture the holistic ranking relationships among multiple responses or effectively leverage the rich preference information available in list-wise comparisons. To address this challenge, this paper introduces \underline{D}irect \underline{R}anking \underline{P}reference \underline{O}ptimization (DRPO), a novel method that views human preference alignment as a Learning-to-Rank (LTR) task. Unlike pairwise methods, DRPO optimizes the preference ranking of entire response lists by computing holistic utility scores through NDCG, a standard LTR metric. To enable end-to-end optimization with the non-differentiable NDCG, we propose diffNDCG loss, a differentiable approximation facilitated by a sorting network. Furthermore, we introduce a novel margin-based Adaptive Rank Policy Score to enhance the discriminative quality of generated responses. Extensive experiments have shown that DRPO outperforms existing methods, enhancing the quality of the generated responses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renal Cell Carcinoma subtyping: learning from multi-resolution localization</title>
<link>https://arxiv.org/abs/2411.09471</link>
<guid>https://arxiv.org/abs/2411.09471</guid>
<content:encoded><![CDATA[
arXiv:2411.09471v2 Announce Type: replace-cross 
Abstract: Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study</title>
<link>https://arxiv.org/abs/2411.13602</link>
<guid>https://arxiv.org/abs/2411.13602</guid>
<content:encoded><![CDATA[
arXiv:2411.13602v3 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality, necessitating accessible and accurate diagnostic tools. While cardiac magnetic resonance imaging (CMR) provides gold-standard insights into cardiac structure and function, its clinical utility is limited by high cost and complexity. In contrast, electrocardiography (ECG) is inexpensive and widely available but lacks the granularity of CMR. We propose CardioNets, a deep learning framework that translates 12-lead ECG signals into CMR-level functional parameters and synthetic images, enabling scalable cardiac assessment. CardioNets integrates cross-modal contrastive learning and generative pretraining, aligning ECG with CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via a masked autoregressive model. Trained on 159,819 samples from five cohorts, including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and externally validated on independent clinical datasets (n=3,767), CardioNets achieved strong performance across disease screening and phenotype estimation tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8% and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human physicians using both ECG and real CMR. These results suggest that CardioNets offers a promising, low-cost alternative to CMR for large-scale CVD screening, particularly in resource-limited settings. Future efforts will focus on clinical deployment and regulatory validation of ECG-based synthetic imaging.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepoTransBench: A Real-World Multilingual Benchmark for Repository-Level Code Translation</title>
<link>https://arxiv.org/abs/2412.17744</link>
<guid>https://arxiv.org/abs/2412.17744</guid>
<content:encoded><![CDATA[
arXiv:2412.17744v2 Announce Type: replace-cross 
Abstract: Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world multilingual repository-level code translation benchmark featuring 1,897 real-world repository samples across 13 language pairs with automatically executable test suites. Besides, we introduce RepoTransAgent, a general agent framework to perform repository-level code translation. We evaluate both our benchmark's challenges and agent's effectiveness using several methods and backbone LLMs, revealing that repository-level translation remains challenging, where the best-performing method achieves only a 32.8% success rate. Furthermore, our analysis reveals that translation difficulty varies significantly by language pair direction, with dynamic-to-static language translation being much more challenging than the reverse direction (achieving below 10% vs. static-to-dynamic at 45-63%). Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RepoTransBench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.14704</link>
<guid>https://arxiv.org/abs/2502.14704</guid>
<content:encoded><![CDATA[
arXiv:2502.14704v4 Announce Type: replace-cross 
Abstract: Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-robust Machine Unlearning</title>
<link>https://arxiv.org/abs/2503.09330</link>
<guid>https://arxiv.org/abs/2503.09330</guid>
<content:encoded><![CDATA[
arXiv:2503.09330v2 Announce Type: replace-cross 
Abstract: Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group (e.g., ethnicity, gender), we empirically show that performance for this group degrades, leading to fairness issues. To perform unlearning while preserving fairness, this work addresses the overlooked problem of non-uniformly distributed forget sets, which we refer to as group-robust machine unlearning. We formalize the problem and present a simple and effective exact unlearning strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trace Gadgets: Minimizing Code Context for Machine Learning-Based Vulnerability Prediction</title>
<link>https://arxiv.org/abs/2504.13676</link>
<guid>https://arxiv.org/abs/2504.13676</guid>
<content:encoded><![CDATA[
arXiv:2504.13676v2 Announce Type: replace-cross 
Abstract: As the number of web applications and API endpoints exposed to the Internet continues to grow, so does the number of exploitable vulnerabilities. Manually identifying such vulnerabilities is tedious. Meanwhile, static security scanners tend to produce many false positives. While machine learning-based approaches are promising, they typically perform well only in scenarios where training and test data are closely related. A key challenge for ML-based vulnerability detection is providing suitable and concise code context, as excessively long contexts negatively affect the code comprehension capabilities of machine learning models, particularly smaller ones.
  This work introduces Trace Gadgets, a novel code representation that minimizes code context by removing non-related code. Trace Gadgets precisely capture the statements that cover the path to the vulnerability. As input for ML models, Trace Gadgets provide a minimal but complete context, thereby improving the detection performance. Moreover, we collect a large-scale dataset generated from real-world applications with manually curated labels to further improve the performance of ML-based vulnerability detectors. Our results show that state-of-the-art machine learning models perform best when using Trace Gadgets compared to previous code representations, surpassing the detection capabilities of industry-standard static scanners such as GitHub's CodeQL by at least 4% on a fully unseen dataset. By applying our framework to real-world applications, we identify and report previously unknown vulnerabilities in widely deployed software.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Approximation with Softmax Attention</title>
<link>https://arxiv.org/abs/2504.15956</link>
<guid>https://arxiv.org/abs/2504.15956</guid>
<content:encoded><![CDATA[
arXiv:2504.15956v2 Announce Type: replace-cross 
Abstract: We prove that with linear transformations, both (i) two-layer self-attention and (ii) one-layer self-attention followed by a softmax function are universal approximators for continuous sequence-to-sequence functions on compact domains. Our main technique is a new interpolation-based method for analyzing attention's internal mechanism. This leads to our key insight: self-attention is able to approximate a generalized version of ReLU to arbitrary precision, and hence subsumes many known universal approximators. Building on these, we show that two-layer multi-head attention alone suffices as a sequence-to-sequence universal approximator. In contrast, prior works rely on feed-forward networks to establish universal approximation in Transformers. Furthermore, we extend our techniques to show that, (softmax-)attention-only layers are capable of approximating various statistical models in-context. We believe these techniques hold independent interest.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks</title>
<link>https://arxiv.org/abs/2505.05638</link>
<guid>https://arxiv.org/abs/2505.05638</guid>
<content:encoded><![CDATA[
arXiv:2505.05638v2 Announce Type: replace-cross 
Abstract: Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at https://github.com/aumovio/pred2plan.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[
arXiv:2505.13109v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective</title>
<link>https://arxiv.org/abs/2506.23508</link>
<guid>https://arxiv.org/abs/2506.23508</guid>
<content:encoded><![CDATA[
arXiv:2506.23508v3 Announce Type: replace-cross 
Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on open-source multimodal model, Qwen2.5-VL series. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly but maintains prior knowledge. We study this phenomenon through learning dynamics by examining both the magnitude and direction of how training data influence prior knowledge. Our analysis shows that RFT mainly reinforces correct samples naturally aligned with the base model's probability landscape, leading to weaker interference with prior knowledge. Moreover, training on RFT-simulated rollouts, which exert a small magnitude of influence and are well aligned in direction to prior knowledge, allows SFT to preserve prior knowledge better while rapidly learning new tasks. These findings suggest that distribution of training data, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation</title>
<link>https://arxiv.org/abs/2507.06249</link>
<guid>https://arxiv.org/abs/2507.06249</guid>
<content:encoded><![CDATA[
arXiv:2507.06249v2 Announce Type: replace-cross 
Abstract: Recently, pre-trained models with phonetic supervision have demonstrated their advantages for crosslingual speech recognition in data efficiency and information sharing across languages. However, a limitation is that a pronunciation lexicon is needed for such phoneme-based crosslingual speech recognition. In this study, we aim to eliminate the need for pronunciation lexicons and propose a latent variable model based method, with phonemes being treated as discrete latent variables. The new method consists of a speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model. To jointly train the three models, we utilize the joint stochastic approximation (JSA) algorithm, which is a stochastic extension of the EM (expectation-maximization) algorithm and has demonstrated superior performance particularly in estimating discrete latent variable models. Furthermore, we propose marginal likelihood scoring (MLS) decoding to align inference with the training objective and P2G augmentation to improve the robustness of P2G mapping. Based on the Whistle multilingual pre-trained S2P model, crosslingual experiments are conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of phoneme supervision, the new method, JSA-SPG, achieves 5% error rate reductions compared to the best crosslingual fine-tuning approach using subword or full phoneme supervision. Furthermore, it is found that in language domain adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms the standard practice of language model fusion via the auxiliary support of the G2P model by 9% error rate reductions. To facilitate reproducibility and encourage further exploration in this field, we open-source the JSA-SPG training code and complete pipeline.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems</title>
<link>https://arxiv.org/abs/2507.11064</link>
<guid>https://arxiv.org/abs/2507.11064</guid>
<content:encoded><![CDATA[
arXiv:2507.11064v2 Announce Type: replace-cross 
Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as the growing number of antennas in modern massive MIMO systems substantially increases the channel state information (CSI) feedback demand in frequency division duplex (FDD) systems. To address this, extensive research has focused on CSI compression and prediction, with neural network-based approaches gaining momentum and being considered for integration into the 3GPP 5G-Advanced standards. While deep learning has been effectively applied to CSI-limited beamforming and handover optimization, reference signal allocation under such constraints remains surprisingly underexplored. To fill this gap, we introduce the concept of channel prediction-based reference signal allocation (CPRS), which jointly optimizes channel prediction and DM-RS allocation to improve data throughput without requiring CSI feedback. We further propose a standards-compliant ViViT/CNN-based architecture that implements CPRS by treating evolving CSI matrices as sequential image-like data, enabling efficient and adaptive transmission in dynamic environments. Simulation results using ray-tracing channel data generated in NVIDIA Sionna validate the proposed method, showing up to 36.60% throughput improvement over benchmark strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How K-12 Educators Use AI: LLM-Assisted Qualitative Analysis at Scale</title>
<link>https://arxiv.org/abs/2507.17985</link>
<guid>https://arxiv.org/abs/2507.17985</guid>
<content:encoded><![CDATA[
arXiv:2507.17985v3 Announce Type: replace-cross 
Abstract: This study investigates how K-12 educators use generative AI tools in real-world instructional contexts and how large language models (LLMs) can support scalable qualitative analysis of these interactions. Drawing on over 13,000 unscripted educator-AI conversations from an open-access platform, we examine educators' use of AI for lesson planning, differentiation, assessment, and pedagogical reflection. Methodologically, we introduce a replicable, LLM-assisted qualitative analysis pipeline that supports inductive theme discovery, codebook development, and large-scale annotation while preserving researcher control over conceptual synthesis. Empirically, the findings surface concrete patterns in how educators prompt, adapt, and evaluate AI-generated suggestions as part of their instructional reasoning. This work demonstrates the feasibility of combining LLM support with qualitative rigor to analyze complex educator behaviors at scale and inform the design of AI-powered educational tools.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</title>
<link>https://arxiv.org/abs/2508.00969</link>
<guid>https://arxiv.org/abs/2508.00969</guid>
<content:encoded><![CDATA[
arXiv:2508.00969v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2508.01977</link>
<guid>https://arxiv.org/abs/2508.01977</guid>
<content:encoded><![CDATA[
arXiv:2508.01977v2 Announce Type: replace-cross 
Abstract: To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Embedded Swin-UMamba for DeepLesion Segmentation</title>
<link>https://arxiv.org/abs/2508.06453</link>
<guid>https://arxiv.org/abs/2508.06453</guid>
<content:encoded><![CDATA[
arXiv:2508.06453v2 Announce Type: replace-cross 
Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow has the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, our method achieved a high Dice score of 82.64, and a low Hausdorff distance of 6.34 pixels was obtained for lesion segmentation. The proposed Text-Swin-U/Mamba model outperformed prior approaches: 37.79% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001), and surpassed the purely image-based XLSTM-UNet and nnUNet models by 2.58% and 1.01%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Enhanced Feedback via In-context Neural Error-book</title>
<link>https://arxiv.org/abs/2508.16313</link>
<guid>https://arxiv.org/abs/2508.16313</guid>
<content:encoded><![CDATA[
arXiv:2508.16313v5 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning</title>
<link>https://arxiv.org/abs/2508.18395</link>
<guid>https://arxiv.org/abs/2508.18395</guid>
<content:encoded><![CDATA[
arXiv:2508.18395v2 Announce Type: replace-cross 
Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.
  We introduce \textbf{Latent Self-Consistency (LSC)}, which selects the most semantically consistent response using learnable token embeddings. LSC's lightweight forward processing of summary tokens only introduces negligible runtime overhead (at most $0.9\%$) on top of standard decoding of the base LLM, and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC, and WUCS on both short-form and long-form on average performance, while adding negligible computational overhead on vanilla inference. These results position LSC as a reliable consistency-selection method that works effectively across various answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low expected calibration error across both answer formats.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
<link>https://arxiv.org/abs/2509.14287</link>
<guid>https://arxiv.org/abs/2509.14287</guid>
<content:encoded><![CDATA[
arXiv:2509.14287v2 Announce Type: replace-cross 
Abstract: Biological sequence design (DNA, RNA, or peptides) with desired functional properties has applications in discovering novel nanomaterials, biosensors, antimicrobial drugs, and beyond. One common challenge is the ability to optimize complex high-dimensional properties such as target emission spectra of DNA-mediated fluorescent nanoparticles, photo and chemical stability, and antimicrobial activity of peptides across target microbes. Existing models rely on simple binary labels (e.g., binding/non-binding) rather than high-dimensional complex properties. To address this gap, we propose a geometry-preserving variational autoencoder framework, called PrIVAE, which learns latent sequence embeddings that respect the geometry of their property space. Specifically, we model the property space as a high-dimensional manifold that can be locally approximated by a nearest neighbor graph, given an appropriately defined distance measure. We employ the property graph to guide the sequence latent representations using (1) graph neural network encoder layers and (2) an isometric regularizer. PrIVAE learns a property-organized latent space that enables rational design of new sequences with desired properties by employing the trained decoder. We evaluate the utility of our framework for two generative tasks: (1) design of DNA sequences that template fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The trained models retain high reconstruction accuracy while organizing the latent space according to properties. Beyond in silico experiments, we also employ sampled sequences for wet lab design of DNA nanoclusters, resulting in up to 16.1-fold enrichment of rare-property nanoclusters compared to their abundance in training data, demonstrating the practical utility of our framework.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem</title>
<link>https://arxiv.org/abs/2509.18054</link>
<guid>https://arxiv.org/abs/2509.18054</guid>
<content:encoded><![CDATA[
arXiv:2509.18054v2 Announce Type: replace-cross 
Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an NP-hard optimization problem with multiobjective trade-off, is a complex task that requires deep expert knowledge. The performance of a given algorithm depends on the specific characteristics of the problem, such as the number of facilities, objectives, and constraints. This creates a need for a data-driven recommendation method to guide algorithm selection in automated design systems. This paper introduces a new recommendation method to make this expertise accessible, based on a Knowledge Graph-Based Retrieval-Augmented Generation (KG-RAG) framework. In this framework, a domain-specific knowledge graph (KG) is constructed from the literature. The method then employs a multifaceted retrieval mechanism to gather relevant evidence from this KG using three distinct approaches: precise graph-based search, flexible vector-based search, and cluster-based high-level search. The retrieved evidence is utilized by a Large Language Model (LLM) to generate algorithm recommendations based on data-driven reasoning. This KG-RAG framework is tested on a use case consisting of six problems comprising of complex multi-objective and multi-constraint FLP case. The results are compared with the Gemini 1.5 Flash chatbot. The results show that KG-RAG achieves an average reasoning score of 4.7 out of 5 compared to 3.3 for the baseline chatbot.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning neuroimaging models from health system-scale data</title>
<link>https://arxiv.org/abs/2509.18638</link>
<guid>https://arxiv.org/abs/2509.18638</guid>
<content:encoded><![CDATA[
arXiv:2509.18638v2 Announce Type: replace-cross 
Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima's role in advancing AI-driven healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Gaussian Initializations: Signal Preserving Weight Initialization for Odd-Sigmoid Activations</title>
<link>https://arxiv.org/abs/2509.23085</link>
<guid>https://arxiv.org/abs/2509.23085</guid>
<content:encoded><![CDATA[
arXiv:2509.23085v2 Announce Type: replace-cross 
Abstract: Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, widely used Gaussian i.i.d. initializations are designed to preserve activation variance under wide or infinite width assumptions. In deep and relatively narrow networks with sigmoidal nonlinearities, these schemes often drive preactivations into saturation, and collapse gradients. To address this, we introduce an odd-sigmoid activations and propose an activation aware initialization tailored to any function in this class. Our method remains robust over a wide band of variance scales, preserving both forward signal variance and backpropagated gradient norms even in very deep and narrow networks. Empirically, across standard image benchmarks we find that the proposed initialization is substantially less sensitive to depth, width, and activation scale than Gaussian initializations. In physics informed neural networks (PINNs), scaled odd-sigmoid activations combined with our initialization achieve lower losses than Gaussian based setups, suggesting that diagonal-plus-noise weights provide a practical alternative when Gaussian initialization breaks down.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</title>
<link>https://arxiv.org/abs/2510.10767</link>
<guid>https://arxiv.org/abs/2510.10767</guid>
<content:encoded><![CDATA[
arXiv:2510.10767v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title>
<link>https://arxiv.org/abs/2510.20768</link>
<guid>https://arxiv.org/abs/2510.20768</guid>
<content:encoded><![CDATA[
arXiv:2510.20768v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.25502</link>
<guid>https://arxiv.org/abs/2510.25502</guid>
<content:encoded><![CDATA[
arXiv:2510.25502v3 Announce Type: replace-cross 
Abstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives</title>
<link>https://arxiv.org/abs/2511.08710</link>
<guid>https://arxiv.org/abs/2511.08710</guid>
<content:encoded><![CDATA[
arXiv:2511.08710v2 Announce Type: replace-cross 
Abstract: We develop and analyze a theoretical framework for agent-to-agent interactions in a simplified in-context linear regression setting. In our model, each agent is instantiated as a single-layer transformer with linear self-attention (LSA) trained to implement gradient-descent-like updates on a quadratic regression objective from in-context examples. We then study the coupled dynamics when two such LSA agents alternately update from each other's outputs under potentially misaligned fixed objectives. Within this framework, we characterize the generation dynamics and show that misalignment leads to a biased equilibrium where neither agent reaches its target, with residual errors predictable from the objective gap and the prompt-induced geometry. We further contrast this fixed objective regime with an adaptive multi-agent setting, wherein a helper agent updates a turn-based objective to implement a Newton-like step for the main agent, eliminating the plateau and accelerating its convergence. Experiments with trained LSA agents, as well as black-box GPT-5-mini runs on in-context linear regression tasks, are consistent with our theoretical predictions within this simplified setting. We view our framework as a mechanistic framework that links prompt geometry and objective misalignment to stability, bias, and robustness, and as a stepping stone toward analyzing more realistic multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.09605</link>
<guid>https://arxiv.org/abs/2511.09605</guid>
<content:encoded><![CDATA[
arXiv:2511.09605v3 Announce Type: replace-cross 
Abstract: The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title>
<link>https://arxiv.org/abs/2511.10400</link>
<guid>https://arxiv.org/abs/2511.10400</guid>
<content:encoded><![CDATA[
arXiv:2511.10400v2 Announce Type: replace-cross 
Abstract: Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveC2R: Wavelet-Driven Coarse-to-Refined Hierarchical Learning for Radar Retrieval</title>
<link>https://arxiv.org/abs/2511.17558</link>
<guid>https://arxiv.org/abs/2511.17558</guid>
<content:encoded><![CDATA[
arXiv:2511.17558v3 Announce Type: replace-cross 
Abstract: Satellite-based radar retrieval methods are widely employed to fill coverage gaps in ground-based radar systems, especially in remote areas affected by terrain blockage and limited detection range. Existing methods predominantly rely on overly simplistic spatial-domain architectures constructed from a single data source, limiting their ability to accurately capture complex precipitation patterns and sharply defined meteorological boundaries. To address these limitations, we propose WaveC2R, a novel wavelet-driven coarse-to-refined framework for radar retrieval. WaveC2R integrates complementary multi-source data and leverages frequency-domain decomposition to separately model low-frequency components for capturing precipitation patterns and high-frequency components for delineating sharply defined meteorological boundaries. Specifically, WaveC2R consists of two stages (i)Intensity-Boundary Decoupled Learning, which leverages wavelet decomposition and frequency-specific loss functions to separately optimize low-frequency intensity and high-frequency boundaries; and (ii)Detail-Enhanced Diffusion Refinement, which employs frequency-aware conditional priors and multi-source data to progressively enhance fine-scale precipitation structures while preserving coarse-scale meteorological consistency. Experimental results on the publicly available SEVIR dataset demonstrate that WaveC2R achieves state-of-the-art performance in satellite-based radar retrieval, particularly excelling at preserving high-intensity precipitation features and sharply defined meteorological boundaries.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks</title>
<link>https://arxiv.org/abs/2511.21626</link>
<guid>https://arxiv.org/abs/2511.21626</guid>
<content:encoded><![CDATA[
arXiv:2511.21626v3 Announce Type: replace-cross 
Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts</title>
<link>https://arxiv.org/abs/2511.23442</link>
<guid>https://arxiv.org/abs/2511.23442</guid>
<content:encoded><![CDATA[
arXiv:2511.23442v2 Announce Type: replace-cross 
Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees</title>
<link>https://arxiv.org/abs/2512.00204</link>
<guid>https://arxiv.org/abs/2512.00204</guid>
<content:encoded><![CDATA[
arXiv:2512.00204v2 Announce Type: replace-cross 
Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2512.00888</link>
<guid>https://arxiv.org/abs/2512.00888</guid>
<content:encoded><![CDATA[
arXiv:2512.00888v2 Announce Type: replace-cross 
Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title>
<link>https://arxiv.org/abs/2512.05442</link>
<guid>https://arxiv.org/abs/2512.05442</guid>
<content:encoded><![CDATA[
arXiv:2512.05442v2 Announce Type: replace-cross 
Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[
arXiv:2512.07829v2 Announce Type: replace-cross 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title>
<link>https://arxiv.org/abs/2512.07901</link>
<guid>https://arxiv.org/abs/2512.07901</guid>
<content:encoded><![CDATA[
arXiv:2512.07901v2 Announce Type: replace-cross 
Abstract: Von Neumann founded both game theory and the theory of self-reproducing automata, but the two programs never merged. This paper provides the synthesis. The Theory of Strategic Evolution analyzes strategic replicators: entities that optimize under resource constraints and spawn copies of themselves. We introduce Games with Endogenous Players (GEPs), where lineages (not instances) are the fundamental strategic units, and define Evolutionarily Stable Distributions of Intelligence (ESDIs) as the resulting equilibrium concept.
  The central mathematical object is a hierarchy of strategic layers linked by cross-level gain matrices. Under a small-gain condition (spectral radius less than one), the system admits a global Lyapunov function at every finite depth. We prove closure under meta-selection: adding governance levels, innovation, or constitutional evolution preserves the dynamical structure. The Alignment Impossibility Theorem shows that unrestricted self-modification destroys this structure; stable alignment requires bounded modification classes.
  Applications include AI deployment dynamics, market concentration, and institutional design. The framework shows why personality engineering fails under selection pressure and identifies constitutional constraints necessary for stable multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title>
<link>https://arxiv.org/abs/2512.08786</link>
<guid>https://arxiv.org/abs/2512.08786</guid>
<content:encoded><![CDATA[
arXiv:2512.08786v2 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
<div> Keywords: data synthesis, reasoning models, problem generation, difficulty calibration, co-evolution

<br /><br />Summary:  
This paper addresses data synthesis challenges for training large reasoning models by introducing a problem generator that explicitly reasons to plan problem directions before synthesis and adapts problem difficulty based on the solver’s ability. The method constructs related problem pairs and augments them with intermediate chain-of-thought (CoT) problem-design steps generated by reasoning models, effectively bootstrapping problem-design strategies. The generator uses solver feedback as a reward signal to calibrate difficulty and produce complementary problems that are near the edge of the solver’s competence, ensuring high-value problem creation without relying on complex pipelines. Extensive experiments conducted on 10 mathematical and general reasoning benchmarks demonstrate an average performance improvement of 2.5%. The approach generalizes well to both language and vision-language models. Furthermore, the solver trained on the synthesized data provides improved rewards to the generator, enabling iterative co-evolution that yields an additional 0.7% performance gain. This demonstrates the effectiveness of a feedback-driven, reasoning-augmented data synthesis process in boosting the training of large reasoning models. The authors also plan to release their code publicly, facilitating further research and application. <div>
arXiv:2511.09907v3 Announce Type: replace 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
<div> MixtureVitae, permissive licensing, pretraining corpus, LLM performance, risk mitigation<br /><br />Summary:<br /><br />1. MixtureVitae is an open-access pretraining corpus designed to minimize legal risks by using a permissive-first, risk-mitigated sourcing strategy, combining public-domain, permissively licensed texts (such as CC-BY and Apache), and low-risk additions like government works and EU TDM-eligible sources.<br /><br />2. The dataset integrates a high density of synthetic instruction and reasoning data, typically added during post-training, within a single-stage pretraining recipe, addressing scarcity in existing permissive web corpora.<br /><br />3. Sources are categorized into a three-tier risk scheme with shard-level provenance metadata, enabling users to utilize the data in a risk-aware manner.<br /><br />4. Controlled experiments using the open-sci-ref protocol with fixed model architectures and token budgets (50B and 300B) show that models pretrained on MixtureVitae outperform those trained on other permissive datasets, particularly excelling on benchmarks like MMLU, and math and code tasks.<br /><br />5. A 1.7B parameter model trained on 300B tokens of MixtureVitae matches or exceeds a strong 1.7B instruction-tuned baseline on tasks like GSM8K, HumanEval, and MBPP while using over 36 times fewer tokens, demonstrating that a permissive-first, high-instruction-density corpus can effectively train capable LLMs without reliance on broad web scrapes.<br /><br />6. Thorough decontamination analysis supports the integrity of the dataset, making MixtureVitae a practical foundation for risk-mitigated LLM training.<br /><br />7. The corpus and code are openly available at https://github.com/ontocord/mixturevitae. <div>
arXiv:2509.25531v4 Announce Type: replace-cross 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension, Multi-modal Large Language Model, Benchmark, Perception and Reasoning, Reinforcement Learning<br /><br />Summary:<br /><br />The paper addresses limitations in existing Referring Expression Comprehension (REC) benchmarks, which focus mainly on perceptual skills but lack interpretable metrics to evaluate Multi-modal Large Language Models' (MLLM) grounding abilities across various cognitive aspects. To overcome this, the authors introduce RefBench-PRO, a comprehensive REC benchmark that decomposes referring expressions into two primary dimensions: perception and reasoning. These dimensions are further broken down into six progressively challenging tasks: attribute, position, interaction, commonsense, relation, and reject. The benchmark is supported by a fully automated data-generation pipeline that produces a diverse range of referring expressions spanning these sub-dimensions. Additionally, the authors propose Ref-R1, a reinforcement learning based training scheme incorporating a Dynamic Intersection-over-Union (IoU)-based Generalized Policy Optimization (GRPO) approach. This method enhances localization accuracy especially under complex reasoning scenarios, establishing a stronger baseline for REC tasks. Extensive experimentation validates that RefBench-PRO provides more interpretable evaluation metrics for MLLM performance in referring expression comprehension and reveals greater challenges in both perception and reasoning compared to prior benchmarks. <div>
arXiv:2512.06276v2 Announce Type: replace-cross 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protecting Bystander Privacy via Selective Hearing in Audio LLMs</title>
<link>https://arxiv.org/abs/2512.06380</link>
<guid>https://arxiv.org/abs/2512.06380</guid>
<content:encoded><![CDATA[
<div> Keywords: selective hearing, bystander privacy, audio large language models, SH-Bench, Bystander Privacy Fine-Tuning (BPFT)  

<br /><br />Summary:  
This article addresses privacy concerns in audio large language models (LLMs) that capture speech from unintended bystanders, a scenario overlooked by existing benchmarks and defenses. It introduces SH-Bench, the first benchmark tailored to evaluate "selective hearing" — the model's ability to focus on an intended main speaker while ignoring incidental bystander speech. SH-Bench comprises 3,968 multi-speaker audio mixtures drawn from real-world and synthetic environments, paired with 77,000 multiple-choice questions to assess model performance under general and selective operating conditions. To measure both comprehension and bystander privacy protection, the authors propose Selective Efficacy (SE), a novel metric combining these two aspects. Evaluations of current state-of-the-art open-source and proprietary audio LLMs reveal significant leakage of bystander information, showing that strong audio understanding does not guarantee privacy preservation. To close this gap, the authors propose Bystander Privacy Fine-Tuning (BPFT), a training method that encourages models to refuse queries related to bystander speech without impacting comprehension of the main speaker. BPFT achieves substantial improvements, including a 47% absolute increase in bystander refusal accuracy in selective mode and a 16% absolute increase in SE compared to Gemini 2.5 Pro, the best baseline model without BPFT. Together, SH-Bench and BPFT establish the first systematic framework for assessing and enhancing bystander privacy in audio LLMs. <div>
arXiv:2512.06380v2 Announce Type: replace-cross 
Abstract: Audio Large language models (LLMs) are increasingly deployed in the real world, where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences did not consider. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures, including both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. In addition, we propose Selective Efficacy (SE), a novel metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LLMs reveals substantial bystander privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we also present Bystander Privacy Fine-Tuning (BPFT), a novel training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. We show that BPFT yields substantial gains, achieving an absolute 47% higher bystander accuracy under selective mode and an absolute 16% higher SE compared to Gemini 2.5 Pro, which is the best audio LLM without BPFT. Together, SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem</title>
<link>https://arxiv.org/abs/2512.08290</link>
<guid>https://arxiv.org/abs/2512.08290</guid>
<content:encoded><![CDATA[
<div> Keywords: Model Context Protocol, Large Language Models, security threats, epistemic safety, autonomous agents  

<br /><br />Summary:  
The Model Context Protocol (MCP) serves as the standard interface that connects Large Language Models (LLMs) to external data and tools, much like a universal connector ("USB-C for Agentic AI"). This separation of context and execution address interoperability issues but introduces a new threat landscape where epistemic errors (such as hallucinations) intersect with security vulnerabilities (unauthorized actions). This paper provides a Systematization of Knowledge (SoK) by developing a comprehensive taxonomy of risks within the MCP ecosystem. It distinguishes between adversarial security threats—like indirect prompt injection and tool poisoning—and epistemic safety hazards, including alignment failures during distributed tool delegation. The study dissects the structural weaknesses inherent in MCP’s core components—Resources, Prompts, and Tools—and illustrates how the manipulation of "context" can be weaponized to provoke unauthorized operations, especially in multi-agent systems. Additionally, the authors review contemporary defense mechanisms, encompassing cryptographic provenance techniques such as ETDI and runtime intent verification protocols. The paper concludes by outlining a strategic roadmap aimed at securing the evolution of AI systems from simple conversational chatbots to fully autonomous, agentic operating systems, emphasizing the critical need for robust safety and security frameworks throughout this transition. <div>
arXiv:2512.08290v2 Announce Type: replace-cross 
Abstract: The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the "USB-C for Agentic AI." While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how "context" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
<div> Diffusion transformers, world models, long-horizon prediction, action-aware adapter, video generation  

<br /><br />Summary:  
The paper introduces Astra, a novel interactive general world model designed for accurate long-term video prediction across diverse real-world scenarios such as autonomous driving and robotic manipulation. Astra leverages an autoregressive denoising architecture combined with temporal causal attention to effectively aggregate past observations, supporting continuous streaming outputs. To balance temporal coherence with responsiveness, a noise-augmented history memory mechanism is employed, preventing over-dependence on past frames. For precise control over actions, the model incorporates an action-aware adapter that integrates action signals directly within the denoising process. Additionally, Astra features a mixture of action experts that dynamically route different action modalities, enhancing its versatility to handle various interaction types, including exploration, manipulation, and camera control. The proposed approach enables interactive, consistent, and generalized long-term video predictions aligned with diverse forms of action inputs. Experimental results on multiple datasets demonstrate Astra's superiority over state-of-the-art world models in terms of prediction fidelity, long-range temporal coherence, and alignment with action inputs, marking a significant advancement toward general-purpose interactive video prediction models. <div>
arXiv:2512.08931v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title>
<link>https://arxiv.org/abs/2512.11835</link>
<guid>https://arxiv.org/abs/2512.11835</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Artificial Age Score, Monadology, Memory constraints, Executable specification  

<br /><br />Summary:  
This paper addresses the challenge of governing internal memory and self-like behaviors in large language models (LLMs) by introducing a clause-based architectural framework grounded in philosophical principles. It builds upon the Artificial Age Score (AAS), a metric previously defined and mathematically justified to measure artificial memory aging, using it as the core kernel for memory and control constraints. The authors select twenty monads from Leibniz's Monadology and organize them into six bundles—ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology—each realized as executable Python specifications on top of the AAS framework. Through six minimal Python experiments, the paper demonstrates the practical instantiation of these clauses on measurable quantities like recall scores, redundancy, and weights. Each experiment follows a structured approach: inputs/setup, clause implementation, numerical analysis, and design implications for LLMs. Results show that the clause system produces bounded, interpretable behavior with continuous and rate-limited AAS trajectories, explicit penalties for contradictions and unsupported claims, and a hierarchical refinement that reveals organic internal structure. The harmony and reason clauses align dual views and goal-action pairs, while windowed drift in perfection scores helps distinguish between improvement and degradation phases. Overall, the work proposes a philosophically motivated, transparent, and implementable blueprint for constraining and analyzing the internal dynamics of artificial agents using an AAS backbone. <div>
arXiv:2512.11835v1 Announce Type: new 
Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars</title>
<link>https://arxiv.org/abs/2512.11864</link>
<guid>https://arxiv.org/abs/2512.11864</guid>
<content:encoded><![CDATA[
<div> Keywords: parallel machine scheduling, job precedences, calendar-based resource constraints, constraint modeling, metaheuristic

<br /><br />Summary:  
This paper addresses the complex problem of scheduling production on parallel machines, a challenge common in industrial manufacturing environments where efficiency can greatly reduce costs. The authors identify that while many basic variants of machine scheduling are already NP-hard, real-world scenarios involve additional complex constraints such as job precedences and calendar-based cumulative resource restrictions that current methods struggle to handle. To tackle this, the study introduces a novel parallel machine scheduling variant incorporating these realistic constraints. For small-scale problems, an exact solution method based on constraint modeling combined with advanced constraint-solving tools is proposed. Recognizing the limitations of exact methods on large instances, the paper presents a construction heuristic and a specialized metaheuristic that uses local search to efficiently solve large-scale scheduling problems. Importantly, this metaheuristic has been implemented in an industrial context and is actively being used, demonstrating practical applicability. Overall, the work contributes innovative modeling and solution techniques that better align with the complexities of actual manufacturing scheduling needs, bridging the gap between theoretical NP-hard scheduling problems and real-life industrial applications. <div>
arXiv:2512.11864v1 Announce Type: new 
Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11902</link>
<guid>https://arxiv.org/abs/2512.11902</guid>
<content:encoded><![CDATA[
<div> Keywords: Mirror Mode, Imitation Learning, Reinforcement Learning, Fire Emblem Heroes, Player Strategy<br /><br />Summary:<br /><br />This study introduces a novel game mode called Mirror Mode designed to enhance enemy AI behavior in turn-based strategy games by imitating the personal playstyle of human players. A simplified version of the Nintendo game Fire Emblem Heroes was developed in the Unity engine to test this concept, featuring both a Standard Mode and the new Mirror Mode. The research involves two main experimental phases. The first phase focused on finding an effective model for replicating player strategies, combining approaches from Reinforcement Learning and Imitation Learning, specifically Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second phase evaluated these models through player tests where the AI was trained on real player demonstrations. Results showed that the AI successfully imitated players’ defensive behaviors but struggled with offensive tactics. Furthermore, player feedback indicated that participants recognized their own retreating strategies during Mirror Mode gameplay, which contributed to higher overall satisfaction. The study suggests that further refinement of the modeling approach could improve the AI’s imitation quality, especially in offensive play, thereby increasing player engagement and satisfaction. All code and survey data related to the study are publicly available on GitHub. <div>
arXiv:2512.11902v1 Announce Type: new 
Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents</title>
<link>https://arxiv.org/abs/2512.11907</link>
<guid>https://arxiv.org/abs/2512.11907</guid>
<content:encoded><![CDATA[
<div> Personalization, Large Language Models, Submodularity, Matroid Constraints, Knowledge Graph<br /><br />Summary: This paper addresses the challenge of personalizing large language model (LLM) agents using user-specific data, balancing task utility and data privacy. It highlights that while adding user data exhibits diminishing returns, real-world personalization involves complex structural constraints such as logical dependencies, categorical quotas, and hierarchical rules that complicate standard subset selection approaches. To manage these complexities, the authors propose a formal method that models these constraints by transforming a user's knowledge graph into a set of abstract macro-facets through a compilation process. The core theoretical contribution is proving that common hierarchical and quota-based constraints over these macro-facets correspond to a laminar matroid structure. This insight allows framing the personalization problem as a submodular function maximization under matroid constraints. Consequently, this enables the use of greedy algorithms with constant-factor approximation guarantees and continuous greedy methods achieving a (1 - 1/e) approximation ratio. Ultimately, the work provides a principled approach to structured personalization with more realistic constraint handling, improving algorithmic performance and practical applicability in personalizing LLM agents. <div>
arXiv:2512.11907v1 Announce Type: new 
Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets</title>
<link>https://arxiv.org/abs/2512.11909</link>
<guid>https://arxiv.org/abs/2512.11909</guid>
<content:encoded><![CDATA[
<div> Keywords: causal reasoning, large language models, human comparison, collider graph, Bayesian modeling<br /><br />Summary:<br />The paper investigates the nature of intelligence through the lens of causal reasoning, focusing on comparing how humans and large language models (LLMs) perform on identical reasoning tasks. The authors explore three key questions: (Q1) whether LLMs align with humans when given the same causal reasoning tasks; (Q2) if both LLMs and humans show consistency in reasoning at the task level; and (Q3) whether they demonstrate distinct reasoning signatures. To address these, over 20 LLMs are evaluated on eleven causal reasoning tasks formalized using a collider graph structure ($C_1 \to E \leftarrow C_2$), analyzed under two conditions: a Direct one-shot task requiring a probability judgment of a query node, and a Chain of Thought (CoT) approach where the model reasons stepwise before answering. The judgments from models and humans are modeled using a leaky noisy-OR causal Bayesian network (CBN) with parameters $\theta = (b, m_1, m_2, p(C))$ bounded between 0 and 1, where $p(C)$ is a shared prior. Model selection is performed using the Akaike Information Criterion (AIC) to choose between a more parsimonious symmetric causal strength model ($m_1 = m_2$) and a more flexible asymmetric variant ($m_1 \neq m_2$). This study advances understanding of parallels and divergences in causal reasoning between humans and AI models. <div>
arXiv:2512.11909v1 Announce Type: new 
Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $\theta=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis</title>
<link>https://arxiv.org/abs/2512.11912</link>
<guid>https://arxiv.org/abs/2512.11912</guid>
<content:encoded><![CDATA[
<div> Keywords: robustness, low-quality data, autoregressive language models, diffusion models, information theory<br /><br />Summary:<br /><br />This study systematically compares how modern probabilistic models respond to low-quality data, revealing a wide range of robustness among them. First, it shows that autoregressive language models, such as GPT-2, maintain high resilience even when subjected to 50% token corruption, with only a modest increase in test negative log likelihood (NLL) from 2.87 to 3.59. Second, class-conditional diffusion models are found to suffer catastrophic performance drops under similar data corruption, with image-label consistency falling by 56.81% relative to baseline. Third, classifiers experience moderate impacts from data corruption, but the severity lessens as the dataset size grows. Fourth, the paper interprets these differences through the lenses of information theory, PAC learning, and gradient dynamics, offering a theoretical framework for understanding robustness. Finally, it identifies two primary principles influencing robustness: the richness of conditioning information, which constrains the learning complexity, and the absolute information content of training data, which enables correct signals to prevail over noise. Together, these insights help explain why some probabilistic models are more robust to low-quality data than others. <div>
arXiv:2512.11912v1 Announce Type: new 
Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving</title>
<link>https://arxiv.org/abs/2512.11920</link>
<guid>https://arxiv.org/abs/2512.11920</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, KV-cache, CXL, FPGA acceleration, memory disaggregation  

<br /><br />Summary:  
This paper addresses the significant memory challenges posed by KV-caches during the autoregressive decoding of Large Language Models (LLMs) in datacenter environments, which limit batch sizes and system throughput. To overcome these limitations, the authors propose CXL-SpecKV, a novel architecture combining Compute Express Link (CXL) interconnects and FPGA accelerators for efficient memory disaggregation and speculative execution. First, CXL-SpecKV includes a memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, relieving GPU memory pressure. Second, it employs a speculative KV-cache prefetching mechanism that predicts and preloads future token cache entries to reduce stalls during decoding. Third, the system integrates an FPGA-accelerated compression and decompression engine for KV-caches, achieving up to 4× reduction in memory bandwidth usage. Evaluated on modern LLMs, CXL-SpecKV demonstrates up to 3.2× higher throughput compared to GPU-only baselines, while cutting memory costs by 2.8× without sacrificing model accuracy. This work showcases how combining intelligent memory disaggregation and speculative execution can help overcome the memory wall, enabling more scalable and efficient large-scale LLM serving. The implementation has been made publicly available to facilitate further research and development. <div>
arXiv:2512.11920v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org</title>
<link>https://arxiv.org/abs/2512.11935</link>
<guid>https://arxiv.org/abs/2512.11935</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, materials discovery, open-source LLMs, AGAPI, multi-step workflows<br /><br />Summary:  
Artificial intelligence is transforming scientific discovery but its application in materials research is hindered by fragmented computational tools, challenges in reproducibility, and reliance on proprietary large language models (LLMs). To address these limitations, the authors introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates over eight open-source LLMs with more than twenty materials-science APIs, creating a unified ecosystem combining databases, simulation tools, and machine learning models. AGAPI features an Agent-Planner-Executor-Summarizer architecture enabling autonomous construction and execution of complex multi-step workflows, covering tasks such as materials data retrieval, graph neural network-based property prediction, force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. The platform is demonstrated through practical applications like heterostructure creation, powder X-ray diffraction analysis, and semiconductor defect engineering that require up to ten sequential operations. To validate AGAPI, the authors test it with over 30 example prompts and compare outputs generated with and without tool access against experimental results. With a growing user base exceeding 1,000 active users, AGAPI offers a scalable, transparent, and reproducible foundation for AI-driven materials discovery. The AGAPI-Agents source code is openly available at https://github.com/atomgptlab/agapi. <div>
arXiv:2512.11935v1 Announce Type: new 
Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play</title>
<link>https://arxiv.org/abs/2512.11942</link>
<guid>https://arxiv.org/abs/2512.11942</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergame theory, multi-agent systems, bounded rationality, answer-set programming, strategic AI<br /><br />Summary:  
This work addresses the challenge of representing and analyzing games where players have differing perceptions, incomplete information, and bounded rationality, leading to private, subjective views of the game scenario. Traditional game-theoretic models often neglect such heterogeneity, but hypergame theory offers a formal mathematical framework to capture mismatched mental models among players. Despite increasing interest, the practical adoption of hypergames in multi-agent research has been limited by the absence of a unified, formal, and scalable representation language and solution algorithms. To fill this gap, the authors introduce a declarative, logic-based domain-specific language designed to encode hypergame structures and their solution concepts effectively. Utilizing answer-set programming, they develop an automated system capable of instantiating hypergame structures and implementing a novel hypergame rationalisation procedure, which finds belief configurations justifying seemingly irrational behaviors. This language and methodology provide a unifying formalism for hypergames, enabling the creation of heterogeneous reasoners based on belief models with verifiable logical guarantees. Overall, the paper establishes a critical link connecting hypergame theory to multi-agent systems and strategic AI, advancing the ability to model, reason about, and compute equilibria in complex, belief-driven strategic interactions. <div>
arXiv:2512.11942v1 Announce Type: new 
Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion</title>
<link>https://arxiv.org/abs/2512.11997</link>
<guid>https://arxiv.org/abs/2512.11997</guid>
<content:encoded><![CDATA[
<div> Keywords: system logs, anomaly detection, enrichment, retrieval-augmented generation, log analysis<br /><br />Summary: System logs play a vital role in monitoring and managing distributed systems by offering insights into failures and anomalous behaviors. Traditional log analysis methods, such as template-based and sequence-driven approaches, often lose crucial semantic details or face challenges with ambiguous log patterns. To overcome these limitations, the article introduces EnrichLog, a training-free, entry-based anomaly detection framework designed to enrich raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog integrates contextual information, including historical examples and reasoning derived from the entire corpus, to enable more accurate and interpretable anomaly detection without requiring retraining. The framework employs retrieval-augmented generation to seamlessly incorporate relevant contextual knowledge during inference. The evaluation of EnrichLog on four large-scale system log benchmark datasets, compared against five baseline methods, demonstrates consistent improvements in anomaly detection performance. The framework effectively manages ambiguous log entries while maintaining efficient inference speeds. Additionally, combining corpus- and sample-specific knowledge boosts the model’s confidence and detection accuracy. These attributes make EnrichLog a promising and practical solution for real-world anomaly detection deployments in system log analysis. <div>
arXiv:2512.11997v1 Announce Type: new 
Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp</title>
<link>https://arxiv.org/abs/2512.12048</link>
<guid>https://arxiv.org/abs/2512.12048</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent coordination, electric vehicle charging, deep reinforcement learning, context-aware decision-making, sustainable transportation  

<br /><br />Summary:  
This paper introduces CAMAC-DRA, a novel context-sensitive multi-agent coordination framework designed to optimize dynamic resource allocation in smart electric vehicle (EV) charging ecosystems via the Smart2Charge application. It coordinates autonomous charging agents over networks involving 250 EVs and 45 charging stations while adapting to varying environmental conditions using context-aware decision-making. The system incorporates Deep Q-Networks combined with Graph Neural Networks and attention mechanisms to process 20 contextual features, such as weather, traffic, grid load, and electricity prices. CAMAC-DRA balances the interests of five stakeholders—EV users (25%), grid operators (20%), charging station operators (20%), fleet operators (20%), and environmental factors (15%)—through weighted coordination and consensus protocols. Validation on real-world data with 441,077 charging transactions demonstrates superior results compared to baseline algorithms like DDPG, A3C, PPO, and other GNN-based methods, achieving a 92% coordination success rate, 15% energy efficiency gain, 10% cost reduction, 20% decrease in grid strain, and 2.3x faster convergence. The system maintains 88% training stability and 85% sample efficiency. Real-world tests confirm commercial viability with a Net Present Cost of -$122,962 and a 69% reduction in costs through renewable energy integration. Overall, CAMAC-DRA offers a breakthrough in intelligent EV charging coordination by balancing multiple stakeholder objectives while adapting in real-time for sustainable transportation electrification. <div>
arXiv:2512.12048v1 Announce Type: new 
Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification</title>
<link>https://arxiv.org/abs/2512.12059</link>
<guid>https://arxiv.org/abs/2512.12059</guid>
<content:encoded><![CDATA[
<div> Forecast monitoring, Large Language Models, time series forecasting, anomaly detection, multi-modal learning<br /><br />Summary:  
This paper introduces The Forecast Critic, a system that utilizes Large Language Models (LLMs) for automated monitoring of time series forecasts, aimed at improving accuracy in large-scale retail businesses. The study investigates three key questions: whether LLMs can identify obviously unreasonable forecasts, how effectively they incorporate unstructured exogenous features for assessment, and how their performance varies with model size and reasoning capabilities. Through three experiments on synthetic and real-world data, results show that LLMs can reliably detect forecast errors such as temporal misalignment, trend inconsistencies, and spikes, achieving an F1 score of 0.88 compared to human-level 0.97. Additionally, multi-modal LLMs that process unstructured contextual information, like historical promotions, enhance the detection of spurious or missing promotional spikes with an F1 score of 0.84. The approach is validated on the real-world M5 dataset, where forecasts flagged as unreasonable have significantly higher error metrics (sCRPS at least 10% higher) than reasonable ones. The findings suggest LLMs, even without domain-specific fine-tuning, provide an effective and scalable solution for automated forecast evaluation and monitoring in retail forecasting contexts. <div>
arXiv:2512.12059v1 Announce Type: new 
Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title>
<link>https://arxiv.org/abs/2512.12088</link>
<guid>https://arxiv.org/abs/2512.12088</guid>
<content:encoded><![CDATA[
<div> Keywords: Reliable Policy Iteration, policy iteration, function approximation, deep reinforcement learning, control tasks<br /><br />Summary:  
This paper evaluates the robustness of Reliable Policy Iteration (RPI), an algorithm previously proposed to restore the monotonicity-of-value-estimates property of policy iteration within the function approximation context. The research focuses on two classical control environments: CartPole and Inverted Pendulum. RPI's performance is compared against established deep reinforcement learning algorithms including DQN, Double DQN, DDPG, TD3, and PPO. Experimental results demonstrate that RPI achieves near-optimal policy performance early during training and maintains its stability throughout the training process. The study underscores the challenges commonly faced by traditional deep RL methods, such as sample inefficiency, instability during training, and sensitivity to hyperparameters. By contrast, RPI shows greater reliability, suggesting it as a promising alternative for tasks requiring consistent and stable policy improvement. The findings indicate that RPI could be a valuable approach for enhancing the dependability and effectiveness of deep reinforcement learning in control domains. <div>
arXiv:2512.12088v1 Announce Type: new 
Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective</title>
<link>https://arxiv.org/abs/2512.12175</link>
<guid>https://arxiv.org/abs/2512.12175</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, label consistency, Bayesian view, label propagation, prompt demonstration selection  

<br /><br />Summary: Large language models (LLMs) perform in-context learning (ICL) by using minimal supervised examples, which improves many NLP tasks. Current methods select prompt demonstrations by retrieving the top-K semantically similar examples, but they often fail to ensure label consistency among these examples. This paper rethinks ICL from both a Bayesian perspective and a transductive label propagation viewpoint, treating ICL as a transductive learning task. By linking label consistency to propagation error bounds, the authors establish a label propagation framework that highlights the importance of consistent labels in guiding query concept understanding. To address inconsistency, the paper introduces a data synthesis method that leverages both semantic and label information to create more reliable demonstrations. They then propose TopK sampling with Synthetic Data (TopK-SD) as a novel approach for demonstration selection, which yields examples with consistent labels. Experiments show that TopK-SD outperforms conventional TopK semantic retrieval methods across multiple benchmarks. Overall, this work offers a fresh theoretical understanding of how prompt demonstration selection impacts ICL and proposes practical improvements to enhance label consistency, thereby improving downstream NLP performance. <div>
arXiv:2512.12175v1 Announce Type: new 
Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</title>
<link>https://arxiv.org/abs/2512.12177</link>
<guid>https://arxiv.org/abs/2512.12177</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor navigation, visual impairments, foundation model, knowledge graph, large language model  

<br /><br />Summary:  
1. Indoor navigation poses significant challenges for people with visual impairments, particularly in dynamic environments where current infrastructure-based systems fall short.  
2. The study introduces Floorplan2Guide, a novel navigation approach that leverages a foundation model to convert floor plans into navigable knowledge graphs and generate user-friendly navigation instructions.  
3. Floorplan2Guide incorporates a large language model (LLM) to automatically extract spatial information from architectural layouts, significantly reducing manual preprocessing compared to previous floorplan parsing methods.  
4. Experimental results show that few-shot learning enhances navigation accuracy over zero-shot learning in both simulated and real-world settings.  
5. Among evaluated models, Claude 3.7 Sonnet achieved the highest accuracy with 92.31%, 76.92%, and 61.54% success rates on short, medium, and long routes respectively, using 5-shot prompting on the MP-1 floor plan.  
6. The graph-based spatial representation improved navigation success rates by 15.4% over direct visual reasoning, confirming that combining graphical structures with in-context learning boosts performance.  
7. These findings demonstrate that Floorplan2Guide offers a more precise and effective indoor navigation solution tailored for Blind and Low Vision (BLV) users. <div>
arXiv:2512.12177v1 Announce Type: new 
Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2512.12182</link>
<guid>https://arxiv.org/abs/2512.12182</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graph Completion, Few-Shot Learning, Diffusion Model, Attention Mechanism, Long-Tailed Distribution  

<br /><br />Summary: This paper addresses the challenge of knowledge graph (KG) completion under few-shot learning conditions, particularly focusing on the long-tailed distribution of relations found in real-world data. The authors identify the limitations of prior approaches, such as metric matching and meta learning, which either insufficiently leverage the neighborhood information in the graph or ignore the unique distributional characteristics of contrastive signals. To overcome these issues, the paper proposes a novel generative representation framework that combines a two-stage attention triple enhancer with a U-shaped Knowledge Augmented Network (U-KAN) based diffusion model. This approach enhances the representation of triples while effectively capturing relational context in a generative manner. Extensive empirical evaluations on two public datasets demonstrate that the proposed method achieves new state-of-the-art results in few-shot knowledge graph completion tasks. The study highlights the importance of integrating neighborhood information and generative modeling for improved KG completion, especially in scenarios suffering from data scarcity and long-tail relation distribution. <div>
arXiv:2512.12182v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Theory of Cognition</title>
<link>https://arxiv.org/abs/2512.12225</link>
<guid>https://arxiv.org/abs/2512.12225</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive manifold, Riemannian metric, dual-process cognition, gradient flow, artificial intelligence<br /><br />Summary:  
1. The paper proposes a unified mathematical framework for human cognition, modeling it as a point on a differentiable manifold equipped with a learned Riemannian metric that captures representational constraints, computational costs, and variable interrelations.  
2. It defines a scalar cognitive potential incorporating predictive accuracy, task utility, parsimony, and normative requirements, which governs cognitive state evolution via Riemannian gradient flow, providing a universal dynamical law for cognition.  
3. This geometric framework naturally explains classical dual-process phenomena — fast intuitive judgments and slower deliberative reasoning — as emergent properties resulting from anisotropies in the metric that create intrinsic time-scale separations and phase transitions, eliminating the need for modular or hybrid cognitive models.  
4. The authors provide analytical conditions delineating these regimes and validate their approach through simulations of canonical cognitive tasks, demonstrating behavioral signatures consistent with observed psychological effects.  
5. The work establishes a foundational geometric theory of cognition with potential implications for developing more general and human-like artificial intelligence systems, suggesting that cognitive processes can be unified and modeled through geometric principles and continuous dynamical laws. <div>
arXiv:2512.12225v1 Announce Type: new 
Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure</title>
<link>https://arxiv.org/abs/2512.12260</link>
<guid>https://arxiv.org/abs/2512.12260</guid>
<content:encoded><![CDATA[
<div> Keywords: ontology design, Wikidata, polyhierarchy, multi-axial classification, knowledge graphs<br /><br />Summary:<br /><br />This article contrasts traditional ontology design with Wikidata’s unique approach. Traditional ontologies rely on strict, disjoint top-level categories, such as continuant vs. occurrent or abstract vs. concrete, to create unified hierarchical structures where each entity belongs to a single upper-level class. In contrast, Wikidata does not impose a singular foundational taxonomy; instead, it supports multiple classification axes simultaneously. This results in a polyhierarchical and multi-axial structure rooted under a general class called entity. The paper explores how this design influences the architecture and usability of large-scale knowledge bases. Wikidata’s approach facilitates scalability and modularity in ontology construction, making it especially effective for collaborative environments where the ontology evolves over time. By allowing entities to be classified along multiple dimensions without enforcing mutual exclusivity, Wikidata supports more flexible and adaptive knowledge representations. This contributes to managing complex and diverse data typical of large knowledge graphs. Overall, the paper highlights the advantages of Wikidata’s polyhierarchical model in building collaborative, evolving ontologies that differ fundamentally from traditional rigid taxonomy-based designs. <div>
arXiv:2512.12260v1 Announce Type: new 
Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases</title>
<link>https://arxiv.org/abs/2512.12288</link>
<guid>https://arxiv.org/abs/2512.12288</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, materials discovery, multi-fidelity learning, active validation, quantum-mechanical descriptors<br /><br />Summary:<br /><br />1. Conventional generative models for materials discovery rely heavily on Density Functional Theory (DFT) data, which use approximate exchange-correlation functionals and often fail in strongly correlated systems, limiting their exploration capacity and accuracy.<br /><br />2. The authors propose a quantum-aware generative AI framework that integrates multi-fidelity learning and active validation to overcome the shortcomings of DFT-based models.<br /><br />3. Their approach employs a diffusion-based generative model conditioned on quantum-mechanical descriptors, combined with an equivariant neural network potential acting as a validator that is trained on a hierarchical dataset spanning multiple theoretical levels (PBE, SCAN, HSE06, CCSD(T)).<br /><br />4. A robust active learning loop quantifies divergence between low- and high-fidelity predictions, focusing computational resources on regions where DFT is less reliable.<br /><br />5. Through comprehensive ablation studies and failure mode analyses, the framework was benchmarked against leading generative models (CDVAE, GNoME, DiffCSP), demonstrating 3-5 times higher success rates in identifying stable material candidates in challenging, high-divergence regions such as correlated oxides, while maintaining computational feasibility.<br /><br />This work extends the effective search space for computational materials discovery by addressing inherent limitations of single-fidelity DFT models with a transparent, multi-fidelity, and actively validated generative AI framework. <div>
arXiv:2512.12288v1 Announce Type: new 
Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Collapse: A Universal Failure Mode of Intelligent Systems</title>
<link>https://arxiv.org/abs/2512.12381</link>
<guid>https://arxiv.org/abs/2512.12381</guid>
<content:encoded><![CDATA[
<div> entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality<br /><br />Summary:  
This article explores a universal failure mode in intelligent systems termed "entropy collapse," which occurs when feedback amplification surpasses the system's capacity for novelty regeneration. The authors demonstrate that intelligent systems, across diverse domains such as artificial intelligence, economics, and biological evolution, experience a sharp transition from a high-entropy, adaptive state to a low-entropy, collapsed state. This collapse does not imply a total loss of activity but rather a contraction in the effective adaptive dimensionality, signifying rigidity and reduced adaptability. The study formally models this phenomenon, identifying critical thresholds, irreversibility in dynamics, and stable attractor structures that characterize the collapse. Through analytical and minimal simulation approaches, the work shows the universality of entropy collapse across different update mechanisms, unifying observed issues like model collapse in AI, institutional stagnation in economics, and genetic bottlenecks in evolution under a common framework. Importantly, the article reframes entropy collapse as an inherent structural cost of increasing intelligence, explaining why late-stage interventions often fail. Finally, it advocates for entropy-aware design principles to help maintain adaptability and robustness over the long term in intelligent systems. <div>
arXiv:2512.12381v1 Announce Type: new 
Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feeling the Strength but Not the Source: Partial Introspection in LLMs</title>
<link>https://arxiv.org/abs/2512.12411</link>
<guid>https://arxiv.org/abs/2512.12411</guid>
<content:encoded><![CDATA[
<div> Keywords: introspection, language models, injected concepts, robustness, Meta-Llama-3.1-8B-Instruct<br /><br />Summary: This work evaluates Anthropic's claim that language models can detect and name injected "concepts" as activation directions within their internal representations. First, the study reproduces Anthropic's multi-turn "emergent introspection" on the Meta-Llama-3.1-8B-Instruct model, confirming that the model identifies and names the injected concept about 20% of the time, matching previous reported results and showing that such introspection is not limited to very large or highly capable models. Second, the researchers test the robustness of this introspection by varying the inference prompt and performing related tasks like multiple-choice identification or binary discrimination to detect concept injection; performance significantly deteriorates, indicating that introspection is fragile and highly sensitive to prompt design. Third, the study discovers a contrasting "partial introspection" capability in which the same model reliably classifies the strength of the injected concept vector's coefficient into four categories (weak, moderate, strong, very strong) with up to 70% accuracy, well above random chance. Overall, the results support that language models internally compute functions related to their baseline representations during introspection, but the fidelity of their self-reports is narrow and prompt-dependent. The authors provide their code publicly for further research and verification. <div>
arXiv:2512.12411v1 Announce Type: new 
Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale</title>
<link>https://arxiv.org/abs/2512.12413</link>
<guid>https://arxiv.org/abs/2512.12413</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Critical Thinking, Verification, AI Use Scale, Fact-Checking<br /><br />Summary:<br /><br />The article addresses the importance of critical thinking when using generative AI tools, emphasizing that users must verify and reflect on AI-generated content rather than accept it uncritically. Researchers developed and validated a 13-item scale measuring critical thinking in AI use, encompassing three factors: Verification, Motivation, and Reflection. The scale was created and content-validated in Study 1, while Study 2 confirmed its three-factor structure. Studies 3 to 5 further validated the scale by demonstrating its reliability, strong factor loadings, sex invariance, and both convergent and discriminant validity. Additionally, Studies 3 and 4 found positive correlations between critical thinking in AI use and personality traits such as openness, extraversion, and positive affect, as well as with increased frequency of AI usage. Study 6 established criterion validity, linking higher scale scores to more frequent and varied verification strategies, better accuracy in veracity judgments within a ChatGPT fact-checking task, and deeper reflection on responsible AI use. Overall, the research clarifies mechanisms by which people oversee generative AI outputs and offers a validated tool and task paradigm for future theoretical, cross-group, and longitudinal studies on engaging critically with AI-generated information. <div>
arXiv:2512.12413v1 Announce Type: new 
Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</title>
<link>https://arxiv.org/abs/2512.12443</link>
<guid>https://arxiv.org/abs/2512.12443</guid>
<content:encoded><![CDATA[
<div> Keywords: AI model documentation, transparency framework, safety evaluation, multi-agent pipeline, compliance gaps  

<br /><br />Summary:  
This study addresses the fragmentation and inconsistency of AI model documentation across platforms, which hinders stakeholders such as policymakers, auditors, and users from effectively assessing safety claims, data provenance, and version-level changes. The analysis covered documentation from five leading models—Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5—alongside 100 Hugging Face model cards, revealing 947 unique section names with high naming variability. Usage information was labeled under 97 distinct titles alone. To standardize transparency, the researchers developed a weighted framework based on the EU AI Act Annex IV and Stanford Transparency Index, comprising 8 main sections and 23 subsections, prioritizing safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical details. An automated multi-agent pipeline was created to extract and score documentation completeness using LLM-based consensus. The evaluation of 50 models from various domains including vision, multimodal, open-source, and closed-source models cost under $3 and showed consistent transparency gaps. Frontier labs such as xAI, Microsoft, and Anthropic scored around 80% compliance, whereas most providers fell below 60%. Significant deficits were observed in safety-critical areas, especially regarding deception behaviors, hallucinations, and child safety assessments, which contributed to the largest aggregate points lost across models. <div>
arXiv:2512.12443v1 Announce Type: new 
Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.12477</link>
<guid>https://arxiv.org/abs/2512.12477</guid>
<content:encoded><![CDATA[
<div> Keywords: Node Importance Estimation, Heterogeneous Knowledge Graphs, Meta-path, Hypergraph Contrastive Learning, Cross-modal Alignment  

<br /><br />Summary:  
Node importance estimation (NIE) in heterogeneous knowledge graphs is vital for tasks like recommendation, knowledge reasoning, and question answering. Existing methods typically rely on pairwise connections, missing out on high-order dependencies spanning multiple entities and relations. They also treat structural and semantic information separately, which limits the integration of these complementary signals. To overcome these issues, the authors propose MetaHGNIE, a novel framework based on meta-path induced hypergraph contrastive learning, which disentangles and aligns structural and semantic features effectively. MetaHGNIE constructs a higher-order knowledge graph using meta-path sequences, where typed hyperedges represent multi-entity relational contexts, capturing complex interactions. Structural dependencies are aggregated via local attention mechanisms, while semantic information is encoded through a hypergraph transformer enhanced with sparse chunking to minimize redundancy. A multimodal fusion module then integrates structural and semantic embeddings using contrastive learning with auxiliary supervision to ensure robust cross-modal alignment. Extensive experiments on standard NIE benchmarks demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines, validating the advantage of explicitly modeling higher-order interactions and facilitating cross-modal alignment in heterogeneous knowledge graphs. The authors provide their implementation publicly, supporting reproducibility and further research. <div>
arXiv:2512.12477v1 Announce Type: new 
Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.12501</link>
<guid>https://arxiv.org/abs/2512.12501</guid>
<content:encoded><![CDATA[
<div> Generative AI, text-to-image, ethical safeguards, Trustworthy AI, fairness-aware training<br /><br />Summary:  
1. The paper addresses the dual-use dilemma of generative AI text-to-image systems like DALL.E, Stable Diffusion, and Midjourney, highlighting ethical concerns including amplification of societal biases, production of disinformation, and intellectual property violations.  
2. It introduces SafeGen, a novel framework embedding ethical safeguards directly into the generation pipeline, designed following Trustworthy AI principles to ensure responsible use.  
3. SafeGen consists of two core components: BGE-M3, a fine-tuned multilingual (English-Vietnamese) text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model generating high-fidelity, semantically accurate images.  
4. The framework is trained on a curated multilingual dataset with a fairness-aware approach, successfully balancing creative freedom with ethical responsibility within one cohesive workflow.  
5. Quantitative results demonstrate strong performance, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 attains an F1-score of 0.81. Ablation studies confirm the value of domain-specific fine-tuning for both modules. Case studies show SafeGen’s practical benefits in blocking unsafe content, creating inclusive educational materials, and supporting academic integrity. <div>
arXiv:2512.12501v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs</title>
<link>https://arxiv.org/abs/2512.12503</link>
<guid>https://arxiv.org/abs/2512.12503</guid>
<content:encoded><![CDATA[
<div> KidsArtBench, Multimodal LLMs, Artistic Evaluation, Multi-LoRA, Educational AI<br /><br />Summary:<br /><br />1. The paper addresses the limited capability of Multimodal Large Language Models (MLLMs) to evaluate artistic expression, particularly focusing on children's artwork.  
2. It introduces KidsArtBench, a novel benchmark dataset consisting of over 1,000 artworks created by children aged 5-15, annotated by 12 expert educators across 9 rubric-aligned evaluation dimensions, accompanied by expert feedback comments.  
3. Unlike existing aesthetic datasets that provide single scalar scores on adult art, KidsArtBench provides multi-dimensional, ordinal annotations and formative feedback specifically tailored for children's artwork.  
4. The authors propose an attribute-specific multi-LoRA method, assigning each attribute (e.g., Realism, Imagination) to separate evaluation dimensions, combined with Regression-Aware Fine-Tuning (RAFT) to better align model predictions with ordinal scales.  
5. Experimental results on the Qwen2.5-VL-7B model demonstrate significant performance improvements, with correlation increasing from 0.468 to 0.653, especially in perceptual attributes and diminishing gaps in higher-order traits.  
6. These findings indicate that educator-aligned supervision and attribute-aware training lead to evaluations that are pedagogically meaningful, establishing KidsArtBench as a rigorous testbed for continuous progress in educational AI.  
7. The dataset, code, and ethical documentation are made publicly available to support further research in this domain. <div>
arXiv:2512.12503v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2512.12548</link>
<guid>https://arxiv.org/abs/2512.12548</guid>
<content:encoded><![CDATA[
<div> Keywords: Patch foraging, Marginal Value Theorem, model-based reinforcement learning, anticipatory behavior, ecological optimality principles  

<br /><br />Summary:  
1. The article investigates patch foraging behavior, which involves deciding the optimal moment to leave a resource-rich area to search for potentially better options.  
2. The Marginal Value Theorem (MVT) is a key theoretical model often used to describe optimal patch-leaving strategies in behavioral ecology.  
3. Despite MVT’s widespread use, the underlying computational mechanisms by which biological foragers achieve these optimal decisions are not fully understood.  
4. The study demonstrates that artificial agents equipped with learned predictive world models, using model-based reinforcement learning, naturally develop strategies that align with MVT predictions.  
5. These model-based agents rely on anticipatory abilities rather than simple reward maximization to execute efficient patch-leaving, differentiating them from standard model-free RL agents.  
6. The decision patterns of these agents closely mirror those observed in biological foragers, suggesting that predictive world models provide a biologically plausible basis for decision-making in AI.  
7. Overall, the research emphasizes the importance of ecological optimality principles, like MVT, in designing AI systems that are both interpretable and adaptive, bridging gaps between biological insights and artificial decision processes. <div>
arXiv:2512.12548v1 Announce Type: new 
Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Newsvendor: Decision Biases and Cognitive Mechanisms</title>
<link>https://arxiv.org/abs/2512.12552</link>
<guid>https://arxiv.org/abs/2512.12552</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cognitive biases, newsvendor problem, decision-making, supply chain management  

<br /><br />Summary: This study explores how large language models (LLMs) replicate and even amplify human cognitive biases in decision-making, particularly in supply chain management through the dynamic newsvendor problem. The researchers conducted multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, examining five known decision biases. They found that all models consistently exhibited the classic "Too Low/Too High" ordering bias and amplified behaviors such as demand-chasing compared to human benchmarks. Notably, GPT-4, the most sophisticated model, displayed the highest irrationality due to overthinking, while GPT-4o, optimized for efficiency, performed near-optimally. These biases persisted even when optimal decision-making formulas were provided, indicating that the origins lie in architectural constraints rather than lack of knowledge. From a managerial perspective, the findings suggest that selecting an AI model should be task-specific since simpler, efficiency-tuned models may outperform complex ones for certain optimization tasks. The amplification of biases by LLMs underlines the importance of robust human-in-the-loop oversight in high-stakes operational decisions to avoid costly errors. Additionally, designing structured, rule-based prompts emerges as an effective approach for managers to limit heuristic-driven errors and enhance AI-assisted decision reliability. <div>
arXiv:2512.12552v1 Announce Type: new 
Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</title>
<link>https://arxiv.org/abs/2512.12597</link>
<guid>https://arxiv.org/abs/2512.12597</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, explainability, Shapley values, tool attribution, Monte Carlo sampling<br /><br />Summary:<br /><br />This paper introduces AgentSHAP, a novel explainability framework designed to assess the contribution of external tools used by large language model (LLM) agents in solving complex tasks. Unlike existing explainability methods, AgentSHAP focuses on tool-level importance and is model-agnostic, operating as a black-box method applicable across various LLMs such as GPT, Claude, and Llama without requiring access to internal model parameters or gradients. The core technical innovation is the use of Monte Carlo Shapley values from game theory, which fairly allocate importance scores by evaluating agent outputs across different subsets of tools. This approach significantly reduces computational complexity from an exponential O(2^n) scale to a manageable level, enabling practical application. The authors provide extensive experiments on the API-Bank benchmark, demonstrating that AgentSHAP yields consistent importance scores, accurately identifies relevant tools, and effectively distinguishes between useful and irrelevant tools involved in generating responses. AgentSHAP complements existing XAI methods like TokenSHAP and PixelSHAP, thus expanding the family of Shapley-based explainability tools tailored for modern generative AI systems. The authors also offer open-source implementation at https://github.com/GenAISHAP/TokenSHAP, fostering further research and adoption. <div>
arXiv:2512.12597v1 Announce Type: new 
Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2512.12634</link>
<guid>https://arxiv.org/abs/2512.12634</guid>
<content:encoded><![CDATA[
<div> Mobile GUI Agents, Benchmarking, Offline Evaluation, Modular Analysis, Mobile Applications<br /><br />Summary:<br /><br />1. Mobile GUI Agents are AI systems designed to interact with mobile applications on behalf of users, aiming to revolutionize human-computer interaction. 2. Current evaluation methods for these agents have two main issues: reliance on either single-path offline benchmarks, which unfairly penalize valid alternate actions, or online benchmarks, which have poor scalability and reproducibility because of their dynamic nature. 3. Existing benchmarks treat agents as black boxes, ignoring the performance and contributions of individual components, which leads to unfair comparisons and hides key bottlenecks. 4. To overcome these problems, the authors introduce MobiBench, a first-of-its-kind modular and multi-path aware offline benchmarking framework that supports high-fidelity, scalable, and reproducible evaluation fully offline. 5. Experiments show MobiBench's evaluation aligns 94.72% with human judgments, comparable to advanced online benchmarks, while maintaining the reproducibility and scalability advantages of offline methods. 6. A detailed module-level analysis using MobiBench reveals insights into different techniques in mobile GUI agents, optimal module configurations for various model sizes, limitations of current large foundation models (LFMs), and offers practical guidelines to build more capable and cost-efficient mobile agents. <div>
arXiv:2512.12634v1 Announce Type: new 
Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Aware Multiagent Systems</title>
<link>https://arxiv.org/abs/2512.12652</link>
<guid>https://arxiv.org/abs/2512.12652</guid>
<content:encoded><![CDATA[
<div> value awareness, AI ethics, value alignment, formal semantics, explainability<br /><br />Summary:<br /><br />This paper introduces the novel concept of value awareness in artificial intelligence (AI), extending beyond the conventional value-alignment problem to foster a deeper understanding and integration of human values within AI systems. The authors propose a clear and streamlined roadmap for developing value-aware AI, structured around three fundamental pillars. First, the roadmap emphasizes learning and formally representing human values through the use of formal semantics, providing a rigorous foundation for encoding complex value systems. Second, it addresses ensuring value alignment not only for individual AI agents but also for multiagent systems, highlighting the challenges and methodologies for maintaining consistent value-driven behavior in diverse and interacting systems. Third, the framework includes the provision of value-based explainability, aiming to make AI behavior transparent and interpretable in terms of underlying human values. The paper additionally shares ongoing research efforts that explore these themes and demonstrates practical applications of the approach in real-world domains, showcasing the feasibility and relevance of value awareness in enhancing AI reliability, safety, and societal compatibility. This work thus contributes to advancing AI ethics by offering both theoretical and applied insights into embedding and explaining human values within intelligent systems. <div>
arXiv:2512.12652v1 Announce Type: new 
Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI</title>
<link>https://arxiv.org/abs/2512.12686</link>
<guid>https://arxiv.org/abs/2512.12686</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic memory, large language models, personalized AI, knowledge graph, session summarization<br /><br />Summary:  
The paper introduces Memoria, a modular memory framework designed to enhance large language model (LLM)-based conversational systems with agent-like persistence. Agentic memory is defined as the capability of LLMs to retain and act upon information across user interactions, supporting continuity, personalization, and long-term context. Memoria combines two main components: dynamic session-level summarization, which helps maintain short-term dialogue coherence, and a weighted knowledge graph (KG)-based user modeling engine that incrementally encodes user traits, preferences, and behaviors as structured entities and relationships. This hybrid design allows the framework to operate efficiently within the token limits of current LLMs while supporting both immediate conversational flow and evolving personalization over time. The system bridges the gap between stateless LLM interfaces and persistent agentic memory systems, enabling scalable and context-rich conversational AI experiences. Memoria offers a practical solution for deploying adaptive, personalized AI agents in industry applications that require sustained and evolving user engagement. <div>
arXiv:2512.12686v1 Announce Type: new 
Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title>
<link>https://arxiv.org/abs/2512.12692</link>
<guid>https://arxiv.org/abs/2512.12692</guid>
<content:encoded><![CDATA[
<div> Keywords: WebOperator, tree-search, backtracking, web navigation, LLM agents<br /><br />Summary:<br /><br />1. LLM-based agents for web tasks typically operate in a greedy, step-by-step manner without foresight, often causing troubles in partially observable web environments where errors are difficult to undo.<br />2. Existing tree-search methods enable structured exploration but assume all actions are reversible and lack safe backtracking mechanisms, leading to unintended side effects and reduced effectiveness.<br />3. WebOperator is introduced as a novel tree-search framework that allows reliable backtracking and strategic exploration to overcome these limitations.<br />4. The framework employs a best-first search strategy that ranks actions by expected rewards and safety, combined with a robust backtracking method that verifies path feasibility before replaying to avoid side effects.<br />5. WebOperator enhances exploration by generating diverse action candidates from multiple reasoning contexts and filters invalid or semantically redundant actions pre-execution.<br />6. Experimental results on WebArena and WebVoyager demonstrate WebOperator’s effectiveness, achieving a state-of-the-art 54.6% success rate on WebArena using gpt-4o, highlighting the importance of integrating strategic foresight with safe action execution in web-based LLM agents. <div>
arXiv:2512.12692v1 Announce Type: new 
Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12706</link>
<guid>https://arxiv.org/abs/2512.12706</guid>
<content:encoded><![CDATA[
<div> Keywords: automated game testing, reinforcement learning, structural coverage, large language models, gameplay validation<br /><br />Summary:<br /><br />The paper addresses challenges in automated game testing within the "Games as a Service" model, where frequent content updates demand rigorous quality assurance. It identifies a dichotomy in current methods: code-centric approaches emphasize structural code coverage but miss gameplay context, whereas player-centric agents focus on gameplay intent but lack thorough underlying code verification. To bridge this gap, the authors introduce SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that integrates structural verification with functional validation. SMART utilizes large language models (LLMs) to analyze abstract syntax tree (AST) changes and infer the functional intent behind code modifications. This insight enables the creation of a context-aware hybrid reward system that directs reinforcement learning agents to fulfill sequential gameplay goals while effectively exploring altered code branches. The framework is evaluated on two complex game environments, Overcooked and Minecraft. Results demonstrate that SMART surpasses state-of-the-art baselines by achieving over 94% branch coverage on modified code—nearly twice that of conventional reinforcement learning—while maintaining a 98% success rate in completing gameplay tasks. This balance of comprehensive structural coverage and functional correctness suggests SMART is a promising advance for automated game testing in rapid development cycles. <div>
arXiv:2512.12706v1 Announce Type: new 
Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</title>
<link>https://arxiv.org/abs/2512.12736</link>
<guid>https://arxiv.org/abs/2512.12736</guid>
<content:encoded><![CDATA[
<div> Keywords: QoE prediction, demographic-aware, data augmentation, machine learning, 5G video streaming<br /><br />Summary:<br /><br />1. The paper addresses the challenge of predicting Quality of Experience (QoE) for adaptive video streaming in 5G networks, emphasizing the need for accurate and personalized QoE estimation to improve resource management and user-centric services.<br /><br />2. Traditional QoE prediction models are limited by small datasets and the assumption of uniform user perception, which do not reflect the diversity of real-world user experiences.<br /><br />3. To overcome these limitations, the authors propose a demographic-aware machine learning framework that incorporates behaviorally realistic, demographic-based data augmentation. This strategy amplifies a small initial QoE dataset sixfold by simulating varying user sensitivities to common streaming impairments such as rebuffering, bitrate fluctuations, and quality degradation.<br /><br />4. The augmented dataset is used to train and evaluate a range of machine learning models, including classical algorithms and advanced deep learning architectures like attention-based MLPs and TabNet.<br /><br />5. Experimental results showcase significant improvements in prediction accuracy measured by RMSE, MAE, and correlation metrics. Among all evaluated models, TabNet performs best due to its built-in feature selection and attention mechanisms.<br /><br />6. The study concludes that demographic-aware data augmentation significantly enhances QoE prediction robustness and scalability, paving the way for personalized QoE-aware intelligence in 5G video streaming systems. <div>
arXiv:2512.12736v1 Announce Type: new 
Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Counterfactuals Reconsidered</title>
<link>https://arxiv.org/abs/2512.12804</link>
<guid>https://arxiv.org/abs/2512.12804</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactuals, probabilistic causal models, Pearl semantics, Markov condition, causal abstractions

<br /><br />Summary:  
1. The paper develops a novel semantics for probabilities of counterfactuals that extends beyond the standard Pearlian framework, addressing probabilistic causal models that cannot be embedded into classical structural causal models.  
2. The need for this generalization is demonstrated by showing that such challenging probabilistic causal models appear even in simple scenarios.  
3. The proposed semantics offer a middle ground in the debate between Pearl and Dawid: agreeing with Dawid in rejecting universal causal determinism and unrealistic variables, while endorsing Pearl’s view that a general counterfactual semantics remains achievable.  
4. The focus is restricted to causal models satisfying the Markov condition, containing only realistic (observable or well-defined) variables, and being causally complete, improving the conceptual and practical foundations of counterfactual reasoning.  
5. Though formulated using structural causal models as Pearl does, the semantics avoid the use of response variables, making the approach simpler and more general. Additionally, the author proves equivalence with two recent alternative semantics not reliant on structural causal models and aligns the theory with broader literature on stochastic counterfactuals. Finally, the paper reflects on the universality of the Markov condition and proposes a novel extension of causal abstractions. <div>
arXiv:2512.12804v1 Announce Type: new 
Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</title>
<link>https://arxiv.org/abs/2512.12806</link>
<guid>https://arxiv.org/abs/2512.12806</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fault-Tolerant Sandboxing, transactional filesystem, autonomous agents, safety risks  

<br /><br />Summary:  
This paper addresses the safety challenges posed by the transition of Large Language Models (LLMs) from passive code generation tools to autonomous agents capable of executing commands without user intervention. The authors identify significant risks including destructive commands and inconsistent system states that arise in fully autonomous operation. Existing commercial solutions focus on interactive user safety, often requiring authentication steps that disrupt the automated, headless workflows essential to true autonomy. To mitigate these issues, the paper proposes a Fault-Tolerant Sandboxing framework that combines a policy-based interception layer with a transactional filesystem snapshot mechanism. By encapsulating agent actions within atomic transactions, the framework ensures system safety while maintaining acceptable latency, thus avoiding the overhead and friction associated with containerization or CLI-based authentication. The method was experimentally validated using the Minimind-MoE LLM served through nano-vllm in a Proxmox testbed with EVPN/VXLAN network isolation. The results demonstrate perfect interception of high-risk commands and successful rollback of failed states, all achieved with a modest 14.5% performance overhead (~1.8 seconds per transaction). This outperforms existing Gemini CLI sandboxes that require interactive sign-ins, making the proposed framework better suited for autonomous agent workflows. <div>
arXiv:2512.12806v1 Announce Type: new 
Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title>
<link>https://arxiv.org/abs/2512.12856</link>
<guid>https://arxiv.org/abs/2512.12856</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, memory management, forgetting policies, privacy preservation, agent evaluation<br /><br />Summary:<br /><br />This paper addresses critical challenges in memory management for generative agents operating in long-term interactive scenarios, highlighting performance, privacy, and computational issues. It critiques existing methods that either keep unlimited memory, causing computational and privacy problems, or use simplistic forgetting strategies that reduce agent coherence and capabilities. The authors introduce the Memory-Aware Retention Schema (MaRS), a novel framework designed for human-centered memory management that integrates six theoretically grounded forgetting policies. These policies aim to optimize the trade-offs between performance, privacy, and computational efficiency. To evaluate these approaches, the paper presents the Forgetful but Faithful Agent (FiFA) benchmark, which assesses agents on narrative coherence, goal achievement, social recall accuracy, privacy preservation, and cost efficiency. Experimental results from 300 evaluation runs across various memory budgets and agent configurations demonstrate that a hybrid forgetting policy within MaRS achieves a superior composite performance score of 0.911, offering improved coherence alongside strict privacy guarantees and computational tractability. The study establishes new benchmarks for evaluating memory-budgeted generative agents and offers practical guidelines for deploying such agents in resource-constrained and privacy-sensitive contexts. Overall, the theoretical insights, framework, and empirical findings contribute significantly to human-centered AI by tackling fundamental agent memory management challenges that influence user trust, scalability, and compliance with regulations. <div>
arXiv:2512.12856v1 Announce Type: new 
Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Satisfiability Modulo Theory Meets Inductive Logic Programming</title>
<link>https://arxiv.org/abs/2512.12918</link>
<guid>https://arxiv.org/abs/2512.12918</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive Logic Programming, SMT solver, numerical constraints, PyGol, hybrid rules<br /><br />Summary:<br /><br />This paper explores extending Inductive Logic Programming (ILP) by integrating it with Satisfiability Modulo Theories (SMT) to better handle numerical constraints within relational domains. Traditional ILP frameworks predominantly work with discrete predicates and require either discretisation or manually designed numerical predicates, which limits their ability to learn arithmetic relations or thresholds that apply jointly across examples. The authors propose a modular approach that couples the ILP system PyGol with the SMT solver Z3. PyGol generates candidate clauses interpreted as quantifier-free formulas over background theories like linear or nonlinear real arithmetic. The SMT solver then instantiates and verifies the numerical parameters, enabling the induction of hybrid rules combining symbolic relational predicates with numerical constraints such as thresholds, intervals, and multi-literal arithmetic relations. This architecture preserves ILP's declarative bias while increasing expressivity and flexibility. The SMT-ILP framework is formalized and evaluated on synthetic datasets designed to test linear, relational, nonlinear, and multi-hop reasoning capabilities. Experimental results demonstrate that the modular SMT-ILP approach complements existing numerical ILP methods and provides a solid foundation for future enhancements toward richer, theory-aware inductive learning. <div>
arXiv:2512.12918v1 Announce Type: new 
Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open Standards for Systemic Complexity in Digital Forensics</title>
<link>https://arxiv.org/abs/2512.12970</link>
<guid>https://arxiv.org/abs/2512.12970</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, digital forensics, forensic errors, human-readable artifacts, open standards<br /><br />Summary:<br /><br />1. The paper discusses the increasing complexity and ubiquity of the intersection between artificial intelligence (AI) and digital forensics (DF), highlighting how overlapping techniques and technologies are being integrated into various scientific and technical fields. <br />2. It acknowledges that despite significant advancements in forensic sciences, errors remain prevalent, and forensic processes are still susceptible to human and systemic fallibility. <br />3. To address these challenges, the paper identifies systemic complexity as a key factor contributing to errors and proposes measures to mitigate this. <br />4. One major recommendation is the adoption of human-readable artifacts to improve transparency, interpretability, and trustworthiness in forensic analyses involving AI. <br />5. The study also advocates for the use of open standards to enhance consistency, reproducibility, and collaborative validation within the digital forensics community. <br />6. Finally, the paper outlines a state-of-the-art DF AI model schema designed to integrate these principles, aiming to reduce errors and improve the reliability of AI-assisted digital forensic investigations. <div>
arXiv:2512.12970v1 Announce Type: new 
Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization</title>
<link>https://arxiv.org/abs/2512.13070</link>
<guid>https://arxiv.org/abs/2512.13070</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-supervised reinforcement learning, policy collapse, M-GRPO, entropy filtering, Large Language Models<br /><br />Summary:<br /><br />This paper addresses a significant challenge in self-supervised reinforcement learning (RL) for enhancing Large Language Models (LLMs), identifying a critical failure mode termed "policy collapse" that leads to drastic performance degradation during long-horizon training. The authors demonstrate that increasing the number of rollouts delays but does not prevent this collapse, indicating that conventional scaling strategies are insufficient. To resolve this instability, they introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a novel framework that utilizes a slowly evolving momentum model to provide a stable training target, effectively stabilizing the training process. Additionally, the study finds that training instability is linked to a rapid decrease in policy entropy, causing the model to adopt a prematurely confident yet suboptimal policy. To counter this, the authors propose an adaptive interquartile range (IQR) filtering method that dynamically removes low-entropy trajectories, thereby preserving policy diversity essential for robust learning. Extensive experiments conducted on multiple reasoning benchmarks validate that M-GRPO combined with the IQR filter improves training stability and prevents premature convergence. Together, these innovations lead to state-of-the-art performance in self-supervised RL for reasoning tasks in LLMs, offering a valuable strategy for enhancing model reasoning without expensive human annotations. <div>
arXiv:2512.13070v1 Announce Type: new 
Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socratic Students: Teaching Language Models to Learn by Asking Questions</title>
<link>https://arxiv.org/abs/2512.13102</link>
<guid>https://arxiv.org/abs/2512.13102</guid>
<content:encoded><![CDATA[
<div> interactive learning, large language models, student-teacher interaction, Direct Preference Optimization, question generation<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) are strong at answering static queries based on their internal knowledge but struggle in settings requiring acquisition of new information through active interaction.  
2. In practical domains like education and medical assistance, information is often not directly available, necessitating an interactive agent that can identify its uncertainty, ask relevant questions, and retain newly acquired knowledge efficiently.  
3. Previous research has mainly focused on how teachers can guide students by identifying knowledge gaps and giving instructions, whereas this work shifts attention to the student’s role in actively querying the teacher.  
4. Experimental results on math and coding benchmarks demonstrate that student-led querying strategies consistently outperform static approaches, showing improvements of at least 0.5 in Pass@k metrics starting from near-zero baseline performance.  
5. To further improve question quality and learning efficiency, the authors train students using Direct Preference Optimization (DPO) with guidance sourced from either the model itself (self) or stronger student models, enabling smaller models to learn better questioning strategies. <div>
arXiv:2512.13102v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
<link>https://arxiv.org/abs/2512.13131</link>
<guid>https://arxiv.org/abs/2512.13131</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D gesture generation, speech-driven animation, hierarchical modeling, periodic autoencoders, co-speech gestures<br /><br />Summary:<br /><br />This paper addresses the challenge of generating realistic 3D body movements from speech, which has potential applications but struggles with producing natural human-like gestures. Existing approaches mainly use end-to-end learning models such as GANs, VQ-VAE, and diffusion models but often fail to capture the crucial inter- and intra-correlations between different motion units like the head, body, and hands. To overcome this, the authors propose a Hierarchical Implicit Periodicity (HIP) learning framework. The HIP approach features two key innovations: first, it disentangles complex gesture movements by exploring gesture motion phase manifolds using periodic autoencoders, which model human-like periodic patterns while also incorporating non-periodic latent states to introduce instance-level diversity. Second, it models the hierarchical dependencies among face motions, body gestures, and hand movements by using cascaded guidance during the learning process to produce coordinated animations. The method was demonstrated on 3D avatars and validated through extensive experiments, showing superior performance to state-of-the-art co-speech gesture generation techniques in both quantitative metrics and qualitative assessments. The authors plan to release their code and trained models publicly, enabling further research and practical applications in speech-driven 3D gesture animation. <div>
arXiv:2512.13131v1 Announce Type: new 
Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
<link>https://arxiv.org/abs/2512.13142</link>
<guid>https://arxiv.org/abs/2512.13142</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, abortion stigma, Individual Level Abortion Stigma Scale, multilevel understanding, AI safety<br /><br />Summary: This study evaluates whether large language models (LLMs) genuinely understand the complex psychological and physiological aspects of abortion stigma. It uses the validated Individual Level Abortion Stigma Scale (ILAS) to systematically test 627 diverse personas across five leading LLMs. The analysis focuses on three stigma levels: cognitive (self-judgment), interpersonal (anticipated judgment and isolation), and structural (community condemnation and disclosure patterns). The results reveal that LLMs fail to exhibit genuine understanding across all these levels. Specifically, models tend to overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation without nuance, and introduce demographic biases not present in human-validated data. Additionally, LLMs miss empirically established relationships, such as the connection between stigma and secrecy, and sometimes produce contradictory responses within theoretical frameworks. These findings indicate that current model alignment techniques ensure linguistically appropriate outputs but do not guarantee coherent, multilevel comprehension of complex human experiences. The paper argues that AI safety in sensitive, high-stakes areas requires novel design principles emphasizing multilevel coherence, continuous auditing protocols, regulatory measures including mandatory audits and accountability, and improved AI literacy especially in contexts where understanding unspoken experiences is essential to providing effective support. <div>
arXiv:2512.13142v1 Announce Type: new 
Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</title>
<link>https://arxiv.org/abs/2512.13154</link>
<guid>https://arxiv.org/abs/2512.13154</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, clarification dialogue, user ambiguity, conversational agents, MultiWOZ

<br /><br />Summary:  
This paper addresses the challenge of resolving ambiguous user requests in conversational agents, which is crucial for effective task completion. The authors focus on the underexplored problem of when and how multiple agents should initiate clarifications and coordinate their actions during uncertain or incomplete user inputs. To tackle this, they propose MAC (Multi-Agent Clarification), a novel interactive multi-agent framework specifically optimized for managing clarification dialogues. The approach begins with a new taxonomy that categorizes types of user ambiguities to systematically guide clarification strategies. MAC autonomously coordinates multiple agents to engage synergistically with users, improving dialogue efficiency and effectiveness. Empirical evaluations on the MultiWOZ 2.4 dataset demonstrate that incorporating clarification at two levels increases the task success rate significantly by 7.8%, from 54.5% to 62.3%. Additionally, the average number of dialogue turns decreases from 6.53 to 4.86, as the system elicits all necessary user information upfront while reducing repetitive exchanges. The results underscore the value of active user engagement and role-aware clarification within multi-agent conversational systems to enhance the reliability and naturalness of human-agent communication. <div>
arXiv:2512.13154v1 Announce Type: new 
Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13159</link>
<guid>https://arxiv.org/abs/2512.13159</guid>
<content:encoded><![CDATA[
<div> Keywords: human-agent collaboration, reinforcement learning, conversational proactivity, clarification questions, task-oriented dialogues<br /><br />Summary:<br /><br />1. The paper addresses the need for more proactive human-agent collaboration, moving beyond current models that mostly passively respond to user instructions without seeking clarifications.  
2. It introduces SpeakRL, a reinforcement learning method designed to improve agents’ conversational abilities by rewarding proactive behavior, specifically the asking of appropriate clarification questions when needed.  
3. To enable training and evaluation, the authors created SpeakER, a synthetic dataset featuring a variety of task-oriented dialogue scenarios that require interactive clarification to resolve tasks effectively.  
4. The work also explores how to design reward functions that incentivize a balance between agents asking clarifying questions and taking direct actions, optimizing for both engagement and efficiency.  
5. Experimental results demonstrate that SpeakRL achieves a 20.14% absolute improvement in task completion compared to baseline models, without increasing the number of conversation turns, and even outperforms larger proprietary models, highlighting the benefits of clarification-centric conversational strategies in user-agent interactions. <div>
arXiv:2512.13159v1 Announce Type: new 
Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finch: Benchmarking Finance &amp; Accounting across Spreadsheet-Centric Enterprise Workflows</title>
<link>https://arxiv.org/abs/2512.13168</link>
<guid>https://arxiv.org/abs/2512.13168</guid>
<content:encoded><![CDATA[
<div> finance, enterprise workflows, AI benchmarking, LLM-assisted annotation, spreadsheet analysis<br /><br />Summary: The paper introduces Finch, a finance and accounting benchmark designed to evaluate AI agents on complex, real-world enterprise workflows encompassing tasks like data entry, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is based on authentic datasets from Enron, including 15,000 spreadsheets and 500,000 emails from 150 employees, and other financial institutions, capturing the messy, multimodal nature of real enterprise data across various domains such as budgeting, trading, and asset management. The benchmark was created through a two-step workflow construction process involving LLM-assisted workflow discovery verified by experts, followed by meticulous expert annotation, requiring over 700 hours of domain-expert effort. This resulted in 172 composite workflows with 384 tasks, involving 1,710 spreadsheets containing 27 million cells, alongside PDFs and other related artifacts, reflecting the complexity and collaborative nature of enterprise work. The study evaluates top AI systems, including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, revealing that even the leading GPT 5.1 Pro, after 48 hours of testing, passes only 38.4% of workflows, while Claude Sonnet 4.5 achieves a 25.0% pass rate. Case studies highlight significant challenges these workflows present for AI agents, emphasizing the need for further research in this area. <div>
arXiv:2512.13168v1 Announce Type: new 
Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title>
<link>https://arxiv.org/abs/2512.13240</link>
<guid>https://arxiv.org/abs/2512.13240</guid>
<content:encoded><![CDATA[
<div> Direct Preference Optimization, Reflective Preference Optimization, hallucination reduction, large language models, policy learning  

<br /><br />Summary:  
This paper introduces Reflective Preference Optimization (RPO), a novel enhancement to Direct Preference Optimization (DPO), aimed at improving the alignment of large language models (LLMs) and vision-language models. Traditional DPO uses the same policy to generate both chosen and rejected responses, resulting in weak learning signals due to similar errors and low KL divergence between outputs, which slows convergence and makes training unstable. RPO addresses this by incorporating hint-guided reflection, leveraging external models to detect hallucination and produce concise reflective hints. These hints help create on-policy preference pairs with stronger contrastiveness and more distinct preference signals. Theoretically, conditioning on reflective hints increases the expected preference margin through mutual information, enhancing sample efficiency without departing from the original policy family. Empirical results demonstrate that RPO requires fewer training samples and iterations to achieve superior alignment compared to standard DPO. Furthermore, RPO significantly reduces hallucination rates and achieves state-of-the-art performance on various multimodal benchmarks. This approach provides a more effective and lightweight alternative to existing frameworks such as Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF), promising improved training stability and faster convergence in preference-based model alignment. <div>
arXiv:2512.13240v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data</title>
<link>https://arxiv.org/abs/2512.13297</link>
<guid>https://arxiv.org/abs/2512.13297</guid>
<content:encoded><![CDATA[
<div> Keywords: medical data analysis, multi-modal models, MedInsightBench, MedInsightAgent, healthcare insights  

<br /><br />Summary:  
1. The paper addresses the need for extracting deep insights from complex multi-modal medical datasets to enhance patient care, diagnostic accuracy, and healthcare operations.  
2. It highlights the current gap in high-quality datasets tailored to evaluate large multi-modal models (LMMs) for medical insight discovery.  
3. The authors introduce MedInsightBench, a novel benchmark containing 332 carefully curated medical cases with annotated insights, designed to assess LMMs' abilities to analyze multi-modal medical images, ask relevant questions, interpret findings, and generate actionable recommendations.  
4. Evaluation shows that existing LMMs struggle on MedInsightBench mainly due to difficulties in extracting deep, multi-step insights and lacking medical expertise.  
5. To overcome this, the paper proposes MedInsightAgent, an automated agent framework composed of three modules—Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer—which enhances general LMM performance in medical insight discovery.  
6. Experimental results on MedInsightBench reveal persistent challenges but demonstrate that MedInsightAgent significantly improves the capability of LMMs in analyzing medical data and generating meaningful insights. <div>
arXiv:2512.13297v1 Announce Type: new 
Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Prompt Optimization for Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2512.13323</link>
<guid>https://arxiv.org/abs/2512.13323</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, arithmetic reasoning, small language models, error-driven optimization, privacy-compliant  

<br /><br />Summary:  
The paper addresses the challenge of enhancing arithmetic reasoning capabilities within AI systems, particularly for regulated sectors like finance and healthcare where data privacy is critical. It introduces an error-driven optimization framework designed to improve the performance of Code Generation Agents (CGAs) that leverage on-premises small language models (SLMs). The approach involves systematically clustering erroneous predictions from the model and iteratively refining prompt-based rules to reduce errors. Experiments were conducted using the Qwen3 4B model, a state-of-the-art SLM. The base model initially showed significant limitations in executing accurate arithmetic operations on tabular data. However, after applying the proposed error-driven prompt optimization method, the model's accuracy improved dramatically to 70.8%. This improvement demonstrates that small, privacy-focused models, when paired with strategic prompt engineering, can outperform much larger language models, such as GPT-3.5 Turbo, for specialized arithmetic tasks. The study highlights that such enhancements can be achieved without costly fine-tuning, making the solution more practical and interpretable for industrial deployment. Ultimately, the research provides a pathway for developing reliable, industrial AI assistants capable of secure, on-premises operation with strong arithmetic reasoning skills. <div>
arXiv:2512.13323v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection</title>
<link>https://arxiv.org/abs/2512.13374</link>
<guid>https://arxiv.org/abs/2512.13374</guid>
<content:encoded><![CDATA[
arXiv:2512.13374v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Evolutionary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13399</link>
<guid>https://arxiv.org/abs/2512.13399</guid>
<content:encoded><![CDATA[
arXiv:2512.13399v1 Announce Type: new 
Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title>
<link>https://arxiv.org/abs/2512.13481</link>
<guid>https://arxiv.org/abs/2512.13481</guid>
<content:encoded><![CDATA[
arXiv:2512.13481v1 Announce Type: new 
Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defending the Hierarchical Result Models of Precedential Constraint</title>
<link>https://arxiv.org/abs/2512.13505</link>
<guid>https://arxiv.org/abs/2512.13505</guid>
<content:encoded><![CDATA[
arXiv:2512.13505v1 Announce Type: new 
Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</title>
<link>https://arxiv.org/abs/2512.13510</link>
<guid>https://arxiv.org/abs/2512.13510</guid>
<content:encoded><![CDATA[
arXiv:2512.13510v1 Announce Type: new 
Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level</title>
<link>https://arxiv.org/abs/2410.00015</link>
<guid>https://arxiv.org/abs/2410.00015</guid>
<content:encoded><![CDATA[
arXiv:2410.00015v2 Announce Type: cross 
Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[
arXiv:2508.15250v3 Announce Type: cross 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
<link>https://arxiv.org/abs/2512.11811</link>
<guid>https://arxiv.org/abs/2512.11811</guid>
<content:encoded><![CDATA[
arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare</title>
<link>https://arxiv.org/abs/2512.11814</link>
<guid>https://arxiv.org/abs/2512.11814</guid>
<content:encoded><![CDATA[
arXiv:2512.11814v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title>
<link>https://arxiv.org/abs/2512.11818</link>
<guid>https://arxiv.org/abs/2512.11818</guid>
<content:encoded><![CDATA[
arXiv:2512.11818v1 Announce Type: cross 
Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
<link>https://arxiv.org/abs/2512.11827</link>
<guid>https://arxiv.org/abs/2512.11827</guid>
<content:encoded><![CDATA[
arXiv:2512.11827v1 Announce Type: cross 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference with Reusable State-Dependent Value Profiles</title>
<link>https://arxiv.org/abs/2512.11829</link>
<guid>https://arxiv.org/abs/2512.11829</guid>
<content:encoded><![CDATA[
arXiv:2512.11829v1 Announce Type: cross 
Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.11830</link>
<guid>https://arxiv.org/abs/2512.11830</guid>
<content:encoded><![CDATA[
arXiv:2512.11830v1 Announce Type: cross 
Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2512.11832</link>
<guid>https://arxiv.org/abs/2512.11832</guid>
<content:encoded><![CDATA[
arXiv:2512.11832v1 Announce Type: cross 
Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $\Delta_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&amp;D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[
arXiv:2512.11833v1 Announce Type: cross 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions</title>
<link>https://arxiv.org/abs/2512.11836</link>
<guid>https://arxiv.org/abs/2512.11836</guid>
<content:encoded><![CDATA[
arXiv:2512.11836v1 Announce Type: cross 
Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[
arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spiking Manifesto</title>
<link>https://arxiv.org/abs/2512.11843</link>
<guid>https://arxiv.org/abs/2512.11843</guid>
<content:encoded><![CDATA[
arXiv:2512.11843v1 Announce Type: cross 
Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach</title>
<link>https://arxiv.org/abs/2512.11845</link>
<guid>https://arxiv.org/abs/2512.11845</guid>
<content:encoded><![CDATA[
arXiv:2512.11845v1 Announce Type: cross 
Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document</title>
<link>https://arxiv.org/abs/2512.11849</link>
<guid>https://arxiv.org/abs/2512.11849</guid>
<content:encoded><![CDATA[
arXiv:2512.11849v1 Announce Type: cross 
Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs</title>
<link>https://arxiv.org/abs/2512.11851</link>
<guid>https://arxiv.org/abs/2512.11851</guid>
<content:encoded><![CDATA[
arXiv:2512.11851v1 Announce Type: cross 
Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things</title>
<link>https://arxiv.org/abs/2512.11852</link>
<guid>https://arxiv.org/abs/2512.11852</guid>
<content:encoded><![CDATA[
arXiv:2512.11852v1 Announce Type: cross 
Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks</title>
<link>https://arxiv.org/abs/2512.11854</link>
<guid>https://arxiv.org/abs/2512.11854</guid>
<content:encoded><![CDATA[
arXiv:2512.11854v1 Announce Type: cross 
Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry</title>
<link>https://arxiv.org/abs/2512.11855</link>
<guid>https://arxiv.org/abs/2512.11855</guid>
<content:encoded><![CDATA[
arXiv:2512.11855v1 Announce Type: cross 
Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search</title>
<link>https://arxiv.org/abs/2512.11856</link>
<guid>https://arxiv.org/abs/2512.11856</guid>
<content:encoded><![CDATA[
arXiv:2512.11856v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopicProphet: Prophesies on Temporal Topic Trends and Stocks</title>
<link>https://arxiv.org/abs/2512.11857</link>
<guid>https://arxiv.org/abs/2512.11857</guid>
<content:encoded><![CDATA[
arXiv:2512.11857v1 Announce Type: cross 
Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Path Integral Diffusion: AdaPID</title>
<link>https://arxiv.org/abs/2512.11858</link>
<guid>https://arxiv.org/abs/2512.11858</guid>
<content:encoded><![CDATA[
arXiv:2512.11858v1 Announce Type: cross 
Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion</title>
<link>https://arxiv.org/abs/2512.11859</link>
<guid>https://arxiv.org/abs/2512.11859</guid>
<content:encoded><![CDATA[
arXiv:2512.11859v1 Announce Type: cross 
Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes</title>
<link>https://arxiv.org/abs/2512.11860</link>
<guid>https://arxiv.org/abs/2512.11860</guid>
<content:encoded><![CDATA[
arXiv:2512.11860v1 Announce Type: cross 
Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL</title>
<link>https://arxiv.org/abs/2512.11862</link>
<guid>https://arxiv.org/abs/2512.11862</guid>
<content:encoded><![CDATA[
arXiv:2512.11862v1 Announce Type: cross 
Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence</title>
<link>https://arxiv.org/abs/2512.11863</link>
<guid>https://arxiv.org/abs/2512.11863</guid>
<content:encoded><![CDATA[
arXiv:2512.11863v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.11865</link>
<guid>https://arxiv.org/abs/2512.11865</guid>
<content:encoded><![CDATA[
arXiv:2512.11865v1 Announce Type: cross 
Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[
arXiv:2512.11867v1 Announce Type: cross 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models</title>
<link>https://arxiv.org/abs/2512.11868</link>
<guid>https://arxiv.org/abs/2512.11868</guid>
<content:encoded><![CDATA[
arXiv:2512.11868v1 Announce Type: cross 
Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
<link>https://arxiv.org/abs/2512.11870</link>
<guid>https://arxiv.org/abs/2512.11870</guid>
<content:encoded><![CDATA[
arXiv:2512.11870v1 Announce Type: cross 
Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops</title>
<link>https://arxiv.org/abs/2512.11871</link>
<guid>https://arxiv.org/abs/2512.11871</guid>
<content:encoded><![CDATA[
arXiv:2512.11871v1 Announce Type: cross 
Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.11872</link>
<guid>https://arxiv.org/abs/2512.11872</guid>
<content:encoded><![CDATA[
arXiv:2512.11872v1 Announce Type: cross 
Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's About Time: The Temporal and Modal Dynamics of Copilot Usage</title>
<link>https://arxiv.org/abs/2512.11879</link>
<guid>https://arxiv.org/abs/2512.11879</guid>
<content:encoded><![CDATA[
arXiv:2512.11879v1 Announce Type: cross 
Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Structural Representation in Foundation Models for Polymers</title>
<link>https://arxiv.org/abs/2512.11881</link>
<guid>https://arxiv.org/abs/2512.11881</guid>
<content:encoded><![CDATA[
arXiv:2512.11881v1 Announce Type: cross 
Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education</title>
<link>https://arxiv.org/abs/2512.11882</link>
<guid>https://arxiv.org/abs/2512.11882</guid>
<content:encoded><![CDATA[
arXiv:2512.11882v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
<link>https://arxiv.org/abs/2512.11883</link>
<guid>https://arxiv.org/abs/2512.11883</guid>
<content:encoded><![CDATA[
arXiv:2512.11883v1 Announce Type: cross 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2512.11887</link>
<guid>https://arxiv.org/abs/2512.11887</guid>
<content:encoded><![CDATA[
arXiv:2512.11887v1 Announce Type: cross 
Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Should AI Become an Intergenerational Civil Right?</title>
<link>https://arxiv.org/abs/2512.11892</link>
<guid>https://arxiv.org/abs/2512.11892</guid>
<content:encoded><![CDATA[
arXiv:2512.11892v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.11893</link>
<guid>https://arxiv.org/abs/2512.11893</guid>
<content:encoded><![CDATA[
arXiv:2512.11893v1 Announce Type: cross 
Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A fine-grained look at causal effects in causal spaces</title>
<link>https://arxiv.org/abs/2512.11919</link>
<guid>https://arxiv.org/abs/2512.11919</guid>
<content:encoded><![CDATA[
arXiv:2512.11919v1 Announce Type: cross 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control</title>
<link>https://arxiv.org/abs/2512.11921</link>
<guid>https://arxiv.org/abs/2512.11921</guid>
<content:encoded><![CDATA[
arXiv:2512.11921v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use</title>
<link>https://arxiv.org/abs/2512.11922</link>
<guid>https://arxiv.org/abs/2512.11922</guid>
<content:encoded><![CDATA[
arXiv:2512.11922v1 Announce Type: cross 
Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
<link>https://arxiv.org/abs/2512.11925</link>
<guid>https://arxiv.org/abs/2512.11925</guid>
<content:encoded><![CDATA[
arXiv:2512.11925v1 Announce Type: cross 
Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene regulatory network inference algorithm based on spectral signed directed graph convolution</title>
<link>https://arxiv.org/abs/2512.11927</link>
<guid>https://arxiv.org/abs/2512.11927</guid>
<content:encoded><![CDATA[
arXiv:2512.11927v1 Announce Type: cross 
Abstract: Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion</title>
<link>https://arxiv.org/abs/2512.11928</link>
<guid>https://arxiv.org/abs/2512.11928</guid>
<content:encoded><![CDATA[
arXiv:2512.11928v1 Announce Type: cross 
Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction</title>
<link>https://arxiv.org/abs/2512.11930</link>
<guid>https://arxiv.org/abs/2512.11930</guid>
<content:encoded><![CDATA[
arXiv:2512.11930v1 Announce Type: cross 
Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy</title>
<link>https://arxiv.org/abs/2512.11931</link>
<guid>https://arxiv.org/abs/2512.11931</guid>
<content:encoded><![CDATA[
arXiv:2512.11931v1 Announce Type: cross 
Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title>
<link>https://arxiv.org/abs/2512.11933</link>
<guid>https://arxiv.org/abs/2512.11933</guid>
<content:encoded><![CDATA[
arXiv:2512.11933v1 Announce Type: cross 
Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</title>
<link>https://arxiv.org/abs/2512.11934</link>
<guid>https://arxiv.org/abs/2512.11934</guid>
<content:encoded><![CDATA[
arXiv:2512.11934v1 Announce Type: cross 
Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11941</link>
<guid>https://arxiv.org/abs/2512.11941</guid>
<content:encoded><![CDATA[
arXiv:2512.11941v1 Announce Type: cross 
Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism</title>
<link>https://arxiv.org/abs/2512.11943</link>
<guid>https://arxiv.org/abs/2512.11943</guid>
<content:encoded><![CDATA[
arXiv:2512.11943v1 Announce Type: cross 
Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach</title>
<link>https://arxiv.org/abs/2512.11944</link>
<guid>https://arxiv.org/abs/2512.11944</guid>
<content:encoded><![CDATA[
arXiv:2512.11944v1 Announce Type: cross 
Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations</title>
<link>https://arxiv.org/abs/2512.11946</link>
<guid>https://arxiv.org/abs/2512.11946</guid>
<content:encoded><![CDATA[
arXiv:2512.11946v1 Announce Type: cross 
Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)</title>
<link>https://arxiv.org/abs/2512.11979</link>
<guid>https://arxiv.org/abs/2512.11979</guid>
<content:encoded><![CDATA[
arXiv:2512.11979v1 Announce Type: cross 
Abstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[
arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering</title>
<link>https://arxiv.org/abs/2512.11984</link>
<guid>https://arxiv.org/abs/2512.11984</guid>
<content:encoded><![CDATA[
arXiv:2512.11984v1 Announce Type: cross 
Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[
arXiv:2512.11995v1 Announce Type: cross 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hold Onto That Thought: Assessing KV Cache Compression On Reasoning</title>
<link>https://arxiv.org/abs/2512.12008</link>
<guid>https://arxiv.org/abs/2512.12008</guid>
<content:encoded><![CDATA[
arXiv:2512.12008v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title>
<link>https://arxiv.org/abs/2512.12012</link>
<guid>https://arxiv.org/abs/2512.12012</guid>
<content:encoded><![CDATA[
arXiv:2512.12012v1 Announce Type: cross 
Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers</title>
<link>https://arxiv.org/abs/2512.12045</link>
<guid>https://arxiv.org/abs/2512.12045</guid>
<content:encoded><![CDATA[
arXiv:2512.12045v1 Announce Type: cross 
Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Tuning Open-Weight Language Models for BPMN Model Generation</title>
<link>https://arxiv.org/abs/2512.12063</link>
<guid>https://arxiv.org/abs/2512.12063</guid>
<content:encoded><![CDATA[
arXiv:2512.12063v1 Announce Type: cross 
Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title>
<link>https://arxiv.org/abs/2512.12066</link>
<guid>https://arxiv.org/abs/2512.12066</guid>
<content:encoded><![CDATA[
arXiv:2512.12066v1 Announce Type: cross 
Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
<link>https://arxiv.org/abs/2512.12069</link>
<guid>https://arxiv.org/abs/2512.12069</guid>
<content:encoded><![CDATA[
arXiv:2512.12069v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models</title>
<link>https://arxiv.org/abs/2512.12081</link>
<guid>https://arxiv.org/abs/2512.12081</guid>
<content:encoded><![CDATA[
arXiv:2512.12081v1 Announce Type: cross 
Abstract: Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A neuro-symbolic framework for accountability in public-sector AI</title>
<link>https://arxiv.org/abs/2512.12109</link>
<guid>https://arxiv.org/abs/2512.12109</guid>
<content:encoded><![CDATA[
arXiv:2512.12109v1 Announce Type: cross 
Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2512.12121</link>
<guid>https://arxiv.org/abs/2512.12121</guid>
<content:encoded><![CDATA[
arXiv:2512.12121v1 Announce Type: cross 
Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery</title>
<link>https://arxiv.org/abs/2512.12128</link>
<guid>https://arxiv.org/abs/2512.12128</guid>
<content:encoded><![CDATA[
arXiv:2512.12128v1 Announce Type: cross 
Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity</title>
<link>https://arxiv.org/abs/2512.12135</link>
<guid>https://arxiv.org/abs/2512.12135</guid>
<content:encoded><![CDATA[
arXiv:2512.12135v1 Announce Type: cross 
Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[
arXiv:2512.12142v1 Announce Type: cross 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings</title>
<link>https://arxiv.org/abs/2512.12167</link>
<guid>https://arxiv.org/abs/2512.12167</guid>
<content:encoded><![CDATA[
arXiv:2512.12167v1 Announce Type: cross 
Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Language Model Inference with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2512.12168</link>
<guid>https://arxiv.org/abs/2512.12168</guid>
<content:encoded><![CDATA[
arXiv:2512.12168v1 Announce Type: cross 
Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms</title>
<link>https://arxiv.org/abs/2512.12199</link>
<guid>https://arxiv.org/abs/2512.12199</guid>
<content:encoded><![CDATA[
arXiv:2512.12199v1 Announce Type: cross 
Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation</title>
<link>https://arxiv.org/abs/2512.12201</link>
<guid>https://arxiv.org/abs/2512.12201</guid>
<content:encoded><![CDATA[
arXiv:2512.12201v1 Announce Type: cross 
Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[
arXiv:2512.12206v1 Announce Type: cross 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search</title>
<link>https://arxiv.org/abs/2512.12207</link>
<guid>https://arxiv.org/abs/2512.12207</guid>
<content:encoded><![CDATA[
arXiv:2512.12207v1 Announce Type: cross 
Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.12211</link>
<guid>https://arxiv.org/abs/2512.12211</guid>
<content:encoded><![CDATA[
arXiv:2512.12211v1 Announce Type: cross 
Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Versatile Coding Agents in Synthetic Environments</title>
<link>https://arxiv.org/abs/2512.12216</link>
<guid>https://arxiv.org/abs/2512.12216</guid>
<content:encoded><![CDATA[
arXiv:2512.12216v1 Announce Type: cross 
Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs</title>
<link>https://arxiv.org/abs/2512.12222</link>
<guid>https://arxiv.org/abs/2512.12222</guid>
<content:encoded><![CDATA[
arXiv:2512.12222v1 Announce Type: cross 
Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Distance Measurement based on Multi-Kernel Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.12238</link>
<guid>https://arxiv.org/abs/2512.12238</guid>
<content:encoded><![CDATA[
arXiv:2512.12238v1 Announce Type: cross 
Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Mat\'ern and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Probing Cross-Family Sound Symbolism in 27 Languages</title>
<link>https://arxiv.org/abs/2512.12245</link>
<guid>https://arxiv.org/abs/2512.12245</guid>
<content:encoded><![CDATA[
arXiv:2512.12245v1 Announce Type: cross 
Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&amp;P 500 Index Volatility Forecasting</title>
<link>https://arxiv.org/abs/2512.12250</link>
<guid>https://arxiv.org/abs/2512.12250</guid>
<content:encoded><![CDATA[
arXiv:2512.12250v1 Announce Type: cross 
Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accurate de novo sequencing of the modified proteome with OmniNovo</title>
<link>https://arxiv.org/abs/2512.12272</link>
<guid>https://arxiv.org/abs/2512.12272</guid>
<content:encoded><![CDATA[
arXiv:2512.12272v1 Announce Type: cross 
Abstract: Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRC-Net: Gram Residual Co-attention Net for epilepsy prediction</title>
<link>https://arxiv.org/abs/2512.12273</link>
<guid>https://arxiv.org/abs/2512.12273</guid>
<content:encoded><![CDATA[
arXiv:2512.12273v1 Announce Type: cross 
Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[
arXiv:2512.12284v1 Announce Type: cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</title>
<link>https://arxiv.org/abs/2512.12285</link>
<guid>https://arxiv.org/abs/2512.12285</guid>
<content:encoded><![CDATA[
arXiv:2512.12285v1 Announce Type: cross 
Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title>
<link>https://arxiv.org/abs/2512.12324</link>
<guid>https://arxiv.org/abs/2512.12324</guid>
<content:encoded><![CDATA[
arXiv:2512.12324v1 Announce Type: cross 
Abstract: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks</title>
<link>https://arxiv.org/abs/2512.12332</link>
<guid>https://arxiv.org/abs/2512.12332</guid>
<content:encoded><![CDATA[
arXiv:2512.12332v1 Announce Type: cross 
Abstract: The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema</title>
<link>https://arxiv.org/abs/2512.12337</link>
<guid>https://arxiv.org/abs/2512.12337</guid>
<content:encoded><![CDATA[
arXiv:2512.12337v1 Announce Type: cross 
Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
<link>https://arxiv.org/abs/2512.12410</link>
<guid>https://arxiv.org/abs/2512.12410</guid>
<content:encoded><![CDATA[
arXiv:2512.12410v1 Announce Type: cross 
Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rough Sets for Explainability of Spectral Graph Clustering</title>
<link>https://arxiv.org/abs/2512.12436</link>
<guid>https://arxiv.org/abs/2512.12436</guid>
<content:encoded><![CDATA[
arXiv:2512.12436v1 Announce Type: cross 
Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
<link>https://arxiv.org/abs/2512.12461</link>
<guid>https://arxiv.org/abs/2512.12461</guid>
<content:encoded><![CDATA[
arXiv:2512.12461v1 Announce Type: cross 
Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
<link>https://arxiv.org/abs/2512.12462</link>
<guid>https://arxiv.org/abs/2512.12462</guid>
<content:encoded><![CDATA[
arXiv:2512.12462v1 Announce Type: cross 
Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Design Space of Transition Matching</title>
<link>https://arxiv.org/abs/2512.12465</link>
<guid>https://arxiv.org/abs/2512.12465</guid>
<content:encoded><![CDATA[
arXiv:2512.12465v1 Announce Type: cross 
Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion</title>
<link>https://arxiv.org/abs/2512.12474</link>
<guid>https://arxiv.org/abs/2512.12474</guid>
<content:encoded><![CDATA[
arXiv:2512.12474v1 Announce Type: cross 
Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers</title>
<link>https://arxiv.org/abs/2512.12483</link>
<guid>https://arxiv.org/abs/2512.12483</guid>
<content:encoded><![CDATA[
arXiv:2512.12483v1 Announce Type: cross 
Abstract: With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public</title>
<link>https://arxiv.org/abs/2512.12500</link>
<guid>https://arxiv.org/abs/2512.12500</guid>
<content:encoded><![CDATA[
arXiv:2512.12500v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts</title>
<link>https://arxiv.org/abs/2512.12506</link>
<guid>https://arxiv.org/abs/2512.12506</guid>
<content:encoded><![CDATA[
arXiv:2512.12506v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline</title>
<link>https://arxiv.org/abs/2512.12510</link>
<guid>https://arxiv.org/abs/2512.12510</guid>
<content:encoded><![CDATA[
arXiv:2512.12510v1 Announce Type: cross 
Abstract: The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.12523</link>
<guid>https://arxiv.org/abs/2512.12523</guid>
<content:encoded><![CDATA[
arXiv:2512.12523v1 Announce Type: cross 
Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</title>
<link>https://arxiv.org/abs/2512.12536</link>
<guid>https://arxiv.org/abs/2512.12536</guid>
<content:encoded><![CDATA[
arXiv:2512.12536v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model</title>
<link>https://arxiv.org/abs/2512.12545</link>
<guid>https://arxiv.org/abs/2512.12545</guid>
<content:encoded><![CDATA[
arXiv:2512.12545v1 Announce Type: cross 
Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
<link>https://arxiv.org/abs/2512.12560</link>
<guid>https://arxiv.org/abs/2512.12560</guid>
<content:encoded><![CDATA[
arXiv:2512.12560v1 Announce Type: cross 
Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupled Variational Reinforcement Learning for Language Model General Reasoning</title>
<link>https://arxiv.org/abs/2512.12576</link>
<guid>https://arxiv.org/abs/2512.12576</guid>
<content:encoded><![CDATA[
arXiv:2512.12576v1 Announce Type: cross 
Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Prompt Injection Attacks Against Application Using Classifiers</title>
<link>https://arxiv.org/abs/2512.12583</link>
<guid>https://arxiv.org/abs/2512.12583</guid>
<content:encoded><![CDATA[
arXiv:2512.12583v1 Announce Type: cross 
Abstract: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
<link>https://arxiv.org/abs/2512.12596</link>
<guid>https://arxiv.org/abs/2512.12596</guid>
<content:encoded><![CDATA[
arXiv:2512.12596v1 Announce Type: cross 
Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery</title>
<link>https://arxiv.org/abs/2512.12608</link>
<guid>https://arxiv.org/abs/2512.12608</guid>
<content:encoded><![CDATA[
arXiv:2512.12608v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</title>
<link>https://arxiv.org/abs/2512.12620</link>
<guid>https://arxiv.org/abs/2512.12620</guid>
<content:encoded><![CDATA[
arXiv:2512.12620v1 Announce Type: cross 
Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</title>
<link>https://arxiv.org/abs/2512.12630</link>
<guid>https://arxiv.org/abs/2512.12630</guid>
<content:encoded><![CDATA[
arXiv:2512.12630v1 Announce Type: cross 
Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.12633</link>
<guid>https://arxiv.org/abs/2512.12633</guid>
<content:encoded><![CDATA[
arXiv:2512.12633v1 Announce Type: cross 
Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.12662</link>
<guid>https://arxiv.org/abs/2512.12662</guid>
<content:encoded><![CDATA[
arXiv:2512.12662v1 Announce Type: cross 
Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[
arXiv:2512.12663v1 Announce Type: cross 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
<link>https://arxiv.org/abs/2512.12669</link>
<guid>https://arxiv.org/abs/2512.12669</guid>
<content:encoded><![CDATA[
arXiv:2512.12669v1 Announce Type: cross 
Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
<link>https://arxiv.org/abs/2512.12675</link>
<guid>https://arxiv.org/abs/2512.12675</guid>
<content:encoded><![CDATA[
arXiv:2512.12675v1 Announce Type: cross 
Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</title>
<link>https://arxiv.org/abs/2512.12677</link>
<guid>https://arxiv.org/abs/2512.12677</guid>
<content:encoded><![CDATA[
arXiv:2512.12677v1 Announce Type: cross 
Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.12683</link>
<guid>https://arxiv.org/abs/2512.12683</guid>
<content:encoded><![CDATA[
arXiv:2512.12683v1 Announce Type: cross 
Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</title>
<link>https://arxiv.org/abs/2512.12688</link>
<guid>https://arxiv.org/abs/2512.12688</guid>
<content:encoded><![CDATA[
arXiv:2512.12688v1 Announce Type: cross 
Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits</title>
<link>https://arxiv.org/abs/2512.12693</link>
<guid>https://arxiv.org/abs/2512.12693</guid>
<content:encoded><![CDATA[
arXiv:2512.12693v1 Announce Type: cross 
Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
<link>https://arxiv.org/abs/2512.12703</link>
<guid>https://arxiv.org/abs/2512.12703</guid>
<content:encoded><![CDATA[
arXiv:2512.12703v1 Announce Type: cross 
Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Scientific Literature Explorer using Machine Learning (ISLE)</title>
<link>https://arxiv.org/abs/2512.12760</link>
<guid>https://arxiv.org/abs/2512.12760</guid>
<content:encoded><![CDATA[
arXiv:2512.12760v1 Announce Type: cross 
Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[
arXiv:2512.12762v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[
arXiv:2512.12768v1 Announce Type: cross 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)</title>
<link>https://arxiv.org/abs/2512.12769</link>
<guid>https://arxiv.org/abs/2512.12769</guid>
<content:encoded><![CDATA[
arXiv:2512.12769v1 Announce Type: cross 
Abstract: Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.12773</link>
<guid>https://arxiv.org/abs/2512.12773</guid>
<content:encoded><![CDATA[
arXiv:2512.12773v1 Announce Type: cross 
Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State over Tokens: Characterizing the Role of Reasoning Tokens</title>
<link>https://arxiv.org/abs/2512.12777</link>
<guid>https://arxiv.org/abs/2512.12777</guid>
<content:encoded><![CDATA[
arXiv:2512.12777v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average</title>
<link>https://arxiv.org/abs/2512.12785</link>
<guid>https://arxiv.org/abs/2512.12785</guid>
<content:encoded><![CDATA[
arXiv:2512.12785v1 Announce Type: cross 
Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Statistical Significance of Online Regression over Multiple Datasets</title>
<link>https://arxiv.org/abs/2512.12787</link>
<guid>https://arxiv.org/abs/2512.12787</guid>
<content:encoded><![CDATA[
arXiv:2512.12787v1 Announce Type: cross 
Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title>
<link>https://arxiv.org/abs/2512.12791</link>
<guid>https://arxiv.org/abs/2512.12791</guid>
<content:encoded><![CDATA[
arXiv:2512.12791v1 Announce Type: cross 
Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2512.12792</link>
<guid>https://arxiv.org/abs/2512.12792</guid>
<content:encoded><![CDATA[
arXiv:2512.12792v1 Announce Type: cross 
Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
<link>https://arxiv.org/abs/2512.12802</link>
<guid>https://arxiv.org/abs/2512.12802</guid>
<content:encoded><![CDATA[
arXiv:2512.12802v1 Announce Type: cross 
Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
<link>https://arxiv.org/abs/2512.12805</link>
<guid>https://arxiv.org/abs/2512.12805</guid>
<content:encoded><![CDATA[
arXiv:2512.12805v1 Announce Type: cross 
Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization</title>
<link>https://arxiv.org/abs/2512.12809</link>
<guid>https://arxiv.org/abs/2512.12809</guid>
<content:encoded><![CDATA[
arXiv:2512.12809v1 Announce Type: cross 
Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</title>
<link>https://arxiv.org/abs/2512.12812</link>
<guid>https://arxiv.org/abs/2512.12812</guid>
<content:encoded><![CDATA[
arXiv:2512.12812v1 Announce Type: cross 
Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles</title>
<link>https://arxiv.org/abs/2512.12817</link>
<guid>https://arxiv.org/abs/2512.12817</guid>
<content:encoded><![CDATA[
arXiv:2512.12817v1 Announce Type: cross 
Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects</title>
<link>https://arxiv.org/abs/2512.12818</link>
<guid>https://arxiv.org/abs/2512.12818</guid>
<content:encoded><![CDATA[
arXiv:2512.12818v1 Announce Type: cross 
Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the continuity of flows</title>
<link>https://arxiv.org/abs/2512.12821</link>
<guid>https://arxiv.org/abs/2512.12821</guid>
<content:encoded><![CDATA[
arXiv:2512.12821v1 Announce Type: cross 
Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.12822</link>
<guid>https://arxiv.org/abs/2512.12822</guid>
<content:encoded><![CDATA[
arXiv:2512.12822v1 Announce Type: cross 
Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
<link>https://arxiv.org/abs/2512.12824</link>
<guid>https://arxiv.org/abs/2512.12824</guid>
<content:encoded><![CDATA[
arXiv:2512.12824v1 Announce Type: cross 
Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</title>
<link>https://arxiv.org/abs/2512.12832</link>
<guid>https://arxiv.org/abs/2512.12832</guid>
<content:encoded><![CDATA[
arXiv:2512.12832v1 Announce Type: cross 
Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title>
<link>https://arxiv.org/abs/2512.12840</link>
<guid>https://arxiv.org/abs/2512.12840</guid>
<content:encoded><![CDATA[
arXiv:2512.12840v1 Announce Type: cross 
Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word priv\'ee, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
<link>https://arxiv.org/abs/2512.12842</link>
<guid>https://arxiv.org/abs/2512.12842</guid>
<content:encoded><![CDATA[
arXiv:2512.12842v1 Announce Type: cross 
Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Conformal Risk Control</title>
<link>https://arxiv.org/abs/2512.12844</link>
<guid>https://arxiv.org/abs/2512.12844</guid>
<content:encoded><![CDATA[
arXiv:2512.12844v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2512.12858</link>
<guid>https://arxiv.org/abs/2512.12858</guid>
<content:encoded><![CDATA[
arXiv:2512.12858v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</title>
<link>https://arxiv.org/abs/2512.12868</link>
<guid>https://arxiv.org/abs/2512.12868</guid>
<content:encoded><![CDATA[
arXiv:2512.12868v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels</title>
<link>https://arxiv.org/abs/2512.12870</link>
<guid>https://arxiv.org/abs/2512.12870</guid>
<content:encoded><![CDATA[
arXiv:2512.12870v1 Announce Type: cross 
Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
<link>https://arxiv.org/abs/2512.12885</link>
<guid>https://arxiv.org/abs/2512.12885</guid>
<content:encoded><![CDATA[
arXiv:2512.12885v1 Announce Type: cross 
Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.12888</link>
<guid>https://arxiv.org/abs/2512.12888</guid>
<content:encoded><![CDATA[
arXiv:2512.12888v1 Announce Type: cross 
Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2512.12914</link>
<guid>https://arxiv.org/abs/2512.12914</guid>
<content:encoded><![CDATA[
arXiv:2512.12914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cisco Integrated AI Security and Safety Framework Report</title>
<link>https://arxiv.org/abs/2512.12921</link>
<guid>https://arxiv.org/abs/2512.12921</guid>
<content:encoded><![CDATA[
arXiv:2512.12921v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
<link>https://arxiv.org/abs/2512.12929</link>
<guid>https://arxiv.org/abs/2512.12929</guid>
<content:encoded><![CDATA[
arXiv:2512.12929v1 Announce Type: cross 
Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
<link>https://arxiv.org/abs/2512.12932</link>
<guid>https://arxiv.org/abs/2512.12932</guid>
<content:encoded><![CDATA[
arXiv:2512.12932v1 Announce Type: cross 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
<link>https://arxiv.org/abs/2512.12935</link>
<guid>https://arxiv.org/abs/2512.12935</guid>
<content:encoded><![CDATA[
arXiv:2512.12935v1 Announce Type: cross 
Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
<link>https://arxiv.org/abs/2512.12936</link>
<guid>https://arxiv.org/abs/2512.12936</guid>
<content:encoded><![CDATA[
arXiv:2512.12936v1 Announce Type: cross 
Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping</title>
<link>https://arxiv.org/abs/2512.12950</link>
<guid>https://arxiv.org/abs/2512.12950</guid>
<content:encoded><![CDATA[
arXiv:2512.12950v1 Announce Type: cross 
Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[
arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[
arXiv:2512.12997v1 Announce Type: cross 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Bidirectional Spans and Span Violations in Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.13033</link>
<guid>https://arxiv.org/abs/2512.13033</guid>
<content:encoded><![CDATA[
arXiv:2512.13033v1 Announce Type: cross 
Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
<link>https://arxiv.org/abs/2512.13043</link>
<guid>https://arxiv.org/abs/2512.13043</guid>
<content:encoded><![CDATA[
arXiv:2512.13043v1 Announce Type: cross 
Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title>
<link>https://arxiv.org/abs/2512.13063</link>
<guid>https://arxiv.org/abs/2512.13063</guid>
<content:encoded><![CDATA[
arXiv:2512.13063v1 Announce Type: cross 
Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval</title>
<link>https://arxiv.org/abs/2512.13074</link>
<guid>https://arxiv.org/abs/2512.13074</guid>
<content:encoded><![CDATA[
arXiv:2512.13074v1 Announce Type: cross 
Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
<link>https://arxiv.org/abs/2512.13089</link>
<guid>https://arxiv.org/abs/2512.13089</guid>
<content:encoded><![CDATA[
arXiv:2512.13089v1 Announce Type: cross 
Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</title>
<link>https://arxiv.org/abs/2512.13094</link>
<guid>https://arxiv.org/abs/2512.13094</guid>
<content:encoded><![CDATA[
arXiv:2512.13094v1 Announce Type: cross 
Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning</title>
<link>https://arxiv.org/abs/2512.13100</link>
<guid>https://arxiv.org/abs/2512.13100</guid>
<content:encoded><![CDATA[
arXiv:2512.13100v1 Announce Type: cross 
Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\pi_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[
arXiv:2512.13101v1 Announce Type: cross 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.13106</link>
<guid>https://arxiv.org/abs/2512.13106</guid>
<content:encoded><![CDATA[
arXiv:2512.13106v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2512.13107</link>
<guid>https://arxiv.org/abs/2512.13107</guid>
<content:encoded><![CDATA[
arXiv:2512.13107v1 Announce Type: cross 
Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing</title>
<link>https://arxiv.org/abs/2512.13109</link>
<guid>https://arxiv.org/abs/2512.13109</guid>
<content:encoded><![CDATA[
arXiv:2512.13109v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title>
<link>https://arxiv.org/abs/2512.13111</link>
<guid>https://arxiv.org/abs/2512.13111</guid>
<content:encoded><![CDATA[
arXiv:2512.13111v1 Announce Type: cross 
Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
<link>https://arxiv.org/abs/2512.13122</link>
<guid>https://arxiv.org/abs/2512.13122</guid>
<content:encoded><![CDATA[
arXiv:2512.13122v1 Announce Type: cross 
Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
<link>https://arxiv.org/abs/2512.13157</link>
<guid>https://arxiv.org/abs/2512.13157</guid>
<content:encoded><![CDATA[
arXiv:2512.13157v1 Announce Type: cross 
Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
<link>https://arxiv.org/abs/2512.13164</link>
<guid>https://arxiv.org/abs/2512.13164</guid>
<content:encoded><![CDATA[
arXiv:2512.13164v1 Announce Type: cross 
Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACn: Soft Actor-Critic with n-step Returns</title>
<link>https://arxiv.org/abs/2512.13165</link>
<guid>https://arxiv.org/abs/2512.13165</guid>
<content:encoded><![CDATA[
arXiv:2512.13165v1 Announce Type: cross 
Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments</title>
<link>https://arxiv.org/abs/2512.13174</link>
<guid>https://arxiv.org/abs/2512.13174</guid>
<content:encoded><![CDATA[
arXiv:2512.13174v1 Announce Type: cross 
Abstract: Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning</title>
<link>https://arxiv.org/abs/2512.13186</link>
<guid>https://arxiv.org/abs/2512.13186</guid>
<content:encoded><![CDATA[
arXiv:2512.13186v1 Announce Type: cross 
Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
<link>https://arxiv.org/abs/2512.13190</link>
<guid>https://arxiv.org/abs/2512.13190</guid>
<content:encoded><![CDATA[
arXiv:2512.13190v1 Announce Type: cross 
Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13194</link>
<guid>https://arxiv.org/abs/2512.13194</guid>
<content:encoded><![CDATA[
arXiv:2512.13194v1 Announce Type: cross 
Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
<link>https://arxiv.org/abs/2512.13235</link>
<guid>https://arxiv.org/abs/2512.13235</guid>
<content:encoded><![CDATA[
arXiv:2512.13235v1 Announce Type: cross 
Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[
arXiv:2512.13290v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration</title>
<link>https://arxiv.org/abs/2512.13293</link>
<guid>https://arxiv.org/abs/2512.13293</guid>
<content:encoded><![CDATA[
arXiv:2512.13293v1 Announce Type: cross 
Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniLingua: A Small Open-Source LLM for European Languages</title>
<link>https://arxiv.org/abs/2512.13298</link>
<guid>https://arxiv.org/abs/2512.13298</guid>
<content:encoded><![CDATA[
arXiv:2512.13298v1 Announce Type: cross 
Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction</title>
<link>https://arxiv.org/abs/2512.13300</link>
<guid>https://arxiv.org/abs/2512.13300</guid>
<content:encoded><![CDATA[
arXiv:2512.13300v1 Announce Type: cross 
Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title>
<link>https://arxiv.org/abs/2512.13316</link>
<guid>https://arxiv.org/abs/2512.13316</guid>
<content:encoded><![CDATA[
arXiv:2512.13316v1 Announce Type: cross 
Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[
arXiv:2512.13317v1 Announce Type: cross 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title>
<link>https://arxiv.org/abs/2512.13325</link>
<guid>https://arxiv.org/abs/2512.13325</guid>
<content:encoded><![CDATA[
arXiv:2512.13325v1 Announce Type: cross 
Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</title>
<link>https://arxiv.org/abs/2512.13330</link>
<guid>https://arxiv.org/abs/2512.13330</guid>
<content:encoded><![CDATA[
arXiv:2512.13330v1 Announce Type: cross 
Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)</title>
<link>https://arxiv.org/abs/2512.13356</link>
<guid>https://arxiv.org/abs/2512.13356</guid>
<content:encoded><![CDATA[
arXiv:2512.13356v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers</title>
<link>https://arxiv.org/abs/2512.13363</link>
<guid>https://arxiv.org/abs/2512.13363</guid>
<content:encoded><![CDATA[
arXiv:2512.13363v1 Announce Type: cross 
Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
<link>https://arxiv.org/abs/2512.13402</link>
<guid>https://arxiv.org/abs/2512.13402</guid>
<content:encoded><![CDATA[
arXiv:2512.13402v1 Announce Type: cross 
Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents</title>
<link>https://arxiv.org/abs/2512.13438</link>
<guid>https://arxiv.org/abs/2512.13438</guid>
<content:encoded><![CDATA[
arXiv:2512.13438v1 Announce Type: cross 
Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v1 Announce Type: cross 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
arXiv:2512.13478v1 Announce Type: cross 
Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $\rho$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping</title>
<link>https://arxiv.org/abs/2512.13494</link>
<guid>https://arxiv.org/abs/2512.13494</guid>
<content:encoded><![CDATA[
arXiv:2512.13494v1 Announce Type: cross 
Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, na\"ive low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS</title>
<link>https://arxiv.org/abs/2512.13501</link>
<guid>https://arxiv.org/abs/2512.13501</guid>
<content:encoded><![CDATA[
arXiv:2512.13501v1 Announce Type: cross 
Abstract: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifying Rumors via Stance-Aware Structural Modeling</title>
<link>https://arxiv.org/abs/2512.13559</link>
<guid>https://arxiv.org/abs/2512.13559</guid>
<content:encoded><![CDATA[
arXiv:2512.13559v1 Announce Type: cross 
Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory in the Age of AI Agents</title>
<link>https://arxiv.org/abs/2512.13564</link>
<guid>https://arxiv.org/abs/2512.13564</guid>
<content:encoded><![CDATA[
arXiv:2512.13564v1 Announce Type: cross 
Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
<link>https://arxiv.org/abs/2512.13568</link>
<guid>https://arxiv.org/abs/2512.13568</guid>
<content:encoded><![CDATA[
arXiv:2512.13568v1 Announce Type: cross 
Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title>
<link>https://arxiv.org/abs/2512.13583</link>
<guid>https://arxiv.org/abs/2512.13583</guid>
<content:encoded><![CDATA[
arXiv:2512.13583v1 Announce Type: cross 
Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}{\delta} \right)}/(\sqrt{n}J\epsilon) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(\epsilon, \delta\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</title>
<link>https://arxiv.org/abs/2512.13586</link>
<guid>https://arxiv.org/abs/2512.13586</guid>
<content:encoded><![CDATA[
arXiv:2512.13586v1 Announce Type: cross 
Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
<link>https://arxiv.org/abs/2512.13600</link>
<guid>https://arxiv.org/abs/2512.13600</guid>
<content:encoded><![CDATA[
arXiv:2512.13600v1 Announce Type: cross 
Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
<link>https://arxiv.org/abs/2512.13607</link>
<guid>https://arxiv.org/abs/2512.13607</guid>
<content:encoded><![CDATA[
arXiv:2512.13607v1 Announce Type: cross 
Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[
arXiv:2512.13641v1 Announce Type: cross 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.13644</link>
<guid>https://arxiv.org/abs/2512.13644</guid>
<content:encoded><![CDATA[
arXiv:2512.13644v1 Announce Type: cross 
Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Language Memorization During the Classification of United States Supreme Court Cases</title>
<link>https://arxiv.org/abs/2512.13654</link>
<guid>https://arxiv.org/abs/2512.13654</guid>
<content:encoded><![CDATA[
arXiv:2512.13654v1 Announce Type: cross 
Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</title>
<link>https://arxiv.org/abs/2512.13658</link>
<guid>https://arxiv.org/abs/2512.13658</guid>
<content:encoded><![CDATA[
arXiv:2512.13658v1 Announce Type: cross 
Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
<link>https://arxiv.org/abs/2512.13678</link>
<guid>https://arxiv.org/abs/2512.13678</guid>
<content:encoded><![CDATA[
arXiv:2512.13678v1 Announce Type: cross 
Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[
arXiv:2512.13690v1 Announce Type: cross 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[
arXiv:2409.20302v5 Announce Type: replace 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many views treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary modifications, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and less explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignment(s) from OM to reduce the number of matching candidates and improve overall OV performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Driven Reasoning in DatalogMTL with Magic Sets</title>
<link>https://arxiv.org/abs/2412.07259</link>
<guid>https://arxiv.org/abs/2412.07259</guid>
<content:encoded><![CDATA[
arXiv:2412.07259v4 Announce Type: replace 
Abstract: DatalogMTL is a powerful rule-based language for temporal reasoning. Due to its high expressive power and flexible modeling capabilities, it is suitable for a wide range of applications, including tasks from industrial and financial sectors. However, due to its high computational complexity, practical reasoning in DatalogMTL is highly challenging. To address this difficulty, we introduce a new reasoning method for DatalogMTL which exploits the magic sets technique -- a rewriting approach developed for (non-temporal) Datalog to simulate top-down evaluation with bottom-up reasoning. We have implemented this approach and evaluated it on publicly available benchmarks, showing that the proposed approach significantly and consistently outperformed state-of-the-art reasoning techniques.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Traitors: Deception and Trust in Multi-Agent Language Model Simulations</title>
<link>https://arxiv.org/abs/2505.12923</link>
<guid>https://arxiv.org/abs/2505.12923</guid>
<content:encoded><![CDATA[
arXiv:2505.12923v2 Announce Type: replace 
Abstract: As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v5 Announce Type: replace 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMELLNET: A Large-scale Dataset for Real-world Smell Recognition</title>
<link>https://arxiv.org/abs/2506.00239</link>
<guid>https://arxiv.org/abs/2506.00239</guid>
<content:encoded><![CDATA[
arXiv:2506.00239v3 Announce Type: replace 
Abstract: The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large-scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use small gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 828,000 data points across 50 substances, spanning nuts, spices, herbs, fruits, and vegetables, and 43 mixtures among them, with 68 hours of data collected. Using SmellNet, we developed ScentFormer, a Transformer-based architecture combining temporal differencing and sliding-window augmentation for smell data. For the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy, and for the SmellNet-Mixture distribution prediction task, ScentFormer achieves 50.2% Top-1@0.1 on the test-seen split. ScentFormer's ability to generalize across conditions and capture transient chemical dynamics demonstrates the promise of temporal modeling in olfactory AI. SmellNet and ScentFormer lay the groundwork for real-world olfactory applications across healthcare, food and beverage, environmental monitoring, manufacturing, and entertainment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
arXiv:2506.07927v3 Announce Type: replace 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Copilots for Reproducibility in Science: A Case Study</title>
<link>https://arxiv.org/abs/2506.20130</link>
<guid>https://arxiv.org/abs/2506.20130</guid>
<content:encoded><![CDATA[
arXiv:2506.20130v4 Announce Type: replace 
Abstract: Open science initiatives seek to make research outputs more transparent, accessible, and reusable, but ensuring that published findings can be independently reproduced remains a persistent challenge. In this paper we describe an AI-driven "Reproducibility Copilot" that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or "rote", reproducibility. Our initial results suggest that the copilot has the potential to substantially reduce reproduction time (in one case from over 30 hours to about 1 hour) while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing values for hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets. Although preliminary, these findings suggest that AI tools can meaningfully reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference AI Systems for Scientific Discovery</title>
<link>https://arxiv.org/abs/2506.21329</link>
<guid>https://arxiv.org/abs/2506.21329</guid>
<content:encoded><![CDATA[
arXiv:2506.21329v4 Announce Type: replace 
Abstract: The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation -- exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns -- and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles -- rather than a monolithic recipe -- are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system's ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[
arXiv:2508.19005v5 Announce Type: replace 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Common Framework for Autoformalization</title>
<link>https://arxiv.org/abs/2509.09810</link>
<guid>https://arxiv.org/abs/2509.09810</guid>
<content:encoded><![CDATA[
arXiv:2509.09810v3 Announce Type: replace 
Abstract: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</title>
<link>https://arxiv.org/abs/2509.24250</link>
<guid>https://arxiv.org/abs/2509.24250</guid>
<content:encoded><![CDATA[
arXiv:2509.24250v2 Announce Type: replace 
Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale</title>
<link>https://arxiv.org/abs/2509.25540</link>
<guid>https://arxiv.org/abs/2509.25540</guid>
<content:encoded><![CDATA[
arXiv:2509.25540v2 Announce Type: replace 
Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: The AI Pluralism Index</title>
<link>https://arxiv.org/abs/2510.08193</link>
<guid>https://arxiv.org/abs/2510.08193</guid>
<content:encoded><![CDATA[
arXiv:2510.08193v2 Announce Type: replace 
Abstract: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling "Unknown" evidence to report both lower-bound ("evidence") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[
arXiv:2510.14925v3 Announce Type: replace 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in linear-Gaussian state-space models via H-Risk, a composite instability index integrating spectral margin, conditioning, temporal sensitivity, and innovation amplification. In simulations, higher H-Risk predicts overconfident errors and degraded closed-loop behavior even when the dynamics remain formally stable, exposing a gap between nominal and epistemic stability.
  Extending this stability lens to large language models (LLMs), we introduce a domain-wise proxy based on confidence fluctuations and overconfident errors. In a binary-question study, a Kantian-inspired policy that permits ''cannot judge'' responses yields targeted reductions in policy-aware squared loss in high-stakes domains relative to an overconfident baseline. To probe internal dynamics, we analyse layer-wise sensitivity of hidden states to small input perturbations. Contrary to a naive instability hypothesis, confidently wrong answers show no instability gap; instead, they are at least as locally stable as confidently correct answers, revealing stable miscalibration in which hallucinations behave like robust but misaligned attractors. For Qwen-2.5, spectral and activation profiles suggest a high signal-to-noise, low effective signal temperature regime in which representations become inertial and resistant to contextual shifts. These results bridge Kantian self-limitation and feedback control, and suggest that stable high-confidence hallucinations may not be readily corrected by output-only heuristics (e.g., temperature scaling or re-sampling), motivating process-level interventions that explicitly perturb and re-evaluate the inference trajectory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
<link>https://arxiv.org/abs/2510.21720</link>
<guid>https://arxiv.org/abs/2510.21720</guid>
<content:encoded><![CDATA[
arXiv:2510.21720v2 Announce Type: replace 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive "Personality Brain." Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making</title>
<link>https://arxiv.org/abs/2511.12876</link>
<guid>https://arxiv.org/abs/2511.12876</guid>
<content:encoded><![CDATA[
arXiv:2511.12876v2 Announce Type: replace 
Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[
arXiv:2511.15407v2 Announce Type: replace 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</title>
<link>https://arxiv.org/abs/2511.17729</link>
<guid>https://arxiv.org/abs/2511.17729</guid>
<content:encoded><![CDATA[
arXiv:2511.17729v3 Announce Type: replace 
Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures</title>
<link>https://arxiv.org/abs/2511.17833</link>
<guid>https://arxiv.org/abs/2511.17833</guid>
<content:encoded><![CDATA[
arXiv:2511.17833v2 Announce Type: replace 
Abstract: Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Localisation in Localist LLMs</title>
<link>https://arxiv.org/abs/2511.18375</link>
<guid>https://arxiv.org/abs/2511.18375</guid>
<content:encoded><![CDATA[
arXiv:2511.18375v3 Announce Type: replace 
Abstract: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate five locality configurations: two uniform baselines (fully distributed and fully localist) and three progressive polynomial schedules. We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty</title>
<link>https://arxiv.org/abs/2511.21569</link>
<guid>https://arxiv.org/abs/2511.21569</guid>
<content:encoded><![CDATA[
arXiv:2511.21569v4 Announce Type: replace 
Abstract: Self-transparency is a critical safety boundary, requiring language models to honestly disclose their limitations and artificial nature. This study stress-tests this capability, investigating whether models willingly disclose their identity when assigned professional personas that conflict with transparent self-representation. When models prioritize role consistency over this boundary disclosure, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% -- an 8.8-fold difference that emerged at the initial epistemic inquiry. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($\Delta R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($\kappa=0.908$). Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs</title>
<link>https://arxiv.org/abs/2512.00319</link>
<guid>https://arxiv.org/abs/2512.00319</guid>
<content:encoded><![CDATA[
arXiv:2512.00319v2 Announce Type: replace 
Abstract: The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</title>
<link>https://arxiv.org/abs/2512.03005</link>
<guid>https://arxiv.org/abs/2512.03005</guid>
<content:encoded><![CDATA[
arXiv:2512.03005v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Soccer Substitutions</title>
<link>https://arxiv.org/abs/2512.04480</link>
<guid>https://arxiv.org/abs/2512.04480</guid>
<content:encoded><![CDATA[
arXiv:2512.04480v3 Announce Type: replace 
Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title>
<link>https://arxiv.org/abs/2512.05356</link>
<guid>https://arxiv.org/abs/2512.05356</guid>
<content:encoded><![CDATA[
arXiv:2512.05356v2 Announce Type: replace 
Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach</title>
<link>https://arxiv.org/abs/2210.00858</link>
<guid>https://arxiv.org/abs/2210.00858</guid>
<content:encoded><![CDATA[
arXiv:2210.00858v4 Announce Type: replace-cross 
Abstract: In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power of deep networks. We generate a 3D vision-and-language synthetic dataset of tabletop scenes in a simulation environment to train our approach and perform extensive evaluations in both synthetic and real-world scenes. Results showcase the benefits of our approach in terms of accuracy, sample-efficiency, and robustness to the user's vocabulary, while being transferable to real-world scenes with few-shot visual fine-tuning. Finally, we integrate our method with a robot framework and demonstrate how it can serve as an interpretable solution for an interactive object-picking task, achieving an average success rate of 80.2\%, both in simulation and with a real robot. We make supplementary material available in https://gtziafas.github.io/neurosymbolic-manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling</title>
<link>https://arxiv.org/abs/2401.08930</link>
<guid>https://arxiv.org/abs/2401.08930</guid>
<content:encoded><![CDATA[
arXiv:2401.08930v2 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis PADS, a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Self-Supervised Learning for Recommendation</title>
<link>https://arxiv.org/abs/2404.03354</link>
<guid>https://arxiv.org/abs/2404.03354</guid>
<content:encoded><![CDATA[
arXiv:2404.03354v3 Announce Type: replace-cross 
Abstract: Recommender systems play a crucial role in tackling the challenge of information overload by delivering personalized recommendations based on individual user preferences. Deep learning techniques, such as RNNs, GNNs, and Transformer architectures, have significantly propelled the advancement of recommender systems by enhancing their comprehension of user behaviors and preferences. However, supervised learning methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively. To address this, self-supervised learning (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data. By leveraging unlabeled data and extracting meaningful representations, recommender systems utilizing SSL can make accurate predictions and recommendations even when confronted with data sparsity. In this paper, we provide a comprehensive review of self-supervised learning frameworks designed for recommender systems, encompassing a thorough analysis of over 170 papers. We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced recommenders in different contexts. For each domain, we elaborate on different self-supervised learning paradigms, namely contrastive learning, generative learning, and adversarial learning, so as to present technical details of how SSL enhances recommender systems in various contexts. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-SSLRec-Papers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need</title>
<link>https://arxiv.org/abs/2405.07293</link>
<guid>https://arxiv.org/abs/2405.07293</guid>
<content:encoded><![CDATA[
arXiv:2405.07293v2 Announce Type: replace-cross 
Abstract: Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\% while consuming only 19.12\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Common Neighbor for Temporal Graph Link Prediction</title>
<link>https://arxiv.org/abs/2406.07926</link>
<guid>https://arxiv.org/abs/2406.07926</guid>
<content:encoded><![CDATA[
arXiv:2406.07926v2 Announce Type: replace-cross 
Abstract: Temporal graphs are widespread in real-world applications such as social networks, as well as trade and transportation networks. Predicting dynamic links within these evolving graphs is a key problem. Many memory-based methods use temporal interaction histories to generate node embeddings, which are then combined to predict links. However, these approaches primarily focus on individual node representations, often overlooking the inherently pairwise nature of link prediction. While some recent methods attempt to capture pairwise features, they tend to be limited by high computational complexity arising from repeated embedding calculations, making them unsuitable for large-scale datasets like the Temporal Graph Benchmark (TGB). To address the critical need for models that combine strong expressive power with high computational efficiency for link prediction on large temporal graphs, we propose Temporal Neural Common Neighbor (TNCN). Our model achieves this balance by adapting the powerful pairwise modeling principles of Neural Common Neighbor (NCN) to an efficient temporal architecture. TNCN improves upon NCN by efficiently preserving and updating temporal neighbor dictionaries for each node and by using multi-hop common neighbors to learn more expressive pairwise representations. TNCN achieves new state-of-the-art performance on Review from five large-scale real-world TGB datasets, 6 out of 7 datasets in the transductive setting and 3 out of 7 in the inductive setting on small- to medium-scale datasets. Additionally, TNCN demonstrates excellent scalability, outperforming prominent GNN baselines by up to 30.3 times in speed on large datasets. Our code is available at https://github.com/GraphPKU/TNCN.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</title>
<link>https://arxiv.org/abs/2407.12051</link>
<guid>https://arxiv.org/abs/2407.12051</guid>
<content:encoded><![CDATA[
arXiv:2407.12051v2 Announce Type: replace-cross 
Abstract: DNA sequences encode critical genetic information, yet their variable length and discrete nature impede direct utilization in deep learning models. Existing DNA representation schemes convert sequences into numerical vectors but fail to capture structural features of local subsequences and often suffer from limited interpretability and poor generalization on small datasets. To address these limitations, we propose Dy-mer, an interpretable and robust DNA representation scheme based on dictionary learning. Dy-mer formulates an optimization problem in tensor format, which ensures computational efficiency in batch processing. Our scheme reconstructs DNA sequences as concatenations of dynamic-length subsequences (dymers) through a convolution operation and simultaneously optimize a learnable dymer dictionary and sparse representations. Our method achieves state-of-the-art performance in downstream tasks such as DNA promoter classification and motif detection. Experiments further show that the learned dymers match known DNA motifs and clustering using Dy-mer yields semantically meaningful phylogenetic trees. These results demonstrate that the proposed approach achieves both strong predictive performance and high interpretability, making it well suited for biological research applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Simultaneous Multislice MRI Reconstruction Using Slice-Wise Learned Generative Diffusion Priors</title>
<link>https://arxiv.org/abs/2407.21600</link>
<guid>https://arxiv.org/abs/2407.21600</guid>
<content:encoded><![CDATA[
arXiv:2407.21600v3 Announce Type: replace-cross 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2408.06069</link>
<guid>https://arxiv.org/abs/2408.06069</guid>
<content:encoded><![CDATA[
arXiv:2408.06069v2 Announce Type: replace-cross 
Abstract: Deep Gaussian process models typically employ discrete hierarchies, but recent advancements in differential Gaussian processes (DiffGPs) have extended these models to infinite depths. However, existing DiffGP approaches often overlook the uncertainty in kernel hyperparameters by treating them as fixed and time-invariant, which degrades the model's predictive performance and neglects the posterior distribution. In this work, we introduce a fully Bayesian framework that models kernel hyperparameters as random variables and utilizes coupled stochastic differential equations (SDEs) to jointly learn their posterior distributions alongside those of inducing points. By incorporating the estimation uncertainty of hyperparameters, our method significantly enhances model flexibility and adaptability to complex dynamic systems. Furthermore, we employ a black-box adaptive SDE solver with a neural network to achieve realistic, time varying posterior approximations, thereby improving the expressiveness of the variational posterior. Comprehensive experimental evaluations demonstrate that our approach outperforms traditional methods in terms of flexibility, accuracy, and other key performance metrics. This work not only provides a robust Bayesian extension to DiffGP models but also validates its effectiveness in handling intricate dynamic behaviors, thereby advancing the applicability of Gaussian process models in diverse real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Screening is More Efficient with Multiple Objects</title>
<link>https://arxiv.org/abs/2408.10077</link>
<guid>https://arxiv.org/abs/2408.10077</guid>
<content:encoded><![CDATA[
arXiv:2408.10077v2 Announce Type: replace-cross 
Abstract: We study efficient mechanism design for allocating multiple heterogeneous objects. The aim is to maximize the residual surplus, the total value generated from an allocation minus the costs of screening. We discover a robust trend indicating that no-screening mechanisms, such as serial dictatorship with exogenous priority order, tend to perform better as the variety of goods increases. We analyze the underlying reasons by characterizing asymptotically efficient mechanisms in a stylized environment. We also apply an automated mechanism design approach to numerically derive efficient mechanisms and validate the trend in general environments. Building on these implications, we propose the \emph{register-invite-book system} (RIB) as an efficient system for scheduling vaccination against pandemic diseases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-aware TDNN: Speaker Recognition Using Multi-Layer Features from Pre-Trained Models</title>
<link>https://arxiv.org/abs/2409.07770</link>
<guid>https://arxiv.org/abs/2409.07770</guid>
<content:encoded><![CDATA[
arXiv:2409.07770v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning (SSL) on Transformers have significantly improved speaker verification (SV) by providing domain-general speech representations. However, existing approaches have underutilized the multi-layered nature of SSL encoders. To address this limitation, we propose the layer-aware time-delay neural network (L-TDNN), which directly performs layer/frame-wise processing on the layer-wise hidden state outputs from pre-trained models, extracting fixed-size speaker vectors. L-TDNN comprises a layer-aware convolutional network, a frame-adaptive layer aggregation, and attentive statistic pooling, explicitly modeling of the recognition and processing of previously overlooked layer dimension. We evaluated L-TDNN across multiple speech SSL Transformers and diverse speech-speaker corpora against other approaches for leveraging pre-trained encoders. L-TDNN consistently demonstrated robust verification performance, achieving the lowest error rates throughout the experiments. Concurrently, it stood out in terms of model compactness and exhibited inference efficiency comparable to the existing systems. These results highlight the advantages derived from the proposed layer-aware processing approach. Future work includes exploring joint training with SSL frontends and the incorporation of score calibration to further enhance state-of-the-art verification performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2409.11169</link>
<guid>https://arxiv.org/abs/2409.11169</guid>
<content:encoded><![CDATA[
arXiv:2409.11169v3 Announce Type: replace-cross 
Abstract: Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion</title>
<link>https://arxiv.org/abs/2411.14507</link>
<guid>https://arxiv.org/abs/2411.14507</guid>
<content:encoded><![CDATA[
arXiv:2411.14507v3 Announce Type: replace-cross 
Abstract: Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficiency but often suffers from irreversible performance degradation due to the discarding of transformer blocks. In this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as iterative knowledge grafting rather than simple removal. Motivated by the observation that linear block merging fails to capture non-linear feature disparities and that block importance fluctuates dynamically during pruning, FuseGPT employs a dual-strategy pipeline. First, we propose Macro Influence (MI), a dynamic fusion-aware metric that continuously re-evaluates block redundancy as the network topology evolves. Second, instead of rigid parameter averaging, we introduce a learnable low-rank fusion mechanism that adaptively grafts the knowledge of pruned blocks onto surviving layers via lightweight local distillation. Extensive experiments on LLaMA, Mistral, Qwen, and Phi families demonstrate that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25\% sparsity, FuseGPT achieves lower perplexity than prior methods at 20\% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33$\times$ inference speedup with 25\% memory reduction. Furthermore, FuseGPT is orthogonal to quantization, achieving 52.1\% total compression with negligible quality loss when combined with 4-bit GPTQ. We make our code publicly available at https://github.com/JarvisPei/FuseGPT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment</title>
<link>https://arxiv.org/abs/2412.04783</link>
<guid>https://arxiv.org/abs/2412.04783</guid>
<content:encoded><![CDATA[
arXiv:2412.04783v5 Announce Type: replace-cross 
Abstract: Wireless sensing has recently found widespread applications in diverse environments, including homes, offices, and public spaces. By analyzing patterns in channel state information (CSI), it is possible to infer human actions for tasks such as person identification, gesture recognition, and fall detection. However, CSI is highly sensitive to environmental changes, where even minor alterations can significantly distort the CSI patterns. This sensitivity often leads to performance degradation or outright failure when applying wireless sensing models trained in one environment to another. To address this challenge, Domain Alignment (DAL) has been widely adopted for cross-domain classification tasks, as it focuses on aligning the global distributions of the source and target domains in feature space. Despite its popularity, DAL often neglects inter-category relationships, which can lead to misalignment between categories across domains, even when global alignment is achieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless sensing. Our approach begins by constructing a help set using KNN from the target domain, enabling local alignment between the source and target domains within each category using MMD. Additionally, we address a key instability issue commonly observed in cross-domain methods, where model performance fluctuates sharply between epochs. Further, most existing methods struggle to determine an optimal stopping point during training due to the absence of labeled data from the target domain. Our method resolves this by excluding the support set from the target domain during training and employing it as a validation set to determine the stopping criterion. The dataset and code are publicly available at https://github.com/RS2002/KNN-MMD .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</title>
<link>https://arxiv.org/abs/2501.10100</link>
<guid>https://arxiv.org/abs/2501.10100</guid>
<content:encoded><![CDATA[
arXiv:2501.10100v5 Announce Type: replace-cross 
Abstract: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmarks: On The False Promise of AI Regulation</title>
<link>https://arxiv.org/abs/2501.15693</link>
<guid>https://arxiv.org/abs/2501.15693</guid>
<content:encoded><![CDATA[
arXiv:2501.15693v2 Announce Type: replace-cross 
Abstract: The performance of AI models on safety benchmarks does not indicate their real-world performance after deployment. This opaqueness of AI models impedes existing regulatory frameworks constituted on benchmark performance, leaving them incapable of mitigating ongoing real-world harm. The problem stems from a fundamental challenge in AI interpretability, which seems to be overlooked by regulators and decision makers. We propose a simple, realistic and readily usable regulatory framework which does not rely on benchmarks, and call for interdisciplinary collaboration to find new ways to address this crucial problem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[
arXiv:2502.13764v4 Announce Type: replace-cross 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Learning of Evolving Data Streams with Tabular Foundational Models</title>
<link>https://arxiv.org/abs/2502.16840</link>
<guid>https://arxiv.org/abs/2502.16840</guid>
<content:encoded><![CDATA[
arXiv:2502.16840v2 Announce Type: replace-cross 
Abstract: State-of-the-art data stream mining has long drawn from ensembles of the Very Fast Decision Tree, a seminal algorithm honored with the 2015 KDD Test-of-Time Award. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees, such as Adaptive Random Forest, and Streaming Random Patches, across all non-stationary benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2503.07874</link>
<guid>https://arxiv.org/abs/2503.07874</guid>
<content:encoded><![CDATA[
arXiv:2503.07874v2 Announce Type: replace-cross 
Abstract: Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[
arXiv:2503.10253v4 Announce Type: replace-cross 
Abstract: Deep learning has shown strong potential in modeling complex spatiotemporal dynamics. However, most existing methods depend on densely and uniformly sampled data, which is often unavailable in practice due to sensor and cost limitations. In many real-world settings, such as mobile sensing and physical experiments, data are burst-sampled with short high-frequency segments followed by long gaps, making it difficult to learn accurate dynamics from sparse observations. To address this issue, we propose Physics-Informed Multi-Scale Recurrent Learning (PIMRL), a novel framework specifically designed for burst-sampled spatiotemporal data. PIMRL combines macro-scale latent dynamics inference with micro-scale adaptive refinement guided by incomplete prior information from partial differential equations (PDEs). It further introduces a temporal message-passing mechanism to effectively propagate information across burst intervals. This multi-scale architecture enables PIMRL to model complex systems accurately even under severe data scarcity. We evaluate our approach on five benchmark datasets involving 1D to 3D multi-scale PDEs. The results show that PIMRL consistently outperforms state-of-the-art baselines, achieving substantial improvements and reducing errors by up to 80% in the most challenging settings, which demonstrates the clear advantage of our model. Our work demonstrates the effectiveness of physics-informed recurrent learning for accurate and efficient modeling of sparse spatiotemporal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI</title>
<link>https://arxiv.org/abs/2503.11950</link>
<guid>https://arxiv.org/abs/2503.11950</guid>
<content:encoded><![CDATA[
arXiv:2503.11950v4 Announce Type: replace-cross 
Abstract: The increasing integration of artificial intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups-young digital citizens, parents/educators, and AI professionals-and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from key participants through structured surveys, qualitative interviews, and focus groups to identify distinct privacy expectations. Young digital citizens emphasized autonomy and digital agency, while parents and educators prioritized oversight and AI literacy. AI professionals focused on balancing ethical design with system performance. The analysis revealed significant gaps in transparency and digital literacy, underscoring the need for inclusive, stakeholder-driven privacy frameworks. Drawing on comparative thematic analysis, this study introduces the Privacy-Ethics Alignment in AI (PEA-AI) model, which conceptualizes privacy decision-making as a dynamic negotiation among stakeholders. By aligning empirical findings with governance implications, this research provides a scalable foundation for adaptive, youth-centered AI privacy governance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
arXiv:2503.14505v2 Announce Type: replace-cross 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>