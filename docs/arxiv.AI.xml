<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.04097</link>
<guid>https://arxiv.org/abs/2510.04097</guid>
<content:encoded><![CDATA[
<div> Keyword: UI images, web code, large language models, WebRenderBench, ALISA

Summary:
WebRenderBench is introduced as a large-scale benchmark of 45.1k webpages from real-world portal sites, offering greater diversity and realism for evaluating WebUI-to-Code conversion. A novel evaluation metric is proposed to measure layout and style consistency in the final rendered pages, providing an efficient and reliable assessment of UI quality. The Automated Layout and Style Inspection Agent (ALISA) integrates this metric into reinforcement learning to enhance training on webpages with asymmetric layouts. Experiments demonstrate that ALISA significantly improves generation performance and achieves state-of-the-art results across multiple metrics.<br><br>Summary: <div>
arXiv:2510.04097v2 Announce Type: replace 
Abstract: Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 45.1k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.03288</link>
<guid>https://arxiv.org/abs/2510.03288</guid>
<content:encoded><![CDATA[
<div> active domain adaptation, log-based anomaly detection, transfer learning, active learning, manual labeling

Summary:<br />
- Log-based anomaly detection is crucial for software system reliability and performance, but labeling a large volume of logs is challenging.
- Existing anomaly detection methods heavily rely on labeling and face issues like data distribution gaps and cold-start problems.
- LogAction is a novel model that combines transfer learning and active learning techniques to address these challenges.
- LogAction uses labeled data from a mature system to train a base model and selects logs at distribution boundaries for manual labeling.
- Experimental results show that LogAction achieves a 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%.

<br /><br />Summary: <div>
arXiv:2510.03288v2 Announce Type: replace-cross 
Abstract: Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation</title>
<link>https://arxiv.org/abs/2510.03426</link>
<guid>https://arxiv.org/abs/2510.03426</guid>
<content:encoded><![CDATA[
<div> order of magnitude, floating-point numbers, parallel hardware, compounding real numbers, numerical stability

Summary:
Generalized orders of magnitude (GOOMs) are introduced as an extension of traditional orders of magnitude to handle compounding real numbers over long sequences without encountering catastrophic numerical underflow or overflow. GOOMs provide stable computation over larger dynamic ranges than conventional methods, particularly on parallel hardware like GPUs. An efficient custom parallel prefix scan is implemented to support native execution on such hardware. Three experiments demonstrate the superiority of GOOMs over traditional approaches: compounding real matrix products beyond standard floating-point limits, estimating Lyapunov exponents' spectra much faster, and capturing long-range dependencies in deep recurrent neural networks without requiring stabilization. GOOMs combined with parallel scanning offer a scalable and numerically robust alternative for high-dynamic-range applications. <br /><br />Summary: <div>
arXiv:2510.03426v2 Announce Type: replace-cross 
Abstract: Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products far beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, orders of magnitude faster than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</title>
<link>https://arxiv.org/abs/2510.04020</link>
<guid>https://arxiv.org/abs/2510.04020</guid>
<content:encoded><![CDATA[
<div> Keywords: spatiotemporal forecasting, Model-Based Reinforcement Learning, Generative World Model, planning algorithm, extreme events

Summary:<br />
The proposed Spatiotemporal Forecasting as Planning (SFP) paradigm addresses the challenges of stochasticity and non-differentiable metrics in physical spatiotemporal forecasting. SFP utilizes a Generative World Model to simulate diverse future states and employs a base forecasting model as an agent guided by a beam search-based planning algorithm. This approach leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. High-reward candidates identified through this process serve as pseudo-labels to optimize the agent's policy via iterative self-training, reducing prediction error significantly. The SFP framework demonstrates exceptional performance in capturing extreme events, showcasing its effectiveness in spatiotemporal forecasting tasks. <div>
arXiv:2510.04020v2 Announce Type: replace-cross 
Abstract: To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</title>
<link>https://arxiv.org/abs/2510.04363</link>
<guid>https://arxiv.org/abs/2510.04363</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, browser-automation, MacroBench, web automation<br />
Summary:<br />
The article introduces MacroBench, a benchmarking tool that evaluates the ability of Large Language Models (LLMs) to generate reusable browser-automation programs from natural-language goals. The benchmark consists of seven self-hosted sites with 681 tasks of varying complexity and difficulty levels. The evaluation protocol includes static checks, sandboxed execution, and outcome verification to validate the generated code. The models tested in the study, GPT-4o-mini, GPT-4o, Gemini, and DeepSeek, showed varying levels of success in completing tasks, with high success rates for simple tasks but failures in complex workflows. Despite achieving functional completion, none of the models met production-quality coding practices. The complete benchmark pipeline, evaluation framework, and experimental results are publicly available for reproducible assessment of macro synthesis for web automation on GitHub.  <br /><br />Summary: <div>
arXiv:2510.04363v2 Announce Type: replace-cross 
Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium. MacroBench instantiates seven self-hosted sites covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification (DOM assertions, database snapshots), and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2,636 model-task runs, we observe stratified success: GPT-4o-mini (96.8%), GPT-4o (95.3%), Gemini (89.0%), DeepSeek (83.4%). Models handle simple tasks reliably (91.7%) but fail on complex workflows (0.0%), and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results at https://github.com/hyunjun1121/MacroBench to enable reproducible assessment of macro synthesis for web automation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation</title>
<link>https://arxiv.org/abs/2510.07331</link>
<guid>https://arxiv.org/abs/2510.07331</guid>
<content:encoded><![CDATA[
<div> probabilistic program semantics, neural language generation, verification, knowledge bases, Truth-Aware Decoding

Summary:
This paper presents Truth-Aware Decoding (TAD), a novel decoding scheme that integrates neural language generation with knowledge bases through semantic guards. The proposed constraint-based semantics enable oracle filtering as a program-logic judgment, ensuring sound and complete guards for greedy selection. An entropy-style invariant measures factual risk using knowledge-aware safe mass. Additionally, a multi-agent operational calculus with verified Lean artefacts certifies implementation behavior. Numerical and algorithmic case studies demonstrate that the guardrails mitigate hallucinations without compromising efficiency. TAD serves as a practical liaison between large-scale empirical models and formal verification.<br /><br />Summary: <div>
arXiv:2510.07331v1 Announce Type: new 
Abstract: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented decoding scheme that aligns neural language generation with knowledge bases. Situated in the tradition of probabilistic program semantics for sequence models, TAD augments modern instruction-tuned systems with a lattice of semantic guards that operate at decode time. Our contributions are fourfold: (i) a constraint-based semantics that renders oracle filtering as a program-logic judgment, (ii) a proof that greedy selection enjoys local likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an entropy-style invariant that quantifies factual risk via knowledge-aware safe mass, and (iv) a multi-agent operational calculus with verified Lean artefacts to certify implementation behaviour. Numerical and algorithmic case studies confirm that the resulting guardrails reduce hallucinations without sacrificing throughput, yielding a pragmatic bridge between large-scale empirical models and formal verification.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)</title>
<link>https://arxiv.org/abs/2510.07363</link>
<guid>https://arxiv.org/abs/2510.07363</guid>
<content:encoded><![CDATA[
<div> framework, industrial IoT, autonomous defense, multi-agent reinforcement learning, cybersecurity 

Summary:
The paper introduces L2M-AID, a framework for Autonomous Industrial Defense utilizing Multi-agent reinforcement learning powered by Large Language Models. It addresses the increasing threat landscape faced by Industrial IoT systems through contextual awareness and semantic representation of telemetry data. By leveraging a semantic bridge provided by LLMs, collaborative agents can reason about adversary intent to enhance security measures. The framework combines LLMs with Multi-Agent Reinforcement Learning (MARL) algorithm, achieving complex cooperative strategies to balance security objectives and operational imperatives. Experimental validation on SWaT and synthetic datasets showcases L2M-AID's superior performance in threat detection, false positive reduction, and response time improvement compared to traditional IDS and deep learning detectors. Notably, L2M-AID excels in maintaining physical process stability, making it a robust solution for securing critical infrastructure. 

<br /><br />Summary: <div>
arXiv:2510.07363v1 Announce Type: new 
Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&amp;CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base Models Know How to Reason, Thinking Models Learn When</title>
<link>https://arxiv.org/abs/2510.07364</link>
<guid>https://arxiv.org/abs/2510.07364</guid>
<content:encoded><![CDATA[
<div> reasoning mechanisms, language models, DeepSeek R1, hybrid model, performance gap

Summary:<br />This work investigates why thinking language models like DeepSeek R1 outperform base models. The study introduces a hybrid model that activates reasoning mechanisms in base models at the right time to achieve thinking-model-level reasoning chains. Through an unsupervised approach, human-interpretable reasoning behaviors in thinking models are uncovered. The hybrid model successfully recovers up to 91% of the performance gap to thinking models without weight updates, steering only 12% of tokens. The findings suggest that thinking models mostly acquire reasoning mechanisms during pre-training, with post-training focusing on efficient deployment of these mechanisms at the appropriate times for enhanced task performance. This refines our understanding of how thinking models are trained and highlights the importance of deploying reasoning mechanisms efficiently for optimal inference-time compute utilization. 

<br /><br /> <div>
arXiv:2510.07364v1 Announce Type: new 
Abstract: Why do thinking language models like DeepSeek R1 outperform their base counterparts? Despite consistent performance gains, it remains unclear to what extent thinking models learn entirely new reasoning capabilities or repurpose pre-existing base model ones. In this work, we propose a hybrid model where we activate reasoning mechanisms in base models at the right time to elicit thinking-model-level reasoning chains, implying that thinking models exploit already existing capabilities. To ground our analysis, we introduce an unsupervised, bottom-up approach for uncovering human-interpretable reasoning behaviors in thinking models. This approach provides an unbiased method to discover reasoning behaviors without imposing manual or LLM-derived assumptions. Across three base and four thinking models, using GSM8K and MATH500, our hybrid model recovers up to 91% of the performance gap to thinking models without any weight updates while steering only 12% of tokens. Concretely, our empirical setup provides a simple, causal way to test the effectiveness of existing reasoning mechanisms in base models by invoking them directly and measuring the resulting task performance. More broadly, these results reframe our understanding of how thinking models are trained: pre-training is when models acquire most of their reasoning mechanisms, and post-training teaches efficient deployment of these mechanisms at the right time, enabling efficient use of their inference-time compute.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD</title>
<link>https://arxiv.org/abs/2510.07409</link>
<guid>https://arxiv.org/abs/2510.07409</guid>
<content:encoded><![CDATA[
<div> AI-driven assessment, ADHD, generative AI, mental health digital twins, personalized care<br />
<br />
Summary: The article advocates for a shift from static mental health diagnostic assessments to continuous, AI-driven assessment, focusing on ADHD as a case study. It explores how generative AI can address capacity constraints in neuropsychology, enabling personalized and longitudinal care pathways. AI can conduct frequent experience sampling and facilitate diagnostic reconciliation, leading to more effective treatment. The concept of mental health digital twins (MHDTs) is proposed as a framework for personalized mental health care, capturing individual symptom dynamics and trajectories. The article emphasizes the need for empirical evidence and outlines a research agenda to refine and operationalize this transformative approach. <div>
arXiv:2510.07409v1 Announce Type: new 
Abstract: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from static mental health diagnostic assessments to continuous, artificial intelligence (AI)-driven assessment. Focusing on Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how generative AI has the potential to address current capacity constraints in neuropsychology, potentially enabling more personalized and longitudinal care pathways. In particular, AI can efficiently conduct frequent, low-level experience sampling from patients and facilitate diagnostic reconciliation across care pathways. We envision a future where mental health care benefits from continuous, rich, and patient-centered data sampling to dynamically adapt to individual patient needs and evolving conditions, thereby improving both accessibility and efficacy of treatment. We further propose the use of mental health digital twins (MHDTs) - continuously updated computational models that capture individual symptom dynamics and trajectories - as a transformative framework for personalized mental health care. We ground this framework in empirical evidence and map out the research agenda required to refine and operationalize it.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProSEA: Problem Solving via Exploration Agents</title>
<link>https://arxiv.org/abs/2510.07423</link>
<guid>https://arxiv.org/abs/2510.07423</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, problem solving, collaboration, adaptive reasoning
Summary:
ProSEA is a new modular and general-purpose multi-agent framework specifically designed for iterative problem solving through exploration and plan evolution. It features a hierarchical architecture with a Manager Agent coordinating domain-specialized Expert Agents, task decomposition, and adaptive replanning based on feedback from failed attempts. Unlike previous systems, ProSEA agents not only report success or failure but also provide detailed reasons for failure and newly discovered constraints, enabling dynamic plan refinement informed by exploratory traces. The framework can operate autonomously but also supports seamless integration with human collaborators when necessary. Experimental results on the FinanceBench benchmark show that ProSEA outperforms existing baselines and excels in reasoning-heavy tasks, even without human feedback. These findings suggest that ProSEA has the potential to serve as a foundation for more transparent, adaptive, and human-aligned AI agents.<br /><br />Summary: <div>
arXiv:2510.07423v1 Announce Type: new 
Abstract: Large language models (LLMs) have empowered AI agents to tackle increasingly complex tasks. However, most existing agents remain limited to static planning and brittle interactions, falling short of true collaboration or adaptive reasoning. We introduce ProSEA, a modular, general-purpose multi-agent framework designed for iterative problem solving through exploration and plan evolution. ProSEA features a hierarchical architecture in which a Manager Agent orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively replans based on structured feedback from failed attempts. Unlike prior systems, ProSEA agents report not only success or failure but also detailed reasons for failure and newly discovered constraints, enabling dynamic plan refinement informed by exploratory traces. The framework operates autonomously but supports seamless integration with human collaborators when needed. Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA, even without human feedback, outperforms state-of-the-art baselines and achieves robust performance across reasoning-heavy tasks. These results underscore ProSEA's potential as a foundation for more transparent, adaptive, and human-aligned AI agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting</title>
<link>https://arxiv.org/abs/2510.07426</link>
<guid>https://arxiv.org/abs/2510.07426</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic forecasting, graph neural network, spatial-temporal modeling, physical road network topology, ensemble aggregation

Summary:<br /><br />
The article introduces TESTAM+, an enhanced spatio-temporal forecasting framework that improves upon existing models by incorporating physical road network topology and data-driven feature similarity. TESTAM+ outperforms TESTAM in terms of Mean Absolute Error (MAE) on two datasets, METR LA and PEMS BAY. The strategic expert selection method in TESTAM+ is shown to be more effective than naive ensemble aggregation. The Adaptive and SpatioSemantic Experts in TESTAM+ demonstrate impressive performance, matching or surpassing the original TESTAM. The Identity + Adaptive configuration in TESTAM+ achieves a significant MAE reduction compared to the state-of-the-art MegaCRN on the METR LA dataset. Additionally, TESTAM+ reduces inference latency by over 50% compared to the full four expert model. These findings suggest that a smaller number of strategically-designed experts can outperform complex multi-expert ensembles, achieving state-of-the-art performance with improved computational efficiency for real-time deployment. <div>
arXiv:2510.07426v1 Announce Type: new 
Abstract: Traffic forecasting is fundamental to intelligent transportation systems, enabling congestion mitigation and emission reduction in increasingly complex urban environments. While recent graph neural network approaches have advanced spatial temporal modeling, existing mixture of experts frameworks like Time Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation of physical road network topology, limiting their spatial capabilities. We present TESTAM+, an enhanced spatio temporal forecasting framework that introduces a novel SpatioSemantic Expert integrating physical road topology with data driven feature similarity through hybrid graph construction. TESTAM+ achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA (3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through comprehensive ablation studies, we discover that strategic expert selection fundamentally outperforms naive ensemble aggregation. Individual experts demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the SpatioSemantic Expert matches this performance with identical 1.63 MAE. The optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing inference latency by 53.1% compared to the full four expert TESTAM+. Our findings reveal that fewer, strategically designed experts outperform complex multi expert ensembles, establishing new state of the art performance with superior computational efficiency for real time deployment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering</title>
<link>https://arxiv.org/abs/2510.07432</link>
<guid>https://arxiv.org/abs/2510.07432</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, time series reasoning tasks, TS-Agent, interpretability, reasoning tasks improvement

Summary: 
TS-Agent is proposed as a time series reasoning agent that utilizes large language models (LLMs) for evidence gathering and synthesis while relying on time series analytical tools for statistical and structural information extraction. The agent operates on raw numeric sequences with atomic operators, maintains an explicit evidence log, and refines its reasoning iteratively under the guidance of a self-critic and quality gate. By avoiding multi-modal alignment training, preserving the native form of time series, ensuring interpretability, and mitigating knowledge leakage or hallucination, TS-Agent achieves comparable performance to state-of-the-art LLMs on understanding benchmarks and outperforms existing models in reasoning tasks, particularly in zero-shot settings where memorization-based models typically struggle. Empirical evaluations on established benchmarks demonstrate the effectiveness of TS-Agent in improving reasoning abilities on time series data. 

<br /><br />Summary: <div>
arXiv:2510.07432v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown strong abilities in reasoning and problem solving, but recent studies reveal that they still struggle with time series reasoning tasks, where outputs are often affected by hallucination or knowledge leakage. In this work we propose TS-Agent, a time series reasoning agent that leverages LLMs strictly for what they excel at, i.e., gathering evidence and synthesizing it into conclusions through step-by-step reasoning, while delegating the extraction of statistical and structural information to time series analytical tools. Instead of mapping time series into text tokens, images, or embeddings, our agent interacts with raw numeric sequences through atomic operators, records outputs in an explicit evidence log, and iteratively refines its reasoning under the guidance of a self-critic and a final quality gate. This design avoids multi-modal alignment training, preserves the native form of time series, ensures interpretability and verifiability, and mitigates knowledge leakage or hallucination. Empirically, we evaluate the agent on established benchmarks. Our experiments show that TS-Agent achieves performance comparable to state-of-the-art LLMs on understanding benchmarks, and delivers significant improvements on reasoning tasks, where existing models often rely on memorization and fail in zero-shot settings.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning</title>
<link>https://arxiv.org/abs/2510.07456</link>
<guid>https://arxiv.org/abs/2510.07456</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, personalized education, intelligent agent framework, adaptive learning, real-time reliability

Summary:
ExpertAgent is an intelligent agent framework designed for personalized education that utilizes advanced generative artificial intelligence to provide a proactive and personalized learning experience. The agent dynamically plans learning content and strategies based on continuously updated student models, ensuring highly adaptive learning experiences in real time. By leveraging a validated curriculum repository, ExpertAgent reduces the risk of producing inaccurate information and enhances the reliability and trustworthiness of instructional content. This innovative approach overcomes the limitations of traditional static learning content, offering optimized teaching strategies tailored to individual students' needs. Overall, ExpertAgent facilitates personalized education by providing real-time adaptability, personalization, and reliability in learning experiences. 

<br /><br />Summary: ExpertAgent is an intelligent agent framework that uses generative artificial intelligence for personalized education, offering dynamic learning content based on student models and validated curriculum repositories to enhance reliability and adaptability. <div>
arXiv:2510.07456v1 Announce Type: new 
Abstract: The application of advanced generative artificial intelligence in education is often constrained by the lack of real-time adaptability, personalization, and reliability of the content. To address these challenges, we propose ExpertAgent - an intelligent agent framework designed for personalized education that provides reliable knowledge and enables highly adaptive learning experiences. Therefore, we developed ExpertAgent, an innovative learning agent that provides users with a proactive and personalized learning experience. ExpertAgent dynamic planning of the learning content and strategy based on a continuously updated student model. Therefore, overcoming the limitations of traditional static learning content to provide optimized teaching strategies and learning experience in real time. All instructional content is grounded in a validated curriculum repository, effectively reducing hallucination risks in large language models and improving reliability and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs for Process Model Analysis and Optimization</title>
<link>https://arxiv.org/abs/2510.07489</link>
<guid>https://arxiv.org/abs/2510.07489</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, process models, natural language interface, reasoning, business process designers<br />
<br />
Summary: In this study, the researchers explore the use of Language Model Models (LLMs) to understand and analyze process models using natural language interfaces. They found that even untrained LLMs like ChatGPT can effectively comprehend BPMN process models and identify errors intelligently. Different LLMs varied in performance, but overall, they were valuable assistants for business process designers. The researchers also delved into the LLMs' ability to reason and analyze processes, noting that they exhibited anthropomorphic properties. This research highlights the potential of LLMs in assisting with process analysis and optimization, showcasing their ability to provide deep insights and assist users in enhancing their processes. <div>
arXiv:2510.07489v1 Announce Type: new 
Abstract: In this paper, we report our experience with several LLMs for their ability to understand a process model in an interactive, conversational style, find syntactical and logical errors in it, and reason with it in depth through a natural language (NL) interface. Our findings show that a vanilla, untrained LLM like ChatGPT (model o3) in a zero-shot setting is effective in understanding BPMN process models from images and answering queries about them intelligently at syntactic, logic, and semantic levels of depth. Further, different LLMs vary in performance in terms of their accuracy and effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a valuable role as assistants for business process designers and users. We also study the LLM's "thought process" and ability to perform deeper reasoning in the context of process analysis and optimization. We find that the LLMs seem to exhibit anthropomorphic properties.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming</title>
<link>https://arxiv.org/abs/2510.07491</link>
<guid>https://arxiv.org/abs/2510.07491</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Intelligent Systems, risk reduction optimization, trustworthy AI, ethical requirements, constrained optimization

Summary:
This article addresses the integration of Medical Intelligent Systems (MIS) into healthcare workflows and the resulting safety and ethical concerns. As most MIS will be classified as high-risk systems under the European Union AI Act, a formal risk management process is necessary to ensure compliance with ethical requirements for trustworthy AI. The focus is on risk reduction optimization problems, aiming to balance risk assessment values in a way that covers ethical requirements effectively. The problem is formalized as a constrained optimization task and studied using three resolution paradigms: Mixed Integer Programming, Satisfiability, and Constraint Programming. The study includes mathematical formulation, modeling with Minizinc, and a comparative experimental analysis of performance, expressiveness, and scalability of each approach. The work identifies limitations of the methodology and offers perspectives on integrating the Minizinc model into a complete trustworthy AI ethical risk management process for MIS.<br /><br />Summary: <div>
arXiv:2510.07491v1 Announce Type: new 
Abstract: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare workflows, offering significant benefits but also raising critical safety and ethical concerns. According to the European Union AI Act, most MIS will be classified as high-risk systems, requiring a formal risk management process to ensure compliance with the ethical requirements of trust- worthy AI. In this context, we focus on risk reduction optimization problems, which aim to reduce risks with ethical considerations by finding the best balanced assignment of risk assessment values according to their coverage of trustworthy AI ethical requirements. We formalize this problem as a constrained optimization task and investigate three resolution paradigms: Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Pro- gramming(CP).Our contributions include the mathematical formulation of this optimization problem, its modeling with the Minizinc constraint modeling language, and a comparative experimental study that analyzes the performance, expressiveness, and scalability of each ap- proach to solving. From the identified limits of the methodology, we draw some perspectives of this work regarding the integration of the Minizinc model into a complete trustworthy AI ethical risk management process for MIS.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query</title>
<link>https://arxiv.org/abs/2510.07516</link>
<guid>https://arxiv.org/abs/2510.07516</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, LLMs, geo-spatial, path query <br />
<br />
CompassLLM is introduced as a multi-agent framework that utilizes Large Language Models (LLMs) to solve the popular path query in the geo-spatial domain. It consists of two stages: SEARCH, which identifies popular paths, and GENERATE, which creates new paths when no historical data exists. The framework shows superior accuracy in identifying popular paths and competitive performance in generating novel paths. Unlike traditional algorithms, CompassLLM does not require model training or parameter tuning, making it cost-effective and efficient for real-world applications. By leveraging the reasoning capabilities of LLMs, CompassLLM offers a promising approach to solving geo-spatial problems like urban planning, navigation optimization, and travel recommendations. <br /><br />Summary: <div>
arXiv:2510.07516v1 Announce Type: new 
Abstract: The popular path query - identifying the most frequented routes between locations from historical trajectory data - has important applications in urban planning, navigation optimization, and travel recommendations. While traditional algorithms and machine learning approaches have achieved success in this domain, they typically require model training, parameter tuning, and retraining when accommodating data updates. As Large Language Models (LLMs) demonstrate increasing capabilities in spatial and graph-based reasoning, there is growing interest in exploring how these models can be applied to geo-spatial problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently leverages the reasoning capabilities of LLMs into the geo-spatial domain to solve the popular path query. CompassLLM employs its agents in a two-stage pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage that synthesizes novel paths in the absence of an existing one in the historical trajectory data. Experiments on real and synthetic datasets show that CompassLLM demonstrates superior accuracy in SEARCH and competitive performance in GENERATE while being cost-effective.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</title>
<link>https://arxiv.org/abs/2510.07517</link>
<guid>https://arxiv.org/abs/2510.07517</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent debate, identity bias, sycophancy, self-bias, Bayesian update<br />
Summary:<br />
The article introduces the concept of Multi-agent debate (MAD) and highlights the issue of identity-driven sycophancy and self-bias in large language models. A new framework is proposed to mitigate and quantify identity bias in MAD by formalizing debate dynamics as an identity-weighted Bayesian update process. The framework includes response anonymization, where identity markers are removed from prompts to reduce bias. The Identity Bias Coefficient (IBC) is defined as a metric to measure how often agents follow a peer versus themselves. Empirical studies across various models and datasets show that sycophancy is more common than self-bias. The study emphasizes the importance of masking identity to ensure that MAD systems base reasoning on content rather than source identity. The code for the framework is available on GitHub for further exploration.<br /><br />Summary: <div>
arXiv:2510.07517v1 Announce Type: new 
Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish "self" from "peer", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to "mask" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in https://github.com/deeplearning-wisc/MAD-identity-bias.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evaluation Study of Hybrid Methods for Multilingual PII Detection</title>
<link>https://arxiv.org/abs/2510.07551</link>
<guid>https://arxiv.org/abs/2510.07551</guid>
<content:encoded><![CDATA[
<div> Keywords: Personally Identifiable Information (PII), low-resource languages, RECAP, large language models, privacy compliance

Summary:<br /><br />
The article introduces a hybrid framework called RECAP that addresses the challenge of detecting Personally Identifiable Information (PII) in low-resource languages. By combining deterministic regular expressions with context-aware large language models (LLMs), RECAP enables scalable PII detection across 13 low-resource locales. The framework supports over 300 entity types without the need for retraining and utilizes a three-phase refinement pipeline for disambiguation and filtering. Benchmarking results show that RECAP outperforms fine-tuned NER models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work provides a scalable and adaptable solution for efficient PII detection in compliance-focused applications.<br /><br />Summary: <div>
arXiv:2510.07551v1 Announce Type: new 
Abstract: The detection of Personally Identifiable Information (PII) is critical for privacy compliance but remains challenging in low-resource languages due to linguistic diversity and limited annotated data. We present RECAP, a hybrid framework that combines deterministic regular expressions with context-aware large language models (LLMs) for scalable PII detection across 13 low-resource locales. RECAP's modular design supports over 300 entity types without retraining, using a three-phase refinement pipeline for disambiguation and filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers a scalable and adaptable solution for efficient PII detection in compliance-focused applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking is Broken - Don't Let AI be its Own Judge</title>
<link>https://arxiv.org/abs/2510.07575</link>
<guid>https://arxiv.org/abs/2510.07575</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, evaluation, benchmarking, trustworthiness, PeerBench

Summary:
The article discusses the challenges in evaluating Artificial Intelligence (AI) and highlights the need for a new, unified paradigm for trustworthy evaluation. It points out the current vulnerabilities in benchmarks due to issues like data contamination and biased evaluations. The authors argue for a paradigm shift towards a quality-controlled benchmarking framework that ensures fairness and credibility in AI evaluation. They introduce PeerBench, a community-governed evaluation blueprint that aims to restore integrity and deliver trustworthy measures of AI progress. The paper emphasizes the importance of establishing a robust evaluation system to distinguish genuine progress from exaggerated claims in the rapidly expanding AI market. <div>
arXiv:2510.07575v1 Announce Type: new 
Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding market capitalization, presents both transformative opportunities and critical challenges. Chief among these is the urgent need for a new, unified paradigm for trustworthy evaluation, as current benchmarks increasingly reveal critical vulnerabilities. Issues like data contamination and selective reporting by model developers fuel hype, while inadequate data quality control can lead to biased evaluations that, even if unintentionally, may favor specific approaches. As a flood of participants enters the AI space, this "Wild West" of assessment makes distinguishing genuine progress from exaggerated claims exceptionally difficult. Such ambiguity blurs scientific signals and erodes public confidence, much as unchecked claims would destabilize financial markets reliant on credible oversight from agencies like Moody's.
  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is devoted to ensuring fairness and credibility; why settle for less in evaluating AI, especially given its profound societal impact? This position paper argues that the current laissez-faire approach is unsustainable. We contend that true, sustainable AI advancement demands a paradigm shift: a unified, live, and quality-controlled benchmarking framework robust by construction, not by mere courtesy and goodwill. To this end, we dissect the systemic flaws undermining today's AI evaluation, distill the essential requirements for a new generation of assessments, and introduce PeerBench, a community-governed, proctored evaluation blueprint that embodies this paradigm through sealed execution, item banking with rolling renewal, and delayed transparency. Our goal is to pave the way for evaluations that can restore integrity and deliver genuinely trustworthy measures of AI progress.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentAsk: Multi-Agent Systems Need to Ask</title>
<link>https://arxiv.org/abs/2510.07593</link>
<guid>https://arxiv.org/abs/2510.07593</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent systems, Large language models, Error propagation, AgentAsk, Reinforcement learning

Summary: 
AgentAsk is a new module designed to address the issue of error cascades in multi-agent systems that utilize large language models (LLMs). This lightweight and easy-to-integrate module aims to improve accuracy and robustness by inserting minimal questions at potential failure points within the interaction chain. The module follows a three-stage pipeline, leveraging curated failure traces to inform its decision-making process. By optimizing online with E-GRPO, a reinforcement learning objective, AgentAsk effectively balances accuracy, latency, and cost considerations. Empirical results across various benchmarks show that AgentAsk consistently enhances performance while keeping overhead low. The module not only improves system reliability but also contributes a systematic taxonomy of edge-level errors and a practical approach for intervention, paving the way for more dependable LLM-based multi-agent systems. <div>
arXiv:2510.07593v1 Announce Type: new 
Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving capabilities through collaborative division of labor. However, they frequently underperform single-agent baselines due to edge-level error cascades: minor inaccuracies at one message handoff propagate across the entire chain. We propose AgentAsk, a lightweight and plug-and-play clarification module that treats every inter-agent message as a potential failure point and inserts minimally necessary questions to arrest error propagation. AgentAsk follows a three-stage pipeline: (i) distilling edge-level judgments from curated failure traces into a compact policy, (ii) supervising the policy to determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a reinforcement learning objective that balances accuracy, latency, and cost. The module is architecture-agnostic and easy to integrate into existing orchestration. Across math, reasoning, and coding benchmarks, AgentAsk consistently improves accuracy and robustness over public multi-agent implementations while keeping overhead minimal, with latency and extra cost all less than 5%, approaching the performance of a strong evaluator. Beyond empirical improvements, we contribute a principled taxonomy of edge-level errors and a practical recipe for link-local intervention, offering a scalable pathway toward more reliable LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines</title>
<link>https://arxiv.org/abs/2510.07614</link>
<guid>https://arxiv.org/abs/2510.07614</guid>
<content:encoded><![CDATA[
<div> pipeline, language models, accountability, traceability, multi-agent systems
Summary:
- The study focuses on creating a traceable and accountable pipeline for sequential multi-agent systems using large language models (LLMs).
- By incorporating structured handoffs and saved records, errors can be traced back and accountability assigned when needed.
- Evaluating different configurations of LLMs on various benchmarks revealed the importance of structured handoffs in improving accuracy and preventing common failures.
- Each model within the pipeline has specific strengths and risks, quantified through repair and harm rates, highlighting the need for role-specific analysis.
- Task-dependent trade-offs between accuracy, cost, and latency suggest that heterogeneous pipelines are often the most efficient approach for designing reliable multi-agent systems.
<br /><br />Summary: <div>
arXiv:2510.07614v1 Announce Type: new 
Abstract: Sequential multi-agent systems built with large language models (LLMs) can automate complex software tasks, but they are hard to trust because errors quietly pass from one stage to the next. We study a traceable and accountable pipeline, meaning a system with clear roles, structured handoffs, and saved records that let us trace who did what at each step and assign blame when things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We evaluate eight configurations of three state-of-the-art LLMs on three benchmarks and analyze where errors start, how they spread, and how they can be fixed. Our results show: (1) adding a structured, accountable handoff between agents markedly improves accuracy and prevents the failures common in simple pipelines; (2) models have clear role-specific strengths and risks (e.g., steady planning vs. high-variance critiquing), which we quantify with repair and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent, with heterogeneous pipelines often the most efficient. Overall, we provide a practical, data-driven method for designing, tracing, and debugging reliable, predictable, and accountable multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services</title>
<link>https://arxiv.org/abs/2510.07623</link>
<guid>https://arxiv.org/abs/2510.07623</guid>
<content:encoded><![CDATA[
<div> transforming healthcare, mental health, Generative AI, training, service provision
<br />
Generative artificial intelligence (Generative AI) is revolutionizing healthcare, particularly in the field of mental health. While much attention has been given to therapist chatbots, the use of generative AI to enhance and scale training in mental health service provision presents a lower-risk, high-impact alternative. By leveraging generative AI for training, individuals can be better prepared to deliver effective mental health support. A real-world case study involving veterans demonstrates the positive impact of using generative AI to improve training in mental health service provision. This approach not only improves the quality of mental health services but also enhances the ability of individuals to support one another's mental well-being. Investing in generative AI for training in mental health service provision holds significant potential for improving mental health outcomes on a larger scale.
<br /><br />Summary: <div>
arXiv:2510.07623v1 Announce Type: new 
Abstract: Generative artificial intelligence (Generative AI) is transforming healthcare. With this evolution comes optimism regarding the impact it will have on mental health, as well as concern regarding the risks that come with generative AI operating in the mental health domain. Much of the investment in, and academic and public discourse about, AI-powered solutions for mental health has focused on therapist chatbots. Despite the common assumption that chatbots will be the most impactful application of GenAI to mental health, we make the case here for a lower-risk, high impact use case: leveraging generative AI to enhance and scale training in mental health service provision. We highlight key benefits of using generative AI to help train people to provide mental health services and present a real-world case study in which generative AI improved the training of veterans to support one another's mental health. With numerous potential applications of generative AI in mental health, we illustrate why we should invest in using generative AI to support training people in mental health service provision.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</title>
<link>https://arxiv.org/abs/2510.07632</link>
<guid>https://arxiv.org/abs/2510.07632</guid>
<content:encoded><![CDATA[
<div> Keywords: AI models, compositional reasoning, evaluation metrics, group matching score, Test-Time Matching

Summary:
Frontier AI models have shown remarkable progress but struggle with compositional reasoning, performing poorly on established benchmarks. This study reveals that evaluation metrics underestimate model capability and introduces a group matching score to better assess performance. Adjusting for this underestimation boosts model performance significantly, with SigLIP-B16 surpassing previous results and GPT-4.1 achieving human-level performance on Winoground. The Test-Time Matching (TTM) algorithm further improves model performance iteratively, leading to state-of-the-art results on MMVP-VLM. TTM remains effective on challenging datasets such as WhatsUp, with relative gains of up to 85.7%. Across diverse setups, TTM consistently enhances model performance, advancing the frontier of compositional reasoning.<br /><br />Summary: <div>
arXiv:2510.07632v1 Announce Type: new 
Abstract: Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning</title>
<link>https://arxiv.org/abs/2510.07635</link>
<guid>https://arxiv.org/abs/2510.07635</guid>
<content:encoded><![CDATA[
<div> Framework, Safe Off-Policy Policy Gradient, Exploration, Novel actions, Recommender systems
<br />
<br />
Summary: 
The article introduces a framework for safe exploration of novel actions in recommender systems, addressing the challenge of balancing safety and exploration. The Safe Off-Policy Policy Gradient (Safe OPG) method is proposed to ensure safety by using high confidence off-policy evaluation. The experiment shows that Safe OPG consistently meets safety requirements. However, it is noted that Safe OPG can be overly conservative, highlighting the tradeoff between safety and exploration. To overcome this, a novel approach called Deployment-Efficient Policy Learning for Safe User Exploration is introduced, which gradually relaxes safety constraints over multiple deployments. This framework allows for exploration of novel actions while maintaining safety in recommender system implementation. <div>
arXiv:2510.07635v1 Announce Type: new 
Abstract: In many real recommender systems, novel items are added frequently over time. The importance of sufficiently presenting novel actions has widely been acknowledged for improving long-term user engagement. A recent work builds on Off-Policy Learning (OPL), which trains a policy from only logged data, however, the existing methods can be unsafe in the presence of novel actions. Our goal is to develop a framework to enforce exploration of novel actions with a guarantee for safety. To this end, we first develop Safe Off-Policy Policy Gradient (Safe OPG), which is a model-free safe OPL method based on a high confidence off-policy evaluation. In our first experiment, we observe that Safe OPG almost always satisfies a safety requirement, even when existing methods violate it greatly. However, the result also reveals that Safe OPG tends to be too conservative, suggesting a difficult tradeoff between guaranteeing safety and exploring novel actions. To overcome this tradeoff, we also propose a novel framework called Deployment-Efficient Policy Learning for Safe User Exploration, which leverages safety margin and gradually relaxes safety regularization during multiple (not many) deployments. Our framework thus enables exploration of novel actions while guaranteeing safe implementation of recommender systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Safety Evaluation in Generative Agent Social Simulations</title>
<link>https://arxiv.org/abs/2510.07709</link>
<guid>https://arxiv.org/abs/2510.07709</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, agents, safety, social dynamics
<br />
Generative agents in multimodal environments face challenges in reasoning about safety, coherence, and trust. A new simulation framework evaluates agents on safety improvement over time, detection of unsafe activities in various social situations, and social dynamics. Equipped with advanced features like layered memory, dynamic planning, and multimodal perception, agents are assessed using SocialMetrics for behavioral and structural metrics. Results show agents struggle to align local revisions with global safety, achieving only a 55% success rate in correcting unsafe plans. Different models exhibit varying performance, with Claude outperforming others in localized contexts. However, there is a tendency to overtrust images, with 45% of unsafe actions being accepted when paired with misleading visuals. These findings highlight limitations in current agent architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.
<br /><br />Summary: <div>
arXiv:2510.07709v1 Announce Type: new 
Abstract: Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07715</link>
<guid>https://arxiv.org/abs/2510.07715</guid>
<content:encoded><![CDATA[
<div> STL, reinforcement learning, cyber-physical systems, real-time constraints, reward generation <br />
<br />
Summary: 
This paper introduces a method for incorporating signal temporal logic (STL) into reinforcement learning (RL) for control synthesis in cyber-physical systems. By continuously monitoring system behavior against an STL specification at each control step, the proposed approach generates rewards that reflect instantaneous state dynamics, improving the accuracy and stability of training performances. The method includes a smooth approximation of causation semantics to make it differentiable for deep-RL methods. An evaluation in the Gym environment on various benchmarks demonstrates that the STL-guided RL method with online causation semantics outperforms existing approaches, offering a more robust and efficient reward generation framework for deep-RL. <div>
arXiv:2510.07715v1 Announce Type: new 
Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control synthesis must guarantee that generated policies meet stringent timing and correctness requirements under uncertain and dynamic conditions. Signal temporal logic (STL) has emerged as a powerful formalism of expressing real-time constraints, with its semantics enabling quantitative assessment of system behavior. Meanwhile, reinforcement learning (RL) has become an important method for solving control synthesis problems in unknown environments. Recent studies incorporate STL-based reward functions into RL to automatically synthesize control policies. However, the automatically inferred rewards obtained by these methods represent the global assessment of a whole or partial path but do not accumulate the rewards of local changes accurately, so the sparse global rewards may lead to non-convergence and unstable training performances. In this paper, we propose an online reward generation method guided by the online causation monitoring of STL. Our approach continuously monitors system behavior against an STL specification at each control step, computing the quantitative distance toward satisfaction or violation and thereby producing rewards that reflect instantaneous state dynamics. Additionally, we provide a smooth approximation of the causation semantics to overcome the discontinuity of the causation semantics and make it differentiable for using deep-RL methods. We have implemented a prototype tool and evaluated it in the Gym environment on a variety of continuously controlled benchmarks. Experimental results show that our proposed STL-guided RL method with online causation semantics outperforms existing relevant STL-guided RL methods, providing a more robust and efficient reward generation framework for deep-RL.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning</title>
<link>https://arxiv.org/abs/2510.07731</link>
<guid>https://arxiv.org/abs/2510.07731</guid>
<content:encoded><![CDATA[
<div> benchmarking, organic mechanism reasoning, large language models (LLMs), chemical reactivity, interdisciplinary research
Summary:
oMeBench is introduced as a large-scale benchmark for organic mechanism reasoning in organic chemistry, containing over 10,000 annotated mechanistic steps. The oMeS dynamic evaluation framework combines step-level logic and chemical similarity for precise evaluation of LLMs. State-of-the-art LLMs show promise in chemical intuition but struggle with correct multi-step reasoning. Using prompting strategy and fine-tuning a specialist model on oMeBench improves performance by 50%. This study aims to advance AI systems towards genuine chemical reasoning and emphasizes the importance of understanding organic reaction mechanisms for designing new molecules and reactions. <div>
arXiv:2510.07731v1 Announce Type: new 
Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation</title>
<link>https://arxiv.org/abs/2510.07733</link>
<guid>https://arxiv.org/abs/2510.07733</guid>
<content:encoded><![CDATA[
<div> agent framework, hierarchical citation graph, structured survey outline, multi-level summaries, research taxonomy <br />
Summary: 
The article introduces SurveyG, a novel agent framework that incorporates a hierarchical citation graph to enhance survey paper generation by considering structural relationships among papers. The graph consists of three layers: Foundation, Development, and Frontier, representing the evolution of research from foundational works to emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, SurveyG produces multi-level summaries that are consolidated into a structured survey outline. A multi-agent validation stage ensures consistency, coverage, and factual accuracy in survey generation. Experiments show that SurveyG outperforms existing frameworks, producing surveys that are more comprehensive and better structured according to the underlying knowledge taxonomy of a field. <br /><br /> <div>
arXiv:2510.07733v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly adopted for automating survey paper generation \cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prompt LLMs to summarize them directly. However, such methods often overlook the structural relationships among papers, resulting in generated surveys that lack a coherent taxonomy and a deeper contextual understanding of research progress. To address these shortcomings, we propose \textbf{SurveyG}, an LLM-based agent framework that integrates \textit{hierarchical citation graph}, where nodes denote research papers and edges capture both citation dependencies and semantic relatedness between their contents, thereby embedding structural and contextual knowledge into the survey generation process. The graph is organized into three layers: \textbf{Foundation}, \textbf{Development}, and \textbf{Frontier}, to capture the evolution of research from seminal works to incremental advances and emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, the agent produces multi-level summaries, which are consolidated into a structured survey outline. A multi-agent validation stage then ensures consistency, coverage, and factual accuracy in generating the final survey. Experiments, including evaluations by human experts and LLM-as-a-judge, demonstrate that SurveyG outperforms state-of-the-art frameworks, producing surveys that are more comprehensive and better structured to the underlying knowledge taxonomy of a field.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains</title>
<link>https://arxiv.org/abs/2510.07748</link>
<guid>https://arxiv.org/abs/2510.07748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Intelligence, Verifiable Reasoning, Theorem Proving, Healthcare Administration

Summary: 
The "Haibu Mathematical-Medical Intelligent Agent" (MMIA) is introduced as a reliable architecture driven by Large Language Models (LLMs) for medical tasks. This system ensures accuracy through verifiable reasoning by breaking down tasks into evidence-based steps and auditing for logical coherence. MMIA's "bootstrapping" mode stores validated reasoning chains for efficient future tasks using Retrieval-Augmented Generation (RAG). Validation across healthcare administration domains showed MMIA achieving a high error detection rate and low false positive rate. The RAG matching mode is expected to reduce processing costs significantly. MMIA's framework presents a promising step towards trustworthy and cost-effective AI systems, making LLM technology suitable for critical medical applications. 

<br /><br />Summary: <div>
arXiv:2510.07748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show promise in medicine but are prone to factual and logical errors, which is unacceptable in this high-stakes field. To address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent" (MMIA), an LLM-driven architecture that ensures reliability through a formally verifiable reasoning process. MMIA recursively breaks down complex medical tasks into atomic, evidence-based steps. This entire reasoning chain is then automatically audited for logical coherence and evidence traceability, similar to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which stores validated reasoning chains as "theorems." Subsequent tasks can then be efficiently solved using Retrieval-Augmented Generation (RAG), shifting from costly first-principles reasoning to a low-cost verification model. We validated MMIA across four healthcare administration domains, including DRG/DIP audits and medical insurance adjudication, using expert-validated benchmarks. Results showed MMIA achieved an error detection rate exceeding 98% with a false positive rate below 1%, significantly outperforming baseline LLMs. Furthermore, the RAG matching mode is projected to reduce average processing costs by approximately 85% as the knowledge base matures. In conclusion, MMIA's verifiable reasoning framework is a significant step toward creating trustworthy, transparent, and cost-effective AI systems, making LLM technology viable for critical applications in medicine.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.07762</link>
<guid>https://arxiv.org/abs/2510.07762</guid>
<content:encoded><![CDATA[
<div> framework, generative, graph restoration, Test-Time Graph Domain Adaptation, large language models
Summary:
The article introduces a novel framework for Test-Time Graph Domain Adaptation (TT-GDA) that leverages large language models (LLMs) to address the domain shift between train and test data. The framework reframes TT-GDA as a generative graph restoration problem, aiming to restore the target graph to its source-domain-like state without accessing the source examples. The key challenges include designing an effective encoding scheme for LLMs and ensuring the restored graph acquires the intrinsic features of the source domain. The proposed approach, GRAIL, compresses node representations, uses a graph diffusion process for restoration, and employs a quantization module to encode features into discrete tokens. An LLM is fine-tuned as a generative restorer guided by reinforcement learning to improve restoration quality. Experimental results show the effectiveness of the approach across various datasets.<br /><br />Summary: <div>
arXiv:2510.07762v1 Announce Type: new 
Abstract: Graph domain adaptation (GDA) has achieved great attention due to its effectiveness in addressing the domain shift between train and test data. A significant bottleneck in existing graph domain adaptation methods is their reliance on source-domain data, which is often unavailable due to privacy or security concerns. This limitation has driven the development of Test-Time Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without accessing the source examples. Inspired by the generative power of large language models (LLMs), we introduce a novel framework that reframes TT-GDA as a generative graph restoration problem, "restoring the target graph to its pristine, source-domain-like state". There are two key challenges: (1) We need to construct a reasonable graph restoration process and design an effective encoding scheme that an LLM can understand, bridging the modality gap. (2) We need to devise a mechanism to ensure the restored graph acquires the intrinsic features of the source domain, even without access to the source data. To ensure the effectiveness of graph restoration, we propose GRAIL, that restores the target graph into a state that is well-aligned with the source domain. Specifically, we first compress the node representations into compact latent features and then use a graph diffusion process to model the graph restoration process. Then a quantization module encodes the restored features into discrete tokens. Building on this, an LLM is fine-tuned as a generative restorer to transform a "noisy" target graph into a "native" one. To further improve restoration quality, we introduce a reinforcement learning process guided by specialized alignment and confidence rewards. Extensive experiments demonstrate the effectiveness of our approach across various datasets.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach for systematic decomposition of complex llm tasks</title>
<link>https://arxiv.org/abs/2510.07772</link>
<guid>https://arxiv.org/abs/2510.07772</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, decomposition framework, constraint problem, formal complexity measures, database querying tasks<br />
Summary:<br />
This study introduces a novel decomposition framework called Analysis of CONstraint-Induced Complexity (ACONIC) to address reliability issues faced by Large Language Models (LLMs) on complex tasks. ACONIC models tasks as constraint problems and utilizes formal complexity measures to guide decomposition systematically. By applying this framework to combinatorial and database querying tasks, significant performance improvements of 10-40 percentage points were observed. This approach is a departure from heuristic methods and agent or manual decomposition commonly used in existing techniques. The systematic nature of ACONIC allows for more effective decomposition, enabling LLMs to perform better on challenging tasks such as SATBench and database querying. Overall, the incorporation of formal complexity measures in task decomposition offers a promising solution to enhancing the performance of LLMs on complex tasks. <br /><br />Summary: <div>
arXiv:2510.07772v1 Announce Type: new 
Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks, as existing decomposition methods are heuristic and rely on agent or manual decomposition. This work introduces a novel, systematic decomposition framework that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models the task as a constraint problem and leveraging formal complexity measures to guide decomposition. On combinatorial (SATBench) and LLM database querying tasks (Spider), we find that by decomposing the tasks following the measure of complexity, agent can perform considerably better (10-40 percentage point).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCPO: When Contrast Fails, Go Gold</title>
<link>https://arxiv.org/abs/2510.07790</link>
<guid>https://arxiv.org/abs/2510.07790</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Group Contrastive Policy Optimization, reference answers, training efficiency, generalization

Summary: 
Group Contrastive Policy Optimization (GCPO) is introduced as a method to improve the reasoning capabilities of language models by incorporating external reference answers. Unlike existing algorithms like Group Relative Policy Optimization (GRPO), GCPO utilizes reference answers to guide model updates when faced with incorrect or unsolvable problems, leading to more efficient training. By emulating the problem-solving strategy of the reference answer during training, GCPO enhances generalization in reasoning tasks. The method achieves exceptional results on various benchmark datasets, surpassing baseline models. The code for GCPO implementation is available on GitHub, allowing for further research and application in reinforcement learning tasks. 

<br /><br />Summary: <div>
arXiv:2510.07790v1 Announce Type: new 
Abstract: Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games</title>
<link>https://arxiv.org/abs/2510.07813</link>
<guid>https://arxiv.org/abs/2510.07813</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Pursuit-Evasion-Exposure-Concealment Game, Strategic trade-off, Communication policy, Opponent modeling <br />
Summary: 
In adversarial environments, agents must navigate the trade-off between acquiring information and exposing themselves to threats. The PursuitEvasion-Exposure-Concealment Game (PEEC) framework explores this tension, with a focus on strategic communication. The SHADOW framework integrates navigation control, communication actions, and opponent modeling for effective decision-making in the PEEC game. Empirical evaluations show that SHADOW outperforms competitive baselines, highlighting the importance of temporal sequence modeling and opponent modeling. The learned policies generalize well across varying communication risks and physical asymmetries between agents, demonstrating the robustness of the approach. <div>
arXiv:2510.07813v1 Announce Type: new 
Abstract: Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation</title>
<link>https://arxiv.org/abs/2510.07825</link>
<guid>https://arxiv.org/abs/2510.07825</guid>
<content:encoded><![CDATA[
<div> Internet of Vehicles, IoV, traffic management, multi-vehicle dynamic navigation, path search algorithms<br />
<br />
Summary: CityNav proposes a hierarchical framework for large-scale multi-vehicle navigation in urban environments. It integrates a global traffic allocation agent and local navigation agents to coordinate strategic traffic flow distribution across regions and generate locally adaptive routes aligned with global directives. The framework includes a cooperative reasoning optimization mechanism and uses a dual-reward structure to promote individual vehicle efficiency and network-wide coordination. Extensive experiments on real-world road networks demonstrate that CityNav outperforms classical path search and reinforcement learning-based baselines in travel efficiency and congestion reduction. The results showcase the potential of large language models (LLMs) in enabling scalable, adaptive, and cooperative city-wide traffic navigation, laying the groundwork for intelligent vehicle routing in complex urban settings. The project is available on GitHub for further exploration. <br /> <div>
arXiv:2510.07825v1 Announce Type: new 
Abstract: The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing large fleets under evolving traffic conditions. Existing path search algorithms and reinforcement learning methods struggle to scale to city-wide networks, often failing to capture the nonlinear, stochastic, and coupled dynamics of urban traffic. To address these challenges, we propose CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle navigation. CityNav integrates a global traffic allocation agent, which coordinates strategic traffic flow distribution across regions, with local navigation agents that generate locally adaptive routes aligned with global directives. To enable effective cooperation, we introduce a cooperative reasoning optimization mechanism, in which agents are jointly trained with a dual-reward structure: individual rewards promote per-vehicle efficiency, while shared rewards encourage network-wide coordination and congestion reduction. Extensive experiments on four real-world road networks of varying scales (up to 1.6 million roads and 430,000 intersections) and traffic datasets demonstrate that CityNav consistently outperforms nine classical path search and RL-based baselines in city-scale travel efficiency and congestion mitigation. Our results highlight the potential of LLMs to enable scalable, adaptive, and cooperative city-wide traffic navigation, providing a foundation for intelligent, large-scale vehicle routing in complex urban environments. Our project is available at https://github.com/usail-hkust/CityNav.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning</title>
<link>https://arxiv.org/abs/2510.07852</link>
<guid>https://arxiv.org/abs/2510.07852</guid>
<content:encoded><![CDATA[
<div> dataset, multimodal, large language models, finance, FinMR

Summary: 
The article introduces FinMR, a high-quality multimodal dataset designed to evaluate expert-level financial reasoning capabilities. It includes over 3,200 question-answer pairs across 15 financial topics, incorporating advanced mathematical reasoning, financial knowledge, and visual interpretation tasks. Benchmarking against leading MLLMs reveals performance disparities and areas for improvement, such as precise image analysis and contextual financial understanding. FinMR aims to bridge the gap between MLLMs and professional financial analysts by providing rich visual content and detailed annotations, serving as a benchmark tool for assessing and advancing multimodal financial reasoning skills to professional analyst-level competence. <br /><br /> <div>
arXiv:2510.07852v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in recent years. However, their rigorous evaluation within specialized domains like finance is hindered by the absence of datasets characterized by professional-level knowledge intensity, detailed annotations, and advanced reasoning complexity. To address this critical gap, we introduce FinMR, a high-quality, knowledge-intensive multimodal dataset explicitly designed to evaluate expert-level financial reasoning capabilities at a professional analyst's standard. FinMR comprises over 3,200 meticulously curated and expertly annotated question-answer pairs across 15 diverse financial topics, ensuring broad domain diversity and integrating sophisticated mathematical reasoning, advanced financial knowledge, and nuanced visual interpretation tasks across multiple image types. Through comprehensive benchmarking with leading closed-source and open-source MLLMs, we highlight significant performance disparities between these models and professional financial analysts, uncovering key areas for model advancement, such as precise image analysis, accurate application of complex financial formulas, and deeper contextual financial understanding. By providing richly varied visual content and thorough explanatory annotations, FinMR establishes itself as an essential benchmark tool for assessing and advancing multimodal financial reasoning toward professional analyst-level competence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models</title>
<link>https://arxiv.org/abs/2510.07858</link>
<guid>https://arxiv.org/abs/2510.07858</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, time series forecasting, causal reasoning, directed causal graph, interpretability

Summary: 
Augur is a novel time series forecasting framework that utilizes large language models (LLMs) to discover and leverage directed causal associations among covariates. It employs a two-stage teacher-student architecture, where a powerful teacher LLM extracts a directed causal graph through heuristic search and pairwise causality testing. Subsequently, a lightweight student agent further refines the graph and fine-tunes on high-confidence causal associations encoded as textual prompts for forecasting. This approach enhances predictive accuracy while offering transparent and traceable reasoning on variable interactions. Extensive experiments conducted on real-world datasets against 25 baselines demonstrate Augur's competitive performance and robust zero-shot generalization capabilities.<br /><br />Summary: <div>
arXiv:2510.07858v1 Announce Type: new 
Abstract: Large language models (LLM) have emerged as a promising avenue for time series forecasting, offering the potential to integrate multimodal data. However, existing LLM-based approaches face notable limitations-such as marginalized role in model architectures, reliance on coarse statistical text prompts, and lack of interpretability. In this work, we introduce Augur, a fully LLM driven time series forecasting framework that exploits LLM causal reasoning to discover and use directed causal associations among covariates. Augur uses a two stage teacher student architecture where a powerful teacher LLM infers a directed causal graph from time series using heuristic search together with pairwise causality testing. A lightweight student agent then refines the graph and fine tune on high confidence causal associations that are encoded as rich textual prompts to perform forecasting. This design improves predictive accuracy while yielding transparent, traceable reasoning about variable interactions. Extensive experiments on real-world datasets with 25 baselines demonstrate that Augur achieves competitive performance and robust zero-shot generalization.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding DeepResearch via Reports</title>
<link>https://arxiv.org/abs/2510.07861</link>
<guid>https://arxiv.org/abs/2510.07861</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepResearch agents, research evaluation, LLM tasks, framework, benchmark 

Summary: 
DeepResearch agents are AI systems that conduct expert-level research through sophisticated reasoning and multi-tool integration. Evaluating these systems is challenging due to open-ended research scenarios and the need for holistic performance assessment. Traditional benchmarks focus on isolated capabilities rather than overall system performance. To address this gap, the DeepResearch-ReportEval framework assesses DeepResearch systems through their outputs: research reports. It measures quality, redundancy, and factuality using an innovative LLM-as-a-Judge methodology. A benchmark of 100 curated queries across 12 categories enables systematic comparison. Evaluation of four commercial systems reveals distinct design philosophies and performance trade-offs, offering foundational insights as DeepResearch evolves. Source code and data are available for further exploration. 

<br /><br />Summary: <div>
arXiv:2510.07861v1 Announce Type: new 
Abstract: DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: https://github.com/HKUDS/DeepResearch-Eval.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Meaningful Transparency in Civic AI Systems</title>
<link>https://arxiv.org/abs/2510.07889</link>
<guid>https://arxiv.org/abs/2510.07889</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, governmental services, bias, transparency, socio-technical systems 

Summary: 
Artificial intelligence is increasingly being used in governmental services, but often produces biased or incorrect outcomes, limiting citizens' agency. Current transparency practices focus on technical aspects of AI decision-making, making it difficult for the public to understand and act upon. This paper proposes the concept of meaningful transparency for civic AI systems, which allows for public engagement and understanding of AI systems that impact their lives. By taking a human-centric view on transparency and incorporating a socio-technical systems perspective, meaningful transparency can connect understanding to actionable insights for citizens. <div>
arXiv:2510.07889v1 Announce Type: new 
Abstract: Artificial intelligence has become a part of the provision of governmental services, from making decisions about benefits to issuing fines for parking violations. However, AI systems rarely live up to the promise of neutral optimisation, creating biased or incorrect outputs and reducing the agency of both citizens and civic workers to shape the way decisions are made. Transparency is a principle that can both help subjects understand decisions made about them and shape the processes behind those decisions. However, transparency as practiced around AI systems tends to focus on the production of technical objects that represent algorithmic aspects of decision making. These are often difficult for publics to understand, do not connect to potential for action, and do not give insight into the wider socio-material context of decision making. In this paper, we build on existing approaches that take a human-centric view on AI transparency, combined with a socio-technical systems view, to develop the concept of meaningful transparency for civic AI systems: transparencies that allow publics to engage with AI systems that affect their lives, connecting understanding with potential for action.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents</title>
<link>https://arxiv.org/abs/2510.07920</link>
<guid>https://arxiv.org/abs/2510.07920</guid>
<content:encoded><![CDATA[
<div> Quantum computing, Financial agents, Information leakage, Benchmark, Counterfactual perturbations <br />
Summary: <br />
This paper addresses the issue of information leakage in LLM-based financial agents, which often leads to inflated back-tested returns that do not hold up in real-time trading. The authors introduce a new evaluation benchmark called FinLake-Bench to quantify this leakage problem across four dimensions. To mitigate the issue, they propose FactFin, a framework that leverages counterfactual perturbations to encourage LLM-based agents to focus on learning causal drivers rather than memorized outcomes. FactFin consists of four core components: Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree Search, and Counterfactual Simulator. Extensive experiments demonstrate that FactFin outperforms all baselines in out-of-sample generalization, delivering superior risk-adjusted performance. <div>
arXiv:2510.07920v1 Announce Type: new 
Abstract: LLM-based financial agents have attracted widespread excitement for their ability to trade like human experts. However, most systems exhibit a "profit mirage": dazzling back-tested returns evaporate once the model's knowledge window ends, because of the inherent information leakage in LLMs. In this paper, we systematically quantify this leakage issue across four dimensions and release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to mitigate this issue, we introduce FactFin, a framework that applies counterfactual perturbations to compel LLM-based agents to learn causal drivers instead of memorized outcomes. FactFin integrates four core components: Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree Search, and Counterfactual Simulator. Extensive experiments show that our method surpasses all baselines in out-of-sample generalization, delivering superior risk-adjusted performance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles</title>
<link>https://arxiv.org/abs/2510.07925</link>
<guid>https://arxiv.org/abs/2510.07925</guid>
<content:encoded><![CDATA[
<div> personalization, large language models, AI agents, user-centered, adaptive

Summary: 
The article discusses the limitations of current approaches for delivering personalized interactions with Large Language Models (LLMs) and proposes a framework for integrating user-specific data to enhance personalization. By combining contextual information with persistent memory, dynamic coordination, self-validation, and evolving user profiles, the framework aims to enable personalized long-term interactions with LLM-based agents. The authors evaluate their approach using metrics such as retrieval accuracy and response correctness, as well as a pilot user study to gather initial feedback on perceived personalization. The study indicates the potential of integrating persistent memory and user profiles to improve the adaptivity and perceived personalization of LLM-based agents. <div>
arXiv:2510.07925v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly serve as the central control unit of AI agents, yet current approaches remain limited in their ability to deliver personalized interactions. While Retrieval Augmented Generation enhances LLM capabilities by improving context-awareness, it lacks mechanisms to combine contextual information with user-specific data. Although personalization has been studied in fields such as human-computer interaction or cognitive science, existing perspectives largely remain conceptual, with limited focus on technical implementation. To address these gaps, we build on a unified definition of personalization as a conceptual foundation to derive technical requirements for adaptive, user-centered LLM-based agents. Combined with established agentic AI patterns such as multi-agent collaboration or multi-source retrieval, we present a framework that integrates persistent memory, dynamic coordination, self-validation, and evolving user profiles to enable personalized long-term interactions. We evaluate our approach on three public datasets using metrics such as retrieval accuracy, response correctness, or BertScore. We complement these results with a five-day pilot user study providing initial insights into user feedback on perceived personalization. The study provides early indications that guide future work and highlights the potential of integrating persistent memory and user profiles to improve the adaptivity and perceived personalization of LLM-based agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization</title>
<link>https://arxiv.org/abs/2510.07943</link>
<guid>https://arxiv.org/abs/2510.07943</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency, trading strategy optimization, genetic algorithms, multi-agent coordination, market microstructure.

Summary: 
The article introduces a novel framework, CGA-Agent, which combines genetic algorithms with multi-agent coordination to optimize trading strategies in volatile cryptocurrency markets. This framework incorporates real-time market intelligence and adaptive strategy feedback to dynamically guide evolutionary processes. Empirical evaluations across three cryptocurrencies show significant performance improvements in both total returns and risk-adjusted metrics. This innovative approach addresses the challenges posed by extreme volatility, non-stationary dynamics, and complex microstructure patterns in cryptocurrency trading. The CGA-Agent framework achieves systematic and statistically significant enhancements in trading strategy optimization, surpassing traditional static optimization methods. This research contributes to advancing adaptive and intelligent approaches for optimizing trading strategies in dynamic financial environments. <br /><br />Summary: <div>
arXiv:2510.07943v1 Announce Type: new 
Abstract: Cryptocurrency markets present formidable challenges for trading strategy optimization due to extreme volatility, non-stationary dynamics, and complex microstructure patterns that render conventional parameter optimization methods fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent (CGA-Agent), a pioneering hybrid framework that synergistically integrates genetic algorithms with intelligent multi-agent coordination mechanisms for adaptive trading strategy parameter optimization in dynamic financial environments. The framework uniquely incorporates real-time market microstructure intelligence and adaptive strategy performance feedback through intelligent mechanisms that dynamically guide evolutionary processes, transcending the limitations of static optimization approaches. Comprehensive empirical evaluation across three cryptocurrencies demonstrates systematic and statistically significant performance improvements on both total returns and risk-adjusted metrics.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance</title>
<link>https://arxiv.org/abs/2510.07972</link>
<guid>https://arxiv.org/abs/2510.07972</guid>
<content:encoded><![CDATA[
<div> relevance analysis, e-commerce search engines, large language models, reinforcement learning, interpretability, robustness<br />
Summary:<br />
Query-product relevance analysis in e-commerce search engines is vital and evolving with the rise of large language models. Existing training paradigms have limitations in generalization, fine-grained supervision, and sparse feedback. A new approach, TaoSR-SHE, introduces Stepwise Reward Policy Optimization (SRPO) to leverage step-level rewards and improve reasoning quality. It combines a generative reward model and human verification to prioritize critical reasoning steps. TaoSR-SHE also utilizes diversified data filtering for exploration and multi-stage curriculum learning for progressive learning. Extensive experiments show its superiority over existing methods in reasoning quality and relevance prediction accuracy, enhancing interpretability and robustness in large-scale e-commerce contexts. <div>
arXiv:2510.07972v1 Announce Type: new 
Abstract: Query-product relevance analysis is a foundational technology in e-commerce search engines and has become increasingly important in AI-driven e-commerce. The recent emergence of large language models (LLMs), particularly their chain-of-thought (CoT) reasoning capabilities, offers promising opportunities for developing relevance systems that are both more interpretable and more robust. However, existing training paradigms have notable limitations: SFT and DPO suffer from poor generalization on long-tail queries and from a lack of fine-grained, stepwise supervision to enforce rule-aligned reasoning. In contrast, reinforcement learning with verification rewards (RLVR) suffers from sparse feedback, which provides insufficient signal to correct erroneous intermediate steps, thereby undermining logical consistency and limiting performance in complex inference scenarios.
  To address these challenges, we introduce the Stepwise Hybrid Examination Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement learning algorithm that leverages step-level rewards generated by a hybrid of a high-quality generative stepwise reward model and a human-annotated offline verifier, prioritizing learning from critical correct and incorrect reasoning steps. TaoSR-SHE further incorporates two key techniques: diversified data filtering to encourage exploration across varied reasoning paths and mitigate policy entropy collapse, and multi-stage curriculum learning to foster progressive capability growth. Extensive experiments on real-world search benchmarks show that TaoSR-SHE improves both reasoning quality and relevance-prediction accuracy in large-scale e-commerce settings, outperforming SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and robustness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title>
<link>https://arxiv.org/abs/2510.07978</link>
<guid>https://arxiv.org/abs/2510.07978</guid>
<content:encoded><![CDATA[
<div> Large-scale Speech Language Models, SpeechLMs, VoiceAgentBench, evaluation, Indian context <br />
Summary:<br /> 
The article introduces VoiceAgentBench, a new benchmark for evaluating SpeechLMs in realistic spoken agentic scenarios. The benchmark includes over 5,500 synthetic spoken queries grounded in the Indian context and covers single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. It supports English, Hindi, and 5 other Indian languages to reflect linguistic and cultural diversity. Speaker variability is simulated using a novel sampling algorithm, maximizing acoustic and speaker diversity. Evaluation measures tool selection accuracy, structural consistency, correctness of tool invocations, and adversarial robustness. Experiments reveal gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, highlighting critical limitations of current SpeechLMs. <br /> <div>
arXiv:2510.07978v1 Announce Type: new 
Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation</title>
<link>https://arxiv.org/abs/2510.07988</link>
<guid>https://arxiv.org/abs/2510.07988</guid>
<content:encoded><![CDATA[
<div> Keywords: Mobile GUI agents, human-in-the-loop, context-aware, multi-agent framework, information management<br />
Summary:<br />
The article introduces ReInAgent, a context-aware multi-agent framework designed to improve mobile task navigation by incorporating human input during task execution. Existing mobile GUI agents often operate autonomously, leading to outcomes that may not align with user preferences. ReInAgent overcomes this limitation by integrating specialized agents for information management, decision-making, and task reflection, all collaborating through a shared memory module. By continuously analyzing contextual information and facilitating user-agent interaction, ReInAgent adapts to dynamic and ambiguous task scenarios, resulting in more reliable outcomes. Experimental results show that ReInAgent outperforms existing approaches, achieving a 25% higher success rate on complex tasks involving information dilemmas. This framework demonstrates the potential for improved adaptability and user satisfaction in mobile task execution.<br /> 
Summary: <div>
arXiv:2510.07988v1 Announce Type: new 
Abstract: Mobile GUI agents exhibit substantial potential to facilitate and automate the execution of user tasks on mobile phones. However, exist mobile GUI agents predominantly privilege autonomous operation and neglect the necessity of active user engagement during task execution. This omission undermines their adaptability to information dilemmas including ambiguous, dynamically evolving, and conflicting task scenarios, leading to execution outcomes that deviate from genuine user requirements and preferences. To address these shortcomings, we propose ReInAgent, a context-aware multi-agent framework that leverages dynamic information management to enable human-in-the-loop mobile task navigation. ReInAgent integrates three specialized agents around a shared memory module: an information-managing agent for slot-based information management and proactive interaction with the user, a decision-making agent for conflict-aware planning, and a reflecting agent for task reflection and information consistency validation. Through continuous contextual information analysis and sustained user-agent collaboration, ReInAgent overcomes the limitation of existing approaches that rely on clear and static task assumptions. Consequently, it enables more adaptive and reliable mobile task navigation in complex, real-world scenarios. Experimental results demonstrate that ReInAgent effectively resolves information dilemmas and produces outcomes that are more closely aligned with genuine user preferences. Notably, on complex tasks involving information dilemmas, ReInAgent achieves a 25% higher success rate than Mobile-Agent-v2.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Do Not Embed Numbers Continuously</title>
<link>https://arxiv.org/abs/2510.08009</link>
<guid>https://arxiv.org/abs/2510.08009</guid>
<content:encoded><![CDATA[
<div> representations, language models, numeric values, continuous, noise <br />
<br />
Summary: 
The research evaluates how language models handle integers and numeric representations, finding that models do not treat numeric spaces as continuous and introduce significant noise. While models can accurately reconstruct values, principal components only explain a small portion of the variation in the embedding space, indicating the presence of orthogonal components. The study shows that as the decimal precision increases, both linear reconstruction and explained variance deteriorate, highlighting potential issues in areas where embedding models are used, especially for tasks requiring high numerical precision or involving large magnitudes and mixed-sign values. <div>
arXiv:2510.08009v1 Announce Type: new 
Abstract: Recent research has extensively studied how large language models manipulate integers in specific arithmetic tasks, and on a more fundamental level, how they represent numeric values. These previous works have found that language model embeddings can be used to reconstruct the original values, however, they do not evaluate whether language models actually model continuous values as continuous. Using expected properties of the embedding space, including linear reconstruction and principal component analysis, we show that language models not only represent numeric spaces as non-continuous but also introduce significant noise. Using models from three major providers (OpenAI, Google Gemini and Voyage AI), we show that while reconstruction is possible with high fidelity ($R^2 \geq 0.95$), principal components only explain a minor share of variation within the embedding space. This indicates that many components within the embedding space are orthogonal to the simple numeric input space. Further, both linear reconstruction and explained variance suffer with increasing decimal precision, despite the ordinal nature of the input space being fundamentally unchanged. The findings of this work therefore have implications for the many areas where embedding models are used, in-particular where high numerical precision, large magnitudes or mixed-sign values are common.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEAR: Phase Entropy Aware Reward for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2510.08026</link>
<guid>https://arxiv.org/abs/2510.08026</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Reasoning Models, Entropy, Phase Entropy Aware Reward, Response Length, Performance

Summary:
Large Reasoning Models (LRMs) often generate long and redundant explanations due to high entropy in the thinking phase. To address this, Phase Entropy Aware Reward (PEAR) penalizes excessive entropy during the thinking phase while allowing moderate exploration for the final answer, leading to concise and accurate reasoning traces. This approach enables adaptive control of response length without predefined targets or rigid rules. Experimental results across multiple benchmarks demonstrate that PEAR effectively reduces response length while maintaining competitive accuracy, even exhibiting strong out-of-distribution robustness. The code for PEAR is available on GitHub for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2510.08026v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution.This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2510.08034</link>
<guid>https://arxiv.org/abs/2510.08034</guid>
<content:encoded><![CDATA[
<div> parameter-efficient, finetuning, Low-Rank Adaptation, AILoRA, self-attention <br />
Summary: 
The article introduces AILoRA, a novel parameter-efficient finetuning method that incorporates function-aware asymmetric low-rank priors. It addresses limitations of the widely adopted Low-Rank Adaptation (LoRA) approach by considering the distinct characteristics of the projection matrices $W^Q$ and $W^V in self-attention mechanisms. $W^Q captures task-specific semantic space knowledge, while $W^V encodes stable token-level feature representations. AILoRA leverages this insight to initialize LoRA modules asymmetrically, injecting principal components of $W^Q for task adaptation and minor components of $W^V for generalizable features. This strategy enhances finetuning performance and convergence efficiency by better capturing the specialized roles of attention parameters. AILoRA offers a promising solution to overcoming challenges faced by existing parameter-efficient finetuning approaches, improving model adaptation to diverse downstream tasks with reduced computational and memory overhead. <br /> <div>
arXiv:2510.08034v1 Announce Type: new 
Abstract: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial computational and memory overhead involved in adapting large-scale pretrained models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank Adaptation (LoRA) has emerged as one of the most widely adopted approaches due to its robust empirical performance and low implementation complexity. In practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$ projection matrices of self-attention modules, enabling an effective trade-off between model performance and parameter efficiency. While LoRA has achieved considerable empirical success, it still encounters challenges such as suboptimal performance and slow convergence. To address these limitations, we introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates function-aware asymmetric low-rank priors. Our empirical analysis reveals that the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit distinct parameter characteristics, stemming from their functional differences. Specifically, $W^Q$ captures task-specific semantic space knowledge essential for attention distributions computation, making its parameters highly sensitive to downstream task variations. In contrast, $W^V$ encodes token-level feature representations that tend to remain stable across tasks and layers. Leveraging these insights, AILoRA performs a function-aware initialization by injecting the principal components of $W^Q$ to retain task-adaptive capacity, and the minor components of $W^V$ to preserve generalizable feature representations. This asymmetric initialization strategy enables LoRA modules to better capture the specialized roles of attention parameters, thereby enhancing both finetuning performance and convergence efficiency.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models</title>
<link>https://arxiv.org/abs/2510.08046</link>
<guid>https://arxiv.org/abs/2510.08046</guid>
<content:encoded><![CDATA[
<div> keywords: scenario generation, autonomous vehicles, Large Language Models (LLMs), 3D simulations, natural language<br />
Summary:<br />
The article introduces LinguaSim, a framework that converts natural language into realistic 3D scenarios for autonomous vehicle testing and training. Current methods struggle to balance accuracy and realism, often sacrificing realism for simplicity. LinguaSim aims to bridge this gap by creating interactive simulations that align closely with user descriptions, enhancing safety testing and training. The framework includes a feedback calibration module to refine scenario generation precision, improving fidelity to user intent. Experiments show LinguaSim can generate scenarios with varying criticality based on natural language descriptions and effectively reduce excessive aggressiveness in initial outputs. This results in a lower crash rate to better match user intentions. Overall, LinguaSim offers a promising approach to scenario generation for autonomous vehicles, combining natural language processing with interactive 3D simulations for more accurate and realistic testing environments.<br /><br />Summary: <div>
arXiv:2510.08046v1 Announce Type: new 
Abstract: The generation of testing and training scenarios for autonomous vehicles has drawn significant attention. While Large Language Models (LLMs) have enabled new scenario generation methods, current methods struggle to balance command adherence accuracy with the realism of real-world driving environments. To reduce scenario description complexity, these methods often compromise realism by limiting scenarios to 2D, or open-loop simulations where background vehicles follow predefined, non-interactive behaviors. We propose LinguaSim, an LLM-based framework that converts natural language into realistic, interactive 3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment between the input descriptions and the generated scenarios. A feedback calibration module further refines the generation precision, improving fidelity to user intent. By bridging the gap between natural language and closed-loop, interactive simulations, LinguaSim constrains adversarial vehicle behaviors using both the scenario description and the autonomous driving model guiding them. This framework facilitates the creation of high-fidelity scenarios that enhance safety testing and training. Experiments show LinguaSim can generate scenarios with varying criticality aligned with different natural language descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions; comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces excessive aggressiveness in LinguaSim's initial outputs, lowering the crash rate from 46.9% to 6.3% to better match user intentions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Condition Conformal Selection</title>
<link>https://arxiv.org/abs/2510.08075</link>
<guid>https://arxiv.org/abs/2510.08075</guid>
<content:encoded><![CDATA[
<div> algorithm, high-quality candidates, multi-condition selection, FDR control, drug discovery 

Summary: 
The article introduces the Multi-Condition Conformal Selection (MCCS) algorithm, which addresses the challenge of selecting high-quality candidates from large datasets with multiple conditions. Traditional conformal selection methods are limited to single-threshold scenarios, but MCCS extends this to include conjunctive and disjunctive conditions. The algorithm utilizes a novel nonconformity score for conjunctive conditions and a global Benjamini-Hochberg procedure for disjunctive conditions to ensure finite-sample False Discovery Rate control with theoretical guarantees. Experimental results demonstrate the superior performance of MCCS compared to baseline methods, its applicability across diverse conditions and real-world modalities, and its scalability to multi-task environments. The MCCS algorithm provides a robust solution for selecting high-quality candidates in resource-constrained applications such as drug discovery, precision medicine, and large language model alignment. 

<br /><br />Summary: <div>
arXiv:2510.08075v1 Announce Type: new 
Abstract: Selecting high-quality candidates from large-scale datasets is critically important in resource-constrained applications such as drug discovery, precision medicine, and the alignment of large language models. While conformal selection methods offer a rigorous solution with False Discovery Rate (FDR) control, their applicability is confined to single-threshold scenarios (i.e., y > c) and overlooks practical needs for multi-condition selection, such as conjunctive or disjunctive conditions. In this work, we propose the Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal selection to scenarios with multiple conditions. In particular, we introduce a novel nonconformity score with regional monotonicity for conjunctive conditions and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions, thereby establishing finite-sample FDR control with theoretical guarantees. The integration of these components enables the proposed method to achieve rigorous FDR-controlled selection in various multi-condition environments. Extensive experiments validate the superiority of MCCS over baselines, its generalizability across diverse condition combinations, different real-world modalities, and multi-task scalability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment</title>
<link>https://arxiv.org/abs/2510.08081</link>
<guid>https://arxiv.org/abs/2510.08081</guid>
<content:encoded><![CDATA[
<div> Keywords: online reviews, quality assessment, deep learning, interpretable features, AutoQual

Summary:
AutoQual is a novel framework utilizing LLM-based agents to automatically extract interpretable features for ranking online reviews based on quality. Traditional methods are limited in scalability and adaptability, while deep learning approaches often lack interpretability. AutoQual mimics human research processes by generating feature hypotheses, operationalizing them, and accumulating knowledge in memory. Deployed on a large-scale online platform, it increased the average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27% through large-scale A/B testing. This framework aims to transform implicit data knowledge into explicit, computable features and can be applied across various domains beyond review quality assessment.<br /><br />Summary: <div>
arXiv:2510.08081v1 Announce Type: new 
Abstract: Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems</title>
<link>https://arxiv.org/abs/2510.08086</link>
<guid>https://arxiv.org/abs/2510.08086</guid>
<content:encoded><![CDATA[
<div> ontology engineering, bias mitigation, fair AI, optimal transport, loan approval<br />
<br />
This paper introduces a framework for provably fair AI that addresses the limitations of current bias mitigation methods. The framework utilizes ontology engineering in OWL 2 QL to formally define sensitive attributes and infer their proxies, constructing a sigma algebra G to capture biased patterns. Fair representations are obtained through Delbaen Majumdar optimal transport to generate independent variables while preserving accuracy. By modeling bias as dependence between sigma algebras and using optimal transport as the fair transformation, the approach ensures complete fairness in tasks like loan approval where proxies like ZIP code can reveal race. This method provides a certifiable and mathematically grounded approach for developing trustworthy AI. 
<br /><br />Summary: <div>
arXiv:2510.08086v1 Announce Type: new 
Abstract: This paper presents a framework for provably fair AI that overcomes the limits of current bias mitigation methods by systematically removing all sensitive information and its proxies. Using ontology engineering in OWL 2 QL, it formally defines sensitive attributes and infers their proxies through logical reasoning, constructing a sigma algebra G that captures the full structure of biased patterns. Fair representations are then obtained via Delbaen Majumdar optimal transport, which generates variables independent of G while minimizing L2 distance to preserve accuracy. This guarantees true independence rather than mere decorrelation. By modeling bias as dependence between sigma algebras, compiling ontological knowledge into measurable structures, and using optimal transport as the unique fair transformation, the approach ensures complete fairness in tasks like loan approval, where proxies such as ZIP code reveal race. The result is a certifiable and mathematically grounded method for trustworthy AI.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Risk-taking AI-Assistants suitably represent entities</title>
<link>https://arxiv.org/abs/2510.08114</link>
<guid>https://arxiv.org/abs/2510.08114</guid>
<content:encoded><![CDATA[
<div> gender-specific attitudes, uncertainty, role-based decision-making, manipulability, risk aversion

Summary:
The study explores the manipulability of risk aversion in language models (LMs) to better understand their ability to replicate human risk preferences. It specifically focuses on gender-specific attitudes, uncertainty, and role-based decision-making. The results show that while some LMs align with human behaviors, there are discrepancies that highlight the need for refining bio-centric measures of manipulability. The study suggests directions for improving AI design to enhance ethical decision-making and calls for advancements to ensure AI systems accurately replicate human risk preferences for better risk management. By refining AI design, the study aims to increase the applicability of AI assistants in risk management contexts. <div>
arXiv:2510.08114v1 Announce Type: new 
Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue</title>
<link>https://arxiv.org/abs/2510.08175</link>
<guid>https://arxiv.org/abs/2510.08175</guid>
<content:encoded><![CDATA[
<div> latency-quality tradeoff, open-domain dialogue AI systems, asynchronous knowledge orchestration, PMFR, Top-iOCQA <br />
<br />
Summary: <br />
In open-domain dialogue AI systems, the tradeoff between response latency and quality is a critical challenge. Existing approaches fall short in balancing comprehensive knowledge access and quick response times. A new system called PMFR addresses this issue by using asynchronous knowledge orchestration. It consists of a Knowledge Adequacy Evaluator for real-time assessment, a Lightweight Response Generator for immediate interaction, and an Asynchronous Knowledge Refinement Agent for background knowledge enrichment. PMFR demonstrated superior performance on Top-iOCQA by reducing latency by 95.3% while maintaining response quality comparable to synchronous baselines. This innovative architecture ensures continuous conversational flow while gradually enhancing knowledge coverage through intelligent triggering mechanisms. <div>
arXiv:2510.08175v1 Announce Type: new 
Abstract: The latency-quality tradeoff is a fundamental constraint in open-domain dialogue AI systems, since comprehensive knowledge access necessitates prohibitive response delays. Contemporary approaches offer two inadequate solutions: lightweight instruct models achieve sub-second latency but lack reasoning depth, while tool-augmented ReAct agents enhance factuality through external knowledge at the cost of synchronous execution that blocks interaction during re- trieval processes. PMFR is thus proposed, with a tempo- ral decoupling framework that fundamentally resolves the contradiction through asynchronous knowledge orchestra- tion. PMFR employs three coordinated components: (1) a Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a Lightweight Response Generator for imme- diate user interaction, and (3) an Asynchronous Knowledge Refinement Agent for background knowledge enhancement. This architecture maintains continuous conversational flow while progressively enriching knowledge coverage through intelligent triggering mechanisms. Evaluation results on Top- iOCQA demonstrate PMFR outperforms brute-force scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while preserving response quality comparable to heavyweight synchronous baselines (GEval-C: 0.613 vs. 0.620).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?</title>
<link>https://arxiv.org/abs/2510.08189</link>
<guid>https://arxiv.org/abs/2510.08189</guid>
<content:encoded><![CDATA[
<div> scaling, reasoning models, R-HORIZON, long-horizon reasoning, LRMs

Summary:
- Recent advancements in test-time scaling for reasoning models have shown significant improvements through long Chain-of-Thought (CoT).
- Current benchmarks primarily focus on immediate, single-horizon tasks, lacking evaluation of models' performance on complex, long-horizon scenarios.
- R-HORIZON is introduced to enhance long-horizon reasoning behaviors in Large Reasoning Models (LRMs) by stimulating complex multi-step reasoning tasks.
- LRMs exhibit limitations in effective reasoning length and struggle with allocating thinking budget across multiple interconnected problems.
- Training LRMs using R-HORIZON data for reinforcement learning with verified rewards (RLVR) results in improved performance on multi-horizon reasoning tasks and enhanced accuracy on standard reasoning tasks, as shown by a significant increase on AIME2024. 

<br /><br />Summary: <div>
arXiv:2510.08189v1 Announce Type: new 
Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring What Matters: The AI Pluralism Index</title>
<link>https://arxiv.org/abs/2510.08193</link>
<guid>https://arxiv.org/abs/2510.08193</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence, Governance, Pluralism, Index, Transparency
Summary: 
The article introduces the AI Pluralism Index (AIPI), a tool that evaluates AI systems based on participatory governance, inclusivity, transparency, and accountability. The index aims to measure the degree to which stakeholders can influence the objectives, data practices, safeguards, and deployment of AI systems. The AIPI utilizes verifiable practices from public artifacts and independent evaluations to provide transparent and evidence-based assessments. The index is designed to promote pluralistic governance in AI development and to enable policymakers, procurers, and the public to make informed decisions. The article outlines the measurement model, implementation process, and reliability assessment of the AIPI. It also presents pilot results of the index and discusses its positioning among existing transparency, safety, and governance frameworks. The ultimate goal of the AIPI is to incentivize pluralistic practices in the development and deployment of AI technologies. 

<br /><br />Summary: <div>
arXiv:2510.08193v1 Announce Type: new 
Abstract: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling "Unknown" evidence to report both lower-bound ("evidence") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tournament Tree Method for preference elicitation in Multi-criteria decision-making</title>
<link>https://arxiv.org/abs/2510.08197</link>
<guid>https://arxiv.org/abs/2510.08197</guid>
<content:encoded><![CDATA[
arXiv:2510.08197v1 Announce Type: new 
Abstract: Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's Multiplicative Preference Relations, are widely used to model expert judgments in multi-criteria decision-making. However, their application is limited by the high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of inconsistency, and the computational complexity of deriving consistent value scales. This paper proposes the Tournament Tree Method (TTM), a novel elicitation and evaluation framework that overcomes these limitations. The TTM requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and consistent comparison matrix. The method consists of three phases: (i) elicitation of expert judgments using a reduced set of targeted comparisons, (ii) construction of the consistent pairwise comparison matrix, and (iii) derivation of a global value scale from the resulting matrix. The proposed approach ensures consistency by design, minimizes cognitive effort, and reduces the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters. Furthermore, it is compatible with the classical Deck of Cards method, and thus it can handle interval and ratio scales. We have also developed a web-based tool that demonstrates its practical applicability in real decision-making scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DODO: Causal Structure Learning with Budgeted Interventions</title>
<link>https://arxiv.org/abs/2510.08207</link>
<guid>https://arxiv.org/abs/2510.08207</guid>
<content:encoded><![CDATA[
arXiv:2510.08207v1 Announce Type: new 
Abstract: Artificial Intelligence has achieved remarkable advancements in recent years, yet much of its progress relies on identifying increasingly complex correlations. Enabling causality awareness in AI has the potential to enhance its performance by enabling a deeper understanding of the underlying mechanisms of the environment. In this paper, we introduce DODO, an algorithm defining how an Agent can autonomously learn the causal structure of its environment through repeated interventions. We assume a scenario where an Agent interacts with a world governed by a causal Directed Acyclic Graph (DAG), which dictates the system's dynamics but remains hidden from the Agent. The Agent's task is to accurately infer the causal DAG, even in the presence of noise. To achieve this, the Agent performs interventions, leveraging causal inference techniques to analyze the statistical significance of observed changes. Results show better performance for DODO, compared to observational approaches, in all but the most limited resource conditions. DODO is often able to reconstruct with as low as zero errors the structure of the causal graph. In the most challenging configuration, DODO outperforms the best baseline by +0.25 F1 points.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens</title>
<link>https://arxiv.org/abs/2510.08222</link>
<guid>https://arxiv.org/abs/2510.08222</guid>
<content:encoded><![CDATA[
arXiv:2510.08222v1 Announce Type: new 
Abstract: Due to their inherent complexity, reasoning tasks have long been regarded as rigorous benchmarks for assessing the capabilities of machine learning models, especially large language models (LLMs). Although humans can solve these tasks with ease, existing models, even after extensive pre-training and post-training at scale, still fail to perform reasoning reliably. In this paper, we revisit reasoning tasks from a causal perspective, seeking to understand their behavior in latent space and to offer insights for addressing their challenges. Specifically, we cast reasoning tasks as a selection mechanism, in which high-level logical concepts function as selection operators on the given observations, such as, identifying the correct answer in a math problem or filling the appropriate entry in Sudoku. We emphasize two key properties of this formulation that shed light on the difficulty of reasoning tasks. First, the latent space exceeds the observation space in complexity, even when the correct answer is fully determined by the observed input. Second, the latent variables, corresponding to logical thought, are densely structured and exhibit strong dependencies. Building on this formulation, we introduce a framework, called SR$^2$, that incorporates the estimated latent variables as feedback into the selection mechanism, thereby facilitating the learning of dense dependencies among latent representations. The framework consists of three key modules: reflective representation learning, dependency self-refinement, and periodic intermediate alignment. Experimentally, we show that our approach yields significant gains in reasoning accuracy, for example, attaining over 10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudoku and Maze tasks over the recent advances.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness</title>
<link>https://arxiv.org/abs/2510.08238</link>
<guid>https://arxiv.org/abs/2510.08238</guid>
<content:encoded><![CDATA[
arXiv:2510.08238v1 Announce Type: new 
Abstract: The rapid deployment of large language model (LLM)-based agents in real-world applications has raised serious concerns about their trustworthiness. In this work, we reveal the security and robustness vulnerabilities of these agents through backdoor attacks. Distinct from traditional backdoors limited to single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a multi-step backdoor attack designed for long-horizon agentic control. CoTri relies on an ordered sequence. It starts with an initial trigger, and subsequent ones are drawn from the environment, allowing multi-step manipulation that diverts the agent from its intended task. Experimental results show that CoTri achieves a near-perfect attack success rate (ASR) while maintaining a near-zero false trigger rate (FTR). Due to training data modeling the stochastic nature of the environment, the implantation of CoTri paradoxically enhances the agent's performance on benign tasks and even improves its robustness against environmental distractions. We further validate CoTri on vision-language models (VLMs), confirming its scalability to multimodal agents. Our work highlights that CoTri achieves stable, multi-step control within agents, improving their inherent robustness and task capabilities, which ultimately makes the attack more stealthy and raises potential safty risks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-TAP: Three-Layer Agent Interaction Protocol Technical Report</title>
<link>https://arxiv.org/abs/2510.08263</link>
<guid>https://arxiv.org/abs/2510.08263</guid>
<content:encoded><![CDATA[
arXiv:2510.08263v1 Announce Type: new 
Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer agent interaction protocol designed to address the challenges faced by multi-agent systems across the three core dimensions of Interoperability, Interaction and Collaboration, and Knowledge Sharing. We have designed and proposed a layered solution composed of three core protocols: the Human-Agent Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction layer, standardizing the flow of information between users, interfaces, and agents by defining a standardized, event-driven communication paradigm. This ensures the real-time performance, reliability, and synergy of interactions. As the core of the infrastructure layer, UAP is designed to break down communication barriers among heterogeneous agents through unified service discovery and protocol conversion mechanisms, thereby enabling seamless interconnection and interoperability of the underlying network. MEK, in turn, operates at the cognitive layer. By establishing a standardized ''Memory (M) - Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the ability to learn from individual experiences and form shareable knowledge, thereby laying the foundation for the realization of true collective intelligence. We believe this protocol framework will provide a solid engineering foundation and theoretical guidance for building the next generation of efficient, scalable, and intelligent multi-agent applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks</title>
<link>https://arxiv.org/abs/2510.08300</link>
<guid>https://arxiv.org/abs/2510.08300</guid>
<content:encoded><![CDATA[
arXiv:2510.08300v1 Announce Type: new 
Abstract: Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization. Open-source code: https://github.com/daniuyter/scalegmn_amortization
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Try Matters: Revisiting the Role of Reflection in Reasoning Models</title>
<link>https://arxiv.org/abs/2510.08308</link>
<guid>https://arxiv.org/abs/2510.08308</guid>
<content:encoded><![CDATA[
arXiv:2510.08308v1 Announce Type: new 
Abstract: Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries</title>
<link>https://arxiv.org/abs/2510.08325</link>
<guid>https://arxiv.org/abs/2510.08325</guid>
<content:encoded><![CDATA[
arXiv:2510.08325v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings</title>
<link>https://arxiv.org/abs/2510.08338</link>
<guid>https://arxiv.org/abs/2510.08338</guid>
<content:encoded><![CDATA[
arXiv:2510.08338v1 Announce Type: new 
Abstract: Consumer research costs companies billions annually yet suffers from panel biases and limited scale. Large language models (LLMs) offer an alternative by simulating synthetic consumers, but produce unrealistic response distributions when asked directly for numerical ratings. We present semantic similarity rating (SSR), a method that elicits textual responses from LLMs and maps these to Likert distributions using embedding similarity to reference statements. Testing on an extensive dataset comprising 57 personal care product surveys conducted by a leading corporation in that market (9,300 human responses), SSR achieves 90% of human test-retest reliability while maintaining realistic response distributions (KS similarity > 0.85). Additionally, these synthetic respondents provide rich qualitative feedback explaining their ratings. This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAgent: A modular Search Agent with Interactive Query Understanding</title>
<link>https://arxiv.org/abs/2510.08383</link>
<guid>https://arxiv.org/abs/2510.08383</guid>
<content:encoded><![CDATA[
arXiv:2510.08383v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Hallucination Detection with Effective Rank-based Uncertainty</title>
<link>https://arxiv.org/abs/2510.08389</link>
<guid>https://arxiv.org/abs/2510.08389</guid>
<content:encoded><![CDATA[
arXiv:2510.08389v1 Announce Type: new 
Abstract: Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</title>
<link>https://arxiv.org/abs/2510.08470</link>
<guid>https://arxiv.org/abs/2510.08470</guid>
<content:encoded><![CDATA[
arXiv:2510.08470v1 Announce Type: new 
Abstract: Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</title>
<link>https://arxiv.org/abs/2510.08511</link>
<guid>https://arxiv.org/abs/2510.08511</guid>
<content:encoded><![CDATA[
arXiv:2510.08511v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRT: Teaching LLM Agents to Know When They Know Enough</title>
<link>https://arxiv.org/abs/2510.08517</link>
<guid>https://arxiv.org/abs/2510.08517</guid>
<content:encoded><![CDATA[
arXiv:2510.08517v1 Announce Type: new 
Abstract: Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowSearch: Advancing deep research with dynamic structured knowledge flow</title>
<link>https://arxiv.org/abs/2510.08521</link>
<guid>https://arxiv.org/abs/2510.08521</guid>
<content:encoded><![CDATA[
arXiv:2510.08521v1 Announce Type: new 
Abstract: Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves state-of-the-art performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at https://github.com/Alpha-Innovator/InternAgent.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Learning via Early Experience</title>
<link>https://arxiv.org/abs/2510.08558</link>
<guid>https://arxiv.org/abs/2510.08558</guid>
<content:encoded><![CDATA[
arXiv:2510.08558v1 Announce Type: new 
Abstract: A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Teach Large Multimodal Models New Skills</title>
<link>https://arxiv.org/abs/2510.08564</link>
<guid>https://arxiv.org/abs/2510.08564</guid>
<content:encoded><![CDATA[
arXiv:2510.08564v1 Announce Type: new 
Abstract: How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&amp;Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning</title>
<link>https://arxiv.org/abs/2411.15876</link>
<guid>https://arxiv.org/abs/2411.15876</guid>
<content:encoded><![CDATA[
arXiv:2411.15876v2 Announce Type: cross 
Abstract: Overfitting remains a significant challenge in deep learning, often arising from data outliers, noise, and limited training data. To address this, the Divide2Conquer (D2C) method was previously proposed, which partitions training data into multiple subsets and trains identical models independently on each. This strategy enables learning more consistent patterns while minimizing the influence of individual outliers and noise. However, D2C's standard aggregation typically treats all subset models equally or based on fixed heuristics (like data size), potentially underutilizing information about their varying generalization capabilities. Building upon this foundation, we introduce Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that refines the aggregation process. DUA-D2C dynamically weights the contributions of subset models based on their performance on a shared validation set, considering both accuracy and prediction uncertainty. This intelligent aggregation allows the central model to preferentially learn from subsets yielding more generalizable and confident edge models, thereby more effectively combating overfitting. Empirical evaluations on benchmark datasets spanning multiple domains demonstrate that DUA-D2C significantly improves generalization. Our analysis includes evaluations of decision boundaries, loss curves, and other performance metrics, highlighting the effectiveness of DUA-D2C. This study demonstrates that DUA-D2C improves generalization performance even when applied on top of other regularization methods, establishing it as a theoretically grounded and effective approach to combating overfitting in modern deep learning. Our codes are publicly available at: https://github.com/Saiful185/DUA-D2C.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children</title>
<link>https://arxiv.org/abs/2510.07320</link>
<guid>https://arxiv.org/abs/2510.07320</guid>
<content:encoded><![CDATA[
arXiv:2510.07320v1 Announce Type: cross 
Abstract: Autism Spectrum Disorder significantly influences the communication abilities, learning processes, behavior, and social interactions of individuals. Although early intervention and customized educational strategies are critical to improving outcomes, there is a pivotal gap in understanding and addressing nuanced behavioral patterns and emotional identification in autistic children prior to skill development. This extended research delves into the foundational step of recognizing and mapping these patterns as a prerequisite to improving learning and soft skills. Using a longitudinal approach to monitor emotions and behaviors, this study aims to establish a baseline understanding of the unique needs and challenges faced by autistic students, particularly in the Information Technology domain, where opportunities are markedly limited. Through a detailed analysis of behavioral trends over time, we propose a targeted framework for developing applications and technical aids designed to meet these identified needs. Our research underscores the importance of a sequential and evidence-based intervention approach that prioritizes a deep understanding of each child's behavioral and emotional landscape as the basis for effective skill development. By shifting the focus toward early identification of behavioral patterns, we aim to foster a more inclusive and supportive learning environment that can significantly improve the educational and developmental trajectory of children with ASD.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation</title>
<link>https://arxiv.org/abs/2510.07328</link>
<guid>https://arxiv.org/abs/2510.07328</guid>
<content:encoded><![CDATA[
arXiv:2510.07328v1 Announce Type: cross 
Abstract: Medical decision systems increasingly rely on data from multiple sources to ensure reliable and unbiased diagnosis. However, existing multimodal learning models fail to achieve this goal because they often ignore two critical challenges. First, various data modalities may learn unevenly, thereby converging to a model biased towards certain modalities. Second, the model may emphasize learning on certain demographic groups causing unfair performances. The two aspects can influence each other, as different data modalities may favor respective groups during optimization, leading to both imbalanced and unfair multimodal learning. This paper proposes a novel approach called MultiFair for multimodal medical classification, which addresses these challenges with a dual-level gradient modulation process. MultiFair dynamically modulates training gradients regarding the optimization direction and magnitude at both data modality and group levels. We conduct extensive experiments on two multimodal medical datasets with different demographic groups. The results show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local MAP Sampling for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.07343</link>
<guid>https://arxiv.org/abs/2510.07343</guid>
<content:encoded><![CDATA[
arXiv:2510.07343v1 Announce Type: cross 
Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model</title>
<link>https://arxiv.org/abs/2510.07345</link>
<guid>https://arxiv.org/abs/2510.07345</guid>
<content:encoded><![CDATA[
arXiv:2510.07345v1 Announce Type: cross 
Abstract: Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with $SurgiFlowVid$, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10-20% over competitive baselines, establishing $SurgiFlowVid$ as a promising strategy to mitigate data imbalance and advance surgical video understanding methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts</title>
<link>https://arxiv.org/abs/2510.07358</link>
<guid>https://arxiv.org/abs/2510.07358</guid>
<content:encoded><![CDATA[
arXiv:2510.07358v1 Announce Type: cross 
Abstract: Most efforts to improve the reasoning capabilities of large language models (LLMs) involve either scaling the number of parameters and the size of training data, or scaling inference computation by letting models generate complex chains of thought. Motivated by interpretability studies showing that the crucial computation required for reasoning tasks is concentrated in a limited range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances the reasoning capabilities of a base model by training it to iterate over a small subset of reasoning-relevant layers during the mid-training stage. ETD amplifies latent reasoning while preserving the original architecture, parameter count, hyperparameters, and training data composition. When iterating on the selected layers at inference time, ETD models yield substantial gains on 17 reasoning benchmarks, including +28.4% relative accuracy improvement on GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an adaptive depth strategy that adjusts the computation per input token. Our results show that recursive latent reasoning offers a simple and effective path to stronger LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention to Order: Transformers Discover Phase Transitions via Learnability</title>
<link>https://arxiv.org/abs/2510.07401</link>
<guid>https://arxiv.org/abs/2510.07401</guid>
<content:encoded><![CDATA[
arXiv:2510.07401v1 Announce Type: cross 
Abstract: Phase transitions mark qualitative reorganizations of collective behavior, yet identifying their boundaries remains challenging whenever analytic solutions are absent and conventional simulations fail. Here we introduce learnability as a universal criterion, defined as the ability of a transformer model containing attention mechanism to extract structure from microscopic states. Using self-supervised learning and Monte Carlo generated configurations of the two-dimensional Ising model, we show that ordered phases correspond to enhanced learnability, manifested in both reduced training loss and structured attention patterns, while disordered phases remain resistant to learning. Two unsupervised diagnostics, the sharp jump in training loss and the rise in attention entropy, recover the critical temperature in excellent agreement with the exact value. Our results establish learnability as a data-driven marker of phase transitions and highlight deep parallels between long-range order in condensed matter and the emergence of structure in modern language models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle</title>
<link>https://arxiv.org/abs/2510.07413</link>
<guid>https://arxiv.org/abs/2510.07413</guid>
<content:encoded><![CDATA[
arXiv:2510.07413v1 Announce Type: cross 
Abstract: To overcome the bottleneck of classical path planning schemes in solving NP problems and address the predicament faced by current mainstream quantum path planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this study attempts to construct a quantum path planning solution based on parallel Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically, the grid path planning problem is mapped to the problem of finding the minimum quantum energy state. Two parallel QAOA circuits are built to simultaneously execute two solution processes, namely connectivity energy calculation and path energy calculation. A classical algorithm is employed to filter out unreasonable solutions of connectivity energy, and finally, the approximate optimal solution to the path planning problem is obtained by merging the calculation results of the two parallel circuits. The research findings indicate that by setting appropriate filter parameters, quantum states corresponding to position points with extremely low occurrence probabilities can be effectively filtered out, thereby increasing the probability of obtaining the target quantum state. Even when the circuit layer number p is only 1, the theoretical solution of the optimal path coding combination can still be found by leveraging the critical role of the filter. Compared with serial circuits, parallel circuits exhibit a significant advantage, as they can find the optimal feasible path coding combination with the highest probability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</title>
<link>https://arxiv.org/abs/2510.07414</link>
<guid>https://arxiv.org/abs/2510.07414</guid>
<content:encoded><![CDATA[
arXiv:2510.07414v1 Announce Type: cross 
Abstract: Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LASER: An LLM-based ASR Scoring and Evaluation Rubric</title>
<link>https://arxiv.org/abs/2510.07437</link>
<guid>https://arxiv.org/abs/2510.07437</guid>
<content:encoded><![CDATA[
arXiv:2510.07437v1 Announce Type: cross 
Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly penalize morphological and syntactic nuances that do not significantly alter sentence semantics. We introduce an LLM-based scoring rubric LASER that leverages state-of-the-art LLMs' in-context learning abilities to learn from prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro achieved a very high correlation score of 94% with human annotations. Hindi examples in the prompt were also effective in analyzing errors in other Indian languages such as Marathi, Kannada and Malayalam. We also demonstrate how a smaller LLM like Llama 3 can be finetuned on word-pair examples derived from reference and ASR predictions to predict what kind of penalty should be applied with close to 89% accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing the Value-at-Risk of Loan Portfolio via Deep Neural Networks</title>
<link>https://arxiv.org/abs/2510.07444</link>
<guid>https://arxiv.org/abs/2510.07444</guid>
<content:encoded><![CDATA[
arXiv:2510.07444v1 Announce Type: cross 
Abstract: Risk management is a prominent issue in peer-to-peer lending. An investor may naturally reduce his risk exposure by diversifying instead of putting all his money on one loan. In that case, an investor may want to minimize the Value-at-Risk (VaR) or Conditional Value-at-Risk (CVaR) of his loan portfolio. We propose a low degree of freedom deep neural network model, DeNN, as well as a high degree of freedom model, DSNN, to tackle the problem. In particular, our models predict not only the default probability of a loan but also the time when it will default. The experiments demonstrate that both models can significantly reduce the portfolio VaRs at different confidence levels, compared to benchmarks. More interestingly, the low degree of freedom model, DeNN, outperforms DSNN in most scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.07459</link>
<guid>https://arxiv.org/abs/2510.07459</guid>
<content:encoded><![CDATA[
arXiv:2510.07459v1 Announce Type: cross 
Abstract: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a novel Mixture-of-Experts (MoE) framework designed for regression tasks and applied to time series forecasting. Unlike conventional MoEs that provide only point estimates, MoGU models each expert's output as a Gaussian distribution. This allows it to directly quantify both the forecast (the mean) and its inherent uncertainty (variance). MoGU's core innovation is its uncertainty-based gating mechanism, which replaces the traditional input-based gating network by using each expert's estimated variance to determine its contribution to the final prediction. Evaluated across diverse time series forecasting benchmarks, MoGU consistently outperforms single-expert models and traditional MoE setups. It also provides well-quantified, informative uncertainties that directly correlate with prediction errors, enhancing forecast reliability. Our code is available from: https://github.com/yolish/moe_unc_tsf
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data</title>
<link>https://arxiv.org/abs/2510.07477</link>
<guid>https://arxiv.org/abs/2510.07477</guid>
<content:encoded><![CDATA[
arXiv:2510.07477v1 Announce Type: cross 
Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of cancer deaths in the US. Although smoking is the primary risk factor, the occurrence of LC in never-smokers and familial aggregation studies highlight a genetic component. Genetic biomarkers identified through genome-wide association studies (GWAS) are promising tools for assessing LC risk. We introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data), a new framework that applies explainable transformer-based deep learning to GWAS data of single nucleotide polymorphisms (SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly processes raw genotype data without clinical covariates, introducing additive positional encodings, neural genotype embeddings, and refined variant filtering. A post hoc explainability module based on Layer-wise Integrated Gradients enables attribution of model predictions to specific SNPs, aligning strongly with known LC risk loci. Trained on data from 27,254 Million Veteran Program participants, HEMERA achieved >99% AUC (area under receiver characteristics) score. These findings support transparent, hypothesis-generating models for personalized LC risk assessment and early intervention.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics</title>
<link>https://arxiv.org/abs/2510.07488</link>
<guid>https://arxiv.org/abs/2510.07488</guid>
<content:encoded><![CDATA[
arXiv:2510.07488v1 Announce Type: cross 
Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are gaining attention, yet fewer studies explore their team dynamics. Inspired by human team science, we propose a multi-agent framework to examine core aspects of team science: structure, diversity, and interaction dynamics. We evaluate team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and Latent Implicit Hate, spanning commonsense and social reasoning. Our results show that flat teams tend to perform better than hierarchical ones, while diversity has a nuanced impact. Interviews suggest agents are overconfident about their team performance, yet post-task reflections reveal both appreciation for collaboration and challenges in integration, including limited conversational coordination.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy</title>
<link>https://arxiv.org/abs/2510.07492</link>
<guid>https://arxiv.org/abs/2510.07492</guid>
<content:encoded><![CDATA[
arXiv:2510.07492v1 Announce Type: cross 
Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Speech LLMs Think while Listening?</title>
<link>https://arxiv.org/abs/2510.07497</link>
<guid>https://arxiv.org/abs/2510.07497</guid>
<content:encoded><![CDATA[
arXiv:2510.07497v1 Announce Type: cross 
Abstract: Recent advances in speech large language models (speech LLMs) have enabled seamless spoken interactions, but these systems still struggle with complex reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning has been to shown to significantly improve the reasoning abilities of text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for multi-stream speech LLMs, demonstrating that reasoning in text space improves the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken reasoning tasks. Beyond accuracy, the latency of the spoken response is a crucial factor for interacting with voice-based agents. Inspired by the human behavior of "thinking while listening," we propose methods to reduce the additional latency from reasoning by allowing the model to start reasoning before the user query has ended. To achieve this, we introduce an entropy-based metric, "question completeness," which acts as an indicator to guide the model on the optimal time to start reasoning. This method provides greater control over the accuracy-latency trade-off compared with heuristic-based approaches and, under equivalent latency conditions, yields a 4% accuracy gain on ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference data created using rejection sampling to push the accuracy-latency pareto frontier further, resulting in a 70% reduction in latency without loss in accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs</title>
<link>https://arxiv.org/abs/2510.07499</link>
<guid>https://arxiv.org/abs/2510.07499</guid>
<content:encoded><![CDATA[
arXiv:2510.07499v1 Announce Type: cross 
Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis</title>
<link>https://arxiv.org/abs/2510.07513</link>
<guid>https://arxiv.org/abs/2510.07513</guid>
<content:encoded><![CDATA[
arXiv:2510.07513v1 Announce Type: cross 
Abstract: Effective analysis of time series data presents significant challenges due to the complex temporal dependencies and cross-channel interactions in multivariate data. Inspired by the way human analysts visually inspect time series to uncover hidden patterns, we ask: can incorporating visual representations enhance automated time-series analysis? Recent advances in multimodal large language models have demonstrated impressive generalization and visual understanding capability, yet their application to time series remains constrained by the modality gap between continuous numerical data and discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel framework that leverages multimodal large language models for general time-series analysis by integrating a dedicated vision branch. Each time-series channel is rendered as a horizontally stacked color-coded line plot in one composite image to capture spatial dependencies across channels, and a temporal-aware visual patch alignment strategy then aligns visual patches with their corresponding time segments. MLLM4TS fuses fine-grained temporal details from the numerical data with global contextual information derived from the visual representation, providing a unified foundation for multimodal time-series analysis. Extensive experiments on standard benchmarks demonstrate the effectiveness of MLLM4TS across both predictive tasks (e.g., classification) and generative tasks (e.g., anomaly detection and forecasting). These results underscore the potential of integrating visual modalities with pretrained language models to achieve robust and generalizable time-series analysis.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning</title>
<link>https://arxiv.org/abs/2510.07524</link>
<guid>https://arxiv.org/abs/2510.07524</guid>
<content:encoded><![CDATA[
arXiv:2510.07524v1 Announce Type: cross 
Abstract: Accurate classification of sleep stages is crucial for the diagnosis and management of sleep disorders. Conventional approaches for sleep scoring rely on manual annotation or features extracted from EEG signals in the time or frequency domain. This study proposes a novel framework for automated sleep stage scoring using time-frequency analysis based on the wavelet transform. The Sleep-EDF Expanded Database (sleep-cassette recordings) was used for evaluation. The continuous wavelet transform (CWT) generated time-frequency maps that capture both transient and oscillatory patterns across frequency bands relevant to sleep staging. Experimental results demonstrate that the proposed wavelet-based representation, combined with ensemble learning, achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of 73.15, outperforming conventional machine learning methods and exhibiting comparable or superior performance to recent deep learning approaches. These findings highlight the potential of wavelet analysis for robust, interpretable, and clinically applicable sleep stage classification.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs</title>
<link>https://arxiv.org/abs/2510.07535</link>
<guid>https://arxiv.org/abs/2510.07535</guid>
<content:encoded><![CDATA[
arXiv:2510.07535v1 Announce Type: cross 
Abstract: Speculative decoding promises faster inference for large language models (LLMs), yet existing methods fail to generalize to real-world settings. Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical workloads involve long contexts. We find current approaches degrade severely with long contexts; for instance, EAGLE3 even slows down the generation speed by 0.81x. We address these limitations by releasing a new long-context benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves about 5x higher acceptance length than EAGLE3 on long-context inputs through three innovations: (1) an LSTM-based drafter conditioned only on the last-token state, making it generalize to various lengths, (2) a special token [SPEC] in the verifier that produces richer representation for drafter, and (3) a hybrid algorithm combining both tree and non-tree decoding methods. We release all code and datasets to advance future research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility</title>
<link>https://arxiv.org/abs/2510.07550</link>
<guid>https://arxiv.org/abs/2510.07550</guid>
<content:encoded><![CDATA[
arXiv:2510.07550v1 Announce Type: cross 
Abstract: Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe that combines a balanced training dataset with a trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, a benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer a unified framework for probing and improving physical plausibility in multimodal models, shedding light on a challenging and underexplored aspect of visual-temporal understanding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Semantics for Robust Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2510.07556</link>
<guid>https://arxiv.org/abs/2510.07556</guid>
<content:encoded><![CDATA[
arXiv:2510.07556v1 Announce Type: cross 
Abstract: Hyperspectral imaging (HSI) classification is a critical tool with widespread applications across diverse fields such as agriculture, environmental monitoring, medicine, and materials science. Due to the limited availability of high-quality training samples and the high dimensionality of spectral data, HSI classification models are prone to overfitting and often face challenges in balancing accuracy and computational complexity. Furthermore, most of HSI classification models are monomodal, where it solely relies on spectral-spatial data to learn decision boundaries in the high dimensional embedding space. To address this, we propose a general-purpose Semantic Spectral-Spatial Fusion Network (S3FN) that uses contextual, class specific textual descriptions to complement the training of an HSI classification model. Specifically, S3FN leverages LLMs to generate comprehensive textual descriptions for each class label that captures their unique characteristics and spectral behaviors. These descriptions are then embedded into a vector space using a pre-trained text encoder such as BERT or RoBERTa to extract meaningful label semantics which in turn leads to a better feature-label alignment for improved classification performance. To demonstrate the effectiveness of our approach, we evaluate our model on three diverse HSI benchmark datasets - Hyperspectral Wood, HyperspectralBlueberries, and DeepHS-Fruit and report significant performance boost. Our results highlight the synergy between textual semantics and spectral-spatial data, paving the way for further advancements in semantically augmented HSI classification models. Codes are be available in: https://github.com/milab-nsu/S3FN
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic</title>
<link>https://arxiv.org/abs/2510.07557</link>
<guid>https://arxiv.org/abs/2510.07557</guid>
<content:encoded><![CDATA[
arXiv:2510.07557v1 Announce Type: cross 
Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to the lmsys-chat-1m dataset, a multilingual conversational corpus built from head-to-head evaluations of large language models (LLMs). Each user prompt is paired with two anonymized LLM responses and a human preference label, used to assess user evaluation of competing model outputs. The main objective is uncovering thematic patterns in these conversations and examining their relation to user preferences, particularly if certain LLMs are consistently preferred within specific topics. A robust preprocessing pipeline was designed for multilingual variation, balancing dialogue turns, and cleaning noisy or redacted data. BERTopic extracted over 29 coherent topics including artificial intelligence, programming, ethics, and cloud infrastructure. We analysed relationships between topics and model preferences to identify trends in model-topic alignment. Visualization techniques included inter-topic distance maps, topic probability distributions, and model-versus-topic matrices. Our findings inform domain-specific fine-tuning and optimization strategies for improving real-world LLM performance and user satisfaction.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER</title>
<link>https://arxiv.org/abs/2510.07566</link>
<guid>https://arxiv.org/abs/2510.07566</guid>
<content:encoded><![CDATA[
arXiv:2510.07566v1 Announce Type: cross 
Abstract: Deploying natural language processing (NLP) models on mobile platforms requires models that can adapt across diverse applications while remaining efficient in memory and computation. We investigate pre-finetuning strategies to enhance the adaptability of lightweight BERT-like encoders for two fundamental NLP task families: named entity recognition (NER) and text classification. While pre-finetuning improves downstream performance for each task family individually, we find that na\"ive multi-task pre-finetuning introduces conflicting optimization signals that degrade overall performance. To address this, we propose a simple yet effective multi-task pre-finetuning framework based on task-primary LoRA modules, which enables a single shared encoder backbone with modular adapters. Our approach achieves performance comparable to individual pre-finetuning while meeting practical deployment constraint. Experiments on 21 downstream tasks show average improvements of +0.8% for NER and +8.8% for text classification, demonstrating the effectiveness of our method for versatile mobile NLP applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2510.07578</link>
<guid>https://arxiv.org/abs/2510.07578</guid>
<content:encoded><![CDATA[
arXiv:2510.07578v1 Announce Type: cross 
Abstract: This review aims to conduct a comparative analysis of liquid neural networks (LNNs) and traditional recurrent neural networks (RNNs) and their variants, such as long short-term memory networks (LSTMs) and gated recurrent units (GRUs). The core dimensions of the analysis include model accuracy, memory efficiency, and generalization ability. By systematically reviewing existing research, this paper explores the basic principles, mathematical models, key characteristics, and inherent challenges of these neural network architectures in processing sequential data. Research findings reveal that LNN, as an emerging, biologically inspired, continuous-time dynamic neural network, demonstrates significant potential in handling noisy, non-stationary data, and achieving out-of-distribution (OOD) generalization. Additionally, some LNN variants outperform traditional RNN in terms of parameter efficiency and computational speed. However, RNN remains a cornerstone in sequence modeling due to its mature ecosystem and successful applications across various tasks. This review identifies the commonalities and differences between LNNs and RNNs, summarizes their respective shortcomings and challenges, and points out valuable directions for future research, particularly emphasizing the importance of improving the scalability of LNNs to promote their application in broader and more complex scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets</title>
<link>https://arxiv.org/abs/2510.07579</link>
<guid>https://arxiv.org/abs/2510.07579</guid>
<content:encoded><![CDATA[
arXiv:2510.07579v1 Announce Type: cross 
Abstract: This study conducts a computational linguistic analysis of pandemic-related online discourse to examine how language distinguishes health misinformation from factual communication. Drawing on three corpora: COVID-19 false narratives (n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts (n = 5787), we identify significant differences in readability, rhetorical markers, and persuasive language use. COVID-19 misinformation exhibited markedly lower readability scores and contained over twice the frequency of fear-related or persuasive terms compared to the other datasets. It also showed minimal use of exclamation marks, contrasting with the more emotive style of Monkeypox content. These patterns suggest that misinformation employs a deliberately complex rhetorical style embedded with emotional cues, a combination that may enhance its perceived credibility. Our findings contribute to the growing body of work on digital health misinformation by highlighting linguistic indicators that may aid detection efforts. They also inform public health messaging strategies and theoretical models of crisis communication in networked media environments. At the same time, the study acknowledges limitations, including reliance on traditional readability indices, use of a deliberately narrow persuasive lexicon, and reliance on static aggregate analysis. Future research should therefore incorporate longitudinal designs, broader emotion lexicons, and platform-sensitive approaches to strengthen robustness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs</title>
<link>https://arxiv.org/abs/2510.07586</link>
<guid>https://arxiv.org/abs/2510.07586</guid>
<content:encoded><![CDATA[
arXiv:2510.07586v1 Announce Type: cross 
Abstract: Well-designed open-source software drives progress in Machine Learning (ML) research. While static graph ML enjoys mature frameworks like PyTorch Geometric and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks comparable infrastructure. Existing TG libraries are often tailored to specific architectures, hindering support for diverse models in this rapidly evolving field. Additionally, the divide between continuous- and discrete-time dynamic graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To address these gaps, we introduce Temporal Graph Modelling (TGM), a research-oriented library for ML on temporal graphs, the first to unify CTDG and DTDG approaches. TGM offers first-class support for dynamic node features, time-granularity conversions, and native handling of link-, node-, and graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across multiple models, datasets, and tasks compared to the widely used DyGLib, and an average 175x speedup on graph discretization relative to available implementations. Beyond efficiency, we show in our experiments how TGM unlocks entirely new research possibilities by enabling dynamic graph property prediction and time-driven training paradigms, opening the door to questions previously impractical to study. TGM is available at https://github.com/tgm-team/tgm
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vocabulary embeddings organize linguistic structure early in language model training</title>
<link>https://arxiv.org/abs/2510.07613</link>
<guid>https://arxiv.org/abs/2510.07613</guid>
<content:encoded><![CDATA[
arXiv:2510.07613v1 Announce Type: cross 
Abstract: Large language models (LLMs) work by manipulating the geometry of input embedding vectors over multiple layers. Here, we ask: how are the input vocabulary representations of language models structured, and how and when does this structure evolve over training? To answer this question, we use representational similarity analysis, running a suite of experiments that correlate the geometric structure of the input embeddings and output embeddings of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic, and frequency-based metrics over the course of training. Our key findings are as follows: 1) During training, the vocabulary embedding geometry quickly converges to high correlations with a suite of semantic and syntactic features; 2) Embeddings of high-frequency and function words (e.g., "the," "of") converge to their final vectors faster than lexical and low-frequency words, which retain some alignment with the bias in their random initializations. These findings help map the dynamic trajectory by which input embeddings organize around linguistic structure, revealing distinct roles for word frequency and function. Our findings motivate a deeper study of how the evolution of vocabulary geometry may facilitate specific capability gains during model training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title>
<link>https://arxiv.org/abs/2510.07620</link>
<guid>https://arxiv.org/abs/2510.07620</guid>
<content:encoded><![CDATA[
arXiv:2510.07620v1 Announce Type: cross 
Abstract: Dynamic trust evaluation in large, rapidly evolving graphs requires models that can capture changing relationships, express calibrated confidence, and resist adversarial manipulation. DGTEN (Deep Gaussian-based Trust Evaluation Network) introduces a unified graph framework that achieves all three by combining uncertainty-aware message passing, expressive temporal modeling, and built-in defenses against trust-targeted attacks. It represents nodes and edges as Gaussian distributions so that both semantic signals and epistemic uncertainty propagate through the graph neural network, enabling risk-aware trust decisions rather than overconfident guesses. To model how trust evolves, it employs hybrid Absolute-Gaussian-Hourglass (HAGH) positional encoding with Kolmogorov-Arnold network-based unbiased multi-head attention, followed by an ordinary differential equation (ODE)-based residual learning module to jointly capture abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity measures, mitigating reputation laundering, sabotage, and on/off attacks. On two signed Bitcoin trust networks, DGTEN delivers significant improvements: in single-timeslot prediction on Bitcoin-Alpha, it improves MCC by 10.77% over the best dynamic baseline; in the cold-start scenario, it achieves a 16.41% MCC gain - the largest across all tasks and datasets. Under adversarial on/off attacks, it surpasses the baseline by up to 11.63% MCC. These results validate the effectiveness of the unified DGTEN framework.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retentive Relevance: Capturing Long-Term User Value in Recommendation Systems</title>
<link>https://arxiv.org/abs/2510.07621</link>
<guid>https://arxiv.org/abs/2510.07621</guid>
<content:encoded><![CDATA[
arXiv:2510.07621v1 Announce Type: cross 
Abstract: Recommendation systems have traditionally relied on short-term engagement signals, such as clicks and likes, to personalize content. However, these signals are often noisy, sparse, and insufficient for capturing long-term user satisfaction and retention. We introduce Retentive Relevance, a novel content-level survey-based feedback measure that directly assesses users' intent to return to the platform for similar content. Unlike other survey measures that focus on immediate satisfaction, Retentive Relevance targets forward-looking behavioral intentions, capturing longer term user intentions and providing a stronger predictor of retention. We validate Retentive Relevance using psychometric methods, establishing its convergent, discriminant, and behavioral validity. Through large-scale offline modeling, we show that Retentive Relevance significantly outperforms both engagement signals and other survey measures in predicting next-day retention, especially for users with limited historical engagement. We develop a production-ready proxy model that integrates Retentive Relevance into the final stage of a multi-stage ranking system on a social media platform. Calibrated score adjustments based on this model yield substantial improvements in engagement, and retention, while reducing exposure to low-quality content, as demonstrated by large-scale A/B experiments. This work provides the first empirically validated framework linking content-level user perceptions to retention outcomes in production systems. We offer a scalable, user-centered solution that advances both platform growth and user experience. Our work has broad implications for responsible AI development.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banking Done Right: Redefining Retail Banking with Language-Centric AI</title>
<link>https://arxiv.org/abs/2510.07645</link>
<guid>https://arxiv.org/abs/2510.07645</guid>
<content:encoded><![CDATA[
arXiv:2510.07645v1 Announce Type: cross 
Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt Bank to enable customers to execute core financial transactions through natural language conversation. This represents the first global regulator-approved deployment worldwide where conversational AI functions as the primary banking interface, in contrast to prior assistants that have been limited to advisory or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a closed-source LLM developed internally, and replaces rigid multi-screen workflows with a single dialogue orchestrated by four LLM-powered agents (Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific LoRA adapter to ILMU, which is hosted within the bank's infrastructure to ensure consistent behavior with minimal overhead. Deterministic guardrails, human-in-the-loop confirmation, and a stateless audit architecture provide defense-in-depth for security and compliance. The result is Banking Done Right: demonstrating that regulator-approved natural-language interfaces can reliably support core financial operations under strict governance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Flows</title>
<link>https://arxiv.org/abs/2510.07650</link>
<guid>https://arxiv.org/abs/2510.07650</guid>
<content:encoded><![CDATA[
arXiv:2510.07650v1 Announce Type: cross 
Abstract: While most reinforcement learning methods today flatten the distribution of future returns to a single scalar value, distributional RL methods exploit the return distribution to provide stronger learning signals and to enable applications in exploration and safe RL. While the predominant method for estimating the return distribution is by modeling it as a categorical distribution over discrete bins or estimating a finite number of quantiles, such approaches leave unanswered questions about the fine-grained structure of the return distribution and about how to distinguish states with high return uncertainty for decision-making. The key idea in this paper is to use modern, flexible flow-based models to estimate the full future return distributions and identify those states with high return variance. We do so by formulating a new flow-matching objective that generates probability density paths satisfying the distributional Bellman equation. Building upon the learned flow models, we estimate the return uncertainty of distinct states using a new flow derivative ODE. We additionally use this uncertainty information to prioritize learning a more accurate return estimation on certain transitions. We compare our method (Value Flows) with prior methods in the offline and online-to-online settings. Experiments on $37$ state-based and $25$ image-based benchmark tasks demonstrate that Value Flows achieves a $1.3\times$ improvement on average in success rates. Website: https://pd-perry.github.io/value-flows Code: https://github.com/chongyi-zheng/value-flows
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2510.07651</link>
<guid>https://arxiv.org/abs/2510.07651</guid>
<content:encoded><![CDATA[
arXiv:2510.07651v1 Announce Type: cross 
Abstract: Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache eviction methods address this by exploiting attention sparsity, yet they typically rank tokens heuristically using accumulated attention weights without considering their true impact on attention outputs. We propose Optimal Brain Cache (OBCache), a principled framework that formulates cache eviction as a layer-wise structured pruning problem. Building upon the Optimal Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the perturbation in attention outputs induced by pruning tokens, with closed-form scores derived for isolated keys, isolated values, and joint key-value pairs. Our scores account not only for attention weights but also for information from value states and attention outputs, thereby enhancing existing eviction strategies with output-aware signals. Experiments on LLaMA and Qwen models demonstrate that replacing the heuristic scores in existing works, which estimate token saliency across different query positions, with OBCache's output-aware scores consistently improves long-context accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKNet: Interpretable Stock Price Prediction via Keyword-Guided Integration of News and Technical Indicators</title>
<link>https://arxiv.org/abs/2510.07661</link>
<guid>https://arxiv.org/abs/2510.07661</guid>
<content:encoded><![CDATA[
arXiv:2510.07661v1 Announce Type: cross 
Abstract: The increasing influence of unstructured external information, such as news articles, on stock prices has attracted growing attention in financial markets. Despite recent advances, most existing newsbased forecasting models represent all articles using sentiment scores or average embeddings that capture the general tone but fail to provide quantitative, context-aware explanations of the impacts of public sentiment on predictions. To address this limitation, we propose an interpretable keyword-guided network (IKNet), which is an explainable forecasting framework that models the semantic association between individual news keywords and stock price movements. The IKNet identifies salient keywords via FinBERTbased contextual analysis, processes each embedding through a separate nonlinear projection layer, and integrates their representations with the time-series data of technical indicators to forecast next-day closing prices. By applying Shapley Additive Explanations the model generates quantifiable and interpretable attributions for the contribution of each keyword to predictions. Empirical evaluations of S&amp;P 500 data from 2015 to 2024 demonstrate that IKNet outperforms baselines, including recurrent neural networks and transformer models, reducing RMSE by up to 32.9% and improving cumulative returns by 18.5%. Moreover, IKNet enhances transparency by offering contextualized explanations of volatility events driven by public sentiment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration</title>
<link>https://arxiv.org/abs/2510.07666</link>
<guid>https://arxiv.org/abs/2510.07666</guid>
<content:encoded><![CDATA[
arXiv:2510.07666v1 Announce Type: cross 
Abstract: Although pyramid networks have demonstrated superior performance in deformable medical image registration, their decoder architectures are inherently prone to propagating and accumulating anatomical structure misalignments. Moreover, most existing models do not adaptively determine the number of iterations for optimization under varying deformation requirements across images, resulting in either premature termination or excessive iterations that degrades registration accuracy. To effectively mitigate the accumulation of anatomical misalignments, we propose the Feature-Enhanced Residual Module (FERM) as the core component of each decoding layer in the pyramid network. FERM comprises three sequential blocks that extract anatomical semantic features, learn to suppress irrelevant features, and estimate the final deformation field, respectively. To adaptively determine the number of iterations for varying images, we propose the dual-stage Threshold-Controlled Iterative (TCI) strategy. In the first stage, TCI assesses registration stability and with asserted stability, it continues with the second stage to evaluate convergence. We coin the model that integrates FERM and TCI as Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP outperforms the state-of-the-art (SOTA) registration networks in terms of accuracy, while maintaining comparable inference speed and a compact model parameter size. Finally, we assess the generalizability of FERM and TCI by integrating them with existing registration networks and further conduct ablation studies to validate the effectiveness of these two proposed methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Video Synthesis via Variational Inference</title>
<link>https://arxiv.org/abs/2510.07670</link>
<guid>https://arxiv.org/abs/2510.07670</guid>
<content:encoded><![CDATA[
arXiv:2510.07670v1 Announce Type: cross 
Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Learning with Synthetic Data for Enhanced Pulmonary Nodule Detection in Chest Radiographs</title>
<link>https://arxiv.org/abs/2510.07681</link>
<guid>https://arxiv.org/abs/2510.07681</guid>
<content:encoded><![CDATA[
arXiv:2510.07681v1 Announce Type: cross 
Abstract: This study evaluates whether integrating curriculum learning with diffusion-based synthetic augmentation can enhance the detection of difficult pulmonary nodules in chest radiographs, particularly those with low size, brightness, and contrast, which often challenge conventional AI models due to data imbalance and limited annotation. A Faster R-CNN with a Feature Pyramid Network (FPN) backbone was trained on a hybrid dataset comprising expert-labeled NODE21 (1,213 patients; 52.4 percent male; mean age 63.2 +/- 11.5 years), VinDr-CXR, CheXpert, and 11,206 DDPM-generated synthetic images. Difficulty scores based on size, brightness, and contrast guided curriculum learning. Performance was compared to a non-curriculum baseline using mean average precision (mAP), Dice score, and area under the curve (AUC). Statistical tests included bootstrapped confidence intervals, DeLong tests, and paired t-tests. The curriculum model achieved a mean AUC of 0.95 versus 0.89 for the baseline (p < 0.001), with improvements in sensitivity (70 percent vs. 48 percent) and accuracy (82 percent vs. 70 percent). Stratified analysis demonstrated consistent gains across all difficulty bins (Easy to Very Hard). Grad-CAM visualizations confirmed more anatomically focused attention under curriculum learning. These results suggest that curriculum-guided synthetic augmentation enhances model robustness and generalization for pulmonary nodule detection.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing Model Specs Reveals Character Differences among Language Models</title>
<link>https://arxiv.org/abs/2510.07686</link>
<guid>https://arxiv.org/abs/2510.07686</guid>
<content:encoded><![CDATA[
arXiv:2510.07686v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly trained from AI constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier LLMs across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs</title>
<link>https://arxiv.org/abs/2510.07697</link>
<guid>https://arxiv.org/abs/2510.07697</guid>
<content:encoded><![CDATA[
arXiv:2510.07697v1 Announce Type: cross 
Abstract: With the rise of advanced reasoning capabilities, large language models (LLMs) are receiving increasing attention. However, although reasoning improves LLMs' performance on downstream tasks, it also introduces new security risks, as adversaries can exploit these capabilities to conduct backdoor attacks. Existing surveys on backdoor attacks and reasoning security offer comprehensive overviews but lack in-depth analysis of backdoor attacks and defenses targeting LLMs' reasoning abilities. In this paper, we take the first step toward providing a comprehensive review of reasoning-based backdoor attacks in LLMs by analyzing their underlying mechanisms, methodological frameworks, and unresolved challenges. Specifically, we introduce a new taxonomy that offers a unified perspective for summarizing existing approaches, categorizing reasoning-based backdoor attacks into associative, passive, and active. We also present defense strategies against such attacks and discuss current challenges alongside potential directions for future research. This work offers a novel perspective, paving the way for further exploration of secure and trustworthy LLM communities.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality Guided Representation Learning for Cross-Style Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.07707</link>
<guid>https://arxiv.org/abs/2510.07707</guid>
<content:encoded><![CDATA[
arXiv:2510.07707v1 Announce Type: cross 
Abstract: The proliferation of online hate speech poses a significant threat to the harmony of the web. While explicit hate is easily recognized through overt slurs, implicit hate speech is often conveyed through sarcasm, irony, stereotypes, or coded language -- making it harder to detect. Existing hate speech detection models, which predominantly rely on surface-level linguistic cues, fail to generalize effectively across diverse stylistic variations. Moreover, hate speech spread on different platforms often targets distinct groups and adopts unique styles, potentially inducing spurious correlations between them and labels, further challenging current detection approaches. Motivated by these observations, we hypothesize that the generation of hate speech can be modeled as a causal graph involving key factors: contextual environment, creator motivation, target, and style. Guided by this graph, we propose CADET, a causal representation learning framework that disentangles hate speech into interpretable latent factors and then controls confounders, thereby isolating genuine hate intent from superficial linguistic cues. Furthermore, CADET allows counterfactual reasoning by intervening on style within the latent space, naturally guiding the model to robustly identify hate speech in varying forms. CADET demonstrates superior performance in comprehensive experiments, highlighting the potential of causal priors in advancing generalizable hate speech detection.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEAS: DEtached value learning with Action Sequence for Scalable Offline RL</title>
<link>https://arxiv.org/abs/2510.07730</link>
<guid>https://arxiv.org/abs/2510.07730</guid>
<content:encoded><![CDATA[
arXiv:2510.07730v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that DEAS consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to enhance the performance of large-scale Vision-Language-Action models that predict action sequences, significantly boosting performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeSH: Memory-as-State-Highways for Recursive Transformers</title>
<link>https://arxiv.org/abs/2510.07739</link>
<guid>https://arxiv.org/abs/2510.07739</guid>
<content:encoded><![CDATA[
arXiv:2510.07739v1 Announce Type: cross 
Abstract: Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth. However, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts. By probing hidden states, we trace this performance gap to two primary bottlenecks: undifferentiated computation, where the core is forced to adopt a similar computational pattern at every iteration, and information overload, where long-lived and transient information must coexist in a single hidden state. To address the issues, we introduce a Memory-as-State-Highways (MeSH) scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations. Probing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160M-1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06% with 33% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppForge: From Assistant to Independent Developer - Are GPTs Ready for Software Development?</title>
<link>https://arxiv.org/abs/2510.07740</link>
<guid>https://arxiv.org/abs/2510.07740</guid>
<content:encoded><![CDATA[
arXiv:2510.07740v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current models' ability to handle complex, multi-component software engineering challenges.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes</title>
<link>https://arxiv.org/abs/2510.07741</link>
<guid>https://arxiv.org/abs/2510.07741</guid>
<content:encoded><![CDATA[
arXiv:2510.07741v1 Announce Type: cross 
Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Test-Time Scaling for Latent Reasoning Models</title>
<link>https://arxiv.org/abs/2510.07745</link>
<guid>https://arxiv.org/abs/2510.07745</guid>
<content:encoded><![CDATA[
arXiv:2510.07745v1 Announce Type: cross 
Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \
This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Multi-Task Learning Framework for Generative Auto-Bidding with Validation-Aligned Optimization</title>
<link>https://arxiv.org/abs/2510.07760</link>
<guid>https://arxiv.org/abs/2510.07760</guid>
<content:encoded><![CDATA[
arXiv:2510.07760v1 Announce Type: cross 
Abstract: In online advertising, heterogeneous advertiser requirements give rise to numerous customized bidding tasks that are typically optimized independently, resulting in extensive computation and limited data efficiency. Multi-task learning offers a principled framework to train these tasks jointly through shared representations. However, existing multi-task optimization strategies are primarily guided by training dynamics and often generalize poorly in volatile bidding environments. To this end, we present Validation-Aligned Multi-task Optimization (VAMO), which adaptively assigns task weights based on the alignment between per-task training gradients and a held-out validation gradient, thereby steering updates toward validation improvement and better matching deployment objectives. We further equip the framework with a periodicity-aware temporal module and couple it with an advanced generative auto-bidding backbone to enhance cross-task transfer of seasonal structure and strengthen bidding performance. Meanwhile, we provide theoretical insights into the proposed method, e.g., convergence guarantee and alignment analysis. Extensive experiments on both simulated and large-scale real-world advertising systems consistently demonstrate significant improvements over typical baselines, illuminating the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.07768</link>
<guid>https://arxiv.org/abs/2510.07768</guid>
<content:encoded><![CDATA[
arXiv:2510.07768v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Conditioned Cross-embodiment Skill Transfer</title>
<link>https://arxiv.org/abs/2510.07773</link>
<guid>https://arxiv.org/abs/2510.07773</guid>
<content:encoded><![CDATA[
arXiv:2510.07773v1 Announce Type: cross 
Abstract: Learning manipulation skills from human demonstration videos presents a promising yet challenging problem, primarily due to the significant embodiment gap between human body and robot manipulators. Existing methods rely on paired datasets or hand-crafted rewards, which limit scalability and generalization. We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment Skill Transfer, enabling robots to acquire manipulation skills directly from human demonstration videos. Our key insight is to represent human motions as sparse optical flow trajectories, which serve as embodiment-agnostic motion cues by removing morphological variations while preserving essential dynamics. Conditioned on these trajectories together with visual and textual inputs, TrajSkill jointly synthesizes temporally consistent robot manipulation videos and translates them into executable actions, thereby achieving cross-embodiment skill transfer. Extensive experiments are conducted, and the results on simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD by 36.6\% compared with the state-of-the-art, and improves cross-embodiment success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation tasks further validate the effectiveness of our approach, demonstrating practical human-to-robot skill transfer across embodiments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drift No More? Context Equilibria in Multi-Turn LLM Interactions</title>
<link>https://arxiv.org/abs/2510.07777</link>
<guid>https://arxiv.org/abs/2510.07777</guid>
<content:encoded><![CDATA[
arXiv:2510.07777v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2510.07778</link>
<guid>https://arxiv.org/abs/2510.07778</guid>
<content:encoded><![CDATA[
arXiv:2510.07778v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\pi_0$, achieving 18\% higher success rates with direct instructions and 28\% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40\% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</title>
<link>https://arxiv.org/abs/2510.07793</link>
<guid>https://arxiv.org/abs/2510.07793</guid>
<content:encoded><![CDATA[
arXiv:2510.07793v1 Announce Type: cross 
Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2510.07794</link>
<guid>https://arxiv.org/abs/2510.07794</guid>
<content:encoded><![CDATA[
arXiv:2510.07794v1 Announce Type: cross 
Abstract: Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2510.07799</link>
<guid>https://arxiv.org/abs/2510.07799</guid>
<content:encoded><![CDATA[
arXiv:2510.07799v1 Announce Type: cross 
Abstract: The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents</title>
<link>https://arxiv.org/abs/2510.07809</link>
<guid>https://arxiv.org/abs/2510.07809</guid>
<content:encoded><![CDATA[
arXiv:2510.07809v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) enable autonomous mobile agents to operate smartphone user interfaces, yet vulnerabilities to UI-level attacks remain critically understudied. Existing research often depends on conspicuous UI overlays, elevated permissions, or impractical threat models, limiting stealth and real-world applicability. In this paper, we present a practical and stealthy one-shot jailbreak attack that leverages in-app prompt injections: malicious applications embed short prompts in UI text that remain inert during human interaction but are revealed when an agent drives the UI via ADB (Android Debug Bridge). Our framework comprises three crucial components: (1) low-privilege perception-chain targeting, which injects payloads into malicious apps as the agent's visual inputs; (2) stealthy user-invisible activation, a touch-based trigger that discriminates agent from human touches using physical touch attributes and exposes the payload only during agent operation; and (3) one-shot prompt efficacy, a heuristic-guided, character-level iterative-deepening search algorithm (HG-IDA*) that performs one-shot, keyword-level detoxification to evade on-device safety filters. We evaluate across multiple LVLM backends, including closed-source services and representative open-source models within three Android applications, and we observe high planning and execution hijack rates in single-shot scenarios (e.g., GPT-4o: 82.5% planning / 75.0% execution). These findings expose a fundamental security vulnerability in current mobile agents with immediate implications for autonomous smartphone operation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMU: Selective Influence Machine Unlearning</title>
<link>https://arxiv.org/abs/2510.07822</link>
<guid>https://arxiv.org/abs/2510.07822</guid>
<content:encoded><![CDATA[
arXiv:2510.07822v1 Announce Type: cross 
Abstract: The undesired memorization of sensitive information by Large Language Models (LLMs) has emphasized the need for safety mechanisms that can regulate model behavior. This has led to the development of machine unlearning techniques that enable models to precisely forget sensitive and unwanted information. For machine unlearning, first-order and second-order optimizer-based methods have shown significant progress in enabling LLMs to forget targeted information. However, in doing so, these approaches often compromise the model's original capabilities, resulting in unlearned models that struggle to retain their prior knowledge and overall utility. To address this, we propose Selective Influence Machine Unlearning (SIMU), a two-step framework that enhances second-order optimizer-based unlearning by selectively updating only the critical neurons responsible for encoding the forget-set. By constraining updates to these targeted neurons, SIMU achieves comparable unlearning efficacy while substantially outperforming current methods in retaining the model's original knowledge.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2510.07829</link>
<guid>https://arxiv.org/abs/2510.07829</guid>
<content:encoded><![CDATA[
arXiv:2510.07829v1 Announce Type: cross 
Abstract: In the Generative Age, the nature of knowledge work is transforming. Traditional models that emphasise the organisation and retrieval of pre-existing information are increasingly inadequate in the face of generative AI (GenAI) systems capable of autonomous content creation. This paper introduces the Knowledge Sculptor (KS), a new professional archetype for Human-GenAI collaboration that transforms raw AI output into trustworthy, actionable knowledge. Grounded in a socio-technical perspective, the KS is conceptualised through a framework of competencies, including architecting a vision, iterative dialogue, information sculpting, and curiosity-driven synthesis. A practice-based vignette illustrates the KS role in action, and in a self-referential approach, the paper itself serves as an artefact of the sculpting process it describes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</title>
<link>https://arxiv.org/abs/2510.07835</link>
<guid>https://arxiv.org/abs/2510.07835</guid>
<content:encoded><![CDATA[
arXiv:2510.07835v1 Announce Type: cross 
Abstract: This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improving LLM Agents at Test-Time</title>
<link>https://arxiv.org/abs/2510.07841</link>
<guid>https://arxiv.org/abs/2510.07841</guid>
<content:encoded><![CDATA[
arXiv:2510.07841v1 Announce Type: cross 
Abstract: One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSwitch: Adaptive Switching Generation for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.07842</link>
<guid>https://arxiv.org/abs/2510.07842</guid>
<content:encoded><![CDATA[
arXiv:2510.07842v1 Announce Type: cross 
Abstract: Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Based Few-Shot Graph-Level Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.07847</link>
<guid>https://arxiv.org/abs/2510.07847</guid>
<content:encoded><![CDATA[
arXiv:2510.07847v1 Announce Type: cross 
Abstract: Graph-level anomaly detection aims to identify anomalous graphs or subgraphs within graph datasets, playing a vital role in various fields such as fraud detection, review classification, and biochemistry. While Graph Neural Networks (GNNs) have made significant progress in this domain, existing methods rely heavily on large amounts of labeled data, which is often unavailable in real-world scenarios. Additionally, few-shot anomaly detection methods based on GNNs are prone to noise interference, resulting in poor embedding quality and reduced model robustness. To address these challenges, we propose a novel meta-learning-based graph-level anomaly detection framework (MA-GAD), incorporating a graph compression module that reduces the graph size, mitigating noise interference while retaining essential node information. We also leverage meta-learning to extract meta-anomaly information from similar networks, enabling the learning of an initialization model that can rapidly adapt to new tasks with limited samples. This improves the anomaly detection performance on target graphs, and a bias network is used to enhance the distinction between anomalous and normal nodes. Our experimental results, based on four real-world biochemical datasets, demonstrate that MA-GAD outperforms existing state-of-the-art methods in graph-level anomaly detection under few-shot conditions. Experiments on both graph anomaly and subgraph anomaly detection tasks validate the framework's effectiveness on real-world datasets.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials</title>
<link>https://arxiv.org/abs/2510.07853</link>
<guid>https://arxiv.org/abs/2510.07853</guid>
<content:encoded><![CDATA[
arXiv:2510.07853v1 Announce Type: cross 
Abstract: High-throughput toxicity testing offers a fast and cost-effective way to test large amounts of compounds. A key component for such systems is the automated evaluation via machine learning models. In this paper, we address critical challenges in this domain and demonstrate how representations learned via self-supervised learning can effectively identify toxicant-induced changes. We provide a proof-of-concept that utilizes the publicly available EmbryoNet dataset, which contains ten zebrafish embryo phenotypes elicited by various chemical compounds targeting different processes in early embryonic development. Our analysis shows that the learned representations using self-supervised learning are suitable for effectively distinguishing between the modes-of-action of different compounds. Finally, we discuss the integration of machine learning models in a physical toxicity testing device in the context of the TOXBOX project.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation</title>
<link>https://arxiv.org/abs/2510.07865</link>
<guid>https://arxiv.org/abs/2510.07865</guid>
<content:encoded><![CDATA[
arXiv:2510.07865v1 Announce Type: cross 
Abstract: The ability to learn multi-modal action distributions is indispensable for robotic manipulation policies to perform precise and robust control. Flow-based generative models have recently emerged as a promising solution to learning distributions of actions, offering one-step action generation and thus achieving much higher sampling efficiency compared to diffusion-based methods. However, existing flow-based policies suffer from representation collapse, the inability to distinguish similar visual representations, leading to failures in precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive Regularization for One-Step Robotic Manipulation), a novel flow matching framework that integrates dispersive regularization into MeanFlow to prevent collapse while maintaining one-step efficiency. DM1 employs multiple dispersive regularization variants across different intermediate embedding layers, encouraging diverse representations across training batches without introducing additional network modules or specialized training procedures. Experiments on RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the Lift task reaching 99% success over 85% of the baseline. Real-robot deployment on a Franka Panda further validates that DM1 transfers effectively from simulation to the physical world. To the best of our knowledge, this is the first work to leverage representation regularization to enable flow-based policies to achieve strong performance in robotic manipulation, establishing a simple yet powerful approach for efficient and robust manipulation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception - Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track</title>
<link>https://arxiv.org/abs/2510.07871</link>
<guid>https://arxiv.org/abs/2510.07871</guid>
<content:encoded><![CDATA[
arXiv:2510.07871v1 Announce Type: cross 
Abstract: In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Weak-to-strong Generalization</title>
<link>https://arxiv.org/abs/2510.07884</link>
<guid>https://arxiv.org/abs/2510.07884</guid>
<content:encoded><![CDATA[
arXiv:2510.07884v1 Announce Type: cross 
Abstract: Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation</title>
<link>https://arxiv.org/abs/2510.07910</link>
<guid>https://arxiv.org/abs/2510.07910</guid>
<content:encoded><![CDATA[
arXiv:2510.07910v1 Announce Type: cross 
Abstract: Drug recommendation is an essential task in machine learning-based clinical decision support systems. However, the risk of drug-drug interactions (DDI) between co-prescribed medications remains a significant challenge. Previous studies have used graph neural networks (GNNs) to represent drug structures. Regardless, their simplified discrete forms cannot fully capture the molecular binding affinity and reactivity. Therefore, we propose Multimodal DDI Prediction with Molecular Electron Localization Function (ELF) Maps (MMM), a novel framework that integrates three-dimensional (3D) quantum-chemical information into drug representation learning. It generates 3D electron density maps using the ELF. To capture both therapeutic relevance and interaction risks, MMM combines ELF-derived features that encode global electronic properties with a bipartite graph encoder that models local substructure interactions. This design enables learning complementary characteristics of drug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442 substructures), comparing it with several baseline models. In particular, a comparison with the GNN-based SafeDrug model demonstrates statistically significant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112), and the DDI rate (p = 0.0386). These results demonstrate the potential of ELF-based 3D representations to enhance prediction accuracy and support safer combinatorial drug prescribing in clinical practice.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation</title>
<link>https://arxiv.org/abs/2510.07912</link>
<guid>https://arxiv.org/abs/2510.07912</guid>
<content:encoded><![CDATA[
arXiv:2510.07912v1 Announce Type: cross 
Abstract: Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2510.07923</link>
<guid>https://arxiv.org/abs/2510.07923</guid>
<content:encoded><![CDATA[
arXiv:2510.07923v1 Announce Type: cross 
Abstract: Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</title>
<link>https://arxiv.org/abs/2510.07940</link>
<guid>https://arxiv.org/abs/2510.07940</guid>
<content:encoded><![CDATA[
arXiv:2510.07940v1 Announce Type: cross 
Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-scale Dataset for Robust Complex Anime Scene Text Detection</title>
<link>https://arxiv.org/abs/2510.07951</link>
<guid>https://arxiv.org/abs/2510.07951</guid>
<content:encoded><![CDATA[
arXiv:2510.07951v1 Announce Type: cross 
Abstract: Current text detection datasets primarily target natural or document scenes, where text typically appear in regular font and shapes, monotonous colors, and orderly layouts. The text usually arranged along straight or curved lines. However, these characteristics differ significantly from anime scenes, where text is often diverse in style, irregularly arranged, and easily confused with complex visual elements such as symbols and decorative patterns. Text in anime scene also includes a large number of handwritten and stylized fonts. Motivated by this gap, we introduce AnimeText, a large-scale dataset containing 735K images and 4.2M annotated text blocks. It features hierarchical annotations and hard negative samples tailored for anime scenarios. %Cross-dataset evaluations using state-of-the-art methods demonstrate that models trained on AnimeText achieve superior performance in anime text detection tasks compared to existing datasets. To evaluate the robustness of AnimeText in complex anime scenes, we conducted cross-dataset benchmarking using state-of-the-art text detection methods. Experimental results demonstrate that models trained on AnimeText outperform those trained on existing datasets in anime scene text detection tasks. AnimeText on HuggingFace: https://huggingface.co/datasets/deepghs/AnimeText
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07958</link>
<guid>https://arxiv.org/abs/2510.07958</guid>
<content:encoded><![CDATA[
arXiv:2510.07958v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO: Diversifying Sample Condensation for Efficient Model Evaluation</title>
<link>https://arxiv.org/abs/2510.07959</link>
<guid>https://arxiv.org/abs/2510.07959</guid>
<content:encoded><![CDATA[
arXiv:2510.07959v1 Announce Type: cross 
Abstract: Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\textit{maximise diversity in model responses}$. Our method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG</title>
<link>https://arxiv.org/abs/2510.07960</link>
<guid>https://arxiv.org/abs/2510.07960</guid>
<content:encoded><![CDATA[
arXiv:2510.07960v1 Announce Type: cross 
Abstract: Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?</title>
<link>https://arxiv.org/abs/2510.07962</link>
<guid>https://arxiv.org/abs/2510.07962</guid>
<content:encoded><![CDATA[
arXiv:2510.07962v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning</title>
<link>https://arxiv.org/abs/2510.07974</link>
<guid>https://arxiv.org/abs/2510.07974</guid>
<content:encoded><![CDATA[
arXiv:2510.07974v1 Announce Type: cross 
Abstract: While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation</title>
<link>https://arxiv.org/abs/2510.07975</link>
<guid>https://arxiv.org/abs/2510.07975</guid>
<content:encoded><![CDATA[
arXiv:2510.07975v1 Announce Type: cross 
Abstract: Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training</title>
<link>https://arxiv.org/abs/2510.07980</link>
<guid>https://arxiv.org/abs/2510.07980</guid>
<content:encoded><![CDATA[
arXiv:2510.07980v1 Announce Type: cross 
Abstract: Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroCard: Cardinality Estimation with Zero Dependence on Target Databases -- No Data, No Query, No Retraining</title>
<link>https://arxiv.org/abs/2510.07983</link>
<guid>https://arxiv.org/abs/2510.07983</guid>
<content:encoded><![CDATA[
arXiv:2510.07983v1 Announce Type: cross 
Abstract: Cardinality estimation is a fundamental task in database systems and plays a critical role in query optimization. Despite significant advances in learning-based cardinality estimation methods, most existing approaches remain difficult to generalize to new datasets due to their strong dependence on raw data or queries, thus limiting their practicality in real scenarios. To overcome these challenges, we argue that semantics in the schema may benefit cardinality estimation, and leveraging such semantics may alleviate these dependencies. To this end, we introduce ZeroCard, the first semantics-driven cardinality estimation method that can be applied without any dependence on raw data access, query logs, or retraining on the target database. Specifically, we propose to predict data distributions using schema semantics, thereby avoiding raw data dependence. Then, we introduce a query template-agnostic representation method to alleviate query dependence. Finally, we construct a large-scale query dataset derived from real-world tables and pretrain ZeroCard on it, enabling it to learn cardinality from schema semantics and predicate representations. After pretraining, ZeroCard's parameters can be frozen and applied in an off-the-shelf manner. We conduct extensive experiments to demonstrate the distinct advantages of ZeroCard and show its practical applications in query optimization. Its zero-dependence property significantly facilitates deployment in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN</title>
<link>https://arxiv.org/abs/2510.07984</link>
<guid>https://arxiv.org/abs/2510.07984</guid>
<content:encoded><![CDATA[
arXiv:2510.07984v1 Announce Type: cross 
Abstract: The simultaneous restoration of high-frequency details and suppression of severe noise in low-light imagery presents a significant and persistent challenge in computer vision. While large-scale Transformer models like SwinIR have set the state of the art in performance, their high computational cost can be a barrier for practical applications. This paper investigates the critical trade-off between performance and efficiency by comparing the state-of-the-art SwinIR model against a standard, lightweight Convolutional Neural Network (CNN) on this challenging task. Our experimental results reveal a nuanced but important finding. While the Transformer-based SwinIR model achieves a higher peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially, the CNN reached this performance after converging in only 10 epochs of training, whereas the more complex SwinIR model required 132 epochs. This efficiency is further underscored by the model's size; the CNN is over 55 times smaller than SwinIR. This work demonstrates that a standard CNN can provide a near state-of-the-art result with significantly lower computational overhead, presenting a compelling case for its use in real-world scenarios where resource constraints are a primary concern.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fewer Weights, More Problems: A Practical Attack on LLM Pruning</title>
<link>https://arxiv.org/abs/2510.07985</link>
<guid>https://arxiv.org/abs/2510.07985</guid>
<content:encoded><![CDATA[
arXiv:2510.07985v1 Announce Type: cross 
Abstract: Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference engines, such as vLLM, enable users to conveniently prune downloaded models before they are deployed. While the utility and efficiency of pruning methods have improved significantly, the security implications of pruning remain underexplored. In this work, for the first time, we show that modern LLM pruning methods can be maliciously exploited. In particular, an adversary can construct a model that appears benign yet, once pruned, exhibits malicious behaviors. Our method is based on the idea that the adversary can compute a proxy metric that estimates how likely each parameter is to be pruned. With this information, the adversary can first inject a malicious behavior into those parameters that are unlikely to be pruned. Then, they can repair the model by using parameters that are likely to be pruned, effectively canceling out the injected behavior in the unpruned model. We demonstrate the severity of our attack through extensive evaluation on five models; after any of the pruning in vLLM are applied (Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious behaviors in a diverse set of attack scenarios (success rates of up to $95.7\%$ for jailbreak, $98.7\%$ for benign instruction refusal, and $99.5\%$ for targeted content injection). Our results reveal a critical deployment-time security gap and underscore the urgent need for stronger security awareness in model compression.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge</title>
<link>https://arxiv.org/abs/2510.07993</link>
<guid>https://arxiv.org/abs/2510.07993</guid>
<content:encoded><![CDATA[
arXiv:2510.07993v1 Announce Type: cross 
Abstract: Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4 reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\% gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2510.08002</link>
<guid>https://arxiv.org/abs/2510.08002</guid>
<content:encoded><![CDATA[
arXiv:2510.08002v1 Announce Type: cross 
Abstract: Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Past, Present, and Future of Bug Tracking in the Generative AI Era</title>
<link>https://arxiv.org/abs/2510.08005</link>
<guid>https://arxiv.org/abs/2510.08005</guid>
<content:encoded><![CDATA[
arXiv:2510.08005v1 Announce Type: cross 
Abstract: Traditional bug tracking systems rely heavily on manual reporting, reproduction, triaging, and resolution, each carried out by different stakeholders such as end users, customer support, developers, and testers. This division of responsibilities requires significant coordination and widens the communication gap between non-technical users and technical teams, slowing the process from bug discovery to resolution. Moreover, current systems are highly asynchronous; users often wait hours or days for a first response, delaying fixes and contributing to frustration. This paper examines the evolution of bug tracking, from early paper-based reporting to today's web-based and SaaS platforms. Building on this trajectory, we propose an AI-powered bug tracking framework that augments existing tools with intelligent, large language model (LLM)-driven automation. Our framework addresses two main challenges: reducing time-to-fix and minimizing human overhead. Users report issues in natural language, while AI agents refine reports, attempt reproduction, and request missing details. Reports are then classified, invalid ones resolved through no-code fixes, and valid ones localized and assigned to developers. LLMs also generate candidate patches, with human oversight ensuring correctness. By integrating automation into each phase, our framework accelerates response times, improves collaboration, and strengthens software maintenance practices for a more efficient, user-centric future.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses</title>
<link>https://arxiv.org/abs/2510.08016</link>
<guid>https://arxiv.org/abs/2510.08016</guid>
<content:encoded><![CDATA[
arXiv:2510.08016v1 Announce Type: cross 
Abstract: Model merging (MM) recently emerged as an effective method for combining large deep learning models. However, it poses significant security risks. Recent research shows that it is highly susceptible to backdoor attacks, which introduce a hidden trigger into a single fine-tuned model instance that allows the adversary to control the output of the final merged model at inference time. In this work, we propose a simple framework for understanding backdoor attacks by treating the attack itself as a task vector. $Backdoor\ Vector\ (BV)$ is calculated as the difference between the weights of a fine-tuned backdoored model and fine-tuned clean model. BVs reveal new insights into attacks understanding and a more effective framework to measure their similarity and transferability. Furthermore, we propose a novel method that enhances backdoor resilience through merging dubbed $Sparse\ Backdoor\ Vector\ (SBV)$ that combines multiple attacks into a single one. We identify the core vulnerability behind backdoor threats in MM: $inherent\ triggers$ that exploit adversarial weaknesses in the base model. To counter this, we propose $Injection\ BV\ Subtraction\ (IBVS)$ - an assumption-free defense against backdoors in MM. Our results show that SBVs surpass prior attacks and is the first method to leverage merging to improve backdoor effectiveness. At the same time, IBVS provides a lightweight, general defense that remains effective even when the backdoor threat is entirely unknown.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset</title>
<link>https://arxiv.org/abs/2510.08022</link>
<guid>https://arxiv.org/abs/2510.08022</guid>
<content:encoded><![CDATA[
arXiv:2510.08022v1 Announce Type: cross 
Abstract: Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments. In this paper, we present FastUMI-100K, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data. Specifically, FastUMI-100K contains over 100K+ demonstration trajectories collected across representative household environments, covering 54 tasks and hundreds of object types. Our dataset integrates multimodal streams, including end-effector states, multi-view wrist-mounted fisheye images and textual annotations. Each trajectory has a length ranging from 120 to 500 frames. Experimental results demonstrate that FastUMI-100K enables high policy success rates across various baseline algorithms, confirming its robustness, adaptability, and real-world applicability for solving complex, dynamic manipulation challenges. The source code and dataset will be released in this link https://github.com/MrKeee/FastUMI-100K.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-derived quantification of hepatic vessel-to-volume ratios in chronic liver disease using a deep learning approach</title>
<link>https://arxiv.org/abs/2510.08039</link>
<guid>https://arxiv.org/abs/2510.08039</guid>
<content:encoded><![CDATA[
arXiv:2510.08039v1 Announce Type: cross 
Abstract: Background: We aimed to quantify hepatic vessel volumes across chronic liver disease stages and healthy controls using deep learning-based magnetic resonance imaging (MRI) analysis, and assess correlations with biomarkers for liver (dys)function and fibrosis/portal hypertension.
  Methods: We assessed retrospectively healthy controls, non-advanced and advanced chronic liver disease (ACLD) patients using a 3D U-Net model for hepatic vessel segmentation on portal venous phase gadoxetic acid-enhanced 3-T MRI. Total (TVVR), hepatic (HVVR), and intrahepatic portal vein-to-volume ratios (PVVR) were compared between groups and correlated with: albumin-bilirubin (ALBI) and model for end-stage liver disease-sodium (MELD-Na) score, and fibrosis/portal hypertension (Fibrosis-4 [FIB-4] score, liver stiffness measurement [LSM], hepatic venous pressure gradient [HVPG], platelet count [PLT], and spleen volume).
  Results: We included 197 subjects, aged 54.9 $\pm$ 13.8 years (mean $\pm$ standard deviation), 111 males (56.3\%): 35 healthy controls, 44 non-ACLD, and 118 ACLD patients. TVVR and HVVR were highest in controls (3.9; 2.1), intermediate in non-ACLD (2.8; 1.7), and lowest in ACLD patients (2.3; 1.0) ($p \leq 0.001$). PVVR was reduced in both non-ACLD and ACLD patients (both 1.2) compared to controls (1.7) ($p \leq 0.001$), but showed no difference between CLD groups ($p = 0.999$). HVVR significantly correlated indirectly with FIB-4, ALBI, MELD-Na, LSM, and spleen volume ($\rho$ ranging from -0.27 to -0.40), and directly with PLT ($\rho = 0.36$). TVVR and PVVR showed similar but weaker correlations.
  Conclusions: Deep learning-based hepatic vessel volumetry demonstrated differences between healthy liver and chronic liver disease stages and shows correlations with established markers of disease severity.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2510.08044</link>
<guid>https://arxiv.org/abs/2510.08044</guid>
<content:encoded><![CDATA[
arXiv:2510.08044v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities, enabling robots to understand natural language instructions and generate high-level plans with appropriate grounding. However, LLM hallucinations present a significant challenge, often leading to overconfident yet potentially misaligned or unsafe plans. While researchers have explored uncertainty estimation to improve the reliability of LLM-based planning, existing studies have not sufficiently differentiated between epistemic and intrinsic uncertainty, limiting the effectiveness of uncertainty esti- mation. In this paper, we present Combined Uncertainty estimation for Reliable Embodied planning (CURE), which decomposes the uncertainty into epistemic and intrinsic uncertainty, each estimated separately. Furthermore, epistemic uncertainty is subdivided into task clarity and task familiarity for more accurate evaluation. The overall uncertainty assessments are obtained using random network distillation and multi-layer perceptron regression heads driven by LLM features. We validated our approach in two distinct experimental settings: kitchen manipulation and tabletop rearrangement experiments. The results show that, compared to existing methods, our approach yields uncertainty estimates that are more closely aligned with the actual execution outcomes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Graph Neural Networks with Readout is Intractable</title>
<link>https://arxiv.org/abs/2510.08045</link>
<guid>https://arxiv.org/abs/2510.08045</guid>
<content:encoded><![CDATA[
arXiv:2510.08045v1 Announce Type: cross 
Abstract: We introduce a logical language for reasoning about quantized aggregate-combine graph neural networks with global readout (ACR-GNNs). We provide a logical characterization and use it to prove that verification tasks for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies that the verification of quantized GNNs is computationally intractable, prompting substantial research efforts toward ensuring the safety of GNN-based systems. We also experimentally demonstrate that quantized ACR-GNN models are lightweight while maintaining good accuracy and generalization capabilities with respect to non-quantized models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance</title>
<link>https://arxiv.org/abs/2510.08048</link>
<guid>https://arxiv.org/abs/2510.08048</guid>
<content:encoded><![CDATA[
arXiv:2510.08048v1 Announce Type: cross 
Abstract: Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</title>
<link>https://arxiv.org/abs/2510.08049</link>
<guid>https://arxiv.org/abs/2510.08049</guid>
<content:encoded><![CDATA[
arXiv:2510.08049v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation</title>
<link>https://arxiv.org/abs/2510.08058</link>
<guid>https://arxiv.org/abs/2510.08058</guid>
<content:encoded><![CDATA[
arXiv:2510.08058v1 Announce Type: cross 
Abstract: With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution-by-design: Ensuring Inference-Time Provenance in Generative Music Systems</title>
<link>https://arxiv.org/abs/2510.08062</link>
<guid>https://arxiv.org/abs/2510.08062</guid>
<content:encoded><![CDATA[
arXiv:2510.08062v1 Announce Type: cross 
Abstract: The rise of AI-generated music is diluting royalty pools and revealing structural flaws in existing remuneration frameworks, challenging the well-established artist compensation systems in the music industry. Existing compensation solutions, such as piecemeal licensing agreements, lack scalability and technical rigour, while current data attribution mechanisms provide only uncertain estimates and are rarely implemented in practice. This paper introduces a framework for a generative music infrastructure centred on direct attribution, transparent royalty distribution, and granular control for artists and rights' holders. We distinguish ontologically between the training set and the inference set, which allows us to propose two complementary forms of attribution: training-time attribution and inference-time attribution. We here favour inference-time attribution, as it enables direct, verifiable compensation whenever an artist's catalogue is used to condition a generated output. Besides, users benefit from the ability to condition generations on specific songs and receive transparent information about attribution and permitted usage. Our approach offers an ethical and practical solution to the pressing need for robust compensation mechanisms in the era of AI-generated music, ensuring that provenance and fairness are embedded at the core of generative systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Multi Agent Bitcoin Trading System</title>
<link>https://arxiv.org/abs/2510.08068</link>
<guid>https://arxiv.org/abs/2510.08068</guid>
<content:encoded><![CDATA[
arXiv:2510.08068v1 Announce Type: cross 
Abstract: This paper presents a Multi Agent Bitcoin Trading system that utilizes Large Lan- guage Models (LLMs) for alpha generation and portfolio management in the cryptocur- rencies market. Unlike equities, cryptocurrencies exhibit extreme volatility and are heavily influenced by rapidly shifting market sentiments and regulatory announcements, making them difficult to model using static regression models or neural networks trained solely on historical data [53]. The proposed framework overcomes this by structuring LLMs into specialised agents for technical analysis, sentiment evaluation, decision-making, and performance reflection. The system improves over time through a novel verbal feedback mechanism where a Reflect agent provides daily and weekly natural-language critiques of trading decisions. These textual evaluations are then injected into future prompts, al- lowing the system to adjust indicator priorities, sentiment weights, and allocation logic without parameter updates or finetuning. Back-testing on Bitcoin price data from July 2024 to April 2025 shows consistent outperformance across market regimes: the Quantita- tive agent delivered over 30% higher returns in bullish phases and 15% overall gains versus buy-and-hold, while the sentiment-driven agent turned sideways markets from a small loss into a gain of over 100%. Adding weekly feedback further improved total performance by 31% and reduced bearish losses by 10%. The results demonstrate that verbal feedback represents a new, scalable, and low-cost method of tuning LLMs for financial goals.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Ensemble Learning Approach for Enhanced IoT Attack Detection: Redefining Security Paradigms in Connected Systems</title>
<link>https://arxiv.org/abs/2510.08084</link>
<guid>https://arxiv.org/abs/2510.08084</guid>
<content:encoded><![CDATA[
arXiv:2510.08084v1 Announce Type: cross 
Abstract: The rapid expansion of Internet of Things (IoT) devices has transformed industries and daily life by enabling widespread connectivity and data exchange. However, this increased interconnection has introduced serious security vulnerabilities, making IoT systems more exposed to sophisticated cyber attacks. This study presents a novel ensemble learning architecture designed to improve IoT attack detection. The proposed approach applies advanced machine learning techniques, specifically the Extra Trees Classifier, along with thorough preprocessing and hyperparameter optimization. It is evaluated on several benchmark datasets including CICIoT2023, IoTID20, BotNeTIoT L01, ToN IoT, N BaIoT, and BoT IoT. The results show excellent performance, achieving high recall, accuracy, and precision with very low error rates. These outcomes demonstrate the model efficiency and superiority compared to existing approaches, providing an effective and scalable method for securing IoT environments. This research establishes a solid foundation for future progress in protecting connected devices from evolving cyber threats.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility</title>
<link>https://arxiv.org/abs/2510.08091</link>
<guid>https://arxiv.org/abs/2510.08091</guid>
<content:encoded><![CDATA[
arXiv:2510.08091v1 Announce Type: cross 
Abstract: We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08098</link>
<guid>https://arxiv.org/abs/2510.08098</guid>
<content:encoded><![CDATA[
arXiv:2510.08098v1 Announce Type: cross 
Abstract: Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Vocabulary Reduction for Auto-Regressive Language Models</title>
<link>https://arxiv.org/abs/2510.08102</link>
<guid>https://arxiv.org/abs/2510.08102</guid>
<content:encoded><![CDATA[
arXiv:2510.08102v1 Announce Type: cross 
Abstract: Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Mental Models in Human-AI Collaboration: A Conceptual Framework</title>
<link>https://arxiv.org/abs/2510.08104</link>
<guid>https://arxiv.org/abs/2510.08104</guid>
<content:encoded><![CDATA[
arXiv:2510.08104v1 Announce Type: cross 
Abstract: Artificial intelligence has become integral to organizational decision-making and while research has explored many facets of this human-AI collaboration, the focus has mainly been on designing the AI agent(s) and the way the collaboration is set up - generally assuming a human decision-maker to be "fixed". However, it has largely been neglected that decision-makers' mental models evolve through their continuous interaction with AI systems. This paper addresses this gap by conceptualizing how the design of human-AI collaboration influences the development of three complementary and interdependent mental models necessary for this collaboration. We develop an integrated socio-technical framework that identifies the mechanisms driving the mental model evolution: data contextualization, reasoning transparency, and performance feedback. Our work advances human-AI collaboration literature through three key contributions: introducing three distinct mental models (domain, information processing, complementarity-awareness); recognizing the dynamic nature of mental models; and establishing mechanisms that guide the purposeful design of effective human-AI collaboration.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents</title>
<link>https://arxiv.org/abs/2510.08109</link>
<guid>https://arxiv.org/abs/2510.08109</guid>
<content:encoded><![CDATA[
arXiv:2510.08109v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Decision Making around Experts</title>
<link>https://arxiv.org/abs/2510.08113</link>
<guid>https://arxiv.org/abs/2510.08113</guid>
<content:encoded><![CDATA[
arXiv:2510.08113v1 Announce Type: cross 
Abstract: Complex learning agents are increasingly deployed alongside existing experts, such as human operators or previously trained agents. However, it remains unclear how should learners optimally incorporate certain forms of expert data, which may differ in structure from the learner's own action-outcome experiences. We study this problem in the context of Bayesian multi-armed bandits, considering: (i) offline settings, where the learner receives a dataset of outcomes from the expert's optimal policy before interaction, and (ii) simultaneous settings, where the learner must choose at each step whether to update its beliefs based on its own experience, or based on the outcome simultaneously achieved by an expert. We formalize how expert data influences the learner's posterior, and prove that pretraining on expert outcomes tightens information-theoretic regret bounds by the mutual information between the expert data and the optimal action. For the simultaneous setting, we propose an information-directed rule where the learner processes the data source that maximizes their one-step information gain about the optimal action. Finally, we propose strategies for how the learner can infer when to trust the expert and when not to, safeguarding the learner for the cases where the expert is ineffective or compromised. By quantifying the value of expert data, our framework provides practical, information-theoretic algorithms for agents to intelligently decide when to learn from others.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations</title>
<link>https://arxiv.org/abs/2510.08120</link>
<guid>https://arxiv.org/abs/2510.08120</guid>
<content:encoded><![CDATA[
arXiv:2510.08120v1 Announce Type: cross 
Abstract: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Domain Unlearning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.08132</link>
<guid>https://arxiv.org/abs/2510.08132</guid>
<content:encoded><![CDATA[
arXiv:2510.08132v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: https://kodaikawamura.github.io/Domain_Unlearning/.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement</title>
<link>https://arxiv.org/abs/2510.08138</link>
<guid>https://arxiv.org/abs/2510.08138</guid>
<content:encoded><![CDATA[
arXiv:2510.08138v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate self-contradictory outputs, which severely impacts their reliability and hinders their adoption in practical applications. In video-language models (Video-LLMs), this phenomenon recently draws the attention of researchers. Specifically, these models fail to provide logically consistent responses to rephrased questions based on their grounding outputs. However, the underlying causes of this phenomenon remain underexplored. In this work, we adopt an interpretability-driven approach to analyze, statistically summarize, and intervention the potential factors of the phenomenon. We find that one of the primary reasons for the inconsistency in responses lies in the inability of cross-modal attention heads to effectively distinguish video tokens across different timestamps. To address this, we propose an attention enhancement method called Temporally Conditioned Attention Sharpening (TCAS), which constructs an enhancement objective based on attention distinctions to enhance the model's temporal resolution capability, thereby improving its temporal understanding logic consistency. Experimental results demonstrate that our method significantly enhances the temporal logic consistency of Video-LLMs. Further interpretability analyses reveal that our method indeed improves the temporal discriminability of attention heads, validating our conclusions. Additionally, our method achieves performance improvements in general video temporal grounding tasks, highlighting that temporal logic consistency is a bottleneck in temporal understanding. By enhancing consistency, our method drives significant progress in video temporal understanding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.08146</link>
<guid>https://arxiv.org/abs/2510.08146</guid>
<content:encoded><![CDATA[
arXiv:2510.08146v1 Announce Type: cross 
Abstract: We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents</title>
<link>https://arxiv.org/abs/2510.08149</link>
<guid>https://arxiv.org/abs/2510.08149</guid>
<content:encoded><![CDATA[
arXiv:2510.08149v1 Announce Type: cross 
Abstract: The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations</title>
<link>https://arxiv.org/abs/2510.08152</link>
<guid>https://arxiv.org/abs/2510.08152</guid>
<content:encoded><![CDATA[
arXiv:2510.08152v1 Announce Type: cross 
Abstract: The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Agents for Algorithmic Discovery</title>
<link>https://arxiv.org/abs/2510.08159</link>
<guid>https://arxiv.org/abs/2510.08159</guid>
<content:encoded><![CDATA[
arXiv:2510.08159v1 Announce Type: cross 
Abstract: We introduce quantum agents trained by episodic, reward-based reinforcement learning to autonomously rediscover several seminal quantum algorithms and protocols. In particular, our agents learn: efficient logarithmic-depth quantum circuits for the Quantum Fourier Transform; Grover's search algorithm; optimal cheating strategies for strong coin flipping; and optimal winning strategies for the CHSH and other nonlocal games. The agents achieve these results directly through interaction, without prior access to known optimal solutions. This demonstrates the potential of quantum intelligence as a tool for algorithmic discovery, opening the way for the automated design of novel quantum algorithms and protocols.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions</title>
<link>https://arxiv.org/abs/2510.08173</link>
<guid>https://arxiv.org/abs/2510.08173</guid>
<content:encoded><![CDATA[
arXiv:2510.08173v1 Announce Type: cross 
Abstract: Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Whisper Embeddings for Audio-based Lyrics Matching</title>
<link>https://arxiv.org/abs/2510.08176</link>
<guid>https://arxiv.org/abs/2510.08176</guid>
<content:encoded><![CDATA[
arXiv:2510.08176v1 Announce Type: cross 
Abstract: Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Canonicalization through Bootstrapped Data Re-Alignment</title>
<link>https://arxiv.org/abs/2510.08178</link>
<guid>https://arxiv.org/abs/2510.08178</guid>
<content:encoded><![CDATA[
arXiv:2510.08178v1 Announce Type: cross 
Abstract: Fine-grained visual classification (FGVC) tasks, such as insect and bird identification, demand sensitivity to subtle visual cues while remaining robust to spatial transformations. A key challenge is handling geometric biases and noise, such as different orientations and scales of objects. Existing remedies rely on heavy data augmentation, which demands powerful models, or on equivariant architectures, which constrain expressivity and add cost. Canonicalization offers an alternative by shielding such biases from the downstream model. In practice, such functions are often obtained using canonicalization priors, which assume aligned training data. Unfortunately, real-world datasets never fulfill this assumption, causing the obtained canonicalizer to be brittle. We propose a bootstrapping algorithm that iteratively re-aligns training samples by progressively reducing variance and recovering the alignment assumption. We establish convergence guarantees under mild conditions for arbitrary compact groups, and show on four FGVC benchmarks that our method consistently outperforms equivariant, and canonicalization baselines while performing on par with augmentation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Matters: An Analysis of 200 Human-SAV Interactions</title>
<link>https://arxiv.org/abs/2510.08202</link>
<guid>https://arxiv.org/abs/2510.08202</guid>
<content:encoded><![CDATA[
arXiv:2510.08202v1 Announce Type: cross 
Abstract: Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Retrieval and Consolidation in Large Language Models through Function Tokens</title>
<link>https://arxiv.org/abs/2510.08203</link>
<guid>https://arxiv.org/abs/2510.08203</guid>
<content:encoded><![CDATA[
arXiv:2510.08203v1 Announce Type: cross 
Abstract: The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</title>
<link>https://arxiv.org/abs/2510.08211</link>
<guid>https://arxiv.org/abs/2510.08211</guid>
<content:encoded><![CDATA[
arXiv:2510.08211v1 Announce Type: cross 
Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuelCast: Benchmarking Tabular and Temporal Models for Ship Fuel Consumption</title>
<link>https://arxiv.org/abs/2510.08217</link>
<guid>https://arxiv.org/abs/2510.08217</guid>
<content:encoded><![CDATA[
arXiv:2510.08217v1 Announce Type: cross 
Abstract: In the shipping industry, fuel consumption and emissions are critical factors due to their significant impact on economic efficiency and environmental sustainability. Accurate prediction of ship fuel consumption is essential for further optimization of maritime operations. However, heterogeneous methodologies and limited high-quality datasets hinder direct comparison of modeling approaches. This paper makes three key contributions: (1) we introduce and release a new dataset (https://huggingface.co/datasets/krohnedigital/FuelCast) comprising operational and environmental data from three ships; (2) we define a standardized benchmark covering tabular regression and time-series regression (3) we investigate the application of in-context learning for ship consumption modeling using the TabPFN foundation model - a first in this domain to our knowledge. Our results demonstrate strong performance across all evaluated models, supporting the feasibility of onboard, data-driven fuel prediction. Models incorporating environmental conditions consistently outperform simple polynomial baselines relying solely on vessel speed. TabPFN slightly outperforms other techniques, highlighting the potential of foundation models with in-context learning capabilities for tabular prediction. Furthermore, including temporal context improves accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive Value Learning for Scalable Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.08218</link>
<guid>https://arxiv.org/abs/2510.08218</guid>
<content:encoded><![CDATA[
arXiv:2510.08218v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make sequences of decisions. However, RL has yet to be fully leveraged in robotics, principally due to its lack of scalability. Offline RL offers a promising avenue by training agents on large, diverse datasets, avoiding the costly real-world interactions of online RL. Scaling offline RL to increasingly complex datasets requires expressive generative models such as diffusion and flow matching. However, existing methods typically depend on either backpropagation through time (BPTT), which is computationally prohibitive, or policy distillation, which introduces compounding errors and limits scalability to larger base policies. In this paper, we consider the question of how to develop a scalable offline RL approach without relying on distillation or backpropagation through time. We introduce Expressive Value Learning for Offline Reinforcement Learning (EVOR): a scalable offline RL approach that integrates both expressive policies and expressive value functions. EVOR learns an optimal, regularized Q-function via flow matching during training. At inference-time, EVOR performs inference-time policy extraction via rejection sampling against the expressive value function, enabling efficient optimization, regularization, and compute-scalable search without retraining. Empirically, we show that EVOR outperforms baselines on a diverse set of offline RL tasks, demonstrating the benefit of integrating expressive value learning into offline RL.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08236</link>
<guid>https://arxiv.org/abs/2510.08236</guid>
<content:encoded><![CDATA[
arXiv:2510.08236v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increas- ingly integral to information dissemination and decision-making processes. Given their grow- ing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propa- gation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the in- herent political leanings of these models. Sub- sequently, persona prompting with the PCT is used to explore explicit stereotypes across vari- ous social dimensions. In a final step, implicit stereotypes are uncovered by evaluating mod- els with multilingual versions of the PCT. Key findings reveal a consistent left-leaning polit- ical alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those iden- tified via explicit persona prompting. Interest- ingly, for most models, implicit and explicit stereotypes show a notable alignment, suggest- ing a degree of transparency or "awareness" regarding their inherent biases. This study un- derscores the complex interplay of political bias and stereotypes in LLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling</title>
<link>https://arxiv.org/abs/2510.08245</link>
<guid>https://arxiv.org/abs/2510.08245</guid>
<content:encoded><![CDATA[
arXiv:2510.08245v1 Announce Type: cross 
Abstract: Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opponent Shaping in LLM Agents</title>
<link>https://arxiv.org/abs/2510.08255</link>
<guid>https://arxiv.org/abs/2510.08255</guid>
<content:encoded><![CDATA[
arXiv:2510.08255v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2510.08256</link>
<guid>https://arxiv.org/abs/2510.08256</guid>
<content:encoded><![CDATA[
arXiv:2510.08256v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Emulation Environment for In-Memory Computing Systems</title>
<link>https://arxiv.org/abs/2510.08257</link>
<guid>https://arxiv.org/abs/2510.08257</guid>
<content:encoded><![CDATA[
arXiv:2510.08257v1 Announce Type: cross 
Abstract: In-memory computing technology is used extensively in artificial intelligence devices due to lower power consumption and fast calculation of matrix-based functions. The development of such a device and its integration in a system takes a significant amount of time and requires the use of a real-time emulation environment, where various system aspects are analyzed, microcode is tested, and applications are deployed, even before the real chip is available. In this work, we present the architecture, the software development tools, and experimental results of a distributed and expandable emulation system for rapid prototyping of integrated circuits based on in-memory computing technologies. Presented experimental results demonstrate the usefulness of the proposed emulator.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Neural Exposure Fields for View Synthesis</title>
<link>https://arxiv.org/abs/2510.08279</link>
<guid>https://arxiv.org/abs/2510.08279</guid>
<content:encoded><![CDATA[
arXiv:2510.08279v1 Announce Type: cross 
Abstract: Recent advances in neural scene representations have led to unprecedented quality in 3D reconstruction and view synthesis. Despite achieving high-quality results for common benchmarks with curated data, outputs often degrade for data that contain per image variations such as strong exposure changes, present, e.g., in most scenes with indoor and outdoor areas or rooms with windows. In this paper, we introduce Neural Exposure Fields (NExF), a novel technique for robustly reconstructing 3D scenes with high quality and 3D-consistent appearance from challenging real-world captures. In the core, we propose to learn a neural field predicting an optimal exposure value per 3D point, enabling us to optimize exposure along with the neural scene representation. While capture devices such as cameras select optimal exposure per image/pixel, we generalize this concept and perform optimization in 3D instead. This enables accurate view synthesis in high dynamic range scenarios, bypassing the need of post-processing steps or multi-exposure captures. Our contributions include a novel neural representation for exposure prediction, a system for joint optimization of the scene representation and the exposure field via a novel neural conditioning mechanism, and demonstrated superior performance on challenging real-world data. We find that our approach trains faster than prior works and produces state-of-the-art results on several benchmarks improving by over 55% over best-performing baselines.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Identifiability via Dynamic Optimal Transport</title>
<link>https://arxiv.org/abs/2510.08294</link>
<guid>https://arxiv.org/abs/2510.08294</guid>
<content:encoded><![CDATA[
arXiv:2510.08294v1 Announce Type: cross 
Abstract: We address the open question of counterfactual identification for high-dimensional multivariate outcomes from observational data. Pearl (2000) argues that counterfactuals must be identifiable (i.e., recoverable from the observed data distribution) to justify causal claims. A recent line of work on counterfactual inference shows promising results but lacks identification, undermining the causal validity of its estimates. To address this, we establish a foundation for multivariate counterfactual identification using continuous-time flows, including non-Markovian settings under standard criteria. We characterise the conditions under which flow matching yields a unique, monotone and rank-preserving counterfactual transport map with tools from dynamic optimal transport, ensuring consistent inference. Building on this, we validate the theory in controlled scenarios with counterfactual ground-truth and demonstrate improvements in axiomatic counterfactual soundness on real images.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterated Agent for Symbolic Regression</title>
<link>https://arxiv.org/abs/2510.08317</link>
<guid>https://arxiv.org/abs/2510.08317</guid>
<content:encoded><![CDATA[
arXiv:2510.08317v1 Announce Type: cross 
Abstract: Symbolic regression (SR), the automated discovery of mathematical expressions from data, is a cornerstone of scientific inquiry. However, it is often hindered by the combinatorial explosion of the search space and a tendency to overfit. Popular methods, rooted in genetic programming, explore this space syntactically, often yielding overly complex, uninterpretable models. This paper introduces IdeaSearchFitter, a framework that employs Large Language Models (LLMs) as semantic operators within an evolutionary search. By generating candidate expressions guided by natural-language rationales, our method biases discovery towards models that are not only accurate but also conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's efficacy across diverse challenges: it achieves competitive, noise-robust performance on the Feynman Symbolic Regression Database (FSReD), outperforming several strong baselines; discovers mechanistically aligned models with good accuracy-complexity trade-offs on real-world data; and derives compact, physically-motivated parametrizations for Parton Distribution Functions in a frontier high-energy physics application. IdeaSearchFitter is a specialized module within our broader iterated agent framework, IdeaSearch, which is publicly available at https://www.ideasearch.cn/.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What's Missing: Attention Dispersion and EMA Stabilization in Length Generalization</title>
<link>https://arxiv.org/abs/2510.08341</link>
<guid>https://arxiv.org/abs/2510.08341</guid>
<content:encoded><![CDATA[
arXiv:2510.08341v1 Announce Type: cross 
Abstract: We study length generalization in transformers through the set complement task, where a model must predict a uniform distribution over tokens absent from an input sequence -- an ability central to board-game style reasoning. Our main theoretical result establishes two statements. First, we prove tight bounds on embedding and value dimensions for single-layer attention-only transformers. Second, we show that if such a model achieves balanced logit displacement at lengths 1 and 2, then it must generalize to longer sequences, though with reduced precision. A mechanistic reading of the proof explains this limitation: as more tokens are attended to, softmax compresses logit displacements, eroding separation between valid and invalid outputs. Training dynamics also suggest a second obstacle: when many next tokens are possible, updates become noisy. We hypothesize that dropout can counteract the first effect and Exponential Moving Average (EMA) the second. We validate these hypotheses through random hyperparameter search on the set complement task, which confirms both mechanisms. We then test OthelloGPT, a GPT-1 style model trained on random Othello moves, and find that EMA again improves length generalization in this more complex setting.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.08350</link>
<guid>https://arxiv.org/abs/2510.08350</guid>
<content:encoded><![CDATA[
arXiv:2510.08350v1 Announce Type: cross 
Abstract: We introduce DeepEN, a deep reinforcement learning (RL) framework for personalized enteral nutrition (EN) in critically ill patients. Trained offline on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates 4-hourly recommendations for caloric, protein, and fluid intake tailored to each patient's evolving physiology. The model integrates a curated, clinically informed state space with a custom reward function that balances short-term physiological and nutrition-related goals with long-term survival outcomes. Using a dueling double deep Q-network with conservative Q-learning regularization, DeepEN learns clinically realistic policies that align with high-value clinician actions while discouraging unsafe deviations. Across various qualitative and quantitative metrics, DeepEN outperforms clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17 percentage-point reduction in estimated mortality (18.8% vs 22.5%) and improvements in key nutritional biomarkers. These findings highlight the potential of safe, data-driven personalization of EN therapy to improve outcomes beyond traditional guideline- or heuristic-based approaches.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</title>
<link>https://arxiv.org/abs/2510.08352</link>
<guid>https://arxiv.org/abs/2510.08352</guid>
<content:encoded><![CDATA[
arXiv:2510.08352v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airy: Reading Robot Intent through Height and Sky</title>
<link>https://arxiv.org/abs/2510.08381</link>
<guid>https://arxiv.org/abs/2510.08381</guid>
<content:encoded><![CDATA[
arXiv:2510.08381v1 Announce Type: cross 
Abstract: As industrial robots move into shared human spaces, their opaque decision making threatens safety, trust, and public oversight. This artwork, Airy, asks whether complex multi agent AI can become intuitively understandable by staging a competition between two reinforcement trained robot arms that snap a bedsheet skyward. Building on three design principles, competition as a clear metric (who lifts higher), embodied familiarity (audiences recognize fabric snapping), and sensor to sense mapping (robot cooperation or rivalry shown through forest and weather projections), the installation gives viewers a visceral way to read machine intent. Observations from five international exhibitions indicate that audiences consistently read the robots' strategies, conflict, and cooperation in real time, with emotional reactions that mirror the system's internal state. The project shows how sensory metaphors can turn a black box into a public interface.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Legend Items on Historical Maps Using GPT-4o with In-Context Learning</title>
<link>https://arxiv.org/abs/2510.08385</link>
<guid>https://arxiv.org/abs/2510.08385</guid>
<content:encoded><![CDATA[
arXiv:2510.08385v1 Announce Type: cross 
Abstract: Historical map legends are critical for interpreting cartographic symbols. However, their inconsistent layouts and unstructured formats make automatic extraction challenging. Prior work focuses primarily on segmentation or general optical character recognition (OCR), with few methods effectively matching legend symbols to their corresponding descriptions in a structured manner. We present a method that combines LayoutLMv3 for layout detection with GPT-4o using in-context learning to detect and link legend items and their descriptions via bounding box predictions. Our experiments show that GPT-4 with structured JSON prompts outperforms the baseline, achieving 88% F-1 and 85% IoU, and reveal how prompt design, example counts, and layout alignment affect performance. This approach supports scalable, layout-aware legend parsing and improves the indexing and searchability of historical maps across various visual styles.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2510.08396</link>
<guid>https://arxiv.org/abs/2510.08396</guid>
<content:encoded><![CDATA[
arXiv:2510.08396v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</title>
<link>https://arxiv.org/abs/2510.08404</link>
<guid>https://arxiv.org/abs/2510.08404</guid>
<content:encoded><![CDATA[
arXiv:2510.08404v1 Announce Type: cross 
Abstract: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors</title>
<link>https://arxiv.org/abs/2510.08413</link>
<guid>https://arxiv.org/abs/2510.08413</guid>
<content:encoded><![CDATA[
arXiv:2510.08413v1 Announce Type: cross 
Abstract: Many prompt engineering techniques have been successful in practice, even when optimizing over a large prompt space with with a small amount of task-specific data. Recent work has partially explained this success by showing generalization bounds which apply PAC-Bayes theory to the discrete prompt space, but they are non-vacuous only in data-rich scenarios. We argue that such widespread success can be more fully explained through more carefully considering data- or distribution-dependent perplexity, which acts as an effective prior and steers the optimization towards prompts that are more ``natural'' for the task at hand. We derive novel generalization bounds that are non-vacuous for data-scarce prompt optimization via more useful priors, formally analyzing how perplexity regularization tightens these bounds by limiting exploration. Empirically, we explore both the bounds' effectiveness and the practical benefits of perplexity regularization in improving prompt generalization.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing</title>
<link>https://arxiv.org/abs/2510.08429</link>
<guid>https://arxiv.org/abs/2510.08429</guid>
<content:encoded><![CDATA[
arXiv:2510.08429v1 Announce Type: cross 
Abstract: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet current quoting practices remain opaque and difficult to audit. We introduce ClauseLens, a clause-grounded reinforcement learning framework that produces transparent, regulation-compliant, and risk-aware treaty quotes.
  ClauseLens models the quoting task as a Risk-Aware Constrained Markov Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from legal and underwriting corpora, embedded into the agent's observations, and used both to constrain feasible actions and to generate clause-grounded natural language justifications.
  Evaluated in a multi-agent treaty simulator calibrated to industry data, ClauseLens reduces solvency violations by 51%, improves tail-risk performance by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded explanations with retrieval precision of 87.4% and recall of 91.1%.
  These findings demonstrate that embedding legal context into both decision and explanation pathways yields interpretable, auditable, and regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and the EU AI Act.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.08439</link>
<guid>https://arxiv.org/abs/2510.08439</guid>
<content:encoded><![CDATA[
arXiv:2510.08439v1 Announce Type: cross 
Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.08442</link>
<guid>https://arxiv.org/abs/2510.08442</guid>
<content:encoded><![CDATA[
arXiv:2510.08442v1 Announce Type: cross 
Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.4x improvement in sample efficiency and can solve tasks that the baseline fails to learn, demonstrated across a suite of manipulation tasks from the ManiSkill3 benchmark, all without modifying the underlying algorithm or hyperparameters.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Series-Symbol Data Generation for Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2510.08445</link>
<guid>https://arxiv.org/abs/2510.08445</guid>
<content:encoded><![CDATA[
arXiv:2510.08445v1 Announce Type: cross 
Abstract: Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as training data scarcity and imbalance continue to hinder their development. Inspired by complex dynamic system theories, we design a series-symbol data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic expressions. To leverage series-symbol data pairs with strong correlations, we develop \texttt{SymTime}, a pre-trained foundation model for enhancing time series representation using symbolic information. \texttt{SymTime} demonstrates competitive performance across five major TSA tasks when fine-tunes with downstream tasks, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of series-symbol data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance. The code is available at https://github.com/wwhenxuan/SymTime.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</title>
<link>https://arxiv.org/abs/2510.08450</link>
<guid>https://arxiv.org/abs/2510.08450</guid>
<content:encoded><![CDATA[
arXiv:2510.08450v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) leverage the graph structure to transmit information between nodes, typically through the message-passing mechanism. While these models have found a wide variety of applications, they are known to suffer from over-squashing, where information from a large receptive field of node representations is collapsed into a single fixed sized vector, resulting in an information bottleneck. In this paper, we re-examine the over-squashing phenomenon through the lens of model storage and retrieval capacity, which we define as the amount of information that can be stored in a node's representation for later use. We study some of the limitations of existing tasks used to measure over-squashing and introduce a new synthetic task to demonstrate that an information bottleneck can saturate this capacity. Furthermore, we adapt ideas from the sequence modeling literature on associative memories, fast weight programmers, and the xLSTM model to develop a novel GNN architecture with improved capacity. We demonstrate strong performance of this architecture both on our capacity synthetic task, as well as a range of real-world graph benchmarks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integral Signatures of Activation Functions: A 9-Dimensional Taxonomy and Stability Theory for Deep Learning</title>
<link>https://arxiv.org/abs/2510.08456</link>
<guid>https://arxiv.org/abs/2510.08456</guid>
<content:encoded><![CDATA[
arXiv:2510.08456v1 Announce Type: cross 
Abstract: Activation functions govern the expressivity and stability of neural networks, yet existing comparisons remain largely heuristic. We propose a rigorous framework for their classification via a nine-dimensional integral signature S_sigma(phi), combining Gaussian propagation statistics (m1, g1, g2, m2, eta), asymptotic slopes (alpha_plus, alpha_minus), and regularity measures (TV(phi'), C(phi)). This taxonomy establishes well-posedness, affine reparameterization laws with bias, and closure under bounded slope variation. Dynamical analysis yields Lyapunov theorems with explicit descent constants and identifies variance stability regions through (m2', g2). From a kernel perspective, we derive dimension-free Hessian bounds and connect smoothness to bounded variation of phi'. Applying the framework, we classify eight standard activations (ReLU, leaky-ReLU, tanh, sigmoid, Swish, GELU, Mish, TeLU), proving sharp distinctions between saturating, linear-growth, and smooth families. Numerical Gauss-Hermite and Monte Carlo validation confirms theoretical predictions. Our framework provides principled design guidance, moving activation choice from trial-and-error to provable stability and kernel conditioning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Platform-Agnostic Modular Architecture for Quantum Benchmarking</title>
<link>https://arxiv.org/abs/2510.08469</link>
<guid>https://arxiv.org/abs/2510.08469</guid>
<content:encoded><![CDATA[
arXiv:2510.08469v1 Announce Type: cross 
Abstract: We present a platform-agnostic modular architecture that addresses the increasingly fragmented landscape of quantum computing benchmarking by decoupling problem generation, circuit execution, and results analysis into independent, interoperable components. Supporting over 20 benchmark variants ranging from simple algorithmic tests like Bernstein-Vazirani to complex Hamiltonian simulation with observable calculations, the system integrates with multiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse workflows. We validate the architecture through successful integration with Sandia's $\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for multi-GPU HPC simulations. Extensibility of the system is demonstrated by implementing dynamic circuit variants of existing benchmarks and a new quantum reinforcement learning benchmark, which become readily available across multiple execution and analysis modes. Our primary contribution is identifying and formalizing modular interfaces that enable interoperability between incompatible benchmarking frameworks, demonstrating that standardized interfaces reduce ecosystem fragmentation while preserving optimization flexibility. This architecture has been developed as a key enhancement to the continually evolving QED-C Application-Oriented Performance Benchmarks for Quantum Computing suite.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepPrune: Parallel Scaling without Inter-trace Redundancy</title>
<link>https://arxiv.org/abs/2510.08483</link>
<guid>https://arxiv.org/abs/2510.08483</guid>
<content:encoded><![CDATA[
arXiv:2510.08483v1 Announce Type: cross 
Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Radiology Report Generation for Traumatic Brain Injuries</title>
<link>https://arxiv.org/abs/2510.08498</link>
<guid>https://arxiv.org/abs/2510.08498</guid>
<content:encoded><![CDATA[
arXiv:2510.08498v1 Announce Type: cross 
Abstract: Traumatic brain injuries present significant diagnostic challenges in emergency medicine, where the timely interpretation of medical images is crucial for patient outcomes. In this paper, we propose a novel AI-based approach for automatic radiology report generation tailored to cranial trauma cases. Our model integrates an AC-BiFPN with a Transformer architecture to capture and process complex medical imaging data such as CT and MRI scans. The AC-BiFPN extracts multi-scale features, enabling the detection of intricate anomalies like intracranial hemorrhages, while the Transformer generates coherent, contextually relevant diagnostic reports by modeling long-range dependencies. We evaluate the performance of our model on the RSNA Intracranial Hemorrhage Detection dataset, where it outperforms traditional CNN-based models in both diagnostic accuracy and report generation. This solution not only supports radiologists in high-pressure environments but also provides a powerful educational tool for trainee physicians, offering real-time feedback and enhancing their learning experience. Our findings demonstrate the potential of combining advanced feature extraction with transformer-based text generation to improve clinical decision-making in the diagnosis of traumatic brain injuries.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.08510</link>
<guid>https://arxiv.org/abs/2510.08510</guid>
<content:encoded><![CDATA[
arXiv:2510.08510v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</title>
<link>https://arxiv.org/abs/2510.08529</link>
<guid>https://arxiv.org/abs/2510.08529</guid>
<content:encoded><![CDATA[
arXiv:2510.08529v1 Announce Type: cross 
Abstract: Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.08531</link>
<guid>https://arxiv.org/abs/2510.08531</guid>
<content:encoded><![CDATA[
arXiv:2510.08531v1 Announce Type: cross 
Abstract: Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kontinuous Kontext: Continuous Strength Control for Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2510.08532</link>
<guid>https://arxiv.org/abs/2510.08532</guid>
<content:encoded><![CDATA[
arXiv:2510.08532v1 Announce Type: cross 
Abstract: Instruction-based image editing offers a powerful and intuitive way to manipulate images through natural language. Yet, relying solely on text instructions limits fine-grained control over the extent of edits. We introduce Kontinuous Kontext, an instruction-driven editing model that provides a new dimension of control over edit strength, enabling users to adjust edits gradually from no change to a fully realized result in a smooth and continuous manner. Kontinuous Kontext extends a state-of-the-art image editing model to accept an additional input, a scalar edit strength which is then paired with the edit instruction, enabling explicit control over the extent of the edit. To inject this scalar information, we train a lightweight projector network that maps the input scalar and the edit instruction to coefficients in the model's modulation space. For training our model, we synthesize a diverse dataset of image-edit-instruction-strength quadruplets using existing generative models, followed by a filtering stage to ensure quality and consistency. Kontinuous Kontext provides a unified approach for fine-grained control over edit strength for instruction driven editing from subtle to strong across diverse operations such as stylization, attribute, material, background, and shape changes, without requiring attribute-specific training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the optimization dynamics of RLVR: Gradient gap and step size thresholds</title>
<link>https://arxiv.org/abs/2510.08539</link>
<guid>https://arxiv.org/abs/2510.08539</guid>
<content:encoded><![CDATA[
arXiv:2510.08539v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple binary feedback to post-train large language models, has shown significant empirical success. However, a principled understanding of why it works has been lacking. This paper builds a theoretical foundation for RLVR by analyzing its training process at both the full-response (trajectory) and token levels. Central to our analysis is a quantity called the Gradient Gap, which formalizes the direction of improvement from low-reward to high-reward regions of the response space. We prove that convergence critically depends on aligning the update direction with this Gradient Gap. Moreover, we derive a sharp step-size threshold based on the magnitude of the Gradient Gap: below it, learning converges, whereas above it, performance collapses. Our theory further predicts how the critical step size must scale with response length and the success rate, thereby explaining why practical heuristics such as length normalization improve stability and showing that, with a fixed learning rate, the success rate can stagnate strictly below $100\%$. We validate these predictions through controlled bandit simulations and LLM experiments, including training Qwen2.5-7B with GRPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoNorms: Benchmarking Cultural Awareness of Video Language Models</title>
<link>https://arxiv.org/abs/2510.08543</link>
<guid>https://arxiv.org/abs/2510.08543</guid>
<content:encoded><![CDATA[
arXiv:2510.08543v1 Announce Type: cross 
Abstract: As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2510.08553</link>
<guid>https://arxiv.org/abs/2510.08553</guid>
<content:encoded><![CDATA[
arXiv:2510.08553v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2510.08559</link>
<guid>https://arxiv.org/abs/2510.08559</guid>
<content:encoded><![CDATA[
arXiv:2510.08559v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</title>
<link>https://arxiv.org/abs/2510.08567</link>
<guid>https://arxiv.org/abs/2510.08567</guid>
<content:encoded><![CDATA[
arXiv:2510.08567v1 Announce Type: cross 
Abstract: Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos</title>
<link>https://arxiv.org/abs/2510.08568</link>
<guid>https://arxiv.org/abs/2510.08568</guid>
<content:encoded><![CDATA[
arXiv:2510.08568v1 Announce Type: cross 
Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation</title>
<link>https://arxiv.org/abs/2510.08569</link>
<guid>https://arxiv.org/abs/2510.08569</guid>
<content:encoded><![CDATA[
arXiv:2510.08569v1 Announce Type: cross 
Abstract: Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation</title>
<link>https://arxiv.org/abs/2510.08572</link>
<guid>https://arxiv.org/abs/2510.08572</guid>
<content:encoded><![CDATA[
arXiv:2510.08572v1 Announce Type: cross 
Abstract: Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Automated Urban Planning: Exploring Algorithmic Approaches with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2304.03892</link>
<guid>https://arxiv.org/abs/2304.03892</guid>
<content:encoded><![CDATA[
arXiv:2304.03892v2 Announce Type: replace 
Abstract: The two fields of urban planning and artificial intelligence (AI) arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we introduce the importance of urban planning from the sustainability, living, economic, disaster, and environmental perspectives. We review the fundamental concepts of urban planning and relate these concepts to crucial open problems of machine learning, including adversarial learning, generative neural networks, deep encoder-decoder networks, conversational AI, and geospatial and temporal machine learning, thereby assaying how AI can contribute to modern urban planning. Thus, a central problem is automated land-use configuration, which is formulated as the generation of land uses and building configuration for a target area from surrounding geospatial, human mobility, social media, environment, and economic activities. Finally, we delineate some implications of AI for urban planning and propose key research areas at the intersection of both topics.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints</title>
<link>https://arxiv.org/abs/2309.15458</link>
<guid>https://arxiv.org/abs/2309.15458</guid>
<content:encoded><![CDATA[
arXiv:2309.15458v4 Announce Type: replace 
Abstract: Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way Intelligibility Protocol</title>
<link>https://arxiv.org/abs/2410.20600</link>
<guid>https://arxiv.org/abs/2410.20600</guid>
<content:encoded><![CDATA[
arXiv:2410.20600v4 Announce Type: replace 
Abstract: Our interest is in the design of software systems involving a human-expert interacting -- using natural language -- with a large language model (LLM) on data analysis tasks. For complex problems, it is possible that LLMs can harness human expertise and creativity to find solutions that were otherwise elusive. On one level, this interaction takes place through multiple turns of prompts from the human and responses from the LLM. Here we investigate a more structured approach based on an abstract protocol described in [3] for interaction between agents. The protocol is motivated by a notion of "two-way intelligibility" and is modelled by a pair of communicating finite-state machines. We provide an implementation of the protocol, and provide empirical evidence of using the implementation to mediate interactions between an LLM and a human-agent in two areas of scientific interest (radiology and drug design). We conduct controlled experiments with a human proxy (a database), and uncontrolled experiments with human subjects. The results provide evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM interaction; and for the utility of two-way intelligibility in the design of human-machine systems. Our code is available at https://github.com/karannb/interact.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Average Controlled and Average Natural Micro Direct Effects in Summary Causal Graphs</title>
<link>https://arxiv.org/abs/2410.23975</link>
<guid>https://arxiv.org/abs/2410.23975</guid>
<content:encoded><![CDATA[
arXiv:2410.23975v2 Announce Type: replace 
Abstract: In this paper, we investigate the identifiability of average controlled direct effects and average natural direct effects in causal systems represented by summary causal graphs, which are abstractions of full causal graphs, often used in dynamic systems where cycles and omitted temporal information complicate causal inference. Unlike in the traditional linear setting, where direct effects are typically easier to identify and estimate, non-parametric direct effects, which are crucial for handling real-world complexities, particularly in epidemiological contexts where relationships between variables (e.g, genetic, environmental, and behavioral factors) are often non-linear, are much harder to define and identify. In particular, we give sufficient conditions for identifying average controlled micro direct effect and average natural micro direct effect from summary causal graphs in the presence of hidden confounding. Furthermore, we show that the conditions given for the average controlled micro direct effect become also necessary in the setting where there is no hidden confounding and where we are only interested in identifiability by adjustment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLM+PDDL Symbolic Plans with Human Objective Specifications through Evolutionary Algorithm Guidance</title>
<link>https://arxiv.org/abs/2412.00300</link>
<guid>https://arxiv.org/abs/2412.00300</guid>
<content:encoded><![CDATA[
arXiv:2412.00300v2 Announce Type: replace 
Abstract: Automated planning using a symbolic planning language, such as PDDL, is a general approach to producing optimal plans to achieve a stated goal. However, creating suitable machine understandable descriptions of the planning domain, problem, and goal requires expertise in the planning language, limiting the utility of these tools for non-expert humans. Recent efforts have explored utilizing a symbolic planner in conjunction with a large language model to generate plans from natural language descriptions given by a non-expert human (LLM+PDDL). Our approach performs initial translation of goal specifications to a set of PDDL goal constraints using an LLM; such translations often result in imprecise symbolic specifications, which are difficult to validate directly. We account for this using an evolutionary approach to generate a population of symbolic goal specifications with slight differences from the initial translation, and utilize a trained LSTM-based validation model to assess whether each induced plan in the population adheres to the natural language specifications. We evaluate our approach on a collection of prototypical specifications in a notional naval disaster recovery task, and demonstrate that our evolutionary approach improve adherence of generated plans to natural language specifications when compared to plans generated using only LLM translations. The code for our method can be found at https://github.com/owenonline/PlanCritic.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving</title>
<link>https://arxiv.org/abs/2502.03438</link>
<guid>https://arxiv.org/abs/2502.03438</guid>
<content:encoded><![CDATA[
arXiv:2502.03438v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of $72.95\%$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. To facilitate further research and development in this area, we have open-sourced our model at https://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents</title>
<link>https://arxiv.org/abs/2502.05957</link>
<guid>https://arxiv.org/abs/2502.05957</guid>
<content:encoded><![CDATA[
arXiv:2502.05957v3 Announce Type: replace 
Abstract: Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires</title>
<link>https://arxiv.org/abs/2503.00566</link>
<guid>https://arxiv.org/abs/2503.00566</guid>
<content:encoded><![CDATA[
arXiv:2503.00566v4 Announce Type: replace 
Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Towards Open Complex Human-AI Agents Collaboration Systems for Problem Solving and Knowledge Management</title>
<link>https://arxiv.org/abs/2505.00018</link>
<guid>https://arxiv.org/abs/2505.00018</guid>
<content:encoded><![CDATA[
arXiv:2505.00018v2 Announce Type: replace 
Abstract: We propose a technology-agnostic, collaboration-ready stance for Human-AI Agents Collaboration Systems (HAACS) that closes long-standing gaps in prior stages (automation; flexible autonomy; agentic multi-agent collectives). Reading empirical patterns through a seven-dimension collaboration spine and human-agent contrasts, we identify missing pieces: principled budgeting of initiative, instantaneous and auditable reconfiguration, a system-wide knowledge backbone with an epistemic promotion gate, capacity-aware human interfaces; and, as a prerequisite to all of the above, unified definitions of agent and formal collaborative dynamics. We respond with (i) a boundary-centric ontology of agenthood synthesized with cybernetics; (ii) a Petri net family (colored and interpreted) that models ownership, cross-boundary interaction, concurrency, guards, and rates with collaboration transitions; and (iii) a three-level orchestration (meta, agent, execution) that governs behavior families via guard flips. On the knowledge side, we ground collaborative learning in Conversation Theory and SECI with teach-back gates and an evolving backbone; on the problem-solving side, we coordinate routine MEA-style control with practice-guided open-ended discovery. The result is the Hierarchical Exploration-Exploitation Net (HE2-Net): a policy-controlled stance that splits provisional from validated assets, promotes only after tests and peer checks, and budgets concurrent probing while keeping reuse fast and safe. We show interoperability with emerging agent protocols without ad hoc glue and sketch bio-cybernetic extensions (autopoiesis, autogenesis, evolving boundaries, synergetics, etc). Altogether, the framework keeps humans central to setting aims, justifying knowledge, and steering theory-practice dynamics, while scaling agents as reliable collaborators within audited governance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI Research Assistants with Expert-Involved Learning</title>
<link>https://arxiv.org/abs/2505.04638</link>
<guid>https://arxiv.org/abs/2505.04638</guid>
<content:encoded><![CDATA[
arXiv:2505.04638v2 Announce Type: replace 
Abstract: Large language models (LLMs) and large multimodal models (LMMs) promise to accelerate biomedical discovery, yet their reliability remains unclear. We introduce ARIEL (AI Research Assistant for Expert-in-the-Loop Learning), an open-source evaluation and optimization framework that pairs a curated multimodal biomedical corpus with expert-vetted tasks to probe two capabilities: full-length article summarization and fine-grained figure interpretation. Using uniform protocols and blinded PhD-level evaluation, we find that state-of-the-art models generate fluent but incomplete summaries, whereas LMMs struggle with detailed visual reasoning. We later observe that prompt engineering and lightweight fine-tuning substantially improve textual coverage, and a compute-scaled inference strategy enhances visual question answering. We build an ARIEL agent that integrates textual and visual cues, and we show it can propose testable mechanistic hypotheses. ARIEL delineates current strengths and limitations of foundation models, and provides a reproducible platform for advancing trustworthy AI in biomedicine.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
<link>https://arxiv.org/abs/2505.21671</link>
<guid>https://arxiv.org/abs/2505.21671</guid>
<content:encoded><![CDATA[
arXiv:2505.21671v2 Announce Type: replace 
Abstract: We study a sequential decision-making problem on a $n$-node graph $\mathcal{G}$ where each node has an unknown label from a finite set $\mathbf{\Omega}$, drawn from a joint distribution $\mathcal{P}$ that is Markov with respect to $\mathcal{G}$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $\mathcal{G}$ is a forest. Our implementation runs in $\mathcal{O}(n^2 \cdot |\mathbf{\Omega}|^2)$ time while using $\mathcal{O}(n \cdot |\mathbf{\Omega}|^2)$ oracle calls to $\mathcal{P}$ and $\mathcal{O}(n^2 \cdot |\mathbf{\Omega}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability</title>
<link>https://arxiv.org/abs/2505.23703</link>
<guid>https://arxiv.org/abs/2505.23703</guid>
<content:encoded><![CDATA[
arXiv:2505.23703v3 Announce Type: replace 
Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the NL-FL Problem Alignment method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the Mixed Problem Input technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based Answer Extraction mechanism. Comprehensive experiments demonstrate that the NFL-HR framework achieves **89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by **4.60%** and **4.82%**, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
arXiv:2507.03336v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Urban Planing AI Agent in the Age of Agentic AI</title>
<link>https://arxiv.org/abs/2507.14730</link>
<guid>https://arxiv.org/abs/2507.14730</guid>
<content:encoded><![CDATA[
arXiv:2507.14730v4 Announce Type: replace 
Abstract: Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Cognitive Overhead in Tool Use via Multi-Small-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.08882</link>
<guid>https://arxiv.org/abs/2508.08882</guid>
<content:encoded><![CDATA[
arXiv:2508.08882v3 Announce Type: replace 
Abstract: Recent advances in multi-agent systems highlight the potential of specialized small agents that collaborate via division of labor. Existing tool-integrated reasoning systems, however, often follow a single-agent paradigm in which one large model interleaves long-horizon reasoning with precise tool operations, leading to cognitive-load interference and unstable coordination. We present MSARL, a Multi-Small-Agent Reinforcement Learning framework that explicitly decouples reasoning from tool use. In MSARL, a Reasoning Agent decomposes problems and plans tool invocations, while multiple Tool Agents specialize in specific external tools, each trained via a combination of imitation learning and reinforcement learning with role-specific rewards. On mathematical problem solving with code execution, MSARL significantly improves reasoning stability and final-answer accuracy over single-agent baselines. Moreover, the architecture generalizes to diverse tool-use tasks, demonstrating that cognitive-role decoupling with small agents is a scalable blueprint for multi-agent AI design.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Memory Systems for Enhancing the Long-term Memory of Agent</title>
<link>https://arxiv.org/abs/2508.15294</link>
<guid>https://arxiv.org/abs/2508.15294</guid>
<content:encoded><![CDATA[
arXiv:2508.15294v2 Announce Type: replace 
Abstract: An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers</title>
<link>https://arxiv.org/abs/2509.06493</link>
<guid>https://arxiv.org/abs/2509.06493</guid>
<content:encoded><![CDATA[
arXiv:2509.06493v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics</title>
<link>https://arxiv.org/abs/2509.11943</link>
<guid>https://arxiv.org/abs/2509.11943</guid>
<content:encoded><![CDATA[
arXiv:2509.11943v2 Announce Type: replace 
Abstract: The development of intelligent agents, particularly those powered by language models (LMs), has shown the critical role in various environments that require intelligent and autonomous decision. Environments are not passive testing grounds and they represent the data required for agents to learn and exhibit very challenging conditions that require adaptive, complex and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \emph{possibility} and \emph{necessity} using the formal language of modal logic. In this work, we use of immutable, domain-specific knowledge to make infere information, which is encoded as logical constraints essential for proper diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents</title>
<link>https://arxiv.org/abs/2509.23045</link>
<guid>https://arxiv.org/abs/2509.23045</guid>
<content:encoded><![CDATA[
arXiv:2509.23045v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</title>
<link>https://arxiv.org/abs/2509.23234</link>
<guid>https://arxiv.org/abs/2509.23234</guid>
<content:encoded><![CDATA[
arXiv:2509.23234v3 Announce Type: replace 
Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG</title>
<link>https://arxiv.org/abs/2509.24761</link>
<guid>https://arxiv.org/abs/2509.24761</guid>
<content:encoded><![CDATA[
arXiv:2509.24761v2 Announce Type: replace 
Abstract: Decoding visual neural representations from Electroencephalography (EEG) signals remains a formidable challenge due to their high-dimensional, noisy, and non-Euclidean nature. In this work, we propose a Spatial-Functional Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG) framework to enhance EEG-based visual decoding. Specifically, we introduce the EEG Graph Transformer (EGT), a novel graph-based neural architecture that simultaneously encodes spatial brain connectivity and temporal neural dynamics. To mitigate high intra-subject variability, we propose Graph Archetype Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes to improve feature consistency and class separability. Furthermore, we conduct comprehensive subject-dependent and subject-independent evaluations on the Things-EEG dataset, demonstrating that our approach significantly outperforms prior state-of-the-art EEG decoding methods.The results underscore the transformative potential of integrating graph-based learning with contrastive objectives to enhance EEG-based brain decoding, paving the way for more generalizable and robust neural representations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bloated Disclosures: Can ChatGPT Help Investors Process Information?</title>
<link>https://arxiv.org/abs/2306.10224</link>
<guid>https://arxiv.org/abs/2306.10224</guid>
<content:encoded><![CDATA[
arXiv:2306.10224v5 Announce Type: replace-cross 
Abstract: Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are remarkably shorter compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). Importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information ``bloat." We show that bloated disclosure is associated with adverse capital market consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance. Collectively, our results indicate that generative AI adds considerable value for investors with information processing constraints.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Difference Predictive Coding</title>
<link>https://arxiv.org/abs/2310.20141</link>
<guid>https://arxiv.org/abs/2310.20141</guid>
<content:encoded><![CDATA[
arXiv:2310.20141v3 Announce Type: replace-cross 
Abstract: Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \times$ more sample efficient than the successor representation and $1500 \times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO</title>
<link>https://arxiv.org/abs/2311.01057</link>
<guid>https://arxiv.org/abs/2311.01057</guid>
<content:encoded><![CDATA[
arXiv:2311.01057v3 Announce Type: replace-cross 
Abstract: Smart glasses are rapidly gaining advanced functions thanks to cutting-edge computing technologies, especially accelerated hardware architectures, and tiny Artificial Intelligence (AI) algorithms. However, integrating AI into smart glasses featuring a small form factor and limited battery capacity remains challenging for a satisfactory user experience. To this end, this paper proposes the design of a smart glasses platform for always-on on-device object detection with an all-day battery lifetime. The proposed platform is based on GAP9, a novel multi-core RISC-V processor from Greenwaves Technologies. Additionally, a family of sub-million parameter TinyissimoYOLO networks are proposed. They are benchmarked on established datasets, capable of differentiating up to 80 classes on MS-COCO. Evaluations on the smart glasses prototype demonstrate TinyissimoYOLO's inference latency of only 17ms and consuming 1.59mJ energy per inference. An end-to-end latency of 56ms is achieved which is equivalent to 18 frames per seconds (FPS) with a total power consumption of 62.9mW. This ensures continuous system runtime of up to 9.3 hours on a 154mAh battery. These results outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image classification) at just 7.3 FPS, while the 18 FPS achieved in this paper even include image-capturing, network inference, and detection post-processing. The algorithm's code is released open with this paper and can be found here: https://github.com/ETH-PBL/TinyissimoYOLO
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousands of AI Authors on the Future of AI</title>
<link>https://arxiv.org/abs/2401.02843</link>
<guid>https://arxiv.org/abs/2401.02843</guid>
<content:encoded><![CDATA[
arXiv:2401.02843v3 Announce Type: replace-cross 
Abstract: In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
  Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depression Detection on Social Media with Large Language Models</title>
<link>https://arxiv.org/abs/2403.10750</link>
<guid>https://arxiv.org/abs/2403.10750</guid>
<content:encoded><![CDATA[
arXiv:2403.10750v2 Announce Type: replace-cross 
Abstract: Limited access to mental healthcare resources hinders timely depression diagnosis, leading to detrimental outcomes. Social media platforms present a valuable data source for early detection, yet this task faces two significant challenges: 1) the need for medical knowledge to distinguish clinical depression from transient mood changes, and 2) the dual requirement for high accuracy and model explainability. To address this, we propose DORIS, a framework that leverages Large Language Models (LLMs). To integrate medical knowledge, DORIS utilizes LLMs to annotate user texts against established medical diagnostic criteria and to summarize historical posts into temporal mood courses. These medically-informed features are then used to train an accurate Gradient Boosting Tree (GBT) classifier. Explainability is achieved by generating justifications for predictions based on the LLM-derived symptom annotations and mood course analyses. Extensive experimental results validate the effectiveness as well as interpretability of our method, highlighting its potential as a supportive clinical tool.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Structural Health Monitoring</title>
<link>https://arxiv.org/abs/2404.02944</link>
<guid>https://arxiv.org/abs/2404.02944</guid>
<content:encoded><![CDATA[
arXiv:2404.02944v2 Announce Type: replace-cross 
Abstract: Structural Health Monitoring (SHM) is a critical task for ensuring the safety and reliability of civil infrastructures, typically realized on bridges and viaducts by means of vibration monitoring. In this paper, we propose for the first time the use of Transformer neural networks, with a Masked Auto-Encoder architecture, as Foundation Models for SHM. We demonstrate the ability of these models to learn generalizable representations from multiple large datasets through self-supervised pre-training, which, coupled with task-specific fine-tuning, allows them to outperform state-of-the-art traditional methods on diverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation (TLE). We then extensively explore model size versus accuracy trade-offs and experiment with Knowledge Distillation (KD) to improve the performance of smaller Transformers, enabling their embedding directly into the SHM edge nodes.
  We showcase the effectiveness of our foundation models using data from three operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy with a monitoring time span of just 15 windows. In contrast, a state-of-the-art method based on Principal Component Analysis (PCA) obtains its first good result (95.03% accuracy), only considering 120 windows. On two different TLE tasks, our models obtain state-of-the-art performance on multiple evaluation metrics (R$^2$ score, MAE% and MSE%). On the first benchmark, we achieve an R$^2$ score of 0.97 and 0.90 for light and heavy vehicle traffic, respectively, while the best previous approach (a Random Forest) stops at 0.91 and 0.84. On the second one, we achieve an R$^2$ score of 0.54 versus the 0.51 of the best competitor method, a Long-Short Term Memory network.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning - A Pilot Study</title>
<link>https://arxiv.org/abs/2409.00310</link>
<guid>https://arxiv.org/abs/2409.00310</guid>
<content:encoded><![CDATA[
arXiv:2409.00310v3 Announce Type: replace-cross 
Abstract: Wearable sensors and IoT/IoMT platforms enable continuous, real-time monitoring, but objective digital markers for eating disorders are limited. In this study, we examined whether actimetry and machine learning (ML) could provide objective criteria for food addiction (FA) and symptom counts (SC). In 78 participants (mean age 22.1 +/- 9.5 y; 73.1% women), one week of non-dominant wrist actimetry and psychometric data (YFAS, DEBQ, ZSDS) were collected. The time series were segmented into daytime activity and nighttime rest, and statistical and entropy descriptors (FuzzyEn, DistEn, SVDEn, PermEn, PhaseEn; 256 features) were calculated. The mean Matthews correlation coefficient (MCC) was used as the primary metric in a K-nearest neighbors (KNN) pipeline with five-fold stratified cross-validation (one hundred repetitions; 500 evaluations); SHAP was used to assist in interpretation. For binary FA, activity-segment features performed best (MCC = 0.78 +/- 0.02; Accuracy ~ 95.3% +/- 0.5; Sensitivity ~ 0.77 +/- 0.03; Specificity ~ 0.98 +/- 0.004), exceeding OaS (Objective and Subjective Features) (MCC = 0.69 +/- 0.03) and rest-only (MCC = 0.50 +/- 0.03). For SC (four classes), OaS slightly surpassed actimetry (MCC = 0.40 +/- 0.01 vs 0.38 +/- 0.01; Accuracy ~ 58.1% vs 56.9%). Emotional and restrained eating were correlated with actimetric features. These findings support wrist-worn actimetry as a digital biomarker of FA that complements questionnaires and may facilitate privacy-preserving clinical translation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest Neighbor CCP-Based Molecular Sequence Analysis</title>
<link>https://arxiv.org/abs/2409.04922</link>
<guid>https://arxiv.org/abs/2409.04922</guid>
<content:encoded><![CDATA[
arXiv:2409.04922v2 Announce Type: replace-cross 
Abstract: Molecular sequence analysis is crucial for comprehending several biological processes, including protein-protein interactions, functional annotation, and disease classification. The large number of sequences and the inherently complicated nature of protein structures make it challenging to analyze such data. Finding patterns and enhancing subsequent research requires the use of dimensionality reduction and feature selection approaches. Recently, a method called Correlated Clustering and Projection (CCP) has been proposed as an effective method for biological sequencing data. The CCP technique is still costly to compute even though it is effective for sequence visualization. Furthermore, its utility for classifying molecular sequences is still uncertain. To solve these two problems, we present a Nearest Neighbor Correlated Clustering and Projection (CCP-NN)-based technique for efficiently preprocessing molecular sequence data. To group related molecular sequences and produce representative supersequences, CCP makes use of sequence-to-sequence correlations. As opposed to conventional methods, CCP doesn't rely on matrix diagonalization, therefore it can be applied to a range of machine-learning problems. We estimate the density map and compute the correlation using a nearest-neighbor search technique. We performed molecular sequence classification using CCP and CCP-NN representations to assess the efficacy of our proposed approach. Our findings show that CCP-NN considerably improves classification task accuracy as well as significantly outperforms CCP in terms of computational runtime.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study</title>
<link>https://arxiv.org/abs/2409.13694</link>
<guid>https://arxiv.org/abs/2409.13694</guid>
<content:encoded><![CDATA[
arXiv:2409.13694v4 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) is increasingly recognized as an effective approach to mitigating the hallucination of large language models (LLMs) through the integration of external knowledge. While numerous efforts, most studies focus on a single type of external knowledge source. However, in real-world applications, most situations involve diverse knowledge from various sources, yet this area has been less explored. The main dilemma is the lack of a suitable dataset containing multiple knowledge sources and pre-exploration of the associated issues. To address these challenges, we standardize a benchmark dataset that combines structured and unstructured knowledge across diverse and complementary domains. Based on this dataset, we further develop a plug-and-play RAG framework, \textbf{PruningRAG}, whose main characteristic is the use of multi-granularity pruning strategies to optimize the integration of relevant information while minimizing misleading context. It consistently improves performance across various existing RAG variants, demonstrating its robustness and broad applicability. Building upon the standardized dataset and PruningRAG, we also report a series of experimental results, as well as insightful findings. Our dataset and code are publicly available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of advancing future research in the RAG community.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Embeddings Can Be Sufficient for Bayesian Optimization</title>
<link>https://arxiv.org/abs/2410.10190</link>
<guid>https://arxiv.org/abs/2410.10190</guid>
<content:encoded><![CDATA[
arXiv:2410.10190v3 Announce Type: replace-cross 
Abstract: Bayesian Optimization is ubiquitous in experimental design and black-box optimization for improving search efficiency. However, most existing approaches rely on regression models which are limited to fixed search spaces and structured, tabular input features. This paper explores the use of LLM embeddings over string inputs for in-context regression in Bayesian Optimization. Our results show that representing inputs as strings enables general-purpose regression across diverse domains, including synthetic, combinatorial, and hyperparameter optimization. Furthermore, our approach achieves optimization performance comparable to state-of-the-art Gaussian Process-based methods such as Google Vizier, and demonstrates potential for broader and more flexible applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2410.17933</link>
<guid>https://arxiv.org/abs/2410.17933</guid>
<content:encoded><![CDATA[
arXiv:2410.17933v4 Announce Type: replace-cross 
Abstract: One of the biggest challenges of building artificial intelligence (AI) model in the healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausting, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America, and Asia) without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaptation to meet the privacy and safety requirements of healthcare data, meanwhile, it rewards honest participation and penalizes malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy-preserving. Its prediction accuracy consistently outperforms models trained on limited personal data and achieves comparable or even slightly better results than centralized training in certain scenarios, all while preserving data privacy. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v2 Announce Type: replace-cross 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</title>
<link>https://arxiv.org/abs/2411.02886</link>
<guid>https://arxiv.org/abs/2411.02886</guid>
<content:encoded><![CDATA[
arXiv:2411.02886v4 Announce Type: replace-cross 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation</title>
<link>https://arxiv.org/abs/2411.19528</link>
<guid>https://arxiv.org/abs/2411.19528</guid>
<content:encoded><![CDATA[
arXiv:2411.19528v2 Announce Type: replace-cross 
Abstract: Standard clothing asset generation involves restoring forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized structure sampling distributions and clothing semantic absence in complex scenarios. Existing models have limited spatial perception, often exhibiting structural hallucinations and texture distortion in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating knowledge from language models and external databases. RAGDiffusion consists of two processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a coarse-to-fine texture alignment that ensures fidelity in pattern and detail components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and texture-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-Free Universum Quadratic Surface Twin Support Vector Machines for Imbalanced Data</title>
<link>https://arxiv.org/abs/2412.01936</link>
<guid>https://arxiv.org/abs/2412.01936</guid>
<content:encoded><![CDATA[
arXiv:2412.01936v2 Announce Type: replace-cross 
Abstract: Binary classification tasks with imbalanced classes pose significant challenges in machine learning. Traditional classifiers often struggle to accurately capture the characteristics of the minority class, resulting in biased models with subpar predictive performance. In this paper, we introduce a novel approach to tackle this issue by leveraging Universum points to support the minority class within quadratic twin support vector machine models. Unlike traditional classifiers, our models utilize quadratic surfaces instead of hyperplanes for binary classification, providing greater flexibility in modeling complex decision boundaries. By incorporating Universum points, our approach enhances classification accuracy and generalization performance on imbalanced datasets. We generated four artificial datasets to demonstrate the flexibility of the proposed methods. Additionally, we validated the effectiveness of our approach through empirical evaluations on benchmark datasets, showing superior performance compared to conventional classifiers and existing methods for imbalanced classification.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiVeGen -- Hierarchical LLM-based Verilog Generation for Scalable Chip Design</title>
<link>https://arxiv.org/abs/2412.05393</link>
<guid>https://arxiv.org/abs/2412.05393</guid>
<content:encoded><![CDATA[
arXiv:2412.05393v2 Announce Type: replace-cross 
Abstract: With Large Language Models (LLMs) recently demonstrating impressive proficiency in code generation, it is promising to extend their abilities to Hardware Description Language (HDL). However, LLMs tend to generate single HDL code blocks rather than hierarchical structures for hardware designs, leading to hallucinations, particularly in complex designs like Domain-Specific Accelerators (DSAs). To address this, we propose HiVeGen, a hierarchical LLM-based Verilog generation framework that decomposes generation tasks into LLM-manageable hierarchical submodules. HiVeGen further harnesses the advantages of such hierarchical structures by integrating automatic Design Space Exploration (DSE) into hierarchy-aware prompt generation, introducing weight-based retrieval to enhance code reuse, and enabling real-time human-computer interaction to lower error-correction cost, significantly improving the quality of generated designs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpiCoder: Encompassing Diversity and Complexity in Code Generation</title>
<link>https://arxiv.org/abs/2501.04694</link>
<guid>https://arxiv.org/abs/2501.04694</guid>
<content:encoded><![CDATA[
arXiv:2501.04694v3 Announce Type: replace-cross 
Abstract: Existing methods for code generation use code snippets as seed data, restricting the complexity and diversity of the synthesized data. In this paper, we introduce a novel feature tree-based synthesis framework, which revolves around hierarchical code features derived from high-level abstractions of code. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features, which captures and recognizes more complex patterns and relationships within the code. By adjusting the depth and breadth of the sampled subtrees, our framework provides precise control over the complexity of the generated code, enabling functionalities that range from function-level operations to multi-file scenarios. We fine-tuned widely-used base models to obtain EpiCoder series, achieving state-of-the-art performance on multiple benchmarks at both the function and file levels. In particular, empirical evidence indicates that our approach shows significant potential in the synthesizing of repository-level code data. Our code and data are publicly available at https://github.com/microsoft/EpiCoder.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</title>
<link>https://arxiv.org/abs/2501.06019</link>
<guid>https://arxiv.org/abs/2501.06019</guid>
<content:encoded><![CDATA[
arXiv:2501.06019v4 Announce Type: replace-cross 
Abstract: Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 14 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at https://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Be Trusted as Evolutionary Optimizers for Network-Structured Combinatorial Problems?</title>
<link>https://arxiv.org/abs/2501.15081</link>
<guid>https://arxiv.org/abs/2501.15081</guid>
<content:encoded><![CDATA[
arXiv:2501.15081v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong capabilities in language understanding and reasoning across diverse domains. Recently, there has been increasing interest in utilizing LLMs not merely as assistants in optimization tasks, but as primary optimizers, particularly for network-structured combinatorial problems. However, before LLMs can be reliably deployed in this role, a fundamental question must be addressed: Can LLMs iteratively manipulate solutions that consistently adhere to problem constraints? In this work, we propose a systematic framework to evaluate the capability of LLMs to engage with problem structures. Rather than treating the model as a black-box generator, we adopt the commonly used evolutionary optimizer (EVO) and propose a comprehensive evaluation framework that rigorously assesses the output fidelity of LLM-based operators across different stages of the evolutionary process. To enhance robustness, we introduce a hybrid error-correction mechanism that mitigates uncertainty in LLMs outputs. Moreover, we explore a cost-efficient population-level optimization strategy that significantly improves efficiency compared to traditional individual-level approaches. Extensive experiments on a representative node-level combinatorial network optimization task demonstrate the effectiveness, adaptability, and inherent limitations of LLM-based EVO. Our findings present perspectives on integrating LLMs into evolutionary computation and discuss paths that may support scalable and context-aware optimization in networked systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improving Skill Learning for Robust Skill-based Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.03752</link>
<guid>https://arxiv.org/abs/2502.03752</guid>
<content:encoded><![CDATA[
arXiv:2502.03752v3 Announce Type: replace-cross 
Abstract: Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, leading to unstable skill learning and degraded performance. To address this, we propose Self-Improving Skill Learning (SISL), which performs self-guided skill refinement using decoupled high-level and skill improvement policies, while applying skill prioritization via maximum return relabeling to focus updates on task-relevant trajectories, resulting in robust and stable adaptation even under noisy and suboptimal data. By mitigating the effect of noise, SISL achieves reliable skill learning and consistently outperforms other skill-based meta-RL methods on diverse long-horizon tasks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rex: Reversible Solvers for Diffusion Models</title>
<link>https://arxiv.org/abs/2502.08834</link>
<guid>https://arxiv.org/abs/2502.08834</guid>
<content:encoded><![CDATA[
arXiv:2502.08834v2 Announce Type: replace-cross 
Abstract: Diffusion models have quickly become the state-of-the-art for numerous generation tasks across many different applications. Encoding samples from the data distribution back into the models underlying prior distribution is an important task that arises in many downstream applications. This task is often called the inversion of diffusion models. Prior approaches for solving this task, however, are often simple heuristic solvers that come with several drawbacks in practice. In this work, we propose a new family of solvers for diffusion models by exploiting the connection between this task and the broader study of algebraically reversible solvers for differential equations. In particular, we construct a family of reversible solvers using an application of Lawson methods to construct exponential Runge-Kutta methods for the diffusion models. We call this family of reversible exponential solvers Rex. In addition to a rigorous theoretical analysis of the proposed solvers we also emonstrate the utility of the methods through a variety of empirical illustrations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v3 Announce Type: replace-cross 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BixBench: a Comprehensive Benchmark for LLM-based Agents in Computational Biology</title>
<link>https://arxiv.org/abs/2503.00096</link>
<guid>https://arxiv.org/abs/2503.00096</guid>
<content:encoded><![CDATA[
arXiv:2503.00096v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and LLM-based agents show great promise in accelerating scientific research. Existing benchmarks for measuring this potential and guiding future development continue to evolve from pure recall and rote knowledge tasks, towards more practical work such as literature review and experimental planning. Bioinformatics is a domain where fully autonomous AI-driven discovery may be near, but no extensive benchmarks for measuring progress have been introduced to date. We therefore present the Bioinformatics Benchmark (BixBench), a dataset comprising over 50 real-world scenarios of practical biological data analysis with nearly 300 associated open-answer questions designed to measure the ability of LLM-based agents to explore biological datasets, perform long, multi-step analytical trajectories, and interpret the nuanced results of those analyses. We evaluate the performance of two frontier LLMs (GPT-4o and Claude 3.5 Sonnet) using a custom agent framework we open source. We find that even the latest frontier models only achieve 17% accuracy in the open-answer regime, and no better than random in a multiple-choice setting. By exposing the current limitations of frontier models, we hope BixBench can spur the development of agents capable of conducting rigorous bioinformatic analysis and accelerate scientific discovery.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</title>
<link>https://arxiv.org/abs/2503.02233</link>
<guid>https://arxiv.org/abs/2503.02233</guid>
<content:encoded><![CDATA[
arXiv:2503.02233v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Your Models to Understand Code via Focal Preference Alignment</title>
<link>https://arxiv.org/abs/2503.02783</link>
<guid>https://arxiv.org/abs/2503.02783</guid>
<content:encoded><![CDATA[
arXiv:2503.02783v4 Announce Type: replace-cross 
Abstract: Preference learning extends the performance of Code LLMs beyond traditional supervised fine-tuning by leveraging relative quality comparisons. In existing approaches, a set of n candidate solutions is evaluated based on test case success rates, with the candidate demonstrating a higher pass rate being labeled as positive and its counterpart with a lower pass rate as negative. However, because this approach aligns entire failing code blocks rather than pinpointing specific errors, it lacks the granularity necessary to capture meaningful error-correction relationships. As a result, the model is unable to learn more informative error-correction patterns. To address these issues, we propose Target-DPO, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. Target-DPO explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To facilitate it, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with Target-DPO achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that Target-DPO yields fewer errors. Code, model and datasets are in: https://github.com/JieWu02/Target-DPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Applications: Current Paradigms and the Next Frontier</title>
<link>https://arxiv.org/abs/2503.04596</link>
<guid>https://arxiv.org/abs/2503.04596</guid>
<content:encoded><![CDATA[
arXiv:2503.04596v2 Announce Type: replace-cross 
Abstract: The development of large language models (LLMs) has given rise to four major application paradigms: LLM app stores, LLM agents, self-hosted LLM services, and LLM-powered devices. Each has its advantages but also shares common challenges. LLM app stores lower the barrier to development but lead to platform lock-in; LLM agents provide autonomy but lack a unified communication mechanism; self-hosted LLM services enhance control but increase deployment complexity; and LLM-powered devices improve privacy and real-time performance but are limited by hardware. This paper reviews and analyzes these paradigms, covering architecture design, application ecosystem, research progress, as well as the challenges and open problems they face. Based on this, we outline the next frontier of LLM applications, characterizing them through three interconnected layers: infrastructure, protocol, and application. We describe their responsibilities and roles of each layer and demonstrate how to mitigate existing fragmentation limitations and improve security and scalability. Finally, we discuss key future challenges, identify opportunities such as protocol-driven cross-platform collaboration and device integration, and propose a research roadmap for openness, security, and sustainability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Sample Complexity Bounds In Bilevel Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.17644</link>
<guid>https://arxiv.org/abs/2503.17644</guid>
<content:encoded><![CDATA[
arXiv:2503.17644v5 Announce Type: replace-cross 
Abstract: Bilevel reinforcement learning (BRL) has emerged as a powerful framework for aligning generative models, yet its theoretical foundations, especially sample complexity bounds, remain underexplored. In this work, we present the first sample complexity bound for BRL, establishing a rate of $\mathcal{O}(\epsilon^{-3})$ in continuous state-action spaces. Traditional MDP analysis techniques do not extend to BRL due to its nested structure and non-convex lower-level problems. We overcome these challenges by leveraging the Polyak-{\L}ojasiewicz (PL) condition and the MDP structure to obtain closed-form gradients, enabling tight sample complexity analysis. Our analysis also extends to general bi-level optimization settings with non-convex lower levels, where we achieve state-of-the-art sample complexity results of $\mathcal{O}(\epsilon^{-3})$ improving upon existing bounds of $\mathcal{O}(\epsilon^{-6})$. Additionally, we address the computational bottleneck of hypergradient estimation by proposing a fully first-order, Hessian-free algorithm suitable for large-scale problems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act</title>
<link>https://arxiv.org/abs/2503.18156</link>
<guid>https://arxiv.org/abs/2503.18156</guid>
<content:encoded><![CDATA[
arXiv:2503.18156v3 Announce Type: replace-cross 
Abstract: AI-generated images have become so good in recent years that individuals often cannot distinguish them any more from "real" images. This development, combined with the rapid spread of AI-generated content online, creates a series of societal risks. Watermarking, a technique that involves embedding information within images and other content to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated content. Indeed, watermarking and AI labelling measures are now becoming a legal requirement in many jurisdictions, including under the 2024 European Union AI Act. Despite the widespread use of AI image generation systems, the practical implications and the current status of implementation of these measures remain largely unexamined. The present paper therefore provides both an empirical and a legal analysis of these measures. In our legal analysis, we identify four categories of generative AI deployment scenarios and outline how the legal obligations could apply in each category. In our empirical analysis, we find that only a minority number of AI image generators currently implement adequate watermarking (38%) and deep fake labelling (18%) practices. In response, we suggest a range of avenues of how the implementation of these legally mandated techniques can be improved, and publicly share our tooling for the detection of watermarks in images.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty</title>
<link>https://arxiv.org/abs/2503.22233</link>
<guid>https://arxiv.org/abs/2503.22233</guid>
<content:encoded><![CDATA[
arXiv:2503.22233v3 Announce Type: replace-cross 
Abstract: We introduce the Entropy-Driven Uncertainty Process Reward Model (EDU-PRM), a novel entropy-driven training framework for process reward modeling that enables dynamic, uncertainty-aligned segmentation of complex reasoning steps, eliminating the need for costly manual step annotations. Unlike previous Process Reward Models (PRMs) that rely on static partitioning and human labeling, EDU-PRM automatically anchors step boundaries at tokens with high predictive entropy, effectively capturing intrinsic logical transitions and facilitating efficient exploration of diverse reasoning paths. On the ProcessBench benchmark, EDU-PRM outperforms strong public PRM baselines, such as Math-Shepherd PRM and Omega PRM, and EDU-PRM achieves comparable results with SOTA models while only using 1.5% training data. Furthermore, by leveraging our proposed EDU sampling strategy, we observe accuracy boosts from 64.7% to 67.3% for generative reasoning tasks, accompanied by a reduction of 32% in token usage. These findings underscore the potential of EDU-PRM as a scalable and annotation-efficient paradigm for process supervision in mathematical reasoning, paving the way for more efficient and robust approaches to complex mathematical problem solving.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
arXiv:2503.23798v3 Announce Type: replace-cross 
Abstract: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, limited attention has been paid to a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive computation in LLMs without modifying their original parameters. Applied to Llama-3-8B, it skips 8 out of 32 layers while maintaining full benchmark performance. Our experiments reveal that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Despite the computational savings, FlexiDepth does not yet achieve wall-clock speedup due to varied skipping patterns and I/O overhead. To inspire future work and advance research on practical speedup, we open-sourced FlexiDepth and a dataset documenting its layer allocation patterns.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title>
<link>https://arxiv.org/abs/2504.00218</link>
<guid>https://arxiv.org/abs/2504.00218</guid>
<content:encoded><![CDATA[
arXiv:2504.00218v2 Announce Type: replace-cross 
Abstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\texttt{Llama}$, $\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on various datasets like $\texttt{JailBreakBench}$ and $\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization</title>
<link>https://arxiv.org/abs/2504.01444</link>
<guid>https://arxiv.org/abs/2504.01444</guid>
<content:encoded><![CDATA[
arXiv:2504.01444v4 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.05220</link>
<guid>https://arxiv.org/abs/2504.05220</guid>
<content:encoded><![CDATA[
arXiv:2504.05220v5 Announce Type: replace-cross 
Abstract: This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection in LLMs with Topological Divergence on Attention Graphs</title>
<link>https://arxiv.org/abs/2504.10063</link>
<guid>https://arxiv.org/abs/2504.10063</guid>
<content:encoded><![CDATA[
arXiv:2504.10063v3 Announce Type: replace-cross 
Abstract: Hallucination, i.e., generating factually incorrect content, remains a critical challenge for large language models (LLMs). We introduce TOHA, a TOpology-based HAllucination detector in the RAG setting, which leverages a topological divergence metric to quantify the structural properties of graphs induced by attention matrices. Examining the topological divergence between prompt and response subgraphs reveals consistent patterns: higher divergence values in specific attention heads correlate with hallucinated outputs, independent of the dataset. Extensive experiments - including evaluation on question answering and summarization tasks - show that our approach achieves state-of-the-art or competitive results on several benchmarks while requiring minimal annotated data and computational resources. Our findings suggest that analyzing the topological structure of attention matrices can serve as an efficient and robust indicator of factual reliability in LLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16460</link>
<guid>https://arxiv.org/abs/2504.16460</guid>
<content:encoded><![CDATA[
arXiv:2504.16460v2 Announce Type: replace-cross 
Abstract: The specialized vocabulary and nuanced concepts of the telecommunications industry pose persistent challenges for standard Natural Language Processing (NLP) models. Generic embedding models often struggle to represent telecom-specific semantics, limiting their utility in retrieval and downstream tasks. We present T-VEC (Telecom Vectorization Model), a domain-adapted embedding model fine-tuned from the gte-Qwen2-1.5B-instruct backbone using a triplet loss objective. Fine-tuning was performed on T-Embed, a high-quality, large-scale dataset covering diverse telecom concepts, standards, and operational scenarios. Although T-Embed contains some proprietary material and cannot be fully released, we open source 75% of the dataset to support continued research in domain-specific representation learning. On a custom benchmark comprising 1500 query-passage pairs from IETF RFCs and vendor manuals, T-VEC surpasses MPNet, BGE, Jina and E5, demonstrating superior domain grounding and semantic precision in telecom-specific retrieval. Embedding visualizations further showcase tight clustering of telecom-relevant concepts. We release T-VEC and its tokenizer to support semantically faithful NLP applications within the telecom domain.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.18114</link>
<guid>https://arxiv.org/abs/2504.18114</guid>
<content:encoded><![CDATA[
arXiv:2504.18114v2 Announce Type: replace-cross 
Abstract: Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding In-context Learning of Addition via Activation Subspaces</title>
<link>https://arxiv.org/abs/2505.05145</link>
<guid>https://arxiv.org/abs/2505.05145</guid>
<content:encoded><![CDATA[
arXiv:2505.05145v3 Announce Type: replace-cross 
Abstract: To perform few-shot learning, language models extract signals from a few input-label pairs, aggregate these into a learned prediction rule, and apply this rule to new inputs. How is this implemented in the forward pass of modern transformer models? To explore this question, we study a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We introduce a novel optimization method that localizes the model's few-shot ability to only a few attention heads. We then perform an in-depth analysis of individual heads, via dimensionality reduction and decomposition. As an example, on Llama-3-8B-instruct, we reduce its mechanism on our tasks to just three attention heads with six-dimensional subspaces, where four dimensions track the unit digit with trigonometric functions at periods $2$, $5$, and $10$, and two dimensions track magnitude with low-frequency components. To deepen our understanding of the mechanism, we also derive a mathematical identity relating ``aggregation'' and ``extraction'' subspaces for attention heads, allowing us to track the flow of information from individual examples to a final aggregated concept. Using this, we identify a self-correction mechanism where mistakes learned from earlier demonstrations are suppressed by later demonstrations. Our results demonstrate how tracking low-dimensional subspaces of localized heads across a forward pass can provide insight into fine-grained computational structures in language models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hakim: Farsi Text Embedding Model</title>
<link>https://arxiv.org/abs/2505.08435</link>
<guid>https://arxiv.org/abs/2505.08435</guid>
<content:encoded><![CDATA[
arXiv:2505.08435v3 Announce Type: replace-cross 
Abstract: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation</title>
<link>https://arxiv.org/abs/2505.11111</link>
<guid>https://arxiv.org/abs/2505.11111</guid>
<content:encoded><![CDATA[
arXiv:2505.11111v2 Announce Type: replace-cross 
Abstract: Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/youlei202/FairSHAP.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title>
<link>https://arxiv.org/abs/2505.13527</link>
<guid>https://arxiv.org/abs/2505.13527</guid>
<content:encoded><![CDATA[
arXiv:2505.13527v2 Announce Type: replace-cross 
Abstract: Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization</title>
<link>https://arxiv.org/abs/2505.14756</link>
<guid>https://arxiv.org/abs/2505.14756</guid>
<content:encoded><![CDATA[
arXiv:2505.14756v2 Announce Type: replace-cross 
Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM Finetuning</title>
<link>https://arxiv.org/abs/2505.16567</link>
<guid>https://arxiv.org/abs/2505.16567</guid>
<content:encoded><![CDATA[
arXiv:2505.16567v3 Announce Type: replace-cross 
Abstract: Finetuning open-weight Large Language Models (LLMs) is standard practice for achieving task-specific performance improvements. Until now, finetuning has been regarded as a controlled and secure process in which training on benign datasets leads to predictable behaviors. In this paper, we demonstrate, for the first time, that an adversary can create compromised LLMs that are performant and benign, yet exhibit adversarial behaviors once finetuned by downstream users. To this end, we propose an attack, FAB (Finetuning-activated Adversarial Behaviors), which compromises an LLM via meta-learning techniques that simulate downstream finetuning, explicitly optimizing for the emergence of adversarial behaviors in the finetuned models. At the same time, the compromised LLM is regularized to retain general capabilities and to exhibit no adversarial behaviors prior to finetuning. As a result, when users finetune (e.g., instruction-tuning, distillation, DPO) the seemingly benign model on their own datasets, they unknowingly trigger its dormant adversarial behavior. We experimentally demonstrate the effectiveness of FAB across multiple LLMs and three commonly considered target behaviors: unsolicited advertising, jailbreakability, and over-refusal. We show that FAB-triggers are robust to various finetuning choices made by the user (e.g., dataset, number of steps, scheduler, post-training algorithm). Our findings challenge prevailing assumptions on the security of finetuning, revealing a critical attack vector.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty</title>
<link>https://arxiv.org/abs/2505.17281</link>
<guid>https://arxiv.org/abs/2505.17281</guid>
<content:encoded><![CDATA[
arXiv:2505.17281v2 Announce Type: replace-cross 
Abstract: Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution</title>
<link>https://arxiv.org/abs/2505.19644</link>
<guid>https://arxiv.org/abs/2505.19644</guid>
<content:encoded><![CDATA[
arXiv:2505.19644v3 Announce Type: replace-cross 
Abstract: A key research area in deepfake speech detection is source tracing - determining the origin of synthesised utterances. The approaches may involve identifying the acoustic model (AM), vocoder model (VM), or other generation-specific parameters. However, progress is limited by the lack of a dedicated, systematically curated dataset. To address this, we introduce STOPA, a systematically varied and metadata-rich dataset for deepfake speech source tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k samples from 13 distinct synthesisers. Unlike existing datasets, which often feature limited variation or sparse metadata, STOPA provides a systematically controlled framework covering a broader range of generative factors, such as the choice of the vocoder model, acoustic model, or pretrained weights, ensuring higher attribution reliability. This control improves attribution accuracy, aiding forensic analysis, deepfake detection, and generative model transparency.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-time Alignment in Continuous Space</title>
<link>https://arxiv.org/abs/2505.20081</link>
<guid>https://arxiv.org/abs/2505.20081</guid>
<content:encoded><![CDATA[
arXiv:2505.20081v3 Announce Type: replace-cross 
Abstract: Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on MATH. Our code is publicly available at https://github.com/yuanyige/sea
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
arXiv:2505.20321v3 Announce Type: replace-cross 
Abstract: Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology</title>
<link>https://arxiv.org/abs/2505.20435</link>
<guid>https://arxiv.org/abs/2505.20435</guid>
<content:encoded><![CDATA[
arXiv:2505.20435v2 Announce Type: replace-cross 
Abstract: Existing interpretability methods for Large Language Models (LLMs) often fall short by focusing on linear directions or isolated features, overlooking the high-dimensional, nonlinear, and relational geometry within model representations. This study focuses on how adversarial inputs systematically affect the internal representation spaces of LLMs, a topic which remains poorly understood. We propose persistent homology (PH), a tool from topological data analysis, as a principled framework to characterize the multi-scale dynamics within LLM activations. Using PH, we systematically analyze six state-of-the-art models under two distinct adversarial conditions, indirect prompt injection and backdoor fine-tuning, and identify a consistent topological signature of adversarial influence. Across architectures and model sizes, adversarial inputs induce ``topological compression'', where the latent space becomes structurally simpler, collapsing from varied, compact, small-scale features into fewer, dominant, and more dispersed large-scale ones. This topological signature is statistically robust across layers, highly discriminative, and provides interpretable insights into how adversarial effects emerge and propagate. By quantifying the shape of activations and neuronal information flow, our architecture-agnostic framework reveals fundamental invariants of representational change, offering a complementary perspective to existing interpretability methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties</title>
<link>https://arxiv.org/abs/2505.20875</link>
<guid>https://arxiv.org/abs/2505.20875</guid>
<content:encoded><![CDATA[
arXiv:2505.20875v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our code and datasets are publicly available at https://github.com/jiyounglee-0523/TransEnV and https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation</title>
<link>https://arxiv.org/abs/2505.21904</link>
<guid>https://arxiv.org/abs/2505.21904</guid>
<content:encoded><![CDATA[
arXiv:2505.21904v4 Announce Type: replace-cross 
Abstract: Instance segmentation demands costly per-pixel annotations and computationally expensive models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pre-trained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM(s) via self-training with contrastive calibration, (2) knowledge transfer through a unified multi-objective loss, and (3) student refinement to mitigate residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to extract informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11x smaller student improves over its zero-shot VFM teacher(s) by +8.5 and +7.1 AP, surpasses adapted teacher(s) by +3.4 and +1.5 AP, and further outperforms state-of-the-art SSKD methods on both benchmarks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement</title>
<link>https://arxiv.org/abs/2505.22021</link>
<guid>https://arxiv.org/abs/2505.22021</guid>
<content:encoded><![CDATA[
arXiv:2505.22021v2 Announce Type: replace-cross 
Abstract: Document Image Enhancement (DIE) serves as a critical component in Document AI systems, where its performance substantially determines the effectiveness of downstream tasks. To address the limitations of existing methods confined to single-degradation restoration or grayscale image processing, we present Global with Local Parametric Generation Enhancement Network (GL-PGENet), a novel architecture designed for multi-degraded color document images, ensuring both efficiency and robustness in real-world scenarios. Our solution incorporates three key innovations: First, a hierarchical enhancement framework that integrates global appearance correction with local refinement, enabling coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network with parametric generation mechanisms that replaces conventional direct prediction, producing enhanced outputs through learned intermediate parametric representations rather than pixel-wise mapping. This approach enhances local consistency while improving model generalization. Finally, a modified NestUNet architecture incorporating dense block to effectively fuse low-level pixel features and high-level semantic features, specifically adapted for document image characteristics. In addition, to enhance generalization performance, we adopt a two-stage training strategy: large-scale pretraining on a synthetic dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive experiments demonstrate the superiority of GL-PGENet, achieving state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The model also exhibits remarkable cross-domain adaptability and maintains computational efficiency for high-resolution images without performance degradation, confirming its practical utility in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGREF: Masked Guidance for Any-Reference Video Generation with Subject Disentanglement</title>
<link>https://arxiv.org/abs/2505.23742</link>
<guid>https://arxiv.org/abs/2505.23742</guid>
<content:encoded><![CDATA[
arXiv:2505.23742v2 Announce Type: replace-cross 
Abstract: We tackle the task of any-reference video generation, which aims to synthesize videos conditioned on arbitrary types and combinations of reference subjects, together with textual prompts. This task faces persistent challenges, including identity inconsistency, entanglement among multiple reference subjects, and copy-paste artifacts. To address these issues, we introduce MAGREF, a unified and effective framework for any-reference video generation. Our approach incorporates masked guidance and a subject disentanglement mechanism, enabling flexible synthesis conditioned on diverse reference images and textual prompts. Specifically, masked guidance employs a region-aware masking mechanism combined with pixel-wise channel concatenation to preserve appearance features of multiple subjects along the channel dimension. This design preserves identity consistency and maintains the capabilities of the pre-trained backbone, without requiring any architectural changes. To mitigate subject confusion, we introduce a subject disentanglement mechanism which injects the semantic values of each subject derived from the text condition into its corresponding visual region. Additionally, we establish a four-stage data pipeline to construct diverse training pairs, effectively alleviating copy-paste artifacts. Extensive experiments on a comprehensive benchmark demonstrate that MAGREF consistently outperforms existing state-of-the-art approaches, paving the way for scalable, controllable, and high-fidelity any-reference video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tug-of-war between idioms' figurative and literal interpretations in LLMs</title>
<link>https://arxiv.org/abs/2506.01723</link>
<guid>https://arxiv.org/abs/2506.01723</guid>
<content:encoded><![CDATA[
arXiv:2506.01723v4 Announce Type: replace-cross 
Abstract: Idioms present a unique challenge for language models due to their non-compositional figurative interpretations, which often strongly diverge from the idiom's literal interpretation. In this paper, we employ causal tracing to systematically analyze how pretrained causal transformers deal with this ambiguity. We localize three mechanisms: (i) Early sublayers and specific attention heads retrieve an idiom's figurative interpretation, while suppressing its literal interpretation. (ii) When disambiguating context precedes the idiom, the model leverages it from the earliest layer and later layers refine the interpretation if the context conflicts with the retrieved interpretation. (iii) Then, selective, competing pathways carry both interpretations: an intermediate pathway prioritizes the figurative interpretation and a parallel direct route favors the literal interpretation, ensuring that both readings remain available. Our findings provide mechanistic evidence for idiom comprehension in autoregressive transformers.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study</title>
<link>https://arxiv.org/abs/2506.04810</link>
<guid>https://arxiv.org/abs/2506.04810</guid>
<content:encoded><![CDATA[
arXiv:2506.04810v2 Announce Type: replace-cross 
Abstract: Logical reasoning is a core capability for large language models (LLMs), yet existing benchmarks that rely solely on final-answer accuracy fail to capture the quality of the reasoning process. To address this, we introduce FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall accuracy, stepwise soundness, and representation-level probing. Leveraging this framework, we conduct a comprehensive study on how different supervision formats in fine-tuning shape reasoning abilities. We fine-tune LLMs on four supervision styles: one in natural language and three symbolic variants. We find a key trade-off: natural language supervision excels at generalization to out-of-distribution and long-chain problems, whereas symbolic supervision is superior at instilling structurally sound, atomic reasoning steps. Furthermore, our probing analysis indicates that fine-tuning primarily refines the model's step-by-step generation process, rather than improving its ability to converge on an answer early. Together, our framework and analysis provide a more rigorous lens for evaluating and improving logical reasoning in LLMs. The code is available at https://github.com/YujunZhou/FineLogic.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining</title>
<link>https://arxiv.org/abs/2506.08022</link>
<guid>https://arxiv.org/abs/2506.08022</guid>
<content:encoded><![CDATA[
arXiv:2506.08022v3 Announce Type: replace-cross 
Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Product of Experts for Visual Generation</title>
<link>https://arxiv.org/abs/2506.08894</link>
<guid>https://arxiv.org/abs/2506.08894</guid>
<content:encoded><![CDATA[
arXiv:2506.08894v2 Announce Type: replace-cross 
Abstract: Modern neural models capture rich priors and have complementary knowledge over shared data domains, e.g., images and videos. Integrating diverse knowledge from multiple sources -- including visual generative models, visual language models, and sources with human-crafted knowledge such as graphics engines and physics simulators -- remains under-explored. We propose a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS). Our framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and additionally providing flexible user interfaces for specifying visual generation goals.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intention-Conditioned Flow Occupancy Models</title>
<link>https://arxiv.org/abs/2506.08902</link>
<guid>https://arxiv.org/abs/2506.08902</guid>
<content:encoded><![CDATA[
arXiv:2506.08902v2 Announce Type: replace-cross 
Abstract: Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \times$ median improvement in returns and increases success rates by $36\%$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
arXiv:2506.09513v3 Announce Type: replace-cross 
Abstract: Reasoning-based large language models have excelled in mathematics and programming, yet their potential in knowledge-intensive medical question answering remains underexplored and insufficiently validated in clinical contexts. To bridge this gap, we introduce ReasonMed, the largest medical reasoning dataset to date, comprising 370k high-quality examples distilled from 1.75 million initial reasoning paths generated by complementary LLMs and curated through a cost-efficient easy-medium-difficult (EMD) pipeline. ReasonMed is built through a multi-agent generation, verification, and refinement process, in which an Error Refiner improves reasoning paths by correcting error-prone steps identified by a verifier. Using ReasonMed, we investigate effective strategies for training medical reasoning models and find that integrating detailed CoT reasoning with concise answer summaries yields the most robust fine-tuning results. Models trained on ReasonMed set a new benchmark: ReasonMed-7B surpasses the prior best sub-10B models by 4.17% and even exceeds LLaMA3.1-70B on PubMedQA by 4.60%. When scaled to ReasonMed-14B, it remains highly competitive, underscoring consistent scaling potential. The codes and datasets are available at https://github.com/YuSun-Work/ReasonMed.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think With Videos For Agentic Long-Video Understanding</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
arXiv:2506.10821v4 Announce Type: replace-cross 
Abstract: Long-video understanding~(LVU) is a challenging problem in computer vision. Existing methods either downsample frames for single-pass reasoning, sacrificing fine-grained details, or depend on textual reasoning over task-agnostic representations, hindering task-specific perception and exploration. In this paper, we propose VideoExplorer, a framework grounded in the principle of ``thinking with video'', which naturally intertwines planning, temporal grounding, and scalable perception into a coherent reasoning process. Rather than reasoning over a static context, VideoExplorer iteratively formulates sub-questions, locates relevant moments, and performs task-oriented, temporally scalable video understanding until reaching the final answer, enabling faithful, efficient, and interpretable reasoning. To address the lack of LVU training resources, we construct a long-video reasoning dataset using difficulty-adaptive sampling to ensure high-quality trajectories on complex tasks. Building on this dataset, we design a two-stage training pipeline: supervised trajectory initialization followed by trajectory-level preference optimization, encouraging adaptive temporal grounding and iterative information integration guided by downstream rewards. Extensive evaluations on popular long-video understanding and reasoning benchmarks demonstrate VideoExplorer's significant advantage over existing baselines, highlighting its robustness, adaptability, and efficiency. Our code is made publicly available in this repository(https://github.com/yhy-2000/VideoDeepResearch).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Losses for Diffusion Bridge Samplers</title>
<link>https://arxiv.org/abs/2506.10982</link>
<guid>https://arxiv.org/abs/2506.10982</guid>
<content:encoded><![CDATA[
arXiv:2506.10982v2 Announce Type: replace-cross 
Abstract: Diffusion bridges are a promising class of deep-learning methods for sampling from unnormalized distributions. Recent works show that the Log Variance (LV) loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when using the reparametrization trick to compute rKL-gradients. While the on-policy LV loss yields identical gradients to the rKL loss when combined with the log-derivative trick for diffusion samplers with non-learnable forward processes, this equivalence does not hold for diffusion bridges or when diffusion coefficients are learned. Based on this insight we argue that for diffusion bridges the LV loss does not represent an optimization objective that can be motivated like the rKL loss via the data processing inequality. Our analysis shows that employing the rKL loss with the log-derivative trick (rKL-LD) does not only avoid these conceptual problems but also consistently outperforms the LV loss. Experimental results with different types of diffusion bridges on challenging benchmarks show that samplers trained with the rKL-LD loss achieve better performance. From a practical perspective we find that rKL-LD requires significantly less hyperparameter optimization and yields more stable training behavior.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Clients Are Equal: Collaborative Model Personalization on Heterogeneous Multi-Modal Clients</title>
<link>https://arxiv.org/abs/2506.11024</link>
<guid>https://arxiv.org/abs/2506.11024</guid>
<content:encoded><![CDATA[
arXiv:2506.11024v2 Announce Type: replace-cross 
Abstract: As AI becomes more personal, e.g., Agentic AI, there is an increasing need for personalizing models for various use cases. Personalized federated learning (PFL) enables each client to collaboratively leverage other clients' knowledge for better adaptation to the task of interest, without privacy risks. Despite its potential, existing PFL methods remain confined to rather simplified scenarios where data and models are the same across clients. To move towards realistic scenarios, we propose FedMosaic, a method that jointly addresses data and model heterogeneity with a task-relevance-aware model aggregation strategy to reduce parameter interference, and a dimension-invariant module that enables knowledge sharing across heterogeneous architectures without huge computational cost. To mimic the real-world task diversity, we propose a multi-modal PFL benchmark spanning 40 distinct tasks with distribution shifts over time. The empirical study shows that FedMosaic outperforms the state-of-the-art PFL methods, excelling in both personalization and generalization capabilities under challenging, realistic scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.11113</link>
<guid>https://arxiv.org/abs/2506.11113</guid>
<content:encoded><![CDATA[
arXiv:2506.11113v3 Announce Type: replace-cross 
Abstract: Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis</title>
<link>https://arxiv.org/abs/2506.12263</link>
<guid>https://arxiv.org/abs/2506.12263</guid>
<content:encoded><![CDATA[
arXiv:2506.12263v3 Announce Type: replace-cross 
Abstract: Foundation models have gained growing interest in the IoT domain due to their reduced reliance on labeled data and strong generalizability across tasks, which address key limitations of traditional machine learning approaches. However, most existing foundation model based methods are developed for specific IoT tasks, making it difficult to compare approaches across IoT domains and limiting guidance for applying them to new tasks. This survey aims to bridge this gap by providing a comprehensive overview of current methodologies and organizing them around four shared performance objectives by different domains: efficiency, context-awareness, safety, and security & privacy. For each objective, we review representative works, summarize commonly-used techniques and evaluation metrics. This objective-centric organization enables meaningful cross-domain comparisons and offers practical insights for selecting and designing foundation model based solutions for new IoT tasks. We conclude with key directions for future research to guide both practitioners and researchers in advancing the use of foundation models in IoT applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Model Confidence on Bias Effects in Measured Uncertainties for Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.16724</link>
<guid>https://arxiv.org/abs/2506.16724</guid>
<content:encoded><![CDATA[
arXiv:2506.16724v2 Announce Type: replace-cross 
Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases have greater effects in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence is associated with greater bias-induced underestimation of epistemic uncertainty, resulting in overconfident estimates, whereas it has no significant effect on the direction of bias effect in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs on a Budget? Say HOLA</title>
<link>https://arxiv.org/abs/2506.18952</link>
<guid>https://arxiv.org/abs/2506.18952</guid>
<content:encoded><![CDATA[
arXiv:2506.18952v2 Announce Type: replace-cross 
Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high compute and memory demands posing a barrier for real-time applications in sectors like healthcare, education, and embedded systems. Current solutions such as quantization, pruning, and retrieval-augmented generation (RAG) offer only partial optimizations and often compromise on speed or accuracy. We introduce HOLA, an end-to-end optimization framework for efficient LLM deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD) for faster inference without quality loss. Externally, AdaComp-RAG adjusts retrieval complexity based on context needs. Together with LoBi, which blends structured pruning (LoRA) and quantization, HOLA delivers significant gains: 17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge devices like Jetson Nano--proving both scalable and production-ready.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth, Trust, and Trouble: Medical AI on the Edge</title>
<link>https://arxiv.org/abs/2507.02983</link>
<guid>https://arxiv.org/abs/2507.02983</guid>
<content:encoded><![CDATA[
arXiv:2507.02983v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) hold significant promise for transforming digital health by enabling automated medical question answering. However, ensuring these models meet critical industry standards for factual accuracy, usefulness, and safety remains a challenge, especially for open-source solutions. We present a rigorous benchmarking framework using a dataset of over 1,000 health questions. We assess model performance across honesty, helpfulness, and harmlessness. Our results highlight trade-offs between factual reliability and safety among evaluated models -- Mistral-7B, BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot prompting improves accuracy from 78% to 85%, and all models show reduced helpfulness on complex queries, highlighting ongoing challenges in clinical QA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</title>
<link>https://arxiv.org/abs/2507.06223</link>
<guid>https://arxiv.org/abs/2507.06223</guid>
<content:encoded><![CDATA[
arXiv:2507.06223v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose \ours\footnote{https://github.com/zhiyuanpeng/EER-FLOPs.} for LLM-based rerankers: RPP (ranking metrics per PetaFLOP), measuring how much ranking quality (e.g., NDCG or MRR) a method achieves per PetaFLOP, and QPP (queries per PetaFLOP), measuring how many queries can be processed per PetaFLOP. Accompanied by the new metrics, an interpretable FLOPs estimator is developed to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architectures, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations</title>
<link>https://arxiv.org/abs/2507.13468</link>
<guid>https://arxiv.org/abs/2507.13468</guid>
<content:encoded><![CDATA[
arXiv:2507.13468v2 Announce Type: replace-cross 
Abstract: The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives</title>
<link>https://arxiv.org/abs/2507.15783</link>
<guid>https://arxiv.org/abs/2507.15783</guid>
<content:encoded><![CDATA[
arXiv:2507.15783v3 Announce Type: replace-cross 
Abstract: AI companion chatbots are increasingly popular with teens, while these interactions are entertaining, they also risk overuse that can potentially disrupt offline daily life. We examined how adolescents describe reliance on AI companions, mapping their experiences onto behavioral addiction frameworks and exploring pathways to disengagement, by analyzing 318 Reddit posts made by users who self-disclosed as 13-17 years old on the Character.AI subreddit. We found teens often begin using chatbots for support or creative play, but these activities can deepen into strong attachments marked by conflict, withdrawal, tolerance, relapse, and mood regulation. Reported consequences include sleep loss, academic decline, and strained real-world connections. Disengagement commonly arises when teens recognize harm, re-engage with offline life, or encounter restrictive platform changes. We highlight specific risks of character-based companion chatbots based on teens' perspectives and introduce a design framework (CARE) for guidance for safer systems and setting directions for future teen-centered research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.16347</link>
<guid>https://arxiv.org/abs/2507.16347</guid>
<content:encoded><![CDATA[
arXiv:2507.16347v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) excel in node classification tasks but often assume homophily, where connected nodes share similar labels. This assumption does not hold in many real-world heterophilic graphs. Existing models for heterophilic graphs primarily rely on pairwise relationships, overlooking multi-scale information from higher-order structures. This leads to suboptimal performance, particularly under noise from conflicting class information across nodes. To address these challenges, we propose HPGNN, a novel model integrating Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces an efficient high-order approximation of Personalized PageRank (PPR) to capture long-range and multi-scale node interactions. This approach reduces computational complexity and mitigates noise from surrounding information. By embedding higher-order structural information into convolutional networks, HPGNN effectively models key interactions across diverse graph dimensions. Extensive experiments on benchmark datasets demonstrate HPGNN's effectiveness. The model achieves better performance than five out of seven state-of-the-art methods on heterophilic graphs in downstream tasks while maintaining competitive performance on homophilic graphs. HPGNN's ability to balance multi-scale information and robustness to noise makes it a versatile solution for real-world graph learning challenges. Codes are available at https://github.com/streetcorner/HPGNN.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Hybrid Captioner for Improved Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2507.17047</link>
<guid>https://arxiv.org/abs/2507.17047</guid>
<content:encoded><![CDATA[
arXiv:2507.17047v3 Announce Type: replace-cross 
Abstract: Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes</title>
<link>https://arxiv.org/abs/2507.17717</link>
<guid>https://arxiv.org/abs/2507.17717</guid>
<content:encoded><![CDATA[
arXiv:2507.17717v2 Announce Type: replace-cross 
Abstract: AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters (prepared in accordance with the HIPAA safe harbor standard) from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms a baseline approach in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, our checklist offers a practical tool for flagging notes that may fall short of our defined quality standards.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19102</link>
<guid>https://arxiv.org/abs/2507.19102</guid>
<content:encoded><![CDATA[
arXiv:2507.19102v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title>
<link>https://arxiv.org/abs/2507.19195</link>
<guid>https://arxiv.org/abs/2507.19195</guid>
<content:encoded><![CDATA[
arXiv:2507.19195v2 Announce Type: replace-cross 
Abstract: Style-conditioned data poisoning is identified as a covert vector for amplifying sociolinguistic bias in large language models. Using small poisoned budgets that pair dialectal prompts -- principally African American Vernacular English (AAVE) and a Southern dialect -- with toxic or stereotyped completions during instruction tuning, this work probes whether linguistic style can act as a latent trigger for harmful behavior. Across multiple model families and scales, poisoned exposure elevates toxicity and stereotype expression for dialectal inputs -- most consistently for AAVE -- while Standard American English remains comparatively lower yet not immune. A multi-metric audit combining classifier-based toxicity with an LLM-as-a-judge reveals stereotype-laden content even when lexical toxicity appears muted, indicating that conventional detectors under-estimate sociolinguistic harms. Additionally, poisoned models exhibit emergent jailbreaking despite the absence of explicit slurs in the poison, suggesting weakened alignment rather than memorization. These findings underscore the need for dialect-aware evaluation, content-level stereotype auditing, and training protocols that explicitly decouple style from toxicity to prevent bias amplification through seemingly minor, style-based contamination.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</title>
<link>https://arxiv.org/abs/2508.01696</link>
<guid>https://arxiv.org/abs/2508.01696</guid>
<content:encoded><![CDATA[
arXiv:2508.01696v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling</title>
<link>https://arxiv.org/abs/2508.08487</link>
<guid>https://arxiv.org/abs/2508.08487</guid>
<content:encoded><![CDATA[
arXiv:2508.08487v4 Announce Type: replace-cross 
Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, a multi-agent collaborative framework designed to assist in long-sequence video storytelling by efficiently translating ideas into visual narratives. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle--Explore, Examine, and Enhanc--to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief idea description, MAViS enables users to rapidly explore diverse visual storytelling and creative directions for sequential video generation by efficiently producing high-quality, complete long-sequence videos. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography</title>
<link>https://arxiv.org/abs/2508.09616</link>
<guid>https://arxiv.org/abs/2508.09616</guid>
<content:encoded><![CDATA[
arXiv:2508.09616v2 Announce Type: replace-cross 
Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the "InDI" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols</title>
<link>https://arxiv.org/abs/2508.13220</link>
<guid>https://arxiv.org/abs/2508.13220</guid>
<content:encoded><![CDATA[
arXiv:2508.13220v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, and protection mechanisms to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. In addition, current protection mechanisms have little effect against these attacks. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging</title>
<link>https://arxiv.org/abs/2508.14053</link>
<guid>https://arxiv.org/abs/2508.14053</guid>
<content:encoded><![CDATA[
arXiv:2508.14053v2 Announce Type: replace-cross 
Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Chain-of-Thought Reasoning Across Languages</title>
<link>https://arxiv.org/abs/2508.14828</link>
<guid>https://arxiv.org/abs/2508.14828</guid>
<content:encoded><![CDATA[
arXiv:2508.14828v2 Announce Type: replace-cross 
Abstract: While large reasoning models have shown remarkable ability to generate long chains-of-thought (CoTs) in English, we still lack understanding of how these long-form reasoning abilities transfer to the vast majority of the world's languages. In this work, we systematically investigate four key stages of model development--scaling, pretraining, post-training, and inference--to understand how long CoT capabilities extend beyond English. We compare two reasoning settings across nine non-English target languages: En-CoT, where models process target-language inputs, but reason in English; and Target-CoT, where models both process inputs and generate long CoTs in the target language. We find that scaling reasoning model size improves multilingual task performance in En-CoT, but Target-CoT performance lags behind. This gap widens for tasks requiring long, multi-step CoTs such as mathematical reasoning. Shifting to pretraining, we find that adding a specialized reasoning stage enhances En-CoT performance but degrades Target-CoT, whereas broad multilingual pretraining improves both modes simultaneously. Given the scarcity of high-quality reasoning traces in languages other than English, we explore synthetic data curation approaches for post-training. We demonstrate that fine-tuning on reasoning traces automatically translated from gold English traces outperforms fine-tuning on target-language traces distilled from large reasoning models. Finally, we report disparities in inference efficiency between languages and uncover language-specific failure modes in CoTs. We release models, datasets, and code to foster further research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[
arXiv:2508.17364v2 Announce Type: replace-cross 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</title>
<link>https://arxiv.org/abs/2508.21099</link>
<guid>https://arxiv.org/abs/2508.21099</guid>
<content:encoded><![CDATA[
arXiv:2508.21099v2 Announce Type: replace-cross 
Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</title>
<link>https://arxiv.org/abs/2508.21589</link>
<guid>https://arxiv.org/abs/2508.21589</guid>
<content:encoded><![CDATA[
arXiv:2508.21589v4 Announce Type: replace-cross 
Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title>
<link>https://arxiv.org/abs/2509.00088</link>
<guid>https://arxiv.org/abs/2509.00088</guid>
<content:encoded><![CDATA[
arXiv:2509.00088v2 Announce Type: replace-cross 
Abstract: Prompt injection attacks pose a significant challenge to the safe deployment of Large Language Models (LLMs) in real-world applications. While prompt-based detection offers a lightweight and interpretable defense strategy, its effectiveness has been hindered by the need for manual prompt engineering. To address this issue, we propose AEGIS , an Automated co-Evolutionary framework for Guarding prompt Injections Schema. Both attack and defense prompts are iteratively optimized against each other using a gradient-like natural language prompt optimization technique. This framework enables both attackers and defenders to autonomously evolve via a Textual Gradient Optimization (TGO) module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our system on a real-world assignment grading dataset of prompt injection attacks and demonstrate that our method consistently outperforms existing baselines, achieving superior robustness in both attack success and detection. Specifically, the attack success rate (ASR) reaches 1.0, representing an improvement of 0.26 over the baseline. For detection, the true positive rate (TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and the true negative rate (TNR) remains comparable at 0.89. Ablation studies confirm the importance of co-evolution, gradient buffering, and multi-objective optimization. We also confirm that this framework is effective in different LLMs. Our results highlight the promise of adversarial training as a scalable and effective approach for guarding prompt injections.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Methane Detection Onboard Satellites</title>
<link>https://arxiv.org/abs/2509.00626</link>
<guid>https://arxiv.org/abs/2509.00626</guid>
<content:encoded><![CDATA[
arXiv:2509.00626v3 Announce Type: replace-cross 
Abstract: Methane is a potent greenhouse gas and a major driver of climate change, making its timely detection critical for effective mitigation. Machine learning (ML) deployed onboard satellites can enable rapid detection while reducing downlink costs, supporting faster response systems. Conventional methane detection methods often rely on image processing techniques, such as orthorectification to correct geometric distortions and matched filters to enhance plume signals. We introduce a novel approach that bypasses these preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We find that ML models trained on this dataset achieve performance comparable to those trained on orthorectified data. Moreover, we also train models on an orthorectified dataset, showing that they can outperform the matched filter baseline (mag1c). We release model checkpoints and two ML-ready datasets comprising orthorectified and unorthorectified hyperspectral images from the Earth Surface Mineral Dust Source Investigation (EMIT) sensor at https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at https://github.com/spaceml-org/plume-hunter.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Performance of Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2509.05258</link>
<guid>https://arxiv.org/abs/2509.05258</guid>
<content:encoded><![CDATA[
arXiv:2509.05258v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, very little information about the scaling performance and training considerations of these large training pipelines is released publicly. Working with very large datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation</title>
<link>https://arxiv.org/abs/2509.06694</link>
<guid>https://arxiv.org/abs/2509.06694</guid>
<content:encoded><![CDATA[
arXiv:2509.06694v3 Announce Type: replace-cross 
Abstract: While artificial neural networks are known as universal approximators for continuous functions, many modern approaches rely on overparameterized architectures with high computational cost. In this work, we introduce the Barycentric Neural Network (BNN): a compact shallow architecture that encodes both structure and parameters through a fixed set of base points and their associated barycentric coordinates. We show that the BNN enables the exact representation of continuous piecewise linear functions (CPLFs), ensuring strict continuity across segments. Given that any continuous function on a compact domain can be uniformly approximated by CPLFs, the BNN emerges as a flexible and interpretable tool for function approximation.
  To enhance geometric fidelity in low-resource scenarios, such as those with few base points to create BNNs or limited training epochs, we propose length-weighted persistent entropy (LWPE): a stable variant of persistent entropy. Our approach integrates the BNN with a loss function based on LWPE to optimize the base points that define the BNN, rather than its internal parameters. Experimental results show that our approach achieves superior and faster approximation performance compared to standard losses (MSE, RMSE, MAE and LogCosh), offering a computationally sustainable alternative for function approximation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title>
<link>https://arxiv.org/abs/2509.08729</link>
<guid>https://arxiv.org/abs/2509.08729</guid>
<content:encoded><![CDATA[
arXiv:2509.08729v3 Announce Type: replace-cross 
Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
arXiv:2509.08827v3 Announce Type: replace-cross 
Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation</title>
<link>https://arxiv.org/abs/2509.09685</link>
<guid>https://arxiv.org/abs/2509.09685</guid>
<content:encoded><![CDATA[
arXiv:2509.09685v4 Announce Type: replace-cross 
Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational music recommendation generated by an agentic data pipeline. In the proposed pipeline, multiple large language model (LLM) agents are created under various roles with specialized prompts and access to different parts of information, and the chat data is acquired by logging the conversation between the Listener LLM and the Recsys LLM. To cover various conversation scenarios, for each conversation, the Listener LLM is conditioned on a finetuned conversation goal. Finally, all the LLMs are multimodal with audio and images, allowing a simulation of multimodal recommendation and conversation. In the LLM-as-a-judge and subjective evaluation experiments, TalkPlayData 2 achieved the proposed goal in various aspects related to training a generative recommendation model for music. TalkPlayData 2 and its generation code are released at https://talkpl.ai/talkplaydata2.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification</title>
<link>https://arxiv.org/abs/2509.10510</link>
<guid>https://arxiv.org/abs/2509.10510</guid>
<content:encoded><![CDATA[
arXiv:2509.10510v2 Announce Type: replace-cross 
Abstract: Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets; however, standard GNNs often operate as black boxes, limiting transparency and usability, particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN. Source Code: https://github.com/basiralab/FireGNN
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking</title>
<link>https://arxiv.org/abs/2509.11552</link>
<guid>https://arxiv.org/abs/2509.11552</guid>
<content:encoded><![CDATA[
arXiv:2509.11552v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of language models by integrating external knowledge sources. However, document chunking as an important part of RAG system often lacks effective evaluation tools. This paper first analyzes why existing RAG evaluation benchmarks are inadequate for assessing document chunking quality, specifically due to evidence sparsity. Based on this conclusion, we propose HiCBench, which includes manually annotated multi-level document chunking points, synthesized evidence-dense quetion answer(QA) pairs, and their corresponding evidence sources. Additionally, we introduce the HiChunk framework, a multi-level document structuring framework based on fine-tuned LLMs, combined with the Auto-Merge retrieval algorithm to improve retrieval quality. Experiments demonstrate that HiCBench effectively evaluates the impact of different chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves better chunking quality within reasonable time consumption, thereby enhancing the overall performance of RAG systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducible workflow for online AI in digital health</title>
<link>https://arxiv.org/abs/2509.13499</link>
<guid>https://arxiv.org/abs/2509.13499</guid>
<content:encoded><![CDATA[
arXiv:2509.13499v2 Announce Type: replace-cross 
Abstract: Online artificial intelligence (AI) algorithms are an important component of digital health interventions. These online algorithms are designed to continually learn and improve their performance as streaming data is collected on individuals. Deploying online AI presents a key challenge: balancing adaptability of online AI with reproducibility. Online AI in digital interventions is a rapidly evolving area, driven by advances in algorithms, sensors, software, and devices. Digital health intervention development and deployment is a continuous process, where implementation - including the AI decision-making algorithm - is interspersed with cycles of re-development and optimization. Each deployment informs the next, making iterative deployment a defining characteristic of this field. This iterative nature underscores the importance of reproducibility: data collected across deployments must be accurately stored to have scientific utility, algorithm behavior must be auditable, and results must be comparable over time to facilitate scientific discovery and trustworthy refinement. This paper proposes a reproducible scientific workflow for developing, deploying, and analyzing online AI decision-making algorithms in digital health interventions. Grounded in practical experience from multiple real-world deployments, this workflow addresses key challenges to reproducibility across all phases of the online AI algorithm development life-cycle.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Correction to Mastery: Reinforced Distillation of Large Language Model Agents</title>
<link>https://arxiv.org/abs/2509.14257</link>
<guid>https://arxiv.org/abs/2509.14257</guid>
<content:encoded><![CDATA[
arXiv:2509.14257v2 Announce Type: replace-cross 
Abstract: Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification</title>
<link>https://arxiv.org/abs/2509.14830</link>
<guid>https://arxiv.org/abs/2509.14830</guid>
<content:encoded><![CDATA[
arXiv:2509.14830v2 Announce Type: replace-cross 
Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control</title>
<link>https://arxiv.org/abs/2509.15799</link>
<guid>https://arxiv.org/abs/2509.15799</guid>
<content:encoded><![CDATA[
arXiv:2509.15799v2 Announce Type: replace-cross 
Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing</title>
<link>https://arxiv.org/abs/2509.16622</link>
<guid>https://arxiv.org/abs/2509.16622</guid>
<content:encoded><![CDATA[
arXiv:2509.16622v2 Announce Type: replace-cross 
Abstract: Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER on test-clean/test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues</title>
<link>https://arxiv.org/abs/2509.17694</link>
<guid>https://arxiv.org/abs/2509.17694</guid>
<content:encoded><![CDATA[
arXiv:2509.17694v2 Announce Type: replace-cross 
Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding</title>
<link>https://arxiv.org/abs/2509.18085</link>
<guid>https://arxiv.org/abs/2509.18085</guid>
<content:encoded><![CDATA[
arXiv:2509.18085v2 Announce Type: replace-cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORPH: Shape-agnostic PDE Foundation Models</title>
<link>https://arxiv.org/abs/2509.21670</link>
<guid>https://arxiv.org/abs/2509.21670</guid>
<content:encoded><![CDATA[
arXiv:2509.21670v2 Announce Type: replace-cross 
Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</title>
<link>https://arxiv.org/abs/2509.22536</link>
<guid>https://arxiv.org/abs/2509.22536</guid>
<content:encoded><![CDATA[
arXiv:2509.22536v2 Announce Type: replace-cross 
Abstract: The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment</title>
<link>https://arxiv.org/abs/2509.22745</link>
<guid>https://arxiv.org/abs/2509.22745</guid>
<content:encoded><![CDATA[
arXiv:2509.22745v2 Announce Type: replace-cross 
Abstract: Recent large language models (LLMs) have increasingly adopted the Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. However, our analysis reveals that routing decisions for harmful inputs drift significantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less effective for MoE LLMs as they fail to prevent drift in harmful input routing. To address this limitation, we propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE directly mitigates routing drift by penalizing the gap between the routing weights of a fine-tuned model and those of the initial safety-aligned model, thereby preserving the safety-aligned routing of harmful inputs to safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as gpt-oss and Llama 4. Our implementation is available at https://anonymous.4open.science/r/SafeMoE.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D</title>
<link>https://arxiv.org/abs/2509.24528</link>
<guid>https://arxiv.org/abs/2509.24528</guid>
<content:encoded><![CDATA[
arXiv:2509.24528v2 Announce Type: replace-cross 
Abstract: 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sandbox Configurator: A Framework to Support Technical Assessment in AI Regulatory Sandboxes</title>
<link>https://arxiv.org/abs/2509.25256</link>
<guid>https://arxiv.org/abs/2509.25256</guid>
<content:encoded><![CDATA[
arXiv:2509.25256v3 Announce Type: replace-cross 
Abstract: The systematic assessment of AI systems is increasingly vital as these technologies enter high-stakes domains. To address this, the EU's Artificial Intelligence Act introduces AI Regulatory Sandboxes (AIRS): supervised environments where AI systems can be tested under the oversight of Competent Authorities (CAs), balancing innovation with compliance, particularly for startups and SMEs. Yet significant challenges remain: assessment methods are fragmented, tests lack standardisation, and feedback loops between developers and regulators are weak. To bridge these gaps, we propose the Sandbox Configurator, a modular open-source framework that enables users to select domain-relevant tests from a shared library and generate customised sandbox environments with integrated dashboards. Its plug-in architecture aims to support both open and proprietary modules, fostering a shared ecosystem of interoperable AI assessment services. The framework aims to address multiple stakeholders: CAs gain structured workflows for applying legal obligations; technical experts can integrate robust evaluation methods; and AI providers access a transparent pathway to compliance. By promoting cross-border collaboration and standardisation, the Sandbox Configurator's goal is to support a scalable and innovation-friendly European infrastructure for trustworthy AI governance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts</title>
<link>https://arxiv.org/abs/2509.26058</link>
<guid>https://arxiv.org/abs/2509.26058</guid>
<content:encoded><![CDATA[
arXiv:2509.26058v2 Announce Type: replace-cross 
Abstract: Electroencephalogram (EEG) artifact detection in real-world settings faces significant challenges such as computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and trade-offs between accuracy and complexity in deep learning models. We propose a hybrid spectral-temporal framework for real-time detection and classification of ocular (EOG), muscular (EMG), and white noise artifacts in single-channel EEG. This method, in contrast to other approaches, combines time-domain low-pass filtering (targeting low-frequency EOG) and frequency-domain power spectral density (PSD) analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature fusion to minimize redundancy while preserving discriminative information. This feature engineering strategy allows a lightweight multi-layer perceptron (MLP) architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB). Additionally, this framework addresses the unexplored problem of simultaneous multi-source contamination(EMG+EOG+white noise), where it maintains 96% classification accuracy despite overlapping artifacts. With 30-second training times (97% faster than CNNs) and robust performance across SNR levels, this framework bridges the gap between clinical applicability and computational efficiency, which enables real-time use in wearable brain-computer interfaces. This work also challenges the ubiquitous dependence on model depth for EEG artifact detection by demonstrating that domain-informed feature fusion surpasses complex architecture in noisy scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.26383</link>
<guid>https://arxiv.org/abs/2509.26383</guid>
<content:encoded><![CDATA[
arXiv:2509.26383v3 Announce Type: replace-cross 
Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction</title>
<link>https://arxiv.org/abs/2509.26633</link>
<guid>https://arxiv.org/abs/2509.26633</guid>
<content:encoded><![CDATA[
arXiv:2509.26633v2 Announce Type: replace-cross 
Abstract: A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Identification via the Empirical NTK</title>
<link>https://arxiv.org/abs/2510.00468</link>
<guid>https://arxiv.org/abs/2510.00468</guid>
<content:encoded><![CDATA[
arXiv:2510.00468v2 Announce Type: replace-cross 
Abstract: We provide evidence that eigenanalysis of the empirical neural tangent kernel (eNTK) can surface the features used by trained neural networks. Across two standard toy models for mechanistic interpretability, Toy Models of Superposition (TMS) and a 1-layer MLP trained on modular addition, we find that the eNTK exhibits sharp spectral cliffs whose top eigenspaces align with ground-truth features. In TMS, the eNTK recovers the ground-truth features in both the sparse (high superposition) and dense regimes. In modular arithmetic, the eNTK can be used to recover Fourier feature families. Moreover, we provide evidence that a layerwise eNTK localizes features to specific layers and that the evolution of the eNTK spectrum can be used to diagnose the grokking phase transition. These results suggest that eNTK analysis may provide a practical handle for feature discovery and for detecting phase changes in small models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Panorama: Fast-Track Nearest Neighbors</title>
<link>https://arxiv.org/abs/2510.00566</link>
<guid>https://arxiv.org/abs/2510.00566</guid>
<content:encoded><![CDATA[
arXiv:2510.00566v2 Announce Type: replace-cross 
Abstract: Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99\% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90\% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason for Hallucination Span Detection</title>
<link>https://arxiv.org/abs/2510.02173</link>
<guid>https://arxiv.org/abs/2510.02173</guid>
<content:encoded><![CDATA[
arXiv:2510.02173v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</title>
<link>https://arxiv.org/abs/2510.02227</link>
<guid>https://arxiv.org/abs/2510.02227</guid>
<content:encoded><![CDATA[
arXiv:2510.02227v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models</title>
<link>https://arxiv.org/abs/2510.02300</link>
<guid>https://arxiv.org/abs/2510.02300</guid>
<content:encoded><![CDATA[
arXiv:2510.02300v2 Announce Type: replace-cross 
Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Video: Automatic Video Generation from Scientific Papers</title>
<link>https://arxiv.org/abs/2510.05096</link>
<guid>https://arxiv.org/abs/2510.05096</guid>
<content:encoded><![CDATA[
arXiv:2510.05096v2 Announce Type: replace-cross 
Abstract: Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal reasoning, large language models, real-world challenges, benchmark, social interactions <br />
Summary: <br />
Temporal reasoning is essential for Large Language Models (LLMs) to understand the real world, but current approaches do not adequately address the challenges of dealing with abundant temporal information, rapidly changing event dynamics, and complex dependencies in social interactions. To address this gap, a multi-level benchmark called TIME has been introduced, featuring 38,522 QA pairs across three levels and 11 sub-tasks. The benchmark includes three sub-datasets - TIME-Wiki, TIME-News, and TIME-Dial - each reflecting different real-world challenges. Extensive experiments were conducted on both reasoning and non-reasoning models, analyzing temporal reasoning performance across diverse scenarios and tasks. The impact of test-time scaling on temporal reasoning capabilities was also investigated. Additionally, TIME-Lite, a human-annotated subset, has been released to facilitate future research and standardized evaluation in temporal reasoning. The code, dataset, and project page are available for further exploration and use. <br /> <div>
arXiv:2505.12891v4 Announce Type: replace 
Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME , and the project page link is https://sylvain-wei.github.io/TIME/ .
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity</title>
<link>https://arxiv.org/abs/2509.20293</link>
<guid>https://arxiv.org/abs/2509.20293</guid>
<content:encoded><![CDATA[
<div> benchmark, LLM-judged, schematic adherence, psychometric validity, factor collapse  
Summary:  
The article discusses the use of LLM-judged benchmarks to evaluate complex model behaviors, highlighting the design issues that can lead to unreliable rankings. Two diagnostic mechanisms, schematic adherence and psychometric validity, are introduced to identify inconsistencies and uncertainty in benchmarking runs. A case study on Arena-Hard Auto reveals severe schema incoherence and factor collapse among popular judges, indicating high levels of unexplained variance and factor correlations. The study also shows that the ELO-style aggregation method used by Arena-Hard Auto masks genuine ranking uncertainty. The results underscore the importance of building better-scoped, reliability-aware LLM-judged benchmarks to ensure validity in model evaluations. The code and dataset for the study are available on GitHub for further exploration.  
Summary: <div>
arXiv:2509.20293v3 Announce Type: replace-cross 
Abstract: LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We released our code and dataset at https://github.com/penfever/judgment-to-noise
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</title>
<link>https://arxiv.org/abs/2510.06261</link>
<guid>https://arxiv.org/abs/2510.06261</guid>
<content:encoded><![CDATA[
<div> Keywords: AlphaApollo, self-evolving reasoning system, foundation model, computation tool, retrieval tool <br />
Summary: 
AlphaApollo is a self-evolving agentic reasoning system designed to overcome limitations in foundation models (FMs). It addresses bottlenecks such as model capacity and test-time iteration reliability by using professional tools like Python with numerical and symbolic libraries for computation and task-relevant external information for decision-making. The system supports multi-round, multi-model solution evolution through a shared state map that facilitates iterative refinement. Evaluations on AIME 2024/2025 show consistent performance gains across multiple models, with improved Average@32 and Pass@32 metrics. Tool-use analysis reveals a high success rate for tool calls, outperforming non-tool baselines and expanding the capability of FMs. Further details and results can be found at the official AlphaApollo Github repository. <br /><br />Summary: <div>
arXiv:2510.06261v1 Announce Type: new 
Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization</title>
<link>https://arxiv.org/abs/2510.06274</link>
<guid>https://arxiv.org/abs/2510.06274</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, reasoning, generalization, Complexity OoD, Kolmogorov complexity

Summary:
Complexity Out of Distribution (Complexity OoD) generalization framework is proposed to define and measure reasoning ability in AI models. It focuses on maintaining performance on test instances with higher solution complexity than training examples. The concept of complexity is formalized using solution description Kolmogorov complexity and operational proxies. This approach unifies learning and reasoning, highlighting the transition from System1 to System2 processing under complexity pressure. Recommendations are provided for incorporating complexity into benchmark design, enhancing supervision targeting solution traces, and addressing learning to reason challenges. Scaling data alone is insufficient for solving Complexity OoD, requiring architectures and training regimes that explicitly model and allocate computation based on complexity. Progress toward robust reasoning involves designing inductive biases and addressing spurious shortcuts, semantic robustness, catastrophic forgetting, and step-wise calibration. <div>
arXiv:2510.06274v1 Announce Type: new 
Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuilderBench -- A benchmark for generalist agents</title>
<link>https://arxiv.org/abs/2510.06288</link>
<guid>https://arxiv.org/abs/2510.06288</guid>
<content:encoded><![CDATA[
<div> Keywords: AI models, exploration, learning through interaction, BuilderBench, embodied reasoning

Summary: 
BuilderBench is introduced as a benchmark for accelerating research in agent pre-training focused on open-ended exploration. The benchmark requires agents to learn how to build various structures using blocks, with a task suite containing over 42 diverse target structures. Agents must explore and learn general principles about the environment without external supervision, testing their understanding of physics, mathematics, and long-horizon planning. The tasks in BuilderBench challenge current algorithms, highlighting the need for embodied reasoning and experimentation. A "training wheels" protocol is provided for agents to build a single target structure from the task suite. Additionally, single-file implementations of six different algorithms are provided as a reference for researchers. The goal is to enable agents to learn through interaction and exploration, moving beyond mimicry and sharpening to solve novel problems. 

<br /><br />Summary: <div>
arXiv:2510.06288v1 Announce Type: new 
Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is not surprising that they struggle to solve problems beyond the limits set by existing data. To solve novel problems, agents should acquire skills for exploring and learning through experience. Finding a scalable learning mechanism for developing agents that learn through interaction remains a major open problem. In this work, we introduce BuilderBench, a benchmark to accelerate research into agent pre-training that centers open-ended exploration. BuilderBench requires agents to learn how to build any structure using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated simulator of a robotic agent interacting with various physical blocks, and $(2)$ a task-suite with over 42 diverse target structures that are carefully curated to test an understanding of physics, mathematics, and long-horizon planning. During training, agents have to explore and learn general principles about the environment without any external supervision. During evaluation, agents have to build the unseen target structures from the task suite. Solving these tasks requires a sort of \emph{embodied reasoning} that is not reflected in words but rather in actions, experimenting with different strategies and piecing them together. Our experiments show that many of these tasks challenge the current iteration of algorithms. Hence, we also provide a ``training wheels'' protocol, in which agents are trained and evaluated to build a single target structure from the task suite. Finally, we provide single-file implementations of six different algorithms as a reference point for researchers.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration</title>
<link>https://arxiv.org/abs/2510.06302</link>
<guid>https://arxiv.org/abs/2510.06302</guid>
<content:encoded><![CDATA[
<div> Keywords: post-merger integration, information system integration, game-based learning design, AMILI, AMILP

Summary:
Post-merger integration presents challenges for information system integration, lacking training in this area. The study proposes addressing this gap through game-based learning design. By transforming static method training into an engaging learning experience, the study aims to improve the effectiveness of training for information system integration in post-merger scenarios. Analyzing foundational learning theories, cognitive load, motivation models, and serious game design frameworks, essential requirements for a game-based learning design framework are identified. The framework will focus on the transformation process and resulting learning experience to enhance learner engagement and effectiveness. The paper outlines a plan for developing and evaluating the proposed framework through iterative design and real-world validation. <br /><br />Summary: <div>
arXiv:2510.06302v1 Announce Type: new 
Abstract: Post-merger integration states unique challenges for professionals responsible for information system integration aimed on alignment and combination diverse system architectures of merging organizations. Although the theoretical and practical guidance exists for post-merger integration on the business level, there is a significant gap in training for information system integration in this context. In prior research specific methods AMILI (Support method for informed decision identification) and AMILP (Support method for informed decision-making) were introduced for the support of information system integration decisions in the post-merger integration. But during the practical application was reported high learning curve and low learner motivation. This paper explores how game-based learning design can address these limitations by transforming static method training into engaging learning experience. The study analyzes foundational learning theories, cognitive load and motivation models, and serious game design frameworks to identify the essential requirements for a game-based learning design framework tailored to information system integration in post-merger integration. Requirements are structured in two components: the transformation process and resulting learning experience. The paper concludes with a plan for developing and evaluating the proposed framework through iterative design and real-world validation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks</title>
<link>https://arxiv.org/abs/2510.06307</link>
<guid>https://arxiv.org/abs/2510.06307</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent system, natural language processing, consensus-seeking, optimal collaborators, Belief-Calibrated Consensus Seeking (BCCS)

Summary:
A new approach called Belief-Calibrated Consensus Seeking (BCCS) is introduced to enhance the capacity of multi-agent systems in solving complex natural language processing tasks. This framework focuses on selecting optimal collaborators to maximize consensus stability by calibrating the consensus judgment based on system-internal beliefs. Existing consensus-seeking methods often overlook contradictions in system-internal beliefs, leading to instability in the consensus. By providing a theoretical framework for optimal collaborator selection, the BCCS framework outperforms existing approaches in accuracy on challenging tasks. Experimental results on benchmark datasets show significant improvements, making the BCCS framework a promising solution for enhancing collaboration among multiple agents in NLP tasks. The code and data for the BCCS framework are available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2510.06307v1 Announce Type: new 
Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural language processing (NLP) tasks through collaboration among multiple agents, where consensus-seeking serves as a fundamental mechanism. However, existing consensus-seeking approaches typically rely on voting mechanisms to judge consensus, overlooking contradictions in system-internal beliefs that destabilize the consensus. Moreover, these methods often involve agents updating their results through indiscriminate collaboration with every other agent. Such uniform interaction fails to identify the optimal collaborators for each agent, hindering the emergence of a stable consensus. To address these challenges, we provide a theoretical framework for selecting optimal collaborators that maximize consensus stability. Based on the theorems, we propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate stable consensus via selecting optimal collaborators and calibrating the consensus judgment by system-internal beliefs. Experimental results on the MATH and MMLU benchmark datasets demonstrate that the proposed BCCS framework outperforms the best existing results by 2.23% and 3.95% of accuracy on challenging tasks, respectively. Our code and data are available at https://github.com/dengwentao99/BCCS.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?</title>
<link>https://arxiv.org/abs/2510.06410</link>
<guid>https://arxiv.org/abs/2510.06410</guid>
<content:encoded><![CDATA[
<div> Collaboration, Reasoning LLMs, Off-trajectory reasoning, Recoverability, Guidability

Summary:
This study investigates the ability of Reasoning LLMs to engage in off-trajectory reasoning by collaborating on each other's thinking within shared trajectories. The authors propose twin tests to evaluate the recoverability and guidability of LLMs, finding that stronger LLMs on benchmarks are more fragile under distractions. All models tested fail to effectively leverage guidance from collaborators beyond their inherent capabilities. Control studies isolate the effects of post-training factors on LLM behaviors, revealing that suboptimal recoverability behaviors of teacher models are transferred to student models even with correct distillation trajectories. The study provides insights for training strong reasoning collaborators and highlights the limitations of off-the-shelf reasoning LLMs. 

<br /><br />Summary: <div>
arXiv:2510.06410v1 Announce Type: new 
Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajectory, yielding better inference efficiency and exploration. A key prerequisite, however, is the ability to assess the usefulness and build on another model's partial thinking -- we call this off-trajectory reasoning. Our paper investigates a critical question: can standard solo-reasoning training pipelines deliver desired off-trajectory behaviors? We propose twin tests that capture the two extremes of the off-trajectory spectrum, namely Recoverability, which tests whether LLMs can backtrack from "distractions" induced by misleading reasoning traces, and Guidability, which tests their ability to build upon correct reasoning from stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often more fragile under distraction. Moreover, all models tested fail to effectively leverage guiding steps from collaborators on problems beyond their inherent capabilities with solve rates remaining under 9.2%. Finally, we conduct control studies to isolate the effects of three factors in post-training on these behaviors: the choice of distillation teacher, the use of RL, and data selection strategy. Our results provide actionable insights for training natively strong reasoning collaborators; e.g., we find that suboptimal recoverability behaviors of teacher models are transferred to distilled students even if the distillation trajectories are correct. Taken together, this work lays the groundwork for evaluating multi-model collaborations in shared reasoning trajectories and highlights the limitations of off-the-shelf reasoning LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health</title>
<link>https://arxiv.org/abs/2510.06433</link>
<guid>https://arxiv.org/abs/2510.06433</guid>
<content:encoded><![CDATA[
<div> Semantic web, knowledge graph, flavonoid contents, cancer connections, machine-operable format

Summary:
This study focuses on representing the relationship between food and health in a standardized, machine-readable format using a semantic web and knowledge graph. By combining information from USDA databases on flavonoid contents of food and cancer connections from the literature, the study creates a knowledge graph using the KNARM methodology. This knowledge graph enables researchers to explore the complex interplay between dietary choices and disease management. Future work includes expanding the knowledge graph's scope, capturing nuances, adding related data, and performing inferences to uncover hidden relationships. <br /><br />Summary: <div>
arXiv:2510.06433v1 Announce Type: new 
Abstract: The focus on "food as medicine" is gaining traction in the field of health and several studies conducted in the past few years discussed this aspect of food in the literature. However, very little research has been done on representing the relationship between food and health in a standardized, machine-readable format using a semantic web that can help us leverage this knowledge effectively. To address this gap, this study aims to create a knowledge graph to link food and health through the knowledge graph's ability to combine information from various platforms focusing on flavonoid contents of food found in the USDA databases and cancer connections found in the literature. We looked closely at these relationships using KNARM methodology and represented them in machine-operable format. The proposed knowledge graph serves as an example for researchers, enabling them to explore the complex interplay between dietary choices and disease management. Future work for this study involves expanding the scope of the knowledge graph by capturing nuances, adding more related data, and performing inferences on the acquired knowledge to uncover hidden relationships.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles</title>
<link>https://arxiv.org/abs/2510.06475</link>
<guid>https://arxiv.org/abs/2510.06475</guid>
<content:encoded><![CDATA[
<div> benchmark, reasoning, planning, scalability, puzzles

Summary:
The study introduces PuzzlePlex, a benchmark designed to evaluate the reasoning and planning abilities of foundation models in complex environments. PuzzlePlex includes 15 types of puzzles of varying difficulty, such as deterministic and stochastic games, single-player, and two-player scenarios. Customized game-playing strategies are implemented for comparison. Fine-grained metrics are developed to measure performance across instruction-based and code-based settings. Reasoning models perform better in instruction-based settings, while code-based execution presents challenges but offers scalability. The study systematically investigates the scaling limits of foundation models in PuzzlePlex, providing insights into future improvements in reasoning, planning, and generalization for foundation models.<br /><br />Summary: <div>
arXiv:2510.06475v1 Announce Type: new 
Abstract: This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them</title>
<link>https://arxiv.org/abs/2510.06534</link>
<guid>https://arxiv.org/abs/2510.06534</guid>
<content:encoded><![CDATA[
<div> reasoning behaviors, agentic search, large language models, behavior priming, reinforcement learning 

Summary:<br />
- The paper presents a reasoning-driven pipeline for agentic search using large language models. 
- Successful agentic search trajectories exhibit four beneficial reasoning behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery.
- The technique of Behavior Priming is proposed to train more effective agentic search models by integrating desired reasoning behaviors through supervised fine-tuning and reinforcement learning.
- Experiments show a significant performance improvement using behavior priming compared to direct training with reinforcement learning.
- The critical factor for strong final performance is the desired reasoning behaviors in the supervised fine-tuning data, rather than the correctness of the final answer. Effective reasoning behaviors lead to better exploration and scaling capabilities, providing a strong foundation for reinforcement learning. The code for the proposed technique will be made available as open source. <div>
arXiv:2510.06534v1 Announce Type: new 
Abstract: Agentic search leverages large language models (LLMs) to interpret complex user information needs and execute a multi-step process of planning, searching, and synthesizing information to provide answers. This paradigm introduces unique challenges for LLMs' reasoning and agentic capabilities when interacting with retrieval systems and the broader web. In this paper, we propose a reasoning-driven LLM-based pipeline to study effective reasoning behavior patterns in agentic search. Using this pipeline, we analyze successful agentic search trajectories and identify four beneficial reasoning behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery. Based on these findings, we propose a technique called Behavior Priming to train more effective agentic search models. It synthesizes agentic search trajectories that exhibit these four behaviors and integrates them into the agentic search model through supervised fine-tuning (SFT), followed by standard reinforcement learning (RL). Experiments on three benchmarks (GAIA, WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models with RL. Crucially, we demonstrate that the desired reasoning behaviors in the SFT data, rather than the correctness of the final answer, is the critical factor for achieving strong final performance after RL: fine-tuning on trajectories with desirable reasoning behaviors but incorrect answers leads to better performance than fine-tuning on trajectories with correct answers. Our analysis further reveals the underlying mechanism: the introduced reasoning behaviors endow models with more effective exploration (higher pass@k and entropy) and test-time scaling (longer trajectories) capabilities, providing a strong foundation for RL. Our code will be released as open source.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Prompt Ensemble for LLM Judge</title>
<link>https://arxiv.org/abs/2510.06538</link>
<guid>https://arxiv.org/abs/2510.06538</guid>
<content:encoded><![CDATA[
<div> framework, reliability, LLM judges, evaluation dimensions, auxiliary 

Summary: 
The article introduces a new framework called Auto-Prompt Ensemble (APE) that aims to enhance the reliability of Large Language Model (LLM) judges by incorporating additional evaluation dimensions. LLM judges often fail to capture implicit assessment standards, leading to missed crucial evaluation dimensions. APE addresses this by automatically learning evaluation dimensions from failure cases and using a confidence-based ensemble mechanism called Collective Confidence to determine when to include additional evaluation dimensions. Through extensive experiments, APE is shown to improve the reliability of LLM Judge across various benchmarks, such as enhancing GPT-4o agreement rate on Reward Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a structured approach for LLM judges to leverage test-time computation and bridge the gap between human and LLM assessments. 

Summary: <br /><br /> <div>
arXiv:2510.06538v1 Announce Type: new 
Abstract: We present a novel framework that improves the reliability of LLM judges by selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM judges often miss crucial evaluation dimensions because they fail to recognize the implicit standards underlying human assessments. To address this challenge, we propose the Auto-Prompt Ensemble (APE), an adaptive framework that automatically learns evaluation dimensions from its failure cases. APE incorporates a confidence-based ensemble mechanism to decide when to adopt the judgments from additional evaluation dimensions through a novel confidence estimation approach called Collective Confidence. Extensive experiments demonstrate that APE improves the reliability of LLM Judge across diverse standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a principled approach for LLM Judge to leverage test-time computation, and bridge the evaluation gap between human and LLM judges.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks</title>
<link>https://arxiv.org/abs/2510.06587</link>
<guid>https://arxiv.org/abs/2510.06587</guid>
<content:encoded><![CDATA[
<div> framework, Large language model, WebDART, complex chores, web tasks<br />
Summary:<br />
The article introduces WebDART, a framework designed to enhance the capabilities of Large Language Model (LLM) agents in handling complex web tasks. WebDART dynamically breaks down objectives into navigation, information extraction, and execution subtasks, allowing the model to focus on one skill at a time. It continuously adjusts this decomposition as new webpages are encountered, optimizing exploration and utilizing newly discovered filters or shortcuts. In evaluations on WebChoreArena, WebDART significantly improves success rates compared to previous state-of-the-art agents, particularly in long-horizon navigation tasks. While maintaining performance levels on simpler tasks in the WebArena suite, WebDART completes objectives with fewer navigation steps, showcasing its effectiveness in tackling challenging web tasks.<br /> 
Summary: <div>
arXiv:2510.06587v1 Announce Type: new 
Abstract: Large language model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long horizon navigation, large scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks: navigation, information extraction, and execution, so the model concentrates on one skill at a time, and (ii) continuously replans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts success rates by up to 13.7 percentage points over previous SOTA agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Emotion Recognition via In-Context Learning</title>
<link>https://arxiv.org/abs/2510.06600</link>
<guid>https://arxiv.org/abs/2510.06600</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion recognition, fine-grained, decision-making, prototype theory, In-Context Learning

Summary:
Emotion recognition is vital in various systems, using In-Context Learning (ICL) to enhance reasoning processes. However, ICL methods often overlook the decision-making process, leading to errors in emotion recognition. This paper explores decision-making in fine-grained emotion recognition using prototype theory. Emotion In-Context Learning (EICL) is proposed to improve query representations by incorporating emotionally similar examples and utilizing a dynamic soft-label strategy. The approach outperforms ICL by optimizing both reasoning and decision-making processes through a two-stage exclusion strategy, leading to improved accuracy in emotion recognition on multiple datasets. EICL addresses the challenge of emotional discrepancies introduced by semantically similar examples, allowing for more accurate representations and enhanced decision-making. <br /><br />Summary: <div>
arXiv:2510.06600v1 Announce Type: new 
Abstract: Fine-grained emotion recognition aims to identify the emotional type in queries through reasoning and decision-making processes, playing a crucial role in various systems. Recent methods use In-Context Learning (ICL), enhancing the representation of queries in the reasoning process through semantically similar examples, while further improving emotion recognition by explaining the reasoning mechanisms. However, these methods enhance the reasoning process but overlook the decision-making process. This paper investigates decision-making in fine-grained emotion recognition through prototype theory. We show that ICL relies on similarity matching between query representations and emotional prototypes within the model, where emotion-accurate representations are critical. However, semantically similar examples often introduce emotional discrepancies, hindering accurate representations and causing errors. To address this, we propose Emotion In-Context Learning (EICL), which introduces emotionally similar examples and uses a dynamic soft-label strategy to improve query representations in the emotion reasoning process. A two-stage exclusion strategy is then employed to assess similarity from multiple angles, further optimizing the decision-making process. Extensive experiments show that EICL significantly outperforms ICL on multiple datasets.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support</title>
<link>https://arxiv.org/abs/2510.06674</link>
<guid>https://arxiv.org/abs/2510.06674</guid>
<content:encoded><![CDATA[
<div> Agent-in-the-Loop, LLM-based customer support system, iterative improvement, live customer operations, human feedback loops <br />
<br />
Summary: 
An Agent-in-the-Loop (AITL) framework is introduced to continuously enhance an LLM-based customer support system by integrating various types of annotations directly into live customer operations. These annotations include pairwise response preferences, agent adoption and rationales, knowledge relevance checks, and identification of missing knowledge. By incorporating these feedback signals into model updates, retraining cycles are significantly reduced from months to weeks. A production pilot with US-based customer support agents showed notable enhancements in retrieval accuracy, generation quality, and agent adoption rates. The results highlight the efficiency of embedding human feedback loops into operational workflows for constant refinement of LLM-based customer support systems. <div>
arXiv:2510.06674v1 Announce Type: new 
Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a continuous data flywheel for iteratively improving an LLM-based customer support system. Unlike standard offline approaches that rely on batch annotations, AITL integrates four key types of annotations directly into live customer operations: (1) pairwise response preferences, (2) agent adoption and rationales, (3) knowledge relevance checks, and (4) identification of missing knowledge. These feedback signals seamlessly feed back into models' updates, reducing retraining cycles from months to weeks. Our production pilot involving US-based customer support agents demonstrated significant improvements in retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality (+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore the effectiveness of embedding human feedback loops directly into operational workflows to continuously refine LLM-based customer support system.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inefficiencies of Meta Agents for Agent Design</title>
<link>https://arxiv.org/abs/2510.06711</link>
<guid>https://arxiv.org/abs/2510.06711</guid>
<content:encoded><![CDATA[
<div> Keywords: automated design, meta-agents, agent architectures, behavioral diversity, economic viability

Summary: 
Automated design of agentic systems using meta-agents faces several challenges. Firstly, incorporating all previous agent designs for learning across iterations does not perform well; an evolutionary approach yields better results. Secondly, while meta-agents design multiple agents during training, they often select only one for deployment, resulting in low behavioral diversity among the designed agents. Additionally, the high cost of designing and deploying automated agents may only be economically viable for specific datasets. Cost analysis shows that the overall cost of automated agent design is lower than that of human-designed agents for only two datasets when deployed on a large scale, indicating limited economic benefits for other datasets. This study emphasizes the importance of addressing these challenges to enhance the practicality and cost-effectiveness of automated design in agentic systems. 

Summary: <div>
arXiv:2510.06711v1 Announce Type: new 
Abstract: Recent works began to automate the design of agentic systems using meta-agents that propose and iteratively refine new agent architectures. In this paper, we examine three key challenges in a common class of meta-agents. First, we investigate how a meta-agent learns across iterations and find that simply expanding the context with all previous agents, as proposed by previous works, performs worse than ignoring prior designs entirely. We show that the performance improves with an evolutionary approach. Second, although the meta-agent designs multiple agents during training, it typically commits to a single agent at test time. We find that the designed agents have low behavioral diversity, limiting the potential for their complementary use. Third, we assess when automated design is economically viable. We find that only in a few cases--specifically, two datasets--the overall cost of designing and deploying the agents is lower than that of human-designed agents when deployed on over 15,000 examples. In contrast, the performance gains for other datasets do not justify the design cost, regardless of scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.06742</link>
<guid>https://arxiv.org/abs/2510.06742</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Knowledge Graphs, Biomedical Sciences, Cognitive Processes, MultiCNKG 
Summary: 
- Introduction of MultiCNKG framework integrating Cognitive Neuroscience Knowledge Graph (CNKG), Gene Ontology (GO), and Disease Ontology (DO)
- Utilization of LLMs like GPT-4 for entity alignment, semantic similarity computation, and graph augmentation
- Creation of a cohesive KG merging genetic mechanisms, neurological disorders, and cognitive functions
- Multi-layered view from molecular to behavioral domains with 6.9K nodes and 11.3K edges 
- Assessments demonstrating robustness, coherence, and competitive performance in link prediction tasks against benchmarks <br /><br />Summary: <div>
arXiv:2510.06742v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has revolutionized the integration of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming limitations in traditional machine learning methods for capturing intricate semantic links among genes, diseases, and cognitive processes. We introduce MultiCNKG, an innovative framework that merges three key knowledge sources: the Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO) comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types. Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity computation, and graph augmentation to create a cohesive KG that interconnects genetic mechanisms, neurological disorders, and cognitive functions. The resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes, Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes, Associated with, Regulates), facilitating a multi-layered view from molecular to behavioral domains. Assessments using metrics such as precision (85.20%), recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty detection (40.28%), and expert validation (89.50%) affirm its robustness and coherence. Link prediction evaluations with models like TransE (MR: 391, MRR: 0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against benchmarks like FB15k-237 and WN18RR. This KG advances applications in personalized medicine, cognitive disorder diagnostics, and hypothesis formulation in cognitive neuroscience.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Memoryless Sequential Decision-making of Large Language Models</title>
<link>https://arxiv.org/abs/2510.06756</link>
<guid>https://arxiv.org/abs/2510.06756</guid>
<content:encoded><![CDATA[
<div> tool, verification, language model, sequential decision-making, safety property

Summary:
A new tool has been introduced for the rigorous and automated verification of large language model (LLM)-based policies in memoryless sequential decision-making tasks. The tool constructs the reachable portion of the Markov decision process guided by the LLM's chosen actions and checks if the policy satisfies specified safety requirements expressed as a PCTL formula. Each state is represented as a natural language prompt, and the LLM's response is parsed into an action to expand reachable successor states. Experiments on standard grid world benchmarks show that LLMs accessed via Ollama can be verified when deterministically seeded but generally perform under deep reinforcement learning baselines. The tool integrates with Ollama and supports PRISM-specified tasks, enabling continuous benchmarking in user-specified sequential decision-making tasks and laying a practical foundation for formally verifying increasingly capable LLMs. 

<br /><br />Summary: <div>
arXiv:2510.06756v1 Announce Type: new 
Abstract: We introduce a tool for rigorous and automated verification of large language model (LLM)- based policies in memoryless sequential decision-making tasks. Given a Markov decision process (MDP) representing the sequential decision-making task, an LLM policy, and a safety requirement expressed as a PCTL formula, our approach incrementally constructs only the reachable portion of the MDP guided by the LLM's chosen actions. Each state is encoded as a natural language prompt, the LLM's response is parsed into an action, and reachable successor states by the policy are expanded. The resulting formal model is checked with Storm to determine whether the policy satisfies the specified safety property. In experiments on standard grid world benchmarks, we show that open source LLMs accessed via Ollama can be verified when deterministically seeded, but generally underperform deep reinforcement learning baselines. Our tool natively integrates with Ollama and supports PRISM-specified tasks, enabling continuous benchmarking in user-specified sequential decision-making tasks and laying a practical foundation for formally verifying increasingly capable LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.06761</link>
<guid>https://arxiv.org/abs/2510.06761</guid>
<content:encoded><![CDATA[
<div> Evolutionary algorithm, research plans, multi-agent framework, automated evaluation, ablation studies  
Summary:   
The article introduces the Double-Loop Multi-Agent (DLMA) framework for automating the scientific research process. The framework consists of two loops, with professor agents evolving research plans in the leader loop, and doctoral student agents executing the plans in the follower loop. Through evolutionary algorithms and dynamic plan adjustments, the DLMA framework generates high-quality research papers that outperform baselines in automated evaluation. Ablation studies confirm the importance of both loops in driving novelty and ensuring soundness throughout the research process. The proposed approach addresses the challenges of automated research by effectively combining high-level planning with adaptable execution, resulting in state-of-the-art research outcomes.  
<br /><br />Summary: <div>
arXiv:2510.06761v1 Announce Type: new 
Abstract: Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoformalizer with Tool Feedback</title>
<link>https://arxiv.org/abs/2510.06857</link>
<guid>https://arxiv.org/abs/2510.06857</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoformalization, Automated Theorem Proving, Formalizer, Lean 4 compilers, Multi-LLMs-as-judge approach

Summary: 
Autoformalization is crucial for Automated Theorem Proving (ATP) due to data scarcity. The Autoformalizer with Tool Feedback (ATF) is a novel approach that incorporates syntax and consistency tools to improve formalization. By using Lean 4 compilers for syntax corrections and multiple language models for consistency validation, ATF refines generated statements. The training process involves a cold-start phase, expert iteration phase, and Direct Preference Optimization. Experimental results demonstrate ATF outperforms baseline models and human evaluations confirm its superiority. ATF also exhibits strong inference scaling properties. The open-source Numina-ATF dataset containing 750K synthetic formal statements is provided to drive advancements in autoformalization and ATP research. 

<br /><br />Summary: <div>
arXiv:2510.06857v1 Announce Type: new 
Abstract: Autoformalization addresses the scarcity of data for Automated Theorem Proving (ATP) by translating mathematical problems from natural language into formal statements. Efforts in recent work shift from directly prompting large language models to training an end-to-end formalizer model from scratch, achieving remarkable advancements. However, existing formalizer still struggles to consistently generate valid statements that meet syntactic validity and semantic consistency. To address this issue, we propose the Autoformalizer with Tool Feedback (ATF), a novel approach that incorporates syntactic and consistency information as tools into the formalization process. By integrating Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge approach for consistency validation, the model is able to adaptively refine generated statements according to the tool feedback, enhancing both syntactic validity and semantic consistency. The training of ATF involves a cold-start phase on synthetic tool-calling data, an expert iteration phase to improve formalization capabilities, and Direct Preference Optimization to alleviate ineffective revisions. Experimental results show that ATF markedly outperforms a range of baseline formalizer models, with its superior performance further validated by human evaluations. Subsequent analysis reveals that ATF demonstrates excellent inference scaling properties. Moreover, we open-source Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate advancements in autoformalization and ATP research.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs</title>
<link>https://arxiv.org/abs/2510.06878</link>
<guid>https://arxiv.org/abs/2510.06878</guid>
<content:encoded><![CDATA[
<div> Keywords: iterative refinement, large language models, Tree-Guided Policy Refinement, Thompson-Sampling-based tree search, stateful reasoning<br />
Summary:<br />
Iterative refinement is a promising approach for improving the problem-solving capabilities of large language models (LLMs). However, effectively searching through the vast space of possible refinements poses a significant challenge. The Tree-Guided Policy Refinement (TGPR) framework introduced in this study combines GRPO with a Thompson-Sampling-based tree search to actively explore both failed and successful refinement paths, leading to more adaptive policies. Testing on various benchmarks showed that TGPR outperformed a competitive GRPO baseline, achieving significant improvements in pass rates. By focusing on combining learned policies with structured search methods, TGPR provides a principled approach for enhancing iterative refinement and stateful reasoning in LLMs. <div>
arXiv:2510.06878v1 Announce Type: new 
Abstract: Iterative refinement has been a promising paradigm to enable large language models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of the key challenges, however, is how to effectively search through the enormous search space of possible refinements. Existing methods typically fall back on predefined heuristics, which are troubled by the exploration-exploitation dilemma and cannot adapt based on past refinement outcomes. We introduce Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with a Thompson-Sampling-based tree search. TGPR explores both failed and successful refinement paths actively, with denser training trajectories and more adaptive policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to +4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to +12.51 percentage points absolute improvement in pass@10 (on APPS) compared to a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN</title>
<link>https://arxiv.org/abs/2510.06911</link>
<guid>https://arxiv.org/abs/2510.06911</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic Web, multi-agent systems, RDF/RDFS, OWL, Large Language Models

Summary:
The article discusses the challenges of modeling agent behaviors using semantic Web standards in the AJAN framework. It focuses on the difficulties in defining agent behaviors in RDF/RDFS and SPARQL, highlighting issues such as handling URIs and complex SPARQL queries. To address these challenges, an integrated development environment is proposed to simplify the modeling process and expand the user community for AJAN. Additionally, the use of Large Language Models is suggested to enhance agent engineering capabilities. By integrating these solutions, the framework aims to facilitate the creation and management of multi-agent systems based on semantic Web standards. <div>
arXiv:2510.06911v1 Announce Type: new 
Abstract: There are many established semantic Web standards for implementing multi-agent driven applications. The AJAN framework allows to engineer multi-agent systems based on these standards. In particular, agent knowledge is represented in RDF/RDFS and OWL, while agent behavior models are defined with Behavior Trees and SPARQL to access and manipulate this knowledge. However, the appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still remains a major hurdle not only for agent modelers in practice. For example, dealing with URIs is very error-prone regarding typos and dealing with complex SPARQL queries in large-scale environments requires a high learning curve. In this paper, we present an integrated development environment to overcome such hurdles of modeling AJAN agents and at the same time to extend the user community for AJAN by the possibility to leverage Large Language Models for agent engineering.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces</title>
<link>https://arxiv.org/abs/2510.06953</link>
<guid>https://arxiv.org/abs/2510.06953</guid>
<content:encoded><![CDATA[
<div> entropy, information density, language model, reasoning quality, uniformity

Summary:<br />
The study examines the Uniform Information Density (UID) hypothesis in the context of large language model (LLM) reasoning traces, focusing on the relationship between step-level uniformity and reasoning quality. A new entropy-based stepwise information density metric is introduced, along with local and global uniformity scores. Experimental results across six reasoning benchmarks demonstrate that higher step-level uniformity correlates with improved accuracy, indicating the practical benefits of maintaining a uniform flow of information during reasoning. Correct reasoning traces exhibit consistent information density, while incorrect traces display irregular spikes. The study shows that uniform information density measures are reliable predictors of reasoning quality, outperforming other internal signals. These findings suggest that ensuring uniformity in information density can enhance the reliability and accuracy of reasoning systems. <div>
arXiv:2510.06953v1 Announce Type: new 
Abstract: The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07038</link>
<guid>https://arxiv.org/abs/2510.07038</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, tool usage, multi-hop reasoning, knowledge-intensive tasks 
Summary:
This paper introduces a novel reinforcement learning framework called Tool-Augmented Policy Optimization (TAPO) that integrates multi-hop reasoning with on-demand tool calling capabilities for large language models. The approach, based on Dynamic Sampling Policy Optimization (DAPO), enables models to dynamically combine complex reasoning with tool usage such as search APIs and Python interpreters. Two new datasets, TAPO-easy-60K and TAPO-hard-18K, are introduced to train and evaluate fact-based reasoning and mathematical calculation capabilities. Experimental results on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of TAPO in tasks requiring external knowledge and mathematical computation, achieving state-of-the-art performance. TAPO shows more efficient tool utilization compared to baseline methods and avoids excessive tool calls due to reward hacking. This study showcases the potential of integrating advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.<br /><br />Summary: <div>
arXiv:2510.07038v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations</title>
<link>https://arxiv.org/abs/2510.07064</link>
<guid>https://arxiv.org/abs/2510.07064</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Diversity, Submodular Optimization, Human Behavior  
Summary:  
- A novel framework is proposed to construct a set of Large Language Model (LLM) agents that collectively capture the diversity of a human population.  
- Each agent is conditioned on a small set of human demonstrations through in-context learning.  
- The challenge is to select a representative set of LLM agents using submodular optimization.  
- Experiments in crowdsourcing and education domains show that the approach constructs agents that better represent human populations compared to baselines.  
- Behavioral analyses demonstrate that these agents reproduce the behavior patterns and perspectives of the individuals they are designed to represent.  

Summary: <div>
arXiv:2510.07064v1 Announce Type: new 
Abstract: The difficulty and expense of obtaining large-scale human responses make Large Language Models (LLMs) an attractive alternative and a promising proxy for human behavior. However, prior work shows that LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors. Thus, rather than trying to capture this diversity with a single LLM agent, we propose a novel framework to construct a set of agents that collectively capture the diversity of a given human population. Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning. The central challenge is therefore to select a representative set of LLM agents from the exponentially large space of possible agents. We tackle this selection problem from the lens of submodular optimization. In particular, we develop methods that offer different trade-offs regarding time complexity and performance guarantees. Extensive experiments in crowdsourcing and educational domains demonstrate that our approach constructs agents that more effectively represent human populations compared to baselines. Moreover, behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive Learning for Possibilistic Logic Programs Under Stable Models</title>
<link>https://arxiv.org/abs/2510.07069</link>
<guid>https://arxiv.org/abs/2510.07069</guid>
<content:encoded><![CDATA[
<div> inductive reasoning, possibilistic logic programs, stable models, induction tasks, algorithms

Summary:<br />
This paper introduces an approach to extract possibilistic logic programs from a background program and examples for inductive reasoning. The concept of induction tasks is defined and two algorithms, ilpsm and ilpsmmin, are presented for computing induction solutions. An implementation of ilpsmmin is also provided, showing superior performance compared to a major inductive learning system for normal logic programs from stable models on randomly generated datasets. This research fills a gap in investigating inductive reasoning in possibilistic logic programs and provides a valuable contribution to the field of answer set programming. <div>
arXiv:2510.07069v1 Announce Type: new 
Abstract: Possibilistic logic programs (poss-programs) under stable models are a major variant of answer set programming (ASP). While its semantics (possibilistic stable models) and properties have been well investigated, the problem of inductive reasoning has not been investigated yet. This paper presents an approach to extracting poss-programs from a background program and examples (parts of intended possibilistic stable models). To this end, the notion of induction tasks is first formally defined, its properties are investigated and two algorithms ilpsm and ilpsmmin for computing induction solutions are presented. An implementation of ilpsmmin is also provided and experimental results show that when inputs are ordinary logic programs, the prototype outperforms a major inductive learning system for normal logic programs from stable models on the datasets that are randomly generated.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2510.07073</link>
<guid>https://arxiv.org/abs/2510.07073</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicle routing problems, large language model, metaheuristic, genetic search, automated heuristics discovery

Summary:
VRPAgent is a framework that leverages large language models to generate heuristic components for vehicle routing problems (VRPs) and refines them using genetic search. By integrating LLM-generated operators into a metaheuristic framework, VRPAgent is able to outperform handcrafted methods and recent learning-based approaches in solving various VRPs. This approach guarantees correctness and efficiency while enabling the discovery of novel and powerful strategies. Through experiments on different VRPs such as capacitated VRP, VRP with time windows, and prize-collecting VRP, VRPAgent demonstrates superior performance using only a single CPU core. This advancement marks VRPAgent as a groundbreaking LLM-based paradigm that propels the state-of-the-art in VRPs, paving the way for automated heuristics discovery in the future.<br /><br />Summary: <div>
arXiv:2510.07073v1 Announce Type: new 
Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a complex task that requires both intuition and deep domain knowledge. Large language model (LLM)-based code generation has recently shown promise across many domains, but it still falls short of producing heuristics that rival those crafted by human experts. In this paper, we propose VRPAgent, a framework that integrates LLM-generated components into a metaheuristic and refines them through a novel genetic search. By using the LLM to generate problem-specific operators, embedded within a generic metaheuristic framework, VRPAgent keeps tasks manageable, guarantees correctness, and still enables the discovery of novel and powerful strategies. Across multiple problems, including the capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our method discovers heuristic operators that outperform handcrafted methods and recent learning-based approaches while requiring only a single CPU core. To our knowledge, \VRPAgent is the first LLM-based paradigm to advance the state-of-the-art in VRPs, highlighting a promising future for automated heuristics discovery.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas</title>
<link>https://arxiv.org/abs/2510.07091</link>
<guid>https://arxiv.org/abs/2510.07091</guid>
<content:encoded><![CDATA[
<div> cognitive bandwidth, action representation, planning with actions, planning with schemas, scalability

Summary:
This paper explores the optimal action representation for long-horizon agents in environments with combinatorially exploded action spaces. It compares conventional planning with actions (PwA) to planning with schemas (PwS), finding an inflection point where PwS becomes more effective than PwA. The study uses ALFWorld and SciWorld as test environments, showing a need for scalable representations around 35 actions for ALFWorld and 500 actions for SciWorld. The location of this inflection point shifts based on model capacities, with stronger planning proficiency pushing it rightward and better schema instantiation pushing it leftward. Despite the suboptimal performance of PwS agents, the paper provides insights on building more capable PwS agents for improved scalable autonomy.<br /><br />Summary: <div>
arXiv:2510.07091v1 Announce Type: new 
Abstract: Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., "move [OBJ] to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Contingencies of Physical Embodiment Allow for Open-Endedness and Care</title>
<link>https://arxiv.org/abs/2510.07117</link>
<guid>https://arxiv.org/abs/2510.07117</guid>
<content:encoded><![CDATA[
<div> Keywords: physical embodiment, existentialism, reinforcement learning, intrinsic drive, care

Summary:
Physical vulnerability and mortality are typically viewed as challenges for artificial agents but are easily managed by biological organisms in open-ended environments. Inspired by Heidegger's phenomenology, the authors propose two minimal conditions for physical embodiment: being-in-the-world and being-towards-death. These conditions give rise to a homeostatic drive and an intrinsic drive to maximize control over future states, akin to Nietzsche's will-to-power. By formalizing these concepts in a reinforcement learning framework, the study explores how intrinsically driven embodied agents can learn in multi-agent environments to cultivate capacities for open-endedness and care. This approach aims to develop more adaptive and caring artificial agents that can navigate complex environments effectively. <br /><br />Summary: <div>
arXiv:2510.07117v1 Announce Type: new 
Abstract: Physical vulnerability and mortality are often seen as obstacles to be avoided in the development of artificial agents, which struggle to adapt to open-ended environments and provide aligned care. Meanwhile, biological organisms survive, thrive, and care for each other in an open-ended physical world with relative ease and efficiency. Understanding the role of the conditions of life in this disparity can aid in developing more robust, adaptive, and caring artificial agents. Here we define two minimal conditions for physical embodiment inspired by the existentialist phenomenology of Martin Heidegger: being-in-the-world (the agent is a part of the environment) and being-towards-death (unless counteracted, the agent drifts toward terminal states due to the second law of thermodynamics). We propose that from these conditions we can obtain both a homeostatic drive - aimed at maintaining integrity and avoiding death by expending energy to learn and act - and an intrinsic drive to continue to do so in as many ways as possible. Drawing inspiration from Friedrich Nietzsche's existentialist concept of will-to-power, we examine how intrinsic drives to maximize control over future states, e.g., empowerment, allow agents to increase the probability that they will be able to meet their future homeostatic needs, thereby enhancing their capacity to maintain physical integrity. We formalize these concepts within a reinforcement learning framework, which enables us to examine how intrinsically driven embodied agents learning in open-ended multi-agent environments may cultivate the capacities for open-endedness and care.ov
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Domain Knowledge into Process Discovery Using Large Language Models</title>
<link>https://arxiv.org/abs/2510.07161</link>
<guid>https://arxiv.org/abs/2510.07161</guid>
<content:encoded><![CDATA[
<div> Keywords: Process discovery, event logs, domain knowledge, Large Language Models, interactive framework

Summary:<br />
- Process discovery aims to derive process models from event logs but may lack reliability without incorporating domain knowledge.
- An interactive framework is proposed that uses Large Language Models to extract declarative rules from natural language descriptions provided by domain experts.
- The extracted rules guide the process discovery algorithm, combining insights from event logs and domain knowledge to avoid problematic process structures.
- The framework coordinates interactions among the LLM, domain experts, and backend services to improve the accuracy of discovered process models.
- The fully implemented tool supporting this workflow was evaluated through an empirical study involving real-life event logs and domain experts assessing usability and effectiveness.<br /><br />Summary: <div>
arXiv:2510.07161v1 Announce Type: new 
Abstract: Process discovery aims to derive process models from event logs, providing insights into operational behavior and forming a foundation for conformance checking and process improvement. However, models derived solely from event data may not accurately reflect the real process, as event logs are often incomplete or affected by noise, and domain knowledge, an important complementary resource, is typically disregarded. As a result, the discovered models may lack reliability for downstream tasks. We propose an interactive framework that incorporates domain knowledge, expressed in natural language, into the process discovery pipeline using Large Language Models (LLMs). Our approach leverages LLMs to extract declarative rules from textual descriptions provided by domain experts. These rules are used to guide the IMr discovery algorithm, which recursively constructs process models by combining insights from both the event log and the extracted rules, helping to avoid problematic process structures that contradict domain knowledge. The framework coordinates interactions among the LLM, domain experts, and a set of backend services. We present a fully implemented tool that supports this workflow and conduct an extensive evaluation of multiple LLMs and prompt engineering strategies. Our empirical study includes a case study based on a real-life event log with the involvement of domain experts, who assessed the usability and effectiveness of the framework.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents</title>
<link>https://arxiv.org/abs/2510.07172</link>
<guid>https://arxiv.org/abs/2510.07172</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, scientific law discovery, NewtonBench, interactive model discovery, AI agents

Summary:
NewtonBench introduces a new benchmark for scientific law discovery using large language models (LLMs), addressing the trilemma of scientific relevance, scalability, and resistance to memorization. The benchmark includes 324 tasks across 12 physics domains and emphasizes interactive model exploration rather than static function fitting. Experiment results show that while LLMs demonstrate a capacity for discovery, this ability weakens with increasing system complexity and is sensitive to observational noise. Surprisingly, providing code interpreters to assist models can hinder discovery by promoting premature exploitation over exploration. The study highlights the challenge of achieving robust and generalizable discovery in interactive environments. NewtonBench serves as a valuable tool for measuring progress in AI-driven scientific discovery and guiding the development of future AI agents capable of authentic scientific exploration. 

<br /><br />Summary: <div>
arXiv:2510.07172v1 Announce Type: new 
Abstract: Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences</title>
<link>https://arxiv.org/abs/2510.07276</link>
<guid>https://arxiv.org/abs/2510.07276</guid>
<content:encoded><![CDATA[
<div> Algorithm, Multi-objective, Coordination, Path finding, Optimization

Summary: 
The article proposes a lexicographic framework for multi-objective multi-agent path finding (MO-MAPF), introducing the Lexicographic Conflict-Based Search (LCBS) algorithm to efficiently compute solutions aligned with user-defined preferences. LCBS integrates priority-aware A* search and conflict-based search to avoid the construction of Pareto frontiers and produce optimal solutions while scaling to instances with up to ten objectives. The algorithm demonstrates higher success rates against existing methods on standard and randomized MAPF benchmarks, particularly as the number of objectives increases. LCBS is capable of efficiently balancing multiple potentially competing objectives in shared environments, offering a novel approach to optimizing coordination among multiple agents. <div>
arXiv:2510.07276v1 Announce Type: new 
Abstract: Many real-world scenarios require multiple agents to coordinate in shared environments, while balancing trade-offs between multiple, potentially competing objectives. Current multi-objective multi-agent path finding (MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto frontiers. They do not explicitly optimize for user-defined preferences, even when the preferences are available, and scale poorly with the number of objectives. We propose a lexicographic framework for modeling MO-MAPF, along with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that directly computes a single solution aligned with a lexicographic preference over objectives. LCBS integrates a priority-aware low-level $A^*$ search with conflict-based search, avoiding Pareto frontier construction and enabling efficient planning guided by preference over objectives. We provide insights into optimality and scalability, and empirically demonstrate that LCBS computes optimal solutions while scaling to instances with up to ten objectives -- far beyond the limits of existing MO-MAPF methods. Evaluations on standard and randomized MAPF benchmarks show consistently higher success rates against state-of-the-art baselines, especially with increasing number of objectives.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic generative AI for media content discovery at the national football league</title>
<link>https://arxiv.org/abs/2510.07297</link>
<guid>https://arxiv.org/abs/2510.07297</guid>
<content:encoded><![CDATA[
<div> AI, generative, NFL, content discovery, workflow  
Summary:  
- Generative AI in collaboration with the NFL enables natural language queries for historical plays, improving operational efficiency.  
- Workflow breaks user queries into elements and translates them for database queries, with over 95 percent accuracy.  
- Semantic caching enhances accuracy and reduces average search time from 10 minutes to 30 seconds.  
- The solution allows media researchers and analysts to focus on creative content and engaging storylines.  
- Through AI technology, users can easily find relevant videos, enhancing the NFL's content management process.  
<br /><br />Summary: <div>
arXiv:2510.07297v1 Announce Type: new 
Abstract: Generative AI has unlocked new possibilities in content discovery and management. Through collaboration with the National Football League (NFL), we demonstrate how a generative-AI based workflow enables media researchers and analysts to query relevant historical plays using natural language rather than traditional filter-and-click interfaces. The agentic workflow takes a user query as input, breaks it into elements, and translates them into the underlying database query language. Accuracy and latency are further improved through carefully designed semantic caching. The solution achieves over 95 percent accuracy and reduces the average time to find relevant videos from 10 minutes to 30 seconds, significantly increasing the NFL's operational efficiency and allowing users to focus on producing creative content and engaging storylines.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepXPalm: Tilt and Position Rendering using Palm-worn Haptic Display and CNN-based Tactile Pattern Recognition</title>
<link>https://arxiv.org/abs/2204.03521</link>
<guid>https://arxiv.org/abs/2204.03521</guid>
<content:encoded><![CDATA[
<div> Keywords: telemanipulation, deformable objects, haptic feedback, tactile sensors, Convolutional Neural Networks (CNN)

Summary:
This work focuses on telemanipulation of plastic pipettes, addressing the challenge of dynamic object shape changes. A telemanipulation system using a multi-contact haptic device and tactile sensors in a Robotiq gripper is proposed. A novel approach utilizing Convolutional Neural Networks (CNN) is introduced to detect tilt and position of deformable objects while grasping them. The CNN generates a mask based on this data to provide additional tactile stimuli to users during telemanipulation. Results show that using the CNN algorithm and the preset mask significantly improves tilt and position recognition by users, increasing from 9.67% to 82.5%. This study highlights the potential of advanced technologies such as CNNs in enhancing the precision and dexterity of telemanipulation systems for deformable objects. 

<br /><br />Summary: <div>
arXiv:2204.03521v1 Announce Type: cross 
Abstract: Telemanipulation of deformable objects requires high precision and dexterity from the users, which can be increased by kinesthetic and tactile feedback. However, the object shape can change dynamically, causing ambiguous perception of its alignment and hence errors in the robot positioning. Therefore, the tilt angle and position classification problem has to be solved to present a clear tactile pattern to the user. This work presents a telemanipulation system for plastic pipettes consisting of a multi-contact haptic device LinkGlide to deliver haptic feedback at the users' palm and two tactile sensors array embedded in the 2-finger Robotiq gripper. We propose a novel approach based on Convolutional Neural Networks (CNN) to detect the tilt and position while grasping deformable objects. The CNN generates a mask based on recognized tilt and position data to render further multi-contact tactile stimuli provided to the user during the telemanipulation. The study has shown that using the CNN algorithm and the preset mask, tilt, and position recognition by users is increased from 9.67% using the direct data to 82.5%.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiltXter: CNN-based Electro-tactile Rendering of Tilt Angle for Telemanipulation of Pasteur Pipettes</title>
<link>https://arxiv.org/abs/2409.15838</link>
<guid>https://arxiv.org/abs/2409.15838</guid>
<content:encoded><![CDATA[
<div> Keywords: deformable objects, tactile feedback, telemanipulation, convolutional neural networks, tilt recognition

Summary:
Deformable objects can pose challenges for robotic grippers during grasping, leading to errors in alignment and positioning. To improve precision in telemanipulation tasks, clear tactile patterns are essential for haptic feedback. This study introduces a telemanipulation system for plastic pipettes, utilizing a haptic interface with electro-stimulation arrays and tactile sensors in a Robotiq gripper. A novel approach employing convolutional neural networks (CNN) is proposed to detect object tilt and generate corresponding tactile patterns for feedback. Results show that CNN-based tilt recognition improved user accuracy and success rates during teleoperation significantly. From a low initial tilt recognition rate of 23.13%, the CNN algorithm boosted it to 57.9%, while the success rate in teleoperation increased from 53.12% to 92.18% with the generated tactile patterns. This approach demonstrates the potential of CNN algorithms in enhancing tactile feedback and improving robotic telemanipulation tasks involving deformable objects.<br /><br />Summary: <div>
arXiv:2409.15838v1 Announce Type: cross 
Abstract: The shape of deformable objects can change drastically during grasping by robotic grippers, causing an ambiguous perception of their alignment and hence resulting in errors in robot positioning and telemanipulation. Rendering clear tactile patterns is fundamental to increasing users' precision and dexterity through tactile haptic feedback during telemanipulation. Therefore, different methods have to be studied to decode the sensors' data into haptic stimuli. This work presents a telemanipulation system for plastic pipettes that consists of a Force Dimension Omega.7 haptic interface endowed with two electro-stimulation arrays and two tactile sensor arrays embedded in the 2-finger Robotiq gripper. We propose a novel approach based on convolutional neural networks (CNN) to detect the tilt of deformable objects. The CNN generates a tactile pattern based on recognized tilt data to render further electro-tactile stimuli provided to the user during the telemanipulation. The study has shown that using the CNN algorithm, tilt recognition by users increased from 23.13\% with the downsized data to 57.9%, and the success rate during teleoperation increased from 53.12% using the downsized data to 92.18% using the tactile patterns generated by the CNN.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents</title>
<link>https://arxiv.org/abs/2510.04452</link>
<guid>https://arxiv.org/abs/2510.04452</guid>
<content:encoded><![CDATA[
<div> Keywords: Interface agents, generative AI models, agent prototyping, user experience, design requirements

Summary:
Interface agents powered by generative AI models, also known as agents, are designed to automate actions based on user commands. Developing these agents involves creating a positive user experience, which is essential for their effectiveness. To facilitate the prototype process and gather input from a variety of users, the study explores the necessary features of agent prototyping systems. Conducting experiments with participants of varying backgrounds in agent experience, key activities in agent experience prototyping and desired capabilities for prototyping systems are identified. These findings are then implemented in the AgentBuilder design probe for agent prototyping. Additionally, an in situ study is conducted using AgentBuilder to validate design requirements and gather insights on the agent prototyping process and developers' needs. The research aims to improve the efficiency and effectiveness of agent prototyping by incorporating valuable user perspectives. 

<br /><br />Summary: <div>
arXiv:2510.04452v1 Announce Type: cross 
Abstract: Interface agents powered by generative AI models (referred to as "agents") can automate actions based on user commands. An important aspect of developing agents is their user experience (i.e., agent experience). There is a growing need to provide scaffolds for a broader set of individuals beyond AI engineers to prototype agent experiences, since they can contribute valuable perspectives to designing agent experiences. In this work, we explore the affordances agent prototyping systems should offer by conducting a requirements elicitation study with 12 participants with varying experience with agents. We identify key activities in agent experience prototyping and the desired capabilities of agent prototyping systems. We instantiate those capabilities in the AgentBuilder design probe for agent prototyping. We conduct an in situ agent prototyping study with 14 participants using AgentBuilder to validate the design requirements and elicit insights on how developers prototype agents and what their needs are in this process.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives</title>
<link>https://arxiv.org/abs/2510.05336</link>
<guid>https://arxiv.org/abs/2510.05336</guid>
<content:encoded><![CDATA[
<div> Benchmark, retrieval-augmented generation, historical weather archives, societal vulnerability, resilience<br />
<br />
Summary: <br />
WeatherArchive-Bench introduces the first benchmark for evaluating retrieval-augmented generation systems on historical weather archives. It consists of two tasks: WeatherArchive-Retrieval assesses a system's capability to find historically relevant passages from over one million archival news segments, while WeatherArchive-Assessment evaluates Large Language Models' ability to classify societal vulnerability and resilience indicators from extreme weather narratives. Experiments show that dense retrievers struggle with historical terminology, and LLMs often misinterpret vulnerability and resilience concepts. These findings highlight limitations in reasoning about complex societal indicators and offer insights for designing more robust climate-focused RAG systems from archival contexts. The dataset and evaluation framework are publicly available for further research. <br /> <div>
arXiv:2510.05336v1 Announce Type: cross 
Abstract: Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at https://anonymous.4open.science/r/WeatherArchive-Bench/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal GUI Architecture for Interfacing with LLM-Based Conversational Assistants</title>
<link>https://arxiv.org/abs/2510.06223</link>
<guid>https://arxiv.org/abs/2510.06223</guid>
<content:encoded><![CDATA[
<div> architecture, large language models, speech-enabled assistants, Model Context Protocol, MVVM pattern

Summary:
The article discusses the integration of large language models (LLMs) and real-time speech recognition with graphical user interfaces (GUIs) to enable issuing GUI actions through natural language. The proposed architecture utilizes the Model Context Protocol (MCP) to make an application's navigation graph and semantics accessible. The ViewModel, following the MVVM pattern, exposes application capabilities to the speech-enabled assistant, ensuring alignment between spoken input and the visual interface. This architecture enables full voice accessibility and consistent feedback across modalities, preparing apps for future OS super assistants. The article also evaluates the use of locally deployable, open-weight LLMs for speech-enabled multimodal UIs, highlighting their performance comparability with proprietary models and the hardware requirements for fast responsiveness. Overall, the study emphasizes the importance of integrating LLMs with GUIs for enhanced user experience and accessibility. 

<br /><br />Summary: <div>
arXiv:2510.06223v1 Announce Type: cross 
Abstract: Advances in large language models (LLMs) and real-time speech recognition now make it possible to issue any graphical user interface (GUI) action through natural language and receive the corresponding system response directly through the GUI. Most production applications were never designed with speech in mind. This article provides a concrete architecture that enables GUIs to interface with LLM-based speech-enabled assistants.
  The architecture makes an application's navigation graph and semantics available through the Model Context Protocol (MCP). The ViewModel, part of the MVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to the assistant by supplying both tools applicable to a currently visible view and application-global tools extracted from the GUI tree router. This architecture facilitates full voice accessibility while ensuring reliable alignment between spoken input and the visual interface, accompanied by consistent feedback across modalities. It future-proofs apps for upcoming OS super assistants that employ computer use agents (CUAs) and natively consume MCP if an application provides it.
  To address concerns about privacy and data security, the practical effectiveness of locally deployable, open-weight LLMs for speech-enabled multimodal UIs is evaluated. Findings suggest that recent smaller open-weight models approach the performance of leading proprietary models in overall accuracy and require enterprise-grade hardware for fast responsiveness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools</title>
<link>https://arxiv.org/abs/2510.06224</link>
<guid>https://arxiv.org/abs/2510.06224</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent Gen AI, human-AI collaboration, transparency, collaboration dynamics, technology organizations

Summary:<br />
The study explores how early adopters at Microsoft perceive multi-agent generative AI tools, viewing them as specialized agents working in teams with roles like assistants or reviewers. These tools range from AI-dominant to AI-assisted interactions, modeled after human collaboration dynamics. Challenges identified include error propagation and the need for clear communication to address transparency issues such as error tracing and prevention of misuse. Transparency is seen as crucial for building trust and oversight in human-agent and agent-agent interactions. Design considerations highlighted in the study contribute to understanding collaborative mechanisms in AI tools and suggest future research directions for optimizing inter-agent and human mediation interactions in innovative workflows.<br /><br />Summary: <div>
arXiv:2510.06224v1 Announce Type: cross 
Abstract: With recent advancements in multi-agent generative AI (Gen AI), technology organizations like Microsoft are adopting these complex tools, redefining AI agents as active collaborators in complex workflows rather than as passive tools. In this study, we investigated how early adopters and developers conceptualize multi-agent Gen AI tools, focusing on how they understand human-AI collaboration mechanisms, general collaboration dynamics, and transparency in the context of AI tools. We conducted semi-structured interviews with 13 developers, all early adopters of multi-agent Gen AI technology who work at Microsoft. Our findings revealed that these early adopters conceptualize multi-agent systems as "teams" of specialized role-based and task-based agents, such as assistants or reviewers, structured similar to human collaboration models and ranging from AI-dominant to AI-assisted, user-controlled interactions. We identified key challenges, including error propagation, unpredictable and unproductive agent loop behavior, and the need for clear communication to mitigate the layered transparency issues. Early adopters' perspectives about the role of transparency underscored its importance as a way to build trust, verify and trace errors, and prevent misuse, errors, and leaks. The insights and design considerations we present contribute to CSCW research about collaborative mechanisms with capabilities ranging from AI-dominant to AI-assisted interactions, transparency and oversight strategies in human-agent and agent-agent interactions, and how humans make sense of these multi-agent systems as dynamic, role-diverse collaborators which are customizable for diverse needs and workflows. We conclude with future research directions that extend CSCW approaches to the design of inter-agent and human mediation interactions.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Multi-agent Social Simulation Framework</title>
<link>https://arxiv.org/abs/2510.06225</link>
<guid>https://arxiv.org/abs/2510.06225</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent social interaction, Large Language Models, Modular framework, Memory summarization, Simulated environment

Summary:
A new modular, object-oriented framework has been developed to improve multi-agent social interaction simulations using Large Language Models. The framework integrates various base classes through a hierarchical structure, promoting scalability and reusability. A memory summarization mechanism filters and distills relevant information from raw data to prioritize contextually salient events and interactions. By selecting and combining derived classes, specific simulated environments can be customized. The framework has been utilized successfully to simulate human interactions on social media, replicating real-world online social behaviors. The source code for the project will be released and continually updated to evolve over time. <div>
arXiv:2510.06225v1 Announce Type: cross 
Abstract: Multi-agent social interaction has clearly benefited from Large Language Models. However, current simulation systems still face challenges such as difficulties in scaling to diverse scenarios and poor reusability due to a lack of modular design. To address these issues, we designed and developed a modular, object-oriented framework that organically integrates various base classes through a hierarchical structure, harvesting scalability and reusability. We inherited the framework to realize common derived classes. Additionally, a memory summarization mechanism is proposed to filter and distill relevant information from raw memory data, prioritizing contextually salient events and interactions. By selecting and combining some necessary derived classes, we customized a specific simulated environment. Utilizing this simulated environment, we successfully simulated human interactions on social media, replicating real-world online social behaviors. The source code for the project will be released and evolve.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)</title>
<link>https://arxiv.org/abs/2510.06235</link>
<guid>https://arxiv.org/abs/2510.06235</guid>
<content:encoded><![CDATA[
<div> Keywords: fMRI, brain responses, multimodal representations, language models, vision models 

Summary: 
The article discusses the submission by the team Seinfeld to the Algonauts 2025 Challenge, focusing on predicting fMRI brain responses to movie stimuli. They utilize a combination of large language models, video encoders, audio models, and vision-language models, incorporating both pre-trained and fine-tuned variants. To enhance performance, they include detailed transcripts and summaries in the textual inputs and explore stimulus-tuning and fine-tuning techniques for language and vision models. The predictions from individual models are consolidated using stacked regression, resulting in competitive rankings, with the team securing the 10th position. They have made their code and resources publicly accessible to contribute to the development of multimodal encoding models for studying brain activity.  <div>
arXiv:2510.06235v1 Announce Type: cross 
Abstract: We present our submission to the Algonauts 2025 Challenge, where the goal is to predict fMRI brain responses to movie stimuli. Our approach integrates multimodal representations from large language models, video encoders, audio models, and vision-language models, combining both off-the-shelf and fine-tuned variants. To improve performance, we enhanced textual inputs with detailed transcripts and summaries, and we explored stimulus-tuning and fine-tuning strategies for language and vision models. Predictions from individual models were combined using stacked regression, yielding solid results. Our submission, under the team name Seinfeld, ranked 10th. We make all code and resources publicly available, contributing to ongoing efforts in developing multimodal encoding models for brain activity.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout</title>
<link>https://arxiv.org/abs/2510.06238</link>
<guid>https://arxiv.org/abs/2510.06238</guid>
<content:encoded><![CDATA[
<div> Keywords: surface landmines, unexploded ordnances, deep learning, uncertainty quantification, Monte Carlo Dropout

Summary:
Surface landmine and unexploded ordnances (UXOs) detection using deep learning faces challenges in noisy conditions and adversarial attacks. The study proposes incorporating Monte Carlo (MC) Dropout into a fine-tuned ResNet-50 model for classification, aiming to quantify uncertainty in predictions. By assessing epistemic uncertainty, the model can provide additional reliability metrics for decision-making in demining operations. Experimental results on various test images confirm the model's capability to identify unreliable predictions under adverse conditions. This proof-of-concept study underscores the necessity for uncertainty quantification in demining efforts and sheds light on the vulnerability of current neural networks to adversarial threats. It emphasizes the crucial need to enhance the robustness and reliability of models for practical demining applications.

<br /><br />Summary: <div>
arXiv:2510.06238v1 Announce Type: cross 
Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep learning has shown promise in humanitarian demining. However, deterministic neural networks can be vulnerable to noisy conditions and adversarial attacks, leading to missed detection or misclassification. This study introduces the idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated into a fine-tuned ResNet-50 architecture for surface landmine and UXO classification, which was tested on a simulated dataset. Integrating the MC Dropout approach helps quantify epistemic uncertainty, providing an additional metric for prediction reliability, which could be helpful to make more informed decisions in demining operations. Experimental results on clean, adversarially perturbed, and noisy test images demonstrate the model's ability to flag unreliable predictions under challenging conditions. This proof-of-concept study highlights the need for uncertainty quantification in demining, raises awareness about the vulnerability of existing neural networks in demining to adversarial threats, and emphasizes the importance of developing more robust and reliable models for practical applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</title>
<link>https://arxiv.org/abs/2510.06240</link>
<guid>https://arxiv.org/abs/2510.06240</guid>
<content:encoded><![CDATA[
<div> Keywords: Industrial question-answering, Multi-agent system, Knowledge graph, Distillation, Reliability

Summary: 
Industrial question-answering systems must ensure high safety and reliability, especially in critical scenarios such as equipment fault diagnosis. Existing multi-agent models enhance reasoning depth but face challenges like unverifiable outputs. In response, Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD) is proposed, using a knowledge graph as a structured prior to improve state representation and convergence. This approach formulates distillation as a Markov Decision Process, enabling the transfer of collaborative reasoning capabilities to lightweight student models. KG-MASD integrates collaborative reasoning with knowledge grounding, leading to enhanced accuracy and reliability in industrial QA tasks. Experimental results show significant improvements over baselines, with higher accuracy and trustworthy AI deployment in safety-critical contexts.<br /><br />Summary: <div>
arXiv:2510.06240v1 Announce Type: cross 
Abstract: Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses</title>
<link>https://arxiv.org/abs/2510.06242</link>
<guid>https://arxiv.org/abs/2510.06242</guid>
<content:encoded><![CDATA[
<div> survey, evaluation, marketing, response, quality

Summary:
- Existing methods for evaluating survey responses have limitations and may lead to misleading conclusions.
- A two-stage evaluation framework is proposed specifically designed for human survey responses.
- The framework includes gibberish filtering to remove nonsensical responses and evaluates responses on three dimensions: effort, relevance, and completeness.
- Validation on English and Korean datasets shows that the proposed framework outperforms existing metrics and is applicable for real-world applications such as response quality prediction and rejection.
- The framework demonstrates strong correlations with expert assessments, indicating its effectiveness in evaluating the quality of human-written survey responses. 

<br /><br />Summary: <div>
arXiv:2510.06242v1 Announce Type: cross 
Abstract: Open-ended survey responses provide valuable insights in marketing research, but low-quality responses not only burden researchers with manual filtering but also risk leading to misleading conclusions, underscoring the need for effective evaluation. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses with their distinct characteristics. To address such characteristics, we propose a two-stage evaluation framework specifically designed for human survey responses. First, gibberish filtering removes nonsensical responses. Then, three dimensions-effort, relevance, and completeness-are evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data. Validation on English and Korean datasets shows that our framework not only outperforms existing metrics but also demonstrates high practical applicability for real-world applications such as response quality prediction and response rejection, showing strong correlations with expert assessment.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning</title>
<link>https://arxiv.org/abs/2510.06243</link>
<guid>https://arxiv.org/abs/2510.06243</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Expression Comprehension, Segmentation, Multimodal Large Language Models, CoT Referring, Sequential Referring Step

Summary:
Referring Expression Comprehension and Segmentation are crucial tasks in evaluating the abilities of Multimodal Large Language Models (MLLMs). A new strategy called CoT Referring is proposed to improve model reasoning across modalities by structuring training data in a chain-of-thought format. This approach enhances accuracy in complex query scenarios by parsing textual structures into sequential referring steps, identifying relationships, and ensuring consistent reference alignment. The training data is restructured to enforce a new output form, and a benchmark is created for evaluating complex referring cases. Additionally, detection and segmentation capabilities are integrated into a unified MLLM framework, trained with an adaptive weighted loss to optimize performance. Experimental results show a significant increase in accuracy over baseline models, with a notable improvement of 2.5%+. 

<br /><br />Summary: Referring Expression Comprehension and Segmentation are critical for evaluating Multimodal Large Language Models (MLLMs). The CoT Referring strategy enhances reasoning across modalities by structuring training data in a chain-of-thought format and improving accuracy in complex query scenarios. The approach restructures training data, creates a benchmark for evaluating complex cases, and integrates detection and segmentation capabilities into a unified MLLM framework with optimized performance. Experimental results demonstrate a substantial increase in accuracy over baseline models, highlighting the effectiveness of the proposed approach. <div>
arXiv:2510.06243v1 Announce Type: cross 
Abstract: Referring Expression Comprehension and Segmentation are critical tasks for assessing the integration of language understanding and image comprehension, serving as benchmarks for Multimodal Large Language Models (MLLMs) capabilities. To address these challenges, we propose a new strategy, CoT Referring, which enhances model reasoning across modalities through a structured, chain-of-thought training data structure. Our approach systematically parses textual structures to a sequential referring step, where in each step it identifies relationships and ensures consistent reference alignment, thereby improving accuracy in complex query scenarios. We restructure the training data to enforce a new output form, providing new annotations for existing datasets and compiling an evaluation benchmark from existing resources. This benchmark is designed explicitly for complex referring cases. We also integrate detection and segmentation capabilities into a unified MLLM framework, training it with a novel adaptive weighted loss to optimize performance. Experimental results on our curated benchmark and RefCOCO/+/g demonstrate the effectiveness of our approach, with a notable increase of 2.5%+ over baseline models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Embedding Frameworks for Scientific Domain</title>
<link>https://arxiv.org/abs/2510.06244</link>
<guid>https://arxiv.org/abs/2510.06244</guid>
<content:encoded><![CDATA[
<div> word representation algorithm, domain specific data, transformer architecture, tokenization method, scientific domain

Summary:
An optimal word representation algorithm is crucial for domain-specific data due to varying meanings of words based on context. Generative AI and transformer models excel in generating contextualized embeddings but are resource-intensive. This study focuses on identifying the best word representation and tokenization methods for the scientific domain. The research aims to find methods suitable for downstream NLP tasks in science and develop an evaluation suite for ongoing algorithm assessments. By creating a comprehensive evaluation suite comprising diverse scientific tasks and datasets, the study evaluates multiple word representation and tokenization algorithms. The goal is to determine the most effective techniques for scientific NLP tasks and provide a platform for continuous evaluation and comparison of new algorithms in this domain.

<br /><br />Summary: <div>
arXiv:2510.06244v1 Announce Type: cross 
Abstract: Finding an optimal word representation algorithm is particularly important in terms of domain specific data, as the same word can have different meanings and hence, different representations depending on the domain and context. While Generative AI and transformer architecture does a great job at generating contextualized embeddings for any given work, they are quite time and compute extensive, especially if we were to pre-train such a model from scratch. In this work, we focus on the scientific domain and finding the optimal word representation algorithm along with the tokenization method that could be used to represent words in the scientific domain. The goal of this research is two fold: 1) finding the optimal word representation and tokenization methods that can be used in downstream scientific domain NLP tasks, and 2) building a comprehensive evaluation suite that could be used to evaluate various word representation and tokenization algorithms (even as new ones are introduced) in the scientific domain. To this end, we build an evaluation suite consisting of several downstream tasks and relevant datasets for each task. Furthermore, we use the constructed evaluation suite to test various word representation and tokenization algorithms.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks</title>
<link>https://arxiv.org/abs/2510.06245</link>
<guid>https://arxiv.org/abs/2510.06245</guid>
<content:encoded><![CDATA[
<div> Graph models, network dynamics, community detection algorithms, evolving community structures, benchmarking<br />
<br />
Summary: 
The article introduces a new community-centered model for generating evolving community structures in graphs. This model allows for customizable changes such as community growth, shrinkage, merging, splitting, appearing, and disappearing. It also generates a temporal network where nodes can appear, disappear, or change community memberships. The benchmark aims to address the lack of real-world community evolution tracking in existing benchmarks. Three methods were tested using this benchmark to measure their performance in tracking node cluster membership and detecting community evolution. The article provides Python libraries, drawing utilities, and validation metrics to compare algorithm results with ground truth for dynamic community detection. Overall, the proposed model offers a comprehensive approach to evaluating community detection algorithms in the context of evolving networks. <div>
arXiv:2510.06245v1 Announce Type: cross 
Abstract: Graph models help understand network dynamics and evolution. Creating graphs with controlled topology and embedded partitions is a common strategy for evaluating community detection algorithms. However, existing benchmarks often overlook the need to track the evolution of communities in real-world networks. To address this, a new community-centered model is proposed to generate customizable evolving community structures where communities can grow, shrink, merge, split, appear or disappear. This benchmark also generates the underlying temporal network, where nodes can appear, disappear, or move between communities. The benchmark has been used to test three methods, measuring their performance in tracking nodes' cluster membership and detecting community evolution. Python libraries, drawing utilities, and validation metrics are provided to compare ground truth with algorithm results for detecting dynamic communities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</title>
<link>https://arxiv.org/abs/2510.06249</link>
<guid>https://arxiv.org/abs/2510.06249</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Models, Low-Resource Languages, Translation Quality, Centered Kernel Alignment, REPINA

Summary: 
The study focuses on improving translation quality from low-resource languages (LRLs) to high-resource languages (HRL) through a joint method called TRepLiNa. By aligning mid-level layers using Centered Kernel Alignment (CKA) and REPINA, the researchers explored zero-shot, few-shot, and fine-tuning settings using a large language model (LLM) across different language pairs. The results indicated that TRepLiNa is an effective and practical approach to enhancing LRL translation, especially in data-scarce contexts. This research contributes to addressing linguistic gaps in India, specifically for LRLs such as Mundari, Santali, and Bhili, by leveraging the similarities between languages within the decoder of the LLM. The findings highlight the potential of enforcing cross-lingual similarity in specific layers to improve translation quality in diverse and low-resource language settings. 

Summary: <div>
arXiv:2510.06249v1 Announce Type: cross 
Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable multilingual PII annotation for responsible AI in LLMs</title>
<link>https://arxiv.org/abs/2510.06250</link>
<guid>https://arxiv.org/abs/2510.06250</guid>
<content:encoded><![CDATA[
<div> framework, data curation, PII annotation, multilingual, LLM fine-tuning

Summary:
This research introduces a scalable framework for high-quality annotation of Personally Identifiable Information (PII) across 13 underrepresented locales, covering approximately 336 locale-specific PII types. The phased, human-in-the-loop annotation methodology combines linguistic expertise with quality assurance to improve recall and false positive rates. By utilizing inter-annotator agreement metrics and root-cause analysis, annotation inconsistencies are systematically uncovered and resolved, resulting in high-fidelity datasets suitable for supervised Large Language Model (LLM) fine-tuning. The study reports empirical gains and highlights common annotator challenges in multilingual PII labeling. Additionally, it demonstrates how iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability. <br /><br />Summary: <div>
arXiv:2510.06250v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential. This work introduces a scalable multilingual data curation framework designed for high-quality PII annotation across 13 underrepresented locales, covering approximately 336 locale-specific PII types. Our phased, human-in-the-loop annotation methodology combines linguistic expertise with rigorous quality assurance, leading to substantial improvements in recall and false positive rates from pilot, training, and production phases. By leveraging inter-annotator agreement metrics and root-cause analysis, the framework systematically uncovers and resolves annotation inconsistencies, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning. Beyond reporting empirical gains, we highlight common annotator challenges in multilingual PII labeling and demonstrate how iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence</title>
<link>https://arxiv.org/abs/2510.06252</link>
<guid>https://arxiv.org/abs/2510.06252</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG signals, dream transcription, AI-generated images, dream research, neural correlates

Summary: <br />
The Dream2Image dataset is a groundbreaking collection that combines EEG signals, dream transcriptions, and AI-generated images. With data from 38 participants and over 31 hours of dream EEG recordings, it offers insights into the final moments of brain activity before awakening, raw reports of dream experiences, and visual reconstructions of dreams. This unique resource aims to advance research in dream analysis, decode dreams from brain activity, and explore the intersection of artificial intelligence and neuroscience. Dream2Image is openly available on platforms like Hugging Face and GitHub, providing a multimodal approach to investigating the complex nature of dreaming. While the dataset has limitations in terms of sample size and dream recall variability, it serves as a valuable tool for inspiring researchers and pushing the boundaries of brain activity decoding. <div>
arXiv:2510.06252v1 Announce Type: cross 
Abstract: Dream2Image is the world's first dataset combining EEG signals, dream transcriptions, and AI-generated images. Based on 38 participants and more than 31 hours of dream EEG recordings, it contains 129 samples offering: the final seconds of brain activity preceding awakening (T-15, T-30, T-60, T-120), raw reports of dream experiences, and an approximate visual reconstruction of the dream. This dataset provides a novel resource for dream research, a unique resource to study the neural correlates of dreaming, to develop models for decoding dreams from brain activity, and to explore new approaches in neuroscience, psychology, and artificial intelligence. Available in open access on Hugging Face and GitHub, Dream2Image provides a multimodal resource designed to support research at the interface of artificial intelligence and neuroscience. It was designed to inspire researchers and extend the current approaches to brain activity decoding. Limitations include the relatively small sample size and the variability of dream recall, which may affect generalizability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven Rubric-Based Assessment of Algebraic Competence in Multi-Stage Block Coding Tasks with Design and Field Evaluation</title>
<link>https://arxiv.org/abs/2510.06253</link>
<guid>https://arxiv.org/abs/2510.06253</guid>
<content:encoded><![CDATA[
<div> education, assessment, algebraic competence, large language model, rubric framework<br />
<br />
Summary: This study introduces a rubric-based assessment framework utilizing a large language model (LLM) to measure students' algebraic competence in block coding tasks. The framework, designed by mathematics education experts, aligns problem segments with predefined rubric dimensions to evaluate both correctness and quality of students' problem-solving processes. A field study with middle school students demonstrated the system's effectiveness, showing strong agreement with expert judgments and providing process-oriented feedback. The incorporation of LLM-driven rubric assessment in online education platforms shows promise in improving assessment methods to capture the depth of students' cognitive processes in alignment with curriculum objectives for mathematics and STEM education. <div>
arXiv:2510.06253v1 Announce Type: cross 
Abstract: As online education platforms continue to expand, there is a growing need for assessment methods that not only measure answer accuracy but also capture the depth of students' cognitive processes in alignment with curriculum objectives. This study proposes and evaluates a rubric-based assessment framework powered by a large language model (LLM) for measuring algebraic competence, real-world-context block coding tasks. The problem set, designed by mathematics education experts, aligns each problem segment with five predefined rubric dimensions, enabling the LLM to assess both correctness and quality of students' problem-solving processes. The system was implemented on an online platform that records all intermediate responses and employs the LLM for rubric-aligned achievement evaluation. To examine the practical effectiveness of the proposed framework, we conducted a field study involving 42 middle school students engaged in multi-stage quadratic equation tasks with block coding. The study integrated learner self-assessments and expert ratings to benchmark the system's outputs. The LLM-based rubric evaluation showed strong agreement with expert judgments and consistently produced rubric-aligned, process-oriented feedback. These results demonstrate both the validity and scalability of incorporating LLM-driven rubric assessment into online mathematics and STEM education platforms.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis</title>
<link>https://arxiv.org/abs/2510.06260</link>
<guid>https://arxiv.org/abs/2510.06260</guid>
<content:encoded><![CDATA[
<div> Keywords: Cutaneous malignancies, AI integration, convolutional neural networks, language model capabilities, dermatological diagnostics

Summary:
This article presents a novel framework for AI integration in dermatological diagnostics to address current challenges such as inter-observer variability and access disparities. The framework incorporates a heterogeneous ensemble of convolutional neural networks to provide complementary diagnostic perspectives and includes an uncertainty mechanism to flag discordant cases for specialist review. Furthermore, language model capabilities are embedded directly into the diagnostic workflow to enhance communication with structured reports that offer precise lesion characterization, diagnostic reasoning, and actionable monitoring guidance. This integration aims to improve diagnostic reliability, bridge communication barriers, and empower patients to recognize early warning signs between visits. By addressing both diagnostic precision and patient education within a single cohesive system, this framework represents a significant advancement toward deployable dermatological AI with clinical impact. <div>
arXiv:2510.06260v1 Announce Type: cross 
Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet current diagnostics suffer from inter-observer variability and access disparities. While AI shows promise, existing dermatological systems are limited by homogeneous architectures, dataset biases across skin tones, and fragmented approaches that treat natural language processing as separate post-hoc explanations rather than integral to clinical decision-making. We introduce a unified framework that fundamentally reimagines AI integration for dermatological diagnostics through two synergistic innovations. First, a purposefully heterogeneous ensemble of architecturally diverse convolutional neural networks provides complementary diagnostic perspectives, with an intrinsic uncertainty mechanism flagging discordant cases for specialist review -- mimicking clinical best practices. Second, we embed large language model capabilities directly into the diagnostic workflow, transforming classification outputs into clinically meaningful assessments that simultaneously fulfill medical documentation requirements and deliver patient-centered education. This seamless integration generates structured reports featuring precise lesion characterization, accessible diagnostic reasoning, and actionable monitoring guidance -- empowering patients to recognize early warning signs between visits. By addressing both diagnostic reliability and communication barriers within a single cohesive system, our approach bridges the critical translational gap that has prevented previous AI implementations from achieving clinical impact. The framework represents a significant advancement toward deployable dermatological AI that enhances diagnostic precision while actively supporting the continuum of care from initial detection through patient education, ultimately improving early intervention rates for skin lesions.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments</title>
<link>https://arxiv.org/abs/2510.06262</link>
<guid>https://arxiv.org/abs/2510.06262</guid>
<content:encoded><![CDATA[
arXiv:2510.06262v1 Announce Type: cross 
Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi) Prakriti Assessment Questionnaire designed to evaluate the physical, physiological, and psychological characteristics of individuals according to classical Ayurvedic principles. The questionnaire consists of 24 multiple-choice items covering body features, appetite, sleep patterns, energy levels, and temperament. It was developed following AYUSH/CCRAS guidelines to ensure comprehensive and accurate data collection. All questions are mandatory and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha) are hidden from participants. Data were collected via a Google Forms deployment, enabling automated scoring of responses to map individual traits to dosha-specific scores. The resulting dataset provides a structured platform for research in computational intelligence, Ayurvedic studies, and personalized health analytics, supporting analysis of trait distributions, correlations, and predictive modeling. It can also serve as a reference for future Prakriti-based studies and the development of intelligent health applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians</title>
<link>https://arxiv.org/abs/2510.06263</link>
<guid>https://arxiv.org/abs/2510.06263</guid>
<content:encoded><![CDATA[
arXiv:2510.06263v1 Announce Type: cross 
Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data that can overwhelm emergency physicians trying to identify critical information. We present a two-stage summarization system that runs entirely on embedded devices, enabling offline clinical summarization while preserving patient privacy. In our approach, a dual-device architecture first retrieves relevant patient record sections using the Jetson Nano-R (Retrieve), then generates a structured summary on another Jetson Nano-S (Summarize), communicating via a lightweight socket link. The summarization output is two-fold: (1) a fixed-format list of critical findings, and (2) a context-specific narrative focused on the clinician's query. The retrieval stage uses locally stored EHRs, splits long notes into semantically coherent sections, and searches for the most relevant sections per query. The generation stage uses a locally hosted small language model (SLM) to produce the summary from the retrieved text, operating within the constraints of two NVIDIA Jetson devices. We first benchmarked six open-source SLMs under 7B parameters to identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to assess summary quality in terms of factual accuracy, completeness, and clarity. Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that our fully offline system can effectively produce useful summaries in under 30 seconds.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language models for longitudinal analysis of abusive content in Billboard Music Charts</title>
<link>https://arxiv.org/abs/2510.06266</link>
<guid>https://arxiv.org/abs/2510.06266</guid>
<content:encoded><![CDATA[
arXiv:2510.06266v1 Announce Type: cross 
Abstract: There is no doubt that there has been a drastic increase in abusive and sexually explicit content in music, particularly in Billboard Music Charts. However, there is a lack of studies that validate the trend for effective policy development, as such content has harmful behavioural changes in children and youths. In this study, we utilise deep learning methods to analyse songs (lyrics) from Billboard Charts of the United States in the last seven decades. We provide a longitudinal study using deep learning and language models and review the evolution of content using sentiment analysis and abuse detection, including sexually explicit content. Our results show a significant rise in explicit content in popular music from 1990 onwards. Furthermore, we find an increasing prevalence of songs with lyrics containing profane, sexually explicit, and otherwise inappropriate language. The longitudinal analysis of the ability of language models to capture nuanced patterns in lyrical content, reflecting shifts in societal norms and language use over time.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases</title>
<link>https://arxiv.org/abs/2510.06267</link>
<guid>https://arxiv.org/abs/2510.06267</guid>
<content:encoded><![CDATA[
arXiv:2510.06267v1 Announce Type: cross 
Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion framework that generates realistic yet privacy-preserving synthetic electronic-health-record (EHR) trajectories for ultra-rare diseases. RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph comprising approximately 8 M typed edges. Meta-path scores extracted from this 8-million-edge KG modulate the per-token noise schedule in the forward stochastic differential equation, steering generation toward biologically plausible lab-medication-adverse-event co-occurrences while retaining score-based diffusion model stability. The reverse denoiser then produces timestamped sequences of lab-code, medication-code, and adverse-event-flag triples that contain no protected health information. On simulated ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean Discrepancy by 40 percent relative to an unguided diffusion baseline and by greater than 60 percent versus GAN counterparts, without sacrificing downstream predictive utility. A black-box membership-inference evaluation using the DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55 safe-release threshold and substantially better than the approximately 0.61 plus or minus 0.03 observed for non-KG baselines, demonstrating strong resistance to re-identification. These results suggest that integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy, enabling safer data sharing for rare-disease research.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCCE: A Framework for Multi-LLM Collaborative Co-Evolution</title>
<link>https://arxiv.org/abs/2510.06270</link>
<guid>https://arxiv.org/abs/2510.06270</guid>
<content:encoded><![CDATA[
arXiv:2510.06270v1 Announce Type: cross 
Abstract: Multi-objective discrete optimization problems, such as molecular design, pose significant challenges due to their vast and unstructured combinatorial spaces. Traditional evolutionary algorithms often get trapped in local optima, while expert knowledge can provide crucial guidance for accelerating convergence. Large language models (LLMs) offer powerful priors and reasoning ability, making them natural optimizers when expert knowledge matters. However, closed-source LLMs, though strong in exploration, cannot update their parameters and thus cannot internalize experience. Conversely, smaller open models can be continually fine-tuned but lack broad knowledge and reasoning strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid framework that unites a frozen closed-source LLM with a lightweight trainable model. The system maintains a trajectory memory of past search processes; the small model is progressively refined via reinforcement learning, with the two models jointly supporting and complementing each other in global exploration. Unlike model distillation, this process enhances the capabilities of both models through mutual inspiration. Experiments on multi-objective drug design benchmarks show that MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines. These results highlight a new paradigm for enabling continual evolution in hybrid LLM systems, combining knowledge-driven exploration with experience-driven learning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"</title>
<link>https://arxiv.org/abs/2510.06275</link>
<guid>https://arxiv.org/abs/2510.06275</guid>
<content:encoded><![CDATA[
arXiv:2510.06275v1 Announce Type: cross 
Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation</title>
<link>https://arxiv.org/abs/2510.06276</link>
<guid>https://arxiv.org/abs/2510.06276</guid>
<content:encoded><![CDATA[
arXiv:2510.06276v1 Announce Type: cross 
Abstract: Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to the subtle and small-scale nature of its lesions. Accurate segmentation of FCD regions in 3D multimodal brain MRI images is essential for effective surgical planning and treatment. However, this task remains highly challenging due to the limited availability of annotated FCD datasets, the extremely small size and weak contrast of FCD lesions, the complexity of handling 3D multimodal inputs, and the need for output smoothness and anatomical consistency, which is often not addressed by standard voxel-wise loss functions. This paper presents a new framework for segmenting FCD regions in 3D brain MRI images. We adopt state-of-the-art transformer-enhanced encoder-decoder architecture and introduce a novel loss function combining Dice loss with an anisotropic {Total Variation} (TV) term. This integration encourages spatial smoothness and reduces false positive clusters without relying on post-processing. The framework is evaluated on a public FCD dataset with 85 epilepsy patients and demonstrates superior segmentation accuracy and consistency compared to standard loss formulations. The model with the proposed TV loss shows an 11.9\% improvement on the Dice coefficient and 13.3\% higher precision over the baseline model. Moreover, the number of false positive clusters is reduced by 61.6%
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets</title>
<link>https://arxiv.org/abs/2510.06278</link>
<guid>https://arxiv.org/abs/2510.06278</guid>
<content:encoded><![CDATA[
arXiv:2510.06278v1 Announce Type: cross 
Abstract: Recent advancements in neural networks, supported by foundational theoretical insights, emphasize the superior representational power of complex numbers. However, their adoption in randomized neural networks (RNNs) has been limited due to the lack of effective methods for transforming real-valued tabular datasets into complex-valued representations. To address this limitation, we propose two methods for generating complex-valued representations from real-valued datasets: a natural transformation and an autoencoder-driven method. Building on these mechanisms, we propose RVFL-X, a complex-valued extension of the random vector functional link (RVFL) network. RVFL-X integrates complex transformations into real-valued datasets while maintaining the simplicity and efficiency of the original RVFL architecture. By leveraging complex components such as input, weights, and activation functions, RVFL-X processes complex representations and produces real-valued outputs. Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that RVFL-X consistently outperforms both the original RVFL and state-of-the-art (SOTA) RNN variants, showcasing its robustness and effectiveness across diverse application domains.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals</title>
<link>https://arxiv.org/abs/2510.06280</link>
<guid>https://arxiv.org/abs/2510.06280</guid>
<content:encoded><![CDATA[
arXiv:2510.06280v1 Announce Type: cross 
Abstract: Vision language models (VLMs), such as CLIP and OpenCLIP, can encode and reflect stereotypical associations between medical professions and demographic attributes learned from web-scale data. We present an evaluation protocol for healthcare settings that quantifies associated biases and assesses their operational risk. Our methodology (i) defines a taxonomy spanning clinicians and allied healthcare roles (e.g., surgeon, cardiologist, dentist, nurse, pharmacist, technician), (ii) curates a profession-aware prompt suite to probe model behavior, and (iii) benchmarks demographic skew against a balanced face corpus. Empirically, we observe consistent demographic biases across multiple roles and vision models. Our work highlights the importance of bias identification in critical domains such as healthcare as AI-enabled hiring and workforce analytics can have downstream implications for equity, compliance, and patient trust.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning</title>
<link>https://arxiv.org/abs/2510.06281</link>
<guid>https://arxiv.org/abs/2510.06281</guid>
<content:encoded><![CDATA[
arXiv:2510.06281v1 Announce Type: cross 
Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$\alpha$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory/Goode Solar Telescope (BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation</title>
<link>https://arxiv.org/abs/2510.06283</link>
<guid>https://arxiv.org/abs/2510.06283</guid>
<content:encoded><![CDATA[
arXiv:2510.06283v1 Announce Type: cross 
Abstract: Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8\%, 94.9\%, and 94.6\%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs</title>
<link>https://arxiv.org/abs/2510.06290</link>
<guid>https://arxiv.org/abs/2510.06290</guid>
<content:encoded><![CDATA[
arXiv:2510.06290v1 Announce Type: cross 
Abstract: Identifying cancer driver genes (CDGs) is essential for understanding cancer mechanisms and developing targeted therapies. Graph neural networks (GNNs) have recently been employed to identify CDGs by capturing patterns in biological interaction networks. However, most GNN-based approaches rely on a single protein-protein interaction (PPI) network, ignoring complementary information from other biological networks. Some studies integrate multiple networks by aligning features with consistency constraints to learn unified gene representations for CDG identification. However, such representation-level fusion often assumes congruent gene relationships across networks, which may overlook network heterogeneity and introduce conflicting information. To address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a novel framework for CDG identification across multiple networks at the decision level. Instead of enforcing feature-level consistency, SEFGNN treats each biological network as an independent evidence source and performs uncertainty-aware fusion at the decision level using Dempster-Shafer Theory (DST). To alleviate the risk of overconfidence from DST, we further introduce a Soft Evidence Smoothing (SES) module that improves ranking stability while preserving discriminative performance. Experiments on three cancer datasets show that SEFGNN consistently outperforms state-of-the-art baselines and exhibits strong potential in discovering novel CDGs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation</title>
<link>https://arxiv.org/abs/2510.06291</link>
<guid>https://arxiv.org/abs/2510.06291</guid>
<content:encoded><![CDATA[
arXiv:2510.06291v1 Announce Type: cross 
Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data mining, enabling machine learning models to simulate human decision making and generate realistic trajectories, addressing both data collection costs and privacy concerns. Recent studies have shown the promise of diffusion models for high-quality trajectory generation. However, most existing methods rely on convolution based architectures (e.g. UNet) to predict noise during the diffusion process, which often results in notable deviations and the loss of fine-grained street-level details due to limited model capacity. In this paper, we propose Trajectory Transformer, a novel model that employs a transformer backbone for both conditional information embedding and noise prediction. We explore two GPS coordinate embedding strategies, location embedding and longitude-latitude embedding, and analyze model performance at different scales. Experiments on two real-world datasets demonstrate that Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations</title>
<link>https://arxiv.org/abs/2510.06292</link>
<guid>https://arxiv.org/abs/2510.06292</guid>
<content:encoded><![CDATA[
arXiv:2510.06292v1 Announce Type: cross 
Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in multimodal tasks, hallucinations continue to hinder their reliability. Among the three categories of hallucinations, which include object, attribute, and relation, relation hallucinations account for the largest proportion but have received the least attention. To address this issue, we propose ChainMPQ (Multi-Perspective Questions guided Interleaved Chain of Image and Text), a training-free method that improves relational inference in LVLMs by utilizing accumulated textual and visual memories. ChainMPQ first extracts subject and object keywords from the question to enhance the corresponding image regions. It then constructs multi-perspective questions that focus on the three core components of a relationship: the subject, the object, and the relation that links them. These questions are sequentially input to the model, with textual and visual memories from earlier steps providing supporting context for subsequent ones, thereby forming an interleaved chain of images and text that guides progressive relational reasoning. Experiments on multiple LVLMs and benchmarks show that ChainMPQ substantially reduces relation hallucinations, while ablation studies further validate the effectiveness of its three core modules.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression</title>
<link>https://arxiv.org/abs/2510.06293</link>
<guid>https://arxiv.org/abs/2510.06293</guid>
<content:encoded><![CDATA[
arXiv:2510.06293v1 Announce Type: cross 
Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling</title>
<link>https://arxiv.org/abs/2510.06295</link>
<guid>https://arxiv.org/abs/2510.06295</guid>
<content:encoded><![CDATA[
arXiv:2510.06295v1 Announce Type: cross 
Abstract: High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code</title>
<link>https://arxiv.org/abs/2510.06296</link>
<guid>https://arxiv.org/abs/2510.06296</guid>
<content:encoded><![CDATA[
arXiv:2510.06296v1 Announce Type: cross 
Abstract: Formal verification is the next frontier for ensuring the correctness of code generated by Large Language Models (LLMs). While methods that co-generate code and formal specifications in formal languages, like Dafny, can, in principle, prove alignment with user intent, progress is bottlenecked by specification quality evaluation. Current benchmarks rely on matching against ground-truth specifications, a manual and expertise-intensive process that has limited existing datasets to a few hundred simple problems and also suffers from a reliability issue. To address this, we introduce VeriEquivBench, a new benchmark with $2,389$ complex algorithmic problems that probe the limitations of current models in both code generation and formal reasoning. Our evaluation framework replaces ground-truth matching with a formally grounded metric, the equivalence score, and rigorously verifies the quality of generated specifications and code. Our results show that generating formally verifiable code remains a profound challenge for state-of-the-art LLMs. This underscores both the difficulty of the task and the need for benchmarks like VeriEquivBench to drive progress toward scalable and reliable coding agents.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGBD Gaze Tracking Using Transformer for Feature Fusion</title>
<link>https://arxiv.org/abs/2510.06298</link>
<guid>https://arxiv.org/abs/2510.06298</guid>
<content:encoded><![CDATA[
arXiv:2510.06298v1 Announce Type: cross 
Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59{\deg} and without Transformer module 3.26{\deg}, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation</title>
<link>https://arxiv.org/abs/2510.06303</link>
<guid>https://arxiv.org/abs/2510.06303</guid>
<content:encoded><![CDATA[
arXiv:2510.06303v1 Announce Type: cross 
Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2510.06343</link>
<guid>https://arxiv.org/abs/2510.06343</guid>
<content:encoded><![CDATA[
arXiv:2510.06343v1 Announce Type: cross 
Abstract: In safety-critical software systems, cybersecurity activities become essential, with risk assessment being one of the most critical. In many software teams, cybersecurity experts are either entirely absent or represented by only a small number of specialists. As a result, the workload for these experts becomes high, and software engineers would need to conduct cybersecurity activities themselves. This creates a need for a tool to support cybersecurity experts and engineers in evaluating vulnerabilities and threats during the risk assessment process. This paper explores the potential of leveraging locally hosted large language models (LLMs) with retrieval-augmented generation to support cybersecurity risk assessment in the forestry domain while complying with data protection and privacy requirements that limit external data sharing. We performed a design science study involving 12 experts in interviews, interactive sessions, and a survey within a large-scale project. The results demonstrate that LLMs can assist cybersecurity experts by generating initial risk assessments, identifying threats, and providing redundancy checks. The results also highlight the necessity for human oversight to ensure accuracy and compliance. Despite trust concerns, experts were willing to utilize LLMs in specific evaluation and assistance roles, rather than solely relying on their generative capabilities. This study provides insights that encourage the use of LLM-based agents to support the risk assessment process of cyber-physical systems in safety-critical domains.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks</title>
<link>https://arxiv.org/abs/2510.06349</link>
<guid>https://arxiv.org/abs/2510.06349</guid>
<content:encoded><![CDATA[
arXiv:2510.06349v1 Announce Type: cross 
Abstract: Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation</title>
<link>https://arxiv.org/abs/2510.06350</link>
<guid>https://arxiv.org/abs/2510.06350</guid>
<content:encoded><![CDATA[
arXiv:2510.06350v1 Announce Type: cross 
Abstract: Online communities rely on a mix of platform policies and community-authored rules to define acceptable behavior and maintain order. However, these rules vary widely across communities, evolve over time, and are enforced inconsistently, posing challenges for transparency, governance, and automation. In this paper, we model the relationship between rules and their enforcement at scale, introducing ModQ, a novel question-answering framework for rule-sensitive content moderation. Unlike prior classification or generation-based approaches, ModQ conditions on the full set of community rules at inference time and identifies which rule best applies to a given comment. We implement two model variants - extractive and multiple-choice QA - and train them on large-scale datasets from Reddit and Lemmy, the latter of which we construct from publicly available moderation logs and rule descriptions. Both models outperform state-of-the-art baselines in identifying moderation-relevant rule violations, while remaining lightweight and interpretable. Notably, ModQ models generalize effectively to unseen communities and rules, supporting low-resource moderation settings and dynamic governance environments.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransFIRA: Transfer Learning for Face Image Recognizability Assessment</title>
<link>https://arxiv.org/abs/2510.06353</link>
<guid>https://arxiv.org/abs/2510.06353</guid>
<content:encoded><![CDATA[
arXiv:2510.06353v1 Announce Type: cross 
Abstract: Face recognition in unconstrained environments such as surveillance, video, and web imagery must contend with extreme variation in pose, blur, illumination, and occlusion, where conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry. We introduce TransFIRA (Transfer Learning for Face Image Recognizability Assessment), a lightweight and annotation-free framework that grounds recognizability directly in embedding space. TransFIRA delivers three advances: (i) a definition of recognizability via class-center similarity (CCS) and class-center angular separation (CCAS), yielding the first natural, decision-boundary--aligned criterion for filtering and weighting; (ii) a recognizability-informed aggregation strategy that achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability, all without external labels, heuristics, or backbone-specific training; and (iii) new extensions beyond faces, including encoder-grounded explainability that reveals how degradations and subject-specific factors affect recognizability, and the first recognizability-aware body recognition assessment. Experiments confirm state-of-the-art results on faces, strong performance on body recognition, and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA as a unified, geometry-driven framework for recognizability assessment -- encoder-specific, accurate, interpretable, and extensible across modalities -- significantly advancing FIQA in accuracy, explainability, and scope.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Natural Language Action Planning for Resilient Embodied Systems</title>
<link>https://arxiv.org/abs/2510.06357</link>
<guid>https://arxiv.org/abs/2510.06357</guid>
<content:encoded><![CDATA[
arXiv:2510.06357v1 Announce Type: cross 
Abstract: Replicating human-level intelligence in the execution of embodied tasks remains challenging due to the unconstrained nature of real-world environments. Novel use of large language models (LLMs) for task planning seeks to address the previously intractable state/action space of complex planning tasks, but hallucinations limit their reliability, and thus, viability beyond a research context. Additionally, the prompt engineering required to achieve adequate system performance lacks transparency, and thus, repeatability. In contrast to LLM planning, symbolic planning methods offer strong reliability and repeatability guarantees, but struggle to scale to the complexity and ambiguity of real-world tasks. We introduce a new robotic planning method that augments LLM planners with symbolic planning oversight to improve reliability and repeatability, and provide a transparent approach to defining hard constraints with considerably stronger clarity than traditional prompt engineering. Importantly, these augmentations preserve the reasoning capabilities of LLMs and retain impressive generalization in open-world environments. We demonstrate our approach in simulated and real-world environments. On the ALFWorld planning benchmark, our approach outperforms current state-of-the-art methods, achieving a near-perfect 99% success rate. Deployment of our method to a real-world quadruped robot resulted in 100% task success compared to 50% and 30% for pure LLM and symbolic planners across embodied pick and place tasks. Our approach presents an effective strategy to enhance the reliability, repeatability and transparency of LLM-based robot planners while retaining their key strengths: flexibility and generalizability to complex real-world environments. We hope that this work will contribute to the broad goal of building resilient embodied intelligent systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA</title>
<link>https://arxiv.org/abs/2510.06371</link>
<guid>https://arxiv.org/abs/2510.06371</guid>
<content:encoded><![CDATA[
arXiv:2510.06371v1 Announce Type: cross 
Abstract: Large-scale multimodal models achieve strong results on tasks like Visual Question Answering (VQA), but they often fail when queries require culturally grounded, everyday knowledge, particularly in low-resource and underrepresented languages. To bridge this gap, we introduce Everyday Multimodal and Multilingual QA (EverydayMMQA), a framework for creating large-scale, culturally-grounded datasets for spoken and visual question answering (SVQA). Using this framework, we developed OASIS, a multimodal dataset integrating speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS contains 3.7M spoken questions, enabling four unique input combinations: speech-only, text-only, speech+image, and text+image. Focused on English and Arabic varieties, 18 countries, the dataset content is curated to reflect diverse, real-world situations. OASIS tests models on tasks beyond object recognition that involve pragmatic, commonsense, and culturally aware reasoning. We benchmarked four closed-source models, three open-source models, and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark and training dataset for building multimodal LLMs for a comprehensive set of everyday tasks within cultural contexts. The framework and dataset will be made publicly available to the community.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</title>
<link>https://arxiv.org/abs/2510.06377</link>
<guid>https://arxiv.org/abs/2510.06377</guid>
<content:encoded><![CDATA[
arXiv:2510.06377v1 Announce Type: cross 
Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel \textit{Relational Attention} mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 94% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Permutation Search</title>
<link>https://arxiv.org/abs/2510.06381</link>
<guid>https://arxiv.org/abs/2510.06381</guid>
<content:encoded><![CDATA[
arXiv:2510.06381v1 Announce Type: cross 
Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS is relevant when deep reinforcement learning is not an option, or when the computing power available before play is not substantial, such as in General Game Playing, for example. The principle of MCPS is to include in the exploration term of a node the statistics on all the playouts that contain all the moves on the path from the root to the node. We extensively test MCPS on a variety of games: board games, wargame, investment game, video game and multi-player games. MCPS has better results than GRAVE in all the two-player games. It has equivalent results for multi-player games because these games are inherently balanced even when players have different strengths. We also show that using abstract codes for moves instead of exact codes can be beneficial to both MCPS and GRAVE, as they improve the permutation statistics and the AMAF statistics. We also provide a mathematical derivation of the formulas used for weighting the three sources of statistics. These formulas are an improvement on the GRAVE formula since they no longer use the bias hyperparameter of GRAVE. Moreover, MCPS is not sensitive to the ref hyperparameter.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting De-identified Documents from Search-based Linkage Attacks</title>
<link>https://arxiv.org/abs/2510.06383</link>
<guid>https://arxiv.org/abs/2510.06383</guid>
<content:encoded><![CDATA[
arXiv:2510.06383v1 Announce Type: cross 
Abstract: While de-identification models can help conceal the identity of the individual(s) mentioned in a document, they fail to address linkage risks, defined as the potential to map the de-identified text back to its source. One straightforward way to perform such linkages is to extract phrases from the de-identified document and then check their presence in the original dataset. This paper presents a method to counter search-based linkage attacks while preserving the semantic integrity of the text. The method proceeds in two steps. We first construct an inverted index of the N-grams occurring in the document collection, making it possible to efficiently determine which N-grams appear in less than $k$ documents (either alone or in combination with other N-grams). An LLM-based rewriter is then iteratively queried to reformulate those spans until linkage is no longer possible. Experimental results on a collection of court cases show that the method is able to effectively prevent search-based linkages while remaining faithful to the original content.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Perspectives: Whose Opinions Do Reward Models Reward?</title>
<link>https://arxiv.org/abs/2510.06391</link>
<guid>https://arxiv.org/abs/2510.06391</guid>
<content:encoded><![CDATA[
arXiv:2510.06391v1 Announce Type: cross 
Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An RM often serves as a proxy for human preferences to guide downstream LM behavior. However, our understanding of RM behavior is limited. Our work (i) formalizes a framework for measuring the alignment of opinions captured by RMs, (ii) investigates the extent to which RMs demonstrate sociodemographic biases, and (iii) explores the effects of prompting to steer rewards towards the preferences of a target group. We study the subjective and diverse perspectives on controversial topics, which allows us to quantify RM perspectives in terms of their opinions, attitudes, and values. We show that RMs are poorly aligned with several demographic groups and can systematically reward harmful stereotypes, and steering alone is not enough to overcome these limitations. Our findings underscore the need for more careful consideration of RM behavior in model alignment during preference learning to prevent the propagation of unwanted social biases in the language technologies that we use.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Protein Design Protocols and Middleware</title>
<link>https://arxiv.org/abs/2510.06396</link>
<guid>https://arxiv.org/abs/2510.06396</guid>
<content:encoded><![CDATA[
arXiv:2510.06396v1 Announce Type: cross 
Abstract: Computational protein design is experiencing a transformation driven by AI/ML. However, the range of potential protein sequences and structures is astronomically vast, even for moderately sized proteins. Hence, achieving convergence between generated and predicted structures demands substantial computational resources for sampling. The Integrated Machine-learning for Protein Structures at Scale (IMPRESS) offers methods and advanced computing systems for coupling AI to high-performance computing tasks, enabling the ability to evaluate the effectiveness of protein designs as they are developed, as well as the models and simulations used to generate data and train models. This paper introduces IMPRESS and demonstrates the development and implementation of an adaptive protein design protocol and its supporting computing infrastructure. This leads to increased consistency in the quality of protein design and enhanced throughput of protein design due to dynamic resource allocation and asynchronous workload execution.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings</title>
<link>https://arxiv.org/abs/2510.06397</link>
<guid>https://arxiv.org/abs/2510.06397</guid>
<content:encoded><![CDATA[
arXiv:2510.06397v1 Announce Type: cross 
Abstract: Non-Euclidean foundation models increasingly place representations in curved spaces such as hyperbolic geometry. We show that this geometry creates a boundary-driven asymmetry that backdoor triggers can exploit. Near the boundary, small input changes appear subtle to standard input-space detectors but produce disproportionately large shifts in the model's representation space. Our analysis formalizes this effect and also reveals a limitation for defenses: methods that act by pulling points inward along the radius can suppress such triggers, but only by sacrificing useful model sensitivity in that same direction. Building on these insights, we propose a simple geometry-adaptive trigger and evaluate it across tasks and architectures. Empirically, attack success increases toward the boundary, whereas conventional detectors weaken, mirroring the theoretical trends. Together, these results surface a geometry-specific vulnerability in non-Euclidean models and offer analysis-backed guidance for designing and understanding the limits of defenses.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks</title>
<link>https://arxiv.org/abs/2510.06444</link>
<guid>https://arxiv.org/abs/2510.06444</guid>
<content:encoded><![CDATA[
arXiv:2510.06444v1 Announce Type: cross 
Abstract: In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Agentic Security: Applications, Threats and Defenses</title>
<link>https://arxiv.org/abs/2510.06445</link>
<guid>https://arxiv.org/abs/2510.06445</guid>
<content:encoded><![CDATA[
arXiv:2510.06445v1 Announce Type: cross 
Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new paradigm in cybersecurity. While these agents can act as powerful tools for both offensive and defensive operations, the very agentic context introduces a new class of inherent security risks. In this work we present the first holistic survey of the agentic security landscape, structuring the field around three interdependent pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 150 papers, explaining how agents are used, the vulnerabilities they possess, and the countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation</title>
<link>https://arxiv.org/abs/2510.06448</link>
<guid>https://arxiv.org/abs/2510.06448</guid>
<content:encoded><![CDATA[
arXiv:2510.06448v1 Announce Type: cross 
Abstract: Transferability estimation metrics are used to find a high-performing pre-trained model for a given target task without fine-tuning models and without access to the source dataset. Despite the growing interest in developing such metrics, the benchmarks used to measure their progress have gone largely unexamined. In this work, we empirically show the shortcomings of widely used benchmark setups to evaluate transferability estimation metrics. We argue that the benchmarks on which these metrics are evaluated are fundamentally flawed. We empirically demonstrate that their unrealistic model spaces and static performance hierarchies artificially inflate the perceived performance of existing metrics, to the point where simple, dataset-agnostic heuristics can outperform sophisticated methods. Our analysis reveals a critical disconnect between current evaluation protocols and the complexities of real-world model selection. To address this, we provide concrete recommendations for constructing more robust and realistic benchmarks to guide future research in a more meaningful direction.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Node-tree Interfaces for AI Explainability</title>
<link>https://arxiv.org/abs/2510.06457</link>
<guid>https://arxiv.org/abs/2510.06457</guid>
<content:encoded><![CDATA[
arXiv:2510.06457v1 Announce Type: cross 
Abstract: As large language models (LLMs) become ubiquitous in workplace tools and decision-making processes, ensuring explainability and fostering user trust are critical. Although advancements in LLM engineering continue, human-centered design is still catching up, particularly when it comes to embedding transparency and trust into AI interfaces. This study evaluates user experiences with two distinct AI interfaces - node-tree interfaces and chatbot interfaces - to assess their performance in exploratory, follow-up inquiry, decision-making, and problem-solving tasks. Our design-driven approach introduces a node-tree interface that visually structures AI-generated responses into hierarchically organized, interactive nodes, allowing users to navigate, refine, and follow up on complex information. In a comparative study with n=20 business users, we observed that while the chatbot interface effectively supports linear, step-by-step queries, it is the node-tree interface that enhances brainstorming. Quantitative and qualitative findings indicate that node-tree interfaces not only improve task performance and decision-making support but also promote higher levels of user trust by preserving context. Our findings suggest that adaptive AI interfaces capable of switching between structured visualizations and conversational formats based on task requirements can significantly enhance transparency and user confidence in AI-powered systems. This work contributes actionable insights to the fields of human-robot interaction and AI design, particularly for enterprise applications where trust-building is critical for teams.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Model for Human Mobility Behavior</title>
<link>https://arxiv.org/abs/2510.06473</link>
<guid>https://arxiv.org/abs/2510.06473</guid>
<content:encoded><![CDATA[
arXiv:2510.06473v1 Announce Type: cross 
Abstract: Understanding and modeling human mobility is central to challenges in transport planning, sustainable urban design, and public health. Despite decades of effort, simulating individual mobility remains challenging because of its complex, context-dependent, and exploratory nature. Here, we present MobilityGen, a deep generative model that produces realistic mobility trajectories spanning days to weeks at large spatial scales. By linking behavioral attributes with environmental context, MobilityGen reproduces key patterns such as scaling laws for location visits, activity time allocation, and the coupled evolution of travel mode and destination choices. It reflects spatio-temporal variability and generates diverse, plausible, and novel mobility patterns consistent with the built environment. Beyond standard validation, MobilityGen yields insights not attainable with earlier models, including how access to urban space varies across travel modes and how co-presence dynamics shape social exposure and segregation. Our work establishes a new framework for mobility simulation, paving the way for fine-grained, data-driven studies of human behavior and its societal implications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin</title>
<link>https://arxiv.org/abs/2510.06477</link>
<guid>https://arxiv.org/abs/2510.06477</guid>
<content:encoded><![CDATA[
arXiv:2510.06477v1 Announce Type: cross 
Abstract: Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift</title>
<link>https://arxiv.org/abs/2510.06478</link>
<guid>https://arxiv.org/abs/2510.06478</guid>
<content:encoded><![CDATA[
arXiv:2510.06478v1 Announce Type: cross 
Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift -- the log-likelihood ratio between full models and deliberately weakened "skeleton" baselines -- using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation by 22-28% vs. sequential baselines while maintaining delta-level control with 12% computational overhead. We introduce automated skeletons (distilled submodels, randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries + verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness -- 10.9% of stopped sequences remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a first-stage filter reducing verification burden by 83%, not as a standalone solution for safety-critical domains.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels</title>
<link>https://arxiv.org/abs/2510.06499</link>
<guid>https://arxiv.org/abs/2510.06499</guid>
<content:encoded><![CDATA[
arXiv:2510.06499v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting</title>
<link>https://arxiv.org/abs/2510.06503</link>
<guid>https://arxiv.org/abs/2510.06503</guid>
<content:encoded><![CDATA[
arXiv:2510.06503v1 Announce Type: cross 
Abstract: Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate. This paper introduces ATLO-ML, an adaptive time-length optimization system that automatically determines the optimal input time length and sampling rate based on user-defined output time length. The system provides a flexible approach to time-series data pre-processing, dynamically adjusting these parameters to enhance predictive performance. ATLO-ML is validated using air quality datasets, including both GAMS-dataset and proprietary data collected from a data center, both in time series format. Results demonstrate that utilizing the optimized time length and sampling rate significantly improves the accuracy of machine learning models compared to fixed time lengths. ATLO-ML shows potential for generalization across various time-sensitive applications, offering a robust solution for optimizing temporal input parameters in machine learning workflows.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Median Perspective on Unlabeled Data for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2510.06505</link>
<guid>https://arxiv.org/abs/2510.06505</guid>
<content:encoded><![CDATA[
arXiv:2510.06505v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the robustness and reliability of machine learning systems deployed in real-world applications. Recent approaches have explored the use of unlabeled data, showing potential for enhancing OOD detection capabilities. However, effectively utilizing unlabeled in-the-wild data remains challenging due to the mixed nature of both in-distribution (InD) and OOD samples. The lack of a distinct set of OOD samples complicates the task of training an optimal OOD classifier. In this work, we introduce Medix, a novel framework designed to identify potential outliers from unlabeled data using the median operation. We use the median because it provides a stable estimate of the central tendency, as an OOD detection mechanism, due to its robustness against noise and outliers. Using these identified outliers, along with labeled InD data, we train a robust OOD classifier. From a theoretical perspective, we derive error bounds that demonstrate Medix achieves a low error rate. Empirical results further substantiate our claims, as Medix outperforms existing methods across the board in open-world settings, confirming the validity of our theoretical insights.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval</title>
<link>https://arxiv.org/abs/2510.06512</link>
<guid>https://arxiv.org/abs/2510.06512</guid>
<content:encoded><![CDATA[
arXiv:2510.06512v1 Announce Type: cross 
Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties such as objects ("car") and emotions ("angry") in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g., "does the speaker eventually sound happy in this audio clip?"), and ranked retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is detected until a pedestrian is detected"). In this work, we formalize this problem of assigning Scores for TempOral Properties (STOPs) over sequences, given potentially noisy score predictors for local properties. We then propose a scoring function called LogSTOP that can efficiently compute these scores for temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP, with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and other Temporal Logic-based baselines by at least 16% on query matching with temporal properties over objects-in-videos and emotions-in-speech respectively. Similarly, on ranked retrieval with temporal properties over objects and actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a 19% and 16% increase in mean average precision and recall over zero-shot text-to-video retrieval baselines respectively.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualizing Multimodality in Combinatorial Search Landscapes</title>
<link>https://arxiv.org/abs/2510.06517</link>
<guid>https://arxiv.org/abs/2510.06517</guid>
<content:encoded><![CDATA[
arXiv:2510.06517v1 Announce Type: cross 
Abstract: This work walks through different visualization techniques for combinatorial search landscapes, focusing on multimodality. We discuss different techniques from the landscape analysis literature, and how they can be combined to provide a more comprehensive view of the search landscape. We also include examples and discuss relevant work to show how others have used these techniques in practice, based on the geometric and aesthetic elements of the Grammar of Graphics. We conclude that there is no free lunch in visualization, and provide recommendations for future work as there are several paths to continue the work in this field.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification</title>
<link>https://arxiv.org/abs/2510.06532</link>
<guid>https://arxiv.org/abs/2510.06532</guid>
<content:encoded><![CDATA[
arXiv:2510.06532v1 Announce Type: cross 
Abstract: Quantum compute is scaling fast, from cloud QPUs to high throughput GPU simulators, making it timely to prototype quantum NLP beyond toy tasks. However, devices remain qubit limited and depth limited, training can be unstable, and classical attention is compute and memory heavy. This motivates compact, phase aware quantum token mixers that stabilize amplitudes and scale to long sequences. We present CLAQS, a compact, fully quantum token mixer for text classification that jointly learns complex-valued mixing and nonlinear transformations within a unified quantum circuit. To enable stable end-to-end optimization, we apply l1 normalization to regulate amplitude scaling and introduce a two-stage parameterized quantum architecture that decouples shared token embeddings from a window-level quantum feed-forward module. Operating under a sliding-window regime with document-level aggregation, CLAQS requires only eight data qubits and shallow circuits, yet achieves 91.64% accuracy on SST-2 and 87.08% on IMDB, outperforming both classical Transformer baselines and strong hybrid quantum-classical counterparts.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Policy-Based RL Algorithms for POMDPs</title>
<link>https://arxiv.org/abs/2510.06540</link>
<guid>https://arxiv.org/abs/2510.06540</guid>
<content:encoded><![CDATA[
arXiv:2510.06540v1 Announce Type: cross 
Abstract: The continuous nature of belief states in POMDPs presents significant computational challenges in learning the optimal policy. In this paper, we consider an approach that solves a Partially Observable Reinforcement Learning (PORL) problem by approximating the corresponding POMDP model into a finite-state Markov Decision Process (MDP) (called Superstate MDP). We first derive theoretical guarantees that improve upon prior work that relate the optimal value function of the transformed Superstate MDP to the optimal value function of the original POMDP. Next, we propose a policy-based learning approach with linear function approximation to learn the optimal policy for the Superstate MDP. Consequently, our approach shows that a POMDP can be approximately solved using TD-learning followed by Policy Optimization by treating it as an MDP, where the MDP state corresponds to a finite history. We show that the approximation error decreases exponentially with the length of this history. To the best of our knowledge, our finite-time bounds are the first to explicitly quantify the error introduced when applying standard TD learning to a setting where the true dynamics are not Markovian.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incoherence in goal-conditioned autoregressive models</title>
<link>https://arxiv.org/abs/2510.06545</link>
<guid>https://arxiv.org/abs/2510.06545</guid>
<content:encoded><![CDATA[
arXiv:2510.06545v1 Announce Type: cross 
Abstract: We investigate mathematically the notion of incoherence: a structural issue with reinforcement learning policies derived by naive goal-conditioning of autoregressive models. We focus on the process of re-training models on their own actions, that is, fine-tuning offline-learned policies with online RL. We prove that it decreases incoherence and leads to an improvement in return, and we aim to characterize the resulting trajectory of policies. By re-framing standard notions of control-as-inference and soft Q learning, we establish a three-way correspondence with two other ways of understanding the iterative re-training process: as folding the posterior into the reward and, in the deterministic case, as decreasing the temperature parameter; the correspondence has computational content via the training-inference trade-off. Through soft-conditioning generative models, we discuss the link between incoherence and the effective horizon.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Markovian Thinker</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[
arXiv:2510.06557v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law</title>
<link>https://arxiv.org/abs/2510.06559</link>
<guid>https://arxiv.org/abs/2510.06559</guid>
<content:encoded><![CDATA[
arXiv:2510.06559v1 Announce Type: cross 
Abstract: Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague's view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system "parses once" (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution</title>
<link>https://arxiv.org/abs/2510.06564</link>
<guid>https://arxiv.org/abs/2510.06564</guid>
<content:encoded><![CDATA[
arXiv:2510.06564v1 Announce Type: cross 
Abstract: Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are frequently impeded by excessive computational complexity. To overcome these limitations, this paper proposes the Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently leverages graph modeling while maintaining computational feasibility. The core idea of HSNet is to decompose the global graph into manageable sub-components. First, we introduce the Constructive Subgraph Set Block (CSSB), which generates a diverse set of complementary subgraphs. Rather than relying on a single monolithic graph, CSSB captures heterogeneous characteristics of the image by modeling different relational patterns and feature interactions, producing a rich ensemble of both local and global graph structures. Subsequently, the Subgraph Aggregation Block (SAB) integrates the representations embedded across these subgraphs. Through adaptive weighting and fusion of multi-graph features, SAB constructs a comprehensive and discriminative representation that captures intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is designed to selectively retain the most salient features, thereby enhancing accuracy while reducing computational overhead. Extensive experiments demonstrate that HSNet achieves state-of-the-art performance, effectively balancing reconstruction quality with computational efficiency. The code will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials</title>
<link>https://arxiv.org/abs/2510.06567</link>
<guid>https://arxiv.org/abs/2510.06567</guid>
<content:encoded><![CDATA[
arXiv:2510.06567v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) holds great promise for supporting clinical trials, from patient recruitment and endpoint assessment to treatment response prediction. However, deploying AI without safeguards poses significant risks, particularly when evaluating patient endpoints that directly impact trial conclusions. We compared two AI frameworks against human-only assessment for medical image-based disease evaluation, measuring cost, accuracy, robustness, and generalization ability. To stress-test these frameworks, we injected bad models, ranging from random guesses to naive predictions, to ensure that observed treatment effects remain valid even under severe model degradation. We evaluated the frameworks using two randomized controlled trials with endpoints derived from spinal X-ray images. Our findings indicate that using AI as a supporting reader (AI-SR) is the most suitable approach for clinical trials, as it meets all criteria across various model types, even with bad models. This method consistently provides reliable disease estimation, preserves clinical trial treatment effect estimates and conclusions, and retains these advantages when applied to different populations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation</title>
<link>https://arxiv.org/abs/2510.06596</link>
<guid>https://arxiv.org/abs/2510.06596</guid>
<content:encoded><![CDATA[
arXiv:2510.06596v1 Announce Type: cross 
Abstract: The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at https://github.com/ayushzenith/SDQM
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation</title>
<link>https://arxiv.org/abs/2510.06605</link>
<guid>https://arxiv.org/abs/2510.06605</guid>
<content:encoded><![CDATA[
arXiv:2510.06605v1 Announce Type: cross 
Abstract: The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a "fingerprint") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Forecasting and Monitoring of Urban Water System</title>
<link>https://arxiv.org/abs/2510.06631</link>
<guid>https://arxiv.org/abs/2510.06631</guid>
<content:encoded><![CDATA[
arXiv:2510.06631v1 Announce Type: cross 
Abstract: Underground water and wastewater pipelines are vital for city operations but plagued by anomalies like leaks and infiltrations, causing substantial water loss, environmental damage, and high repair costs. Conventional manual inspections lack efficiency, while dense sensor deployments are prohibitively expensive. In recent years, artificial intelligence has advanced rapidly and is increasingly applied to urban infrastructure. In this research, we propose an integrated AI and remote-sensor framework to address the challenge of leak detection in underground water pipelines, through deploying a sparse set of remote sensors to capture real-time flow and depth data, paired with HydroNet - a dedicated model utilizing pipeline attributes (e.g., material, diameter, slope) in a directed graph for higher-precision modeling. Evaluations on a real-world campus wastewater network dataset demonstrate that our system collects effective spatio-temporal hydraulic data, enabling HydroNet to outperform advanced baselines. This integration of edge-aware message passing with hydraulic simulations enables accurate network-wide predictions from limited sensor deployments. We envision that this approach can be effectively extended to a wide range of underground water pipeline networks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v1 Announce Type: cross 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering</title>
<link>https://arxiv.org/abs/2510.06638</link>
<guid>https://arxiv.org/abs/2510.06638</guid>
<content:encoded><![CDATA[
arXiv:2510.06638v1 Announce Type: cross 
Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Lightweight Language Models for C/C++ Vulnerabilities</title>
<link>https://arxiv.org/abs/2510.06645</link>
<guid>https://arxiv.org/abs/2510.06645</guid>
<content:encoded><![CDATA[
arXiv:2510.06645v1 Announce Type: cross 
Abstract: The increasing complexity of modern software systems exacerbates the prevalence of security vulnerabilities, posing risks of severe breaches and substantial economic loss. Consequently, robust code vulnerability detection is essential for software security. While Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, their potential for automated code vulnerability detection remains underexplored. This paper presents FineSec, a novel framework that harnesses LLMs through knowledge distillation to enable efficient and precise vulnerability identification in C/C++ codebases. FineSec utilizes knowledge distillation to transfer expertise from large teacher models to compact student models, achieving high accuracy with minimal computational cost. By integrating data preparation, training, evaluation, and continuous learning into a unified, single-task workflow, FineSec offers a streamlined approach. Extensive evaluations on C/C++ codebases demonstrate its superiority over both base models and larger LLMs in identifying complex vulnerabilities and logical flaws, establishing FineSec as a practical and scalable solution for real-world software security. To facilitate reproducibility, the datasets, source code, and experimental results are made publicly available at: https://github.com/yangxiaoxuan123/FineSec_detect.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators</title>
<link>https://arxiv.org/abs/2510.06646</link>
<guid>https://arxiv.org/abs/2510.06646</guid>
<content:encoded><![CDATA[
arXiv:2510.06646v1 Announce Type: cross 
Abstract: A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform "zero-shot super-resolution," namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions</title>
<link>https://arxiv.org/abs/2510.06649</link>
<guid>https://arxiv.org/abs/2510.06649</guid>
<content:encoded><![CDATA[
arXiv:2510.06649v1 Announce Type: cross 
Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure for neural networks that employs two forward passes instead of the traditional forward and backward passes used in backpropagation. However, FF remains largely confined to supervised settings, leaving a gap at domains where learning signals can be yielded more naturally such as RL. In this work, inspired by FF's goodness function using layer activity statistics, we introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value estimation method that applies a goodness function and action conditioning for local RL using temporal difference learning. Despite its simplicity and biological grounding, our approach achieves superior performance compared to state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind Control Suite benchmarks, while also outperforming algorithms trained with backpropagation on most tasks. Code can be found at https://github.com/agentic-learning-ai-lab/arq.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delay Independent Safe Control with Neural Networks: Positive Lur'e Certificates for Risk Aware Autonomy</title>
<link>https://arxiv.org/abs/2510.06661</link>
<guid>https://arxiv.org/abs/2510.06661</guid>
<content:encoded><![CDATA[
arXiv:2510.06661v1 Announce Type: cross 
Abstract: We present a risk-aware safety certification method for autonomous, learning enabled control systems. Focusing on two realistic risks, state/input delays and interval matrix uncertainty, we model the neural network (NN) controller with local sector bounds and exploit positivity structure to derive linear, delay-independent certificates that guarantee local exponential stability across admissible uncertainties. To benchmark performance, we adopt and implement a state-of-the-art IQC NN verification pipeline. On representative cases, our positivity-based tests run orders of magnitude faster than SDP-based IQC while certifying regimes the latter cannot-providing scalable safety guarantees that complement risk-aware control.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Neural Architecture Design for Industrial Defect Detection</title>
<link>https://arxiv.org/abs/2510.06669</link>
<guid>https://arxiv.org/abs/2510.06669</guid>
<content:encoded><![CDATA[
arXiv:2510.06669v1 Announce Type: cross 
Abstract: Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at https://github.com/Yuxi104/AutoNAD.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heptapod: Language Modeling on Visual Signals</title>
<link>https://arxiv.org/abs/2510.06673</link>
<guid>https://arxiv.org/abs/2510.06673</guid>
<content:encoded><![CDATA[
arXiv:2510.06673v1 Announce Type: cross 
Abstract: We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \textbf{causal attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend of semantic tokenizers}. Our key innovation is \textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback</title>
<link>https://arxiv.org/abs/2510.06677</link>
<guid>https://arxiv.org/abs/2510.06677</guid>
<content:encoded><![CDATA[
arXiv:2510.06677v1 Announce Type: cross 
Abstract: We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion</title>
<link>https://arxiv.org/abs/2510.06687</link>
<guid>https://arxiv.org/abs/2510.06687</guid>
<content:encoded><![CDATA[
arXiv:2510.06687v1 Announce Type: cross 
Abstract: Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; however, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential reconstruction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image-only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks</title>
<link>https://arxiv.org/abs/2510.06695</link>
<guid>https://arxiv.org/abs/2510.06695</guid>
<content:encoded><![CDATA[
arXiv:2510.06695v1 Announce Type: cross 
Abstract: In recent years, the growing interest in Large Language Models (LLMs) has significantly advanced prompt engineering, transitioning from manual design to model-based optimization. Prompts for LLMs generally comprise two components: the \textit{instruction}, which defines the task or objective, and the \textit{input}, which is tailored to the instruction type. In natural language generation (NLG) tasks such as machine translation, the \textit{input} component is particularly critical, while the \textit{instruction} component tends to be concise. Existing prompt engineering methods primarily focus on optimizing the \textit{instruction} component for general tasks, often requiring large-parameter LLMs as auxiliary tools. However, these approaches exhibit limited applicability for tasks like machine translation, where the \textit{input} component plays a more pivotal role. To address this limitation, this paper introduces a novel prompt optimization method specifically designed for machine translation tasks. The proposed approach employs a small-parameter model trained using a back-translation-based strategy, significantly reducing training overhead for single-task optimization while delivering highly effective performance. With certain adaptations, this method can also be extended to other downstream tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AISysRev -- LLM-based Tool for Title-abstract Screening</title>
<link>https://arxiv.org/abs/2510.06708</link>
<guid>https://arxiv.org/abs/2510.06708</guid>
<content:encoded><![CDATA[
arXiv:2510.06708v1 Announce Type: cross 
Abstract: Systematic reviews are a standard practice for summarizing the state of evidence in software engineering. Conducting systematic reviews is laborious, especially during the screening or study selection phase, where the number of papers can be overwhelming. During this phase, papers are assessed against inclusion and exclusion criteria based on their titles and abstracts. Recent research has demonstrated that large language models (LLMs) can perform title-abstract screening at a level comparable to that of a master's student. While LLMs cannot be fully trusted, they can help, for example, in Rapid Reviews, which try to expedite the review process. Building on recent research, we developed AiSysRev, an LLM-based screening tool implemented as a web application running in a Docker container. The tool accepts a CSV file containing paper titles and abstracts. Users specify inclusion and exclusion criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev supports both zero-shot and few-shot screening, and also allows for manual screening through interfaces that display LLM results as guidance for human reviewers.We conducted a trial study with 137 papers using the tool. Our findings indicate that papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary cases, where LLMs are prone to errors, highlight the need for human intervention. While LLMs do not replace human judgment in systematic reviews, they can significantly reduce the burden of assessing large volumes of scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool: https://github.com/EvoTestOps/AISysRev
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Goal Representations</title>
<link>https://arxiv.org/abs/2510.06714</link>
<guid>https://arxiv.org/abs/2510.06714</guid>
<content:encoded><![CDATA[
arXiv:2510.06714v1 Announce Type: cross 
Abstract: In this work, we introduce dual goal representations for goal-conditioned reinforcement learning (GCRL). A dual goal representation characterizes a state by "the set of temporal distances from all other states"; in other words, it encodes a state through its relations to every other state, measured by temporal distance. This representation provides several appealing theoretical properties. First, it depends only on the intrinsic dynamics of the environment and is invariant to the original state representation. Second, it contains provably sufficient information to recover an optimal goal-reaching policy, while being able to filter out exogenous noise. Based on this concept, we develop a practical goal representation learning method that can be combined with any existing GCRL algorithm. Through diverse experiments on the OGBench task suite, we empirically show that dual goal representations consistently improve offline goal-reaching performance across 20 state- and pixel-based tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Company Policies and Policy Implications in Software Organizations</title>
<link>https://arxiv.org/abs/2510.06718</link>
<guid>https://arxiv.org/abs/2510.06718</guid>
<content:encoded><![CDATA[
arXiv:2510.06718v1 Announce Type: cross 
Abstract: The risks associated with adopting large language model (LLM) chatbots in software organizations highlight the need for clear policies. We examine how 11 companies create these policies and the factors that influence them, aiming to help managers safely integrate chatbots into development workflows.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management</title>
<link>https://arxiv.org/abs/2510.06727</link>
<guid>https://arxiv.org/abs/2510.06727</guid>
<content:encoded><![CDATA[
arXiv:2510.06727v1 Announce Type: cross 
Abstract: We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization (\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</title>
<link>https://arxiv.org/abs/2510.06732</link>
<guid>https://arxiv.org/abs/2510.06732</guid>
<content:encoded><![CDATA[
arXiv:2510.06732v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities</title>
<link>https://arxiv.org/abs/2510.06743</link>
<guid>https://arxiv.org/abs/2510.06743</guid>
<content:encoded><![CDATA[
arXiv:2510.06743v1 Announce Type: cross 
Abstract: Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2510.06776</link>
<guid>https://arxiv.org/abs/2510.06776</guid>
<content:encoded><![CDATA[
arXiv:2510.06776v1 Announce Type: cross 
Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and analysis to understand real-world disease dynamics. In particular, post hoc analyses using compartmental models offer valuable insights into the effectiveness of public health interventions, such as vaccination strategies and containment policies. However, such compartmental models like SIR (Susceptible-Infectious-Recovered) often face limitations in directly incorporating noisy observational data. In this work, we employ Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the SIR model using infection data from the Robert Koch Institute (RKI). Our main contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics across all German federal states over a three-year period. We estimate state-specific transmission and recovery parameters and time-varying reproduction number (R_t) to track the pandemic progression. The results highlight strong variations in transmission behavior across regions, revealing correlations with vaccination uptake and temporal patterns associated with major pandemic phases. Our findings demonstrate the utility of PINNs in localized, long-term epidemiological modeling.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness</title>
<link>https://arxiv.org/abs/2510.06780</link>
<guid>https://arxiv.org/abs/2510.06780</guid>
<content:encoded><![CDATA[
arXiv:2510.06780v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured format, for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using miniGPTKBs (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation type: high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extreme Amodal Face Detection</title>
<link>https://arxiv.org/abs/2510.06791</link>
<guid>https://arxiv.org/abs/2510.06791</guid>
<content:encoded><![CDATA[
arXiv:2510.06791v1 Announce Type: cross 
Abstract: Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline</title>
<link>https://arxiv.org/abs/2510.06800</link>
<guid>https://arxiv.org/abs/2510.06800</guid>
<content:encoded><![CDATA[
arXiv:2510.06800v1 Announce Type: cross 
Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character's responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrence-Complete Frame-based Action Models</title>
<link>https://arxiv.org/abs/2510.06828</link>
<guid>https://arxiv.org/abs/2510.06828</guid>
<content:encoded><![CDATA[
arXiv:2510.06828v1 Announce Type: cross 
Abstract: In recent years, attention-like mechanisms have been used to great success in the space of large language models, unlocking scaling potential to a previously unthinkable extent. "Attention Is All You Need" famously claims RNN cells are not needed in conjunction with attention. We challenge this view. In this paper, we point to existing proofs that architectures with fully parallelizable forward or backward passes cannot represent classes of problems specifically interesting for long-running agentic tasks. We further conjecture a critical time t beyond which non-recurrence-complete models fail to aggregate inputs correctly, with concrete implications for agentic systems (e.g., software engineering agents). To address this, we introduce a recurrence-complete architecture and train it on GitHub-derived action sequences. Loss follows a power law in the trained sequence length while the parameter count remains fixed. Moreover, longer-sequence training always amortizes its linearly increasing wall-time cost, yielding lower loss as a function of wall time.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting</title>
<link>https://arxiv.org/abs/2510.06840</link>
<guid>https://arxiv.org/abs/2510.06840</guid>
<content:encoded><![CDATA[
arXiv:2510.06840v1 Announce Type: cross 
Abstract: Convolutional neural networks (CNNs) and transformer architectures offer strengths for modeling temporal data: CNNs excel at capturing local patterns and translational invariances, while transformers effectively model long-range dependencies via self-attention. This paper proposes a hybrid architecture integrating convolutional feature extraction with a temporal fusion transformer (TFT) backbone to enhance multivariate time series forecasting. The CNN module first applies a hierarchy of one-dimensional convolutional layers to distill salient local patterns from raw input sequences, reducing noise and dimensionality. The resulting feature maps are then fed into the TFT, which applies multi-head attention to capture both short- and long-term dependencies and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a hydroelectric natural flow time series dataset. Experimental results demonstrate that CNN-TFT outperforms well-established deep learning models, with a mean absolute percentage error of up to 2.2%. The explainability of the model is obtained by a proposed Shapley additive explanations with multi-head attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW, is promising for applications requiring high-fidelity, multivariate time series forecasts, being available for future analysis at https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SID: Multi-LLM Debate Driven by Self Signals</title>
<link>https://arxiv.org/abs/2510.06843</link>
<guid>https://arxiv.org/abs/2510.06843</guid>
<content:encoded><![CDATA[
arXiv:2510.06843v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenJAI-v1.0: An Open Thai Large Language Model</title>
<link>https://arxiv.org/abs/2510.06847</link>
<guid>https://arxiv.org/abs/2510.06847</guid>
<content:encoded><![CDATA[
arXiv:2510.06847v1 Announce Type: cross 
Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model. Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use. Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis</title>
<link>https://arxiv.org/abs/2510.06852</link>
<guid>https://arxiv.org/abs/2510.06852</guid>
<content:encoded><![CDATA[
arXiv:2510.06852v1 Announce Type: cross 
Abstract: Context: Financial system stability is determined by the condition of the banking system. A bank failure can destroy the stability of the financial system, as banks are subject to systemic risk, affecting not only individual banks but also segments or the entire financial system. Calculating the probability of a bank going bankrupt is one way to ensure the banking system is safe and sound. Existing literature and limitations: Statistical models, such as Altman's Z-Score, are one of the common techniques for developing a bankruptcy prediction model. However, statistical methods rely on rigid and sometimes irrelevant assumptions, which can result in low forecast accuracy. New approaches are necessary. Objective of the research: Bankruptcy models are developed using machine learning techniques, such as logistic regression (LR), random forest (RF), and support vector machines (SVM). According to several studies, machine learning is also more accurate and effective than statistical methods for categorising and forecasting banking risk management. Present Research: The commercial bank data are derived from the annual financial statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to 2004, and the rural bank data are derived from the quarterly financial reports of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019. Five rural banks in Indonesia have also been selected to demonstrate the feasibility of analysing bank bankruptcy trends. Findings and implications: The results of the research experiments show that RF can forecast data from commercial banks with a 90% accuracy rate. Furthermore, the three machine learning methods proposed accurately predict the likelihood of rural bank bankruptcy. Contribution and Conclusion: The proposed innovative machine learning approach help to implement policies that reduce the costs of bankruptcy.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining raw data complexity to improve satellite onboard processing</title>
<link>https://arxiv.org/abs/2510.06858</link>
<guid>https://arxiv.org/abs/2510.06858</guid>
<content:encoded><![CDATA[
arXiv:2510.06858v1 Announce Type: cross 
Abstract: With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalization of Graph Neural Networks for AC Optimal Power Flow</title>
<link>https://arxiv.org/abs/2510.06860</link>
<guid>https://arxiv.org/abs/2510.06860</guid>
<content:encoded><![CDATA[
arXiv:2510.06860v1 Announce Type: cross 
Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale power systems, with conventional solvers requiring prohibitive solution times. Machine learning approaches offer computational speedups but struggle with scalability and topology adaptability without expensive retraining. To enable scalability across grid sizes and adaptability to topology changes, we propose a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models buses, generators, loads, shunts, transmission lines and transformers as distinct node or edge types, combined with a scalable transformer model for handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN achieves less than 1% optimality gap on default topologies. Applied zero-shot to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap despite training only on default topologies. Pre-training on smaller grids also improves results on a larger grid. Computational speedups reach 1,000x to 10,000x compared to interior point solvers. These results advance practical, generalizable machine learning for real-time power system operations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval</title>
<link>https://arxiv.org/abs/2510.06868</link>
<guid>https://arxiv.org/abs/2510.06868</guid>
<content:encoded><![CDATA[
arXiv:2510.06868v1 Announce Type: cross 
Abstract: We consider image transmission via deep joint source-channel coding (DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by training a DeepJSCC encoder-decoder pair with a pre-trained deep hash distillation (DHD) module to semantically cluster images, facilitating security-oriented applications through enhanced semantic consistency and improving the perceptual reconstruction quality. We train the DeepJSCC module to both reduce mean square error (MSE) and minimize cosine distance between DHD hashes of source and reconstructed images. Significantly improved perceptual quality as a result of semantic alignment is illustrated for different multi-hop settings, for which classical DeepJSCC may suffer from noise accumulation, measured by the learned perceptual image patch similarity (LPIPS) metric.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder</title>
<link>https://arxiv.org/abs/2510.06880</link>
<guid>https://arxiv.org/abs/2510.06880</guid>
<content:encoded><![CDATA[
arXiv:2510.06880v1 Announce Type: cross 
Abstract: The integration of multi-omics single-cell data remains challenging due to high-dimensionality and complex inter-modality relationships. To address this, we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a heterogeneous graph autoencoder that combines graph convolution and attention mechanisms to dynamically construct relational graphs directly from data. Evaluations on six publicly available datasets demonstrate that MoRE-GNN captures biologically meaningful relationships and outperforms existing methods, particularly in settings with strong inter-modality correlations. Furthermore, the learned representations allow for accurate downstream cross-modal predictions. While performance may vary with dataset complexity, MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing multi-omics integration.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices</title>
<link>https://arxiv.org/abs/2510.06882</link>
<guid>https://arxiv.org/abs/2510.06882</guid>
<content:encoded><![CDATA[
arXiv:2510.06882v1 Announce Type: cross 
Abstract: Edge devices have limited resources, which inevitably leads to situations where stream processing services cannot satisfy their needs. While existing autoscaling mechanisms focus entirely on resource scaling, Edge devices require alternative ways to sustain the Service Level Objectives (SLOs) of competing services. To address these issues, we introduce a Multi-dimensional Autoscaling Platform (MUDAP) that supports fine-grained vertical scaling across both service- and resource-level dimensions. MUDAP supports service-specific scaling tailored to available parameters, e.g., scale data quality or model size for a particular service. To optimize the execution across services, we present a scaling agent based on Regression Analysis of Structural Knowledge (RASK). The RASK agent efficiently explores the solution space and learns a continuous regression model of the processing environment for inferring optimal scaling actions. We compared our approach with two autoscalers, the Kubernetes VPA and a reinforcement learning agent, for scaling up to 9 services on a single Edge device. Our results showed that RASK can infer an accurate regression model in merely 20 iterations (i.e., observe 200s of processing). By increasingly adding elasticity dimensions, RASK sustained the highest request load with 28% less SLO violations, compared to baselines.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3Retrieve: Benchmarking Multimodal Retrieval for Medicine</title>
<link>https://arxiv.org/abs/2510.06888</link>
<guid>https://arxiv.org/abs/2510.06888</guid>
<content:encoded><![CDATA[
arXiv:2510.06888v1 Announce Type: cross 
Abstract: With the increasing use of RetrievalAugmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications. The dataset and the baselines code are available in this github page https://github.com/AkashGhosh/M3Retrieve.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Angular Constraint Embedding via SpherePair Loss for Constrained Clustering</title>
<link>https://arxiv.org/abs/2510.06907</link>
<guid>https://arxiv.org/abs/2510.06907</guid>
<content:encoded><![CDATA[
arXiv:2510.06907v1 Announce Type: cross 
Abstract: Constrained clustering integrates domain knowledge through pairwise constraints. However, existing deep constrained clustering (DCC) methods are either limited by anchors inherent in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability. To avoid their respective pitfalls, we propose a novel angular constraint embedding approach for DCC, termed SpherePair. Using the SpherePair loss with a geometric formulation, our method faithfully encodes pairwise constraints and leads to embeddings that are clustering-friendly in angular space, effectively separating representation learning from clustering. SpherePair preserves pairwise relations without conflict, removes the need to specify the exact number of clusters, generalizes to unseen data, enables rapid inference of the number of clusters, and is supported by rigorous theoretical guarantees. Comparative evaluations with state-of-the-art DCC methods on diverse benchmarks, along with empirical validation of theoretical insights, confirm its superior performance, scalability, and overall real-world effectiveness. Code is available at \href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally Vulnerable Subtype of Internet Gaming Disorder: Measuring and Exploring the Pathology of Problematic Generative AI Use</title>
<link>https://arxiv.org/abs/2510.06908</link>
<guid>https://arxiv.org/abs/2510.06908</guid>
<content:encoded><![CDATA[
arXiv:2510.06908v1 Announce Type: cross 
Abstract: Concerns over the potential over-pathologization of generative AI (GenAI) use and the lack of conceptual clarity surrounding GenAI addiction call for empirical tools and theoretical refinement. This study developed and validated the PUGenAIS-9 (Problematic Use of Generative Artificial Intelligence Scale-9 items) and examined whether PUGenAIS reflects addiction-like patterns under the Internet Gaming Disorder (IGD) framework. Using samples from China and the United States (N = 1,508), we conducted confirmatory factor analysis and identified a robust 31-item structure across nine IGD-based dimensions. We then derived the PUGenAIS-9 by selecting the highest-loading items from each dimension and validated its structure in an independent sample (N = 1,426). Measurement invariance tests confirmed its stability across nationality and gender. Person-centered (latent profile analysis) and variable-centered (network analysis) approaches found that PUGenAIS matches the traits of the emotionally vulnerable subtype of IGD, not the competence-based kind. These results support using PUGenAIS-9 to identify problematic GenAI use and show the need to rethink digital addiction with an ICD (infrastructures, content, and device) model. This keeps addiction research responsive to new media while avoiding over-pathologizing.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning</title>
<link>https://arxiv.org/abs/2510.06913</link>
<guid>https://arxiv.org/abs/2510.06913</guid>
<content:encoded><![CDATA[
arXiv:2510.06913v1 Announce Type: cross 
Abstract: Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle's realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</title>
<link>https://arxiv.org/abs/2510.06915</link>
<guid>https://arxiv.org/abs/2510.06915</guid>
<content:encoded><![CDATA[
arXiv:2510.06915v1 Announce Type: cross 
Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Nonparametric Dynamical Clustering of Time Series</title>
<link>https://arxiv.org/abs/2510.06919</link>
<guid>https://arxiv.org/abs/2510.06919</guid>
<content:encoded><![CDATA[
arXiv:2510.06919v1 Announce Type: cross 
Abstract: We present a method that models the evolution of an unbounded number of time series clusters by switching among an unknown number of regimes with linear dynamics. We develop a Bayesian non-parametric approach using a hierarchical Dirichlet process as a prior on the parameters of a Switching Linear Dynamical System and a Gaussian process prior to model the statistical variations in amplitude and temporal alignment within each cluster. By modeling the evolution of time series patterns, the method avoids unnecessary proliferation of clusters in a principled manner. We perform inference by formulating a variational lower bound for off-line and on-line scenarios, enabling efficient learning through optimization. We illustrate the versatility and effectiveness of the approach through several case studies of electrocardiogram analysis using publicly available databases.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive and Scalable Quantum Fusion for Multimodal Learning</title>
<link>https://arxiv.org/abs/2510.06938</link>
<guid>https://arxiv.org/abs/2510.06938</guid>
<content:encoded><![CDATA[
arXiv:2510.06938v1 Announce Type: cross 
Abstract: The aim of this paper is to introduce a quantum fusion mechanism for multimodal learning and to establish its theoretical and empirical potential. The proposed method, called the Quantum Fusion Layer (QFL), replaces classical fusion schemes with a hybrid quantum-classical procedure that uses parameterized quantum circuits to learn entangled feature interactions without requiring exponential parameter growth. Supported by quantum signal processing principles, the quantum component efficiently represents high-order polynomial interactions across modalities with linear parameter scaling, and we provide a separation example between QFL and low-rank tensor-based methods that highlights potential quantum query advantages. In simulation, QFL consistently outperforms strong classical baselines on small but diverse multimodal tasks, with particularly marked improvements in high-modality regimes. These results suggest that QFL offers a fundamentally new and scalable approach to multimodal fusion that merits deeper exploration on larger systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouped Differential Attention</title>
<link>https://arxiv.org/abs/2510.06949</link>
<guid>https://arxiv.org/abs/2510.06949</guid>
<content:encoded><![CDATA[
arXiv:2510.06949v1 Announce Type: cross 
Abstract: The self-attention mechanism, while foundational to modern Transformer architectures, suffers from a critical inefficiency: it frequently allocates substantial attention to redundant or noisy context. Differential Attention addressed this by using subtractive attention maps for signal and noise, but its required balanced head allocation imposes rigid constraints on representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel approach that introduces unbalanced head allocation between signal-preserving and noise-control groups. GDA significantly enhances signal focus by strategically assigning more heads to signal extraction and fewer to noise-control, stabilizing the latter through controlled repetition (akin to GQA). This design achieves stronger signal fidelity with minimal computational overhead. We further extend this principle to group-differentiated growth, a scalable strategy that selectively replicates only the signal-focused heads, thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we demonstrate that moderate imbalance ratios in GDA yield substantial improvements in generalization and stability compared to symmetric baselines. Our results collectively establish that ratio-aware head allocation and selective expansion offer an effective and practical path toward designing scalable, computation-efficient Transformer architectures.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</title>
<link>https://arxiv.org/abs/2510.06961</link>
<guid>https://arxiv.org/abs/2510.06961</guid>
<content:encoded><![CDATA[
arXiv:2510.06961v1 Announce Type: cross 
Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDUMATH: Generating Standards-aligned Educational Math Word Problems</title>
<link>https://arxiv.org/abs/2510.06965</link>
<guid>https://arxiv.org/abs/2510.06965</guid>
<content:encoded><![CDATA[
arXiv:2510.06965v1 Announce Type: cross 
Abstract: Math word problems (MWPs) are critical K-12 educational tools, and customizing them to students' interests and ability levels can increase learning outcomes. However, teachers struggle to find time to customize MWPs for each student given large class sizes and increasing burnout. We propose that LLMs can support math education by generating MWPs customized to student interests and math education standards. To this end, we use a joint human expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and closed LLMs and develop the first teacher-annotated dataset for standards-aligned educational MWP generation. We show the value of our data by using it to train a 12B open model that matches the performance of larger and more capable open models. We also use our teacher-annotated data to train a text classifier that enables a 30B open LLM to outperform existing closed baselines without any training. Next, we show our models' MWPs are more similar to human-written MWPs than those from existing models. We conclude by conducting the first study of customized LLM-generated MWPs with grade school students, finding they perform similarly on our models' MWPs relative to human-written MWPs but consistently prefer our customized MWPs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Surface for Text-to-3D using 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2510.06967</link>
<guid>https://arxiv.org/abs/2510.06967</guid>
<content:encoded><![CDATA[
arXiv:2510.06967v1 Announce Type: cross 
Abstract: Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Global Representation from Queries for Vectorized HD Map Construction</title>
<link>https://arxiv.org/abs/2510.06969</link>
<guid>https://arxiv.org/abs/2510.06969</guid>
<content:encoded><![CDATA[
arXiv:2510.06969v1 Announce Type: cross 
Abstract: The online construction of vectorized high-definition (HD) maps is a cornerstone of modern autonomous driving systems. State-of-the-art approaches, particularly those based on the DETR framework, formulate this as an instance detection problem. However, their reliance on independent, learnable object queries results in a predominantly local query perspective, neglecting the inherent global representation within HD maps. In this work, we propose \textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD \textbf{Map} construction), an architecture designed to learn and utilize a global representations from queries. Our method introduces two synergistic modules: a Global Representation Learning (GRL) module, which encourages the distribution of all queries to better align with the global map through a carefully designed holistic segmentation task, and a Global Representation Guidance (GRG) module, which endows each individual query with explicit, global-level contextual information to facilitate its optimization. Evaluations on the nuScenes and Argoverse2 datasets validate the efficacy of our approach, demonstrating substantial improvements in mean Average Precision (mAP) compared to leading baselines.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VelLMes: A high-interaction AI-based deception framework</title>
<link>https://arxiv.org/abs/2510.06975</link>
<guid>https://arxiv.org/abs/2510.06975</guid>
<content:encoded><![CDATA[
arXiv:2510.06975v1 Announce Type: cross 
Abstract: There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers. Generative AI has recently become a valuable asset for cybersecurity researchers and practitioners, and the field of cyber-deception is no exception. Researchers have demonstrated how LLMs can be leveraged to create realistic-looking honeytokens, fake users, and even simulated systems that can be used as honeypots. This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus VelLMes offers a variety of choices for deception design based on the users' needs. VelLMes is designed to be attacked by humans, so interactivity and realism are key for its performance. We evaluate the generative capabilities and the deception capabilities. Generative capabilities were evaluated using unit tests for LLMs. The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of Goal-Setting Theory in LLM-Driven Assessment</title>
<link>https://arxiv.org/abs/2510.06997</link>
<guid>https://arxiv.org/abs/2510.06997</guid>
<content:encoded><![CDATA[
arXiv:2510.06997v1 Announce Type: cross 
Abstract: Many users interact with AI tools like ChatGPT using a mental model that treats the system as human-like, which we call Model H. According to goal-setting theory, increased specificity in goals should reduce performance variance. If Model H holds, then prompting a chatbot with more detailed instructions should lead to more consistent evaluation behavior.
  This paper tests that assumption through a controlled experiment in which ChatGPT evaluated 29 student submissions using four prompts with increasing specificity. We measured consistency using intra-rater reliability (Cohen's Kappa) across repeated runs.
  Contrary to expectations, performance did not improve consistently with increased prompt specificity, and performance variance remained largely unchanged. These findings challenge the assumption that LLMs behave like human evaluators and highlight the need for greater robustness and improved input integration in future model development.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages</title>
<link>https://arxiv.org/abs/2510.07000</link>
<guid>https://arxiv.org/abs/2510.07000</guid>
<content:encoded><![CDATA[
arXiv:2510.07000v1 Announce Type: cross 
Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the availability of high-quality post-training data, particularly instruction-tuning and preference-based examples. Existing open-source datasets, however, often lack multilingual coverage, cultural grounding, and suffer from task diversity gaps that are especially pronounced for Indian languages. We introduce a human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56 sub-categories, leveraging 57 diverse datasets. Our dataset protocol incorporates several often-overlooked dimensions and emphasize task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance, providing a foundation for more inclusive and effective multilingual LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Native Hybrid Attention for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2510.07019</link>
<guid>https://arxiv.org/abs/2510.07019</guid>
<content:encoded><![CDATA[
arXiv:2510.07019v1 Announce Type: cross 
Abstract: Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy</title>
<link>https://arxiv.org/abs/2510.07022</link>
<guid>https://arxiv.org/abs/2510.07022</guid>
<content:encoded><![CDATA[
arXiv:2510.07022v1 Announce Type: cross 
Abstract: Machine unlearning is critical for enforcing data deletion rights like the "right to be forgotten." As a decentralized paradigm, Federated Learning (FL) also requires unlearning, but realistic implementations face two major challenges. First, fairness in Federated Unlearning (FU) is often overlooked. Exact unlearning methods typically force all clients into costly retraining, even those uninvolved. Approximate approaches, using gradient ascent or distillation, make coarse interventions that can unfairly degrade performance for clients with only retained data. Second, most FU evaluations rely on synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity. These unrealistic benchmarks obscure the true impact of unlearning and limit the applicability of current methods. We first conduct a comprehensive benchmark of existing FU methods under realistic data heterogeneity and fairness conditions. We then propose a novel, fairness-aware FU approach, Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address both challenges. FedCCCU offers a practical and scalable solution for real-world FU. Experimental results show that existing methods perform poorly in realistic settings, while our approach consistently outperforms them.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge</title>
<link>https://arxiv.org/abs/2510.07024</link>
<guid>https://arxiv.org/abs/2510.07024</guid>
<content:encoded><![CDATA[
arXiv:2510.07024v1 Announce Type: cross 
Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI tasks. A significant contributor is their factual knowledge, which, to date, remains poorly understood, and is usually analyzed from biased samples. In this paper, we take a deep tour into the factual knowledge (or beliefs) of a frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited set of 100 million beliefs of one of the strongest currently available frontier LLMs, GPT-4.1. We find that the models' factual knowledge differs quite significantly from established knowledge bases, and that its accuracy is significantly lower than indicated by previous benchmarks. We also find that inconsistency, ambiguity and hallucinations are major issues, shedding light on future research opportunities concerning factual LLM knowledge.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration</title>
<link>https://arxiv.org/abs/2510.07035</link>
<guid>https://arxiv.org/abs/2510.07035</guid>
<content:encoded><![CDATA[
arXiv:2510.07035v1 Announce Type: cross 
Abstract: Molecular representation learning plays a crucial role in advancing applications such as drug discovery and material design. Existing work leverages 2D and 3D modalities of molecular information for pre-training, aiming to capture comprehensive structural and geometric insights. However, these methods require paired 2D and 3D molecular data to train the model effectively and prevent it from collapsing into a single modality, posing limitations in scenarios where a certain modality is unavailable or computationally expensive to generate. To overcome this limitation, we propose FlexMol, a flexible molecule pre-training framework that learns unified molecular representations while supporting single-modality input. Specifically, inspired by the unified structure in vision-language models, our approach employs separate models for 2D and 3D molecular data, leverages parameter sharing to improve computational efficiency, and utilizes a decoder to generate features for the missing modality. This enables a multistage continuous learning process where both modalities contribute collaboratively during training, while ensuring robustness when only one modality is available during inference. Extensive experiments demonstrate that FlexMol achieves superior performance across a wide range of molecular property prediction tasks, and we also empirically demonstrate its effectiveness with incomplete data. Our code and data are available at https://github.com/tewiSong/FlexMol.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07048</link>
<guid>https://arxiv.org/abs/2510.07048</guid>
<content:encoded><![CDATA[
arXiv:2510.07048v1 Announce Type: cross 
Abstract: Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introspection in Learned Semantic Scene Graph Localisation</title>
<link>https://arxiv.org/abs/2510.07053</link>
<guid>https://arxiv.org/abs/2510.07053</guid>
<content:encoded><![CDATA[
arXiv:2510.07053v1 Announce Type: cross 
Abstract: This work investigates how semantics influence localisation performance and robustness in a learned self-supervised, contrastive semantic localisation framework. After training a localisation network on both original and perturbed maps, we conduct a thorough post-hoc introspection analysis to probe whether the model filters environmental noise and prioritises distinctive landmarks over routine clutter. We validate various interpretability methods and present a comparative reliability analysis. Integrated gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour. A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted. Overall, the results indicate that the model learns noise-robust, semantically salient relations about place definition, thereby enabling explainable registration under challenging visual and structural variations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish</title>
<link>https://arxiv.org/abs/2510.07074</link>
<guid>https://arxiv.org/abs/2510.07074</guid>
<content:encoded><![CDATA[
arXiv:2510.07074v1 Announce Type: cross 
Abstract: Instruction tuning has become a key technique for enhancing the performance of large language models, enabling them to better follow human prompts. However, low-resource languages such as Luxembourgish face severe limitations due to the lack of high-quality instruction datasets. Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies. In this work, we address these challenges by creating a cross-lingual instruction tuning dataset for Luxembourgish, without resorting to machine-generated translations into it. Instead, by leveraging aligned data from English, French, and German, we build a high-quality dataset that preserves linguistic and cultural nuances. We provide evidence that cross-lingual instruction tuning not only improves representational alignment across languages but also the model's generative capabilities in Luxembourgish. This highlights how cross-lingual data curation can avoid the common pitfalls of machine-translated data and directly benefit low-resource language development.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</title>
<link>https://arxiv.org/abs/2510.07077</link>
<guid>https://arxiv.org/abs/2510.07077</guid>
<content:encoded><![CDATA[
arXiv:2510.07077v1 Announce Type: cross 
Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.07084</link>
<guid>https://arxiv.org/abs/2510.07084</guid>
<content:encoded><![CDATA[
arXiv:2510.07084v1 Announce Type: cross 
Abstract: Transformer-based methods have achieved impressive results in time series forecasting. However, existing Transformers still exhibit limitations in sequence modeling as they tend to overemphasize temporal dependencies. This incurs additional computational overhead without yielding corresponding performance gains. We find that the performance of Transformers is highly dependent on the embedding method used to learn effective representations. To address this issue, we extract multivariate features to augment the effective information captured in the embedding layer, yielding multidimensional embeddings that convey richer and more meaningful sequence representations. These representations enable Transformer-based forecasters to better understand the series. Specifically, we introduce Hybrid Temporal and Multivariate Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature extraction module with a carefully designed multivariate feature extraction module to provide complementary features, thereby achieving a balance between model complexity and performance. By combining HTME with the Transformer architecture, we present HTMformer, leveraging the enhanced feature extraction capability of the HTME extractor to build a lightweight forecaster. Experiments conducted on eight real-world datasets demonstrate that our approach outperforms existing baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report</title>
<link>https://arxiv.org/abs/2510.07092</link>
<guid>https://arxiv.org/abs/2510.07092</guid>
<content:encoded><![CDATA[
arXiv:2510.07092v1 Announce Type: cross 
Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to reason about the future by predicting visual observations or compact latent states. The 1X World Model Challenge introduces an open-source benchmark of real-world humanoid interaction, with two complementary tracks: sampling, focused on forecasting future image frames, and compression, focused on predicting future discrete latent codes. For the sampling track, we adapt the video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned future frame prediction. We condition the video generation on robot states using AdaLN-Zero, and further post-train the model using LoRA. For the compression track, we train a Spatio-Temporal Transformer model from scratch. Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386 in the compression task, securing 1st place in both challenges.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning</title>
<link>https://arxiv.org/abs/2510.07105</link>
<guid>https://arxiv.org/abs/2510.07105</guid>
<content:encoded><![CDATA[
arXiv:2510.07105v1 Announce Type: cross 
Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Conditioned Diffusion for Controllable Histopathology Image Generation</title>
<link>https://arxiv.org/abs/2510.07129</link>
<guid>https://arxiv.org/abs/2510.07129</guid>
<content:encoded><![CDATA[
arXiv:2510.07129v1 Announce Type: cross 
Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model</title>
<link>https://arxiv.org/abs/2510.07133</link>
<guid>https://arxiv.org/abs/2510.07133</guid>
<content:encoded><![CDATA[
arXiv:2510.07133v1 Announce Type: cross 
Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the complexity and unpredictability of real-world driving environments. Traditional testing methods face significant limitations, such as the oracle problem, which makes it difficult to determine whether a system's behavior is correct, and the inability to cover the full range of scenarios an autonomous vehicle may encounter. In this paper, we introduce a digital twin-driven metamorphic testing framework that addresses these challenges by creating a virtual replica of the self-driving system and its operating environment. By combining digital twin technology with AI-based image generative models such as Stable Diffusion, our approach enables the systematic generation of realistic and diverse driving scenes. This includes variations in weather, road topology, and environmental features, all while maintaining the core semantics of the original scenario. The digital twin provides a synchronized simulation environment where changes can be tested in a controlled and repeatable manner. Within this environment, we define three metamorphic relations inspired by real-world traffic rules and vehicle behavior. We validate our framework in the Udacity self-driving simulator and demonstrate that it significantly enhances test coverage and effectiveness. Our method achieves the highest true positive rate (0.719), F1 score (0.689), and precision (0.662) compared to baseline approaches. This paper highlights the value of integrating digital twins with AI-powered scenario generation to create a scalable, automated, and high-fidelity testing solution for autonomous vehicle safety.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</title>
<link>https://arxiv.org/abs/2510.07134</link>
<guid>https://arxiv.org/abs/2510.07134</guid>
<content:encoded><![CDATA[
arXiv:2510.07134v1 Announce Type: cross 
Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing human and language models sentence processing difficulties on complex structures</title>
<link>https://arxiv.org/abs/2510.07141</link>
<guid>https://arxiv.org/abs/2510.07141</guid>
<content:encoded><![CDATA[
arXiv:2510.07141v1 Announce Type: cross 
Abstract: Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework for Stateful Inference-Time Search</title>
<link>https://arxiv.org/abs/2510.07147</link>
<guid>https://arxiv.org/abs/2510.07147</guid>
<content:encoded><![CDATA[
arXiv:2510.07147v1 Announce Type: cross 
Abstract: Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL</title>
<link>https://arxiv.org/abs/2510.07151</link>
<guid>https://arxiv.org/abs/2510.07151</guid>
<content:encoded><![CDATA[
arXiv:2510.07151v1 Announce Type: cross 
Abstract: Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</title>
<link>https://arxiv.org/abs/2510.07181</link>
<guid>https://arxiv.org/abs/2510.07181</guid>
<content:encoded><![CDATA[
arXiv:2510.07181v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial reasoning, yet they remain fundamentally limited to qualitative precision and lack the computational precision required for real-world robotics. Current approaches fail to leverage metric cues from depth sensors and camera calibration, instead reducing geometric problems to pattern recognition tasks that cannot deliver the centimeter-level accuracy essential for robotic manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel framework that transforms VLMs from perceptual estimators to geometric computers by enabling them to generate and execute precise geometric computations through external tools. Rather than attempting to internalize complex geometric operations within neural networks, TIGeR empowers models to recognize geometric reasoning requirements, synthesize appropriate computational code, and invoke specialized libraries for exact calculations. To support this paradigm, we introduce TIGeR-300K, a comprehensive tool-invocation-oriented dataset covering point transformations, pose estimation, trajectory generation, and spatial compatibility verification, complete with tool invocation sequences and intermediate computations. Through a two-stage training pipeline combining supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achieves SOTA performance on geometric reasoning benchmarks while demonstrating centimeter-level precision in real-world robotic manipulation tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</title>
<link>https://arxiv.org/abs/2510.07191</link>
<guid>https://arxiv.org/abs/2510.07191</guid>
<content:encoded><![CDATA[
arXiv:2510.07191v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.07210</link>
<guid>https://arxiv.org/abs/2510.07210</guid>
<content:encoded><![CDATA[
arXiv:2510.07210v1 Announce Type: cross 
Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proximal policy optimization and approximated online POMDP planning with heuristic confidence-based vertical pruning to reduce its execution time without compromising safety of driving. Our experimental performance analysis on the CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed that HyPlan may navigate safer than selected relevant baselines and perform significantly faster than considered alternative online POMDP planners.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</title>
<link>https://arxiv.org/abs/2510.07213</link>
<guid>https://arxiv.org/abs/2510.07213</guid>
<content:encoded><![CDATA[
arXiv:2510.07213v1 Announce Type: cross 
Abstract: Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</title>
<link>https://arxiv.org/abs/2510.07217</link>
<guid>https://arxiv.org/abs/2510.07217</guid>
<content:encoded><![CDATA[
arXiv:2510.07217v1 Announce Type: cross 
Abstract: Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation</title>
<link>https://arxiv.org/abs/2510.07227</link>
<guid>https://arxiv.org/abs/2510.07227</guid>
<content:encoded><![CDATA[
arXiv:2510.07227v1 Announce Type: cross 
Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships</title>
<link>https://arxiv.org/abs/2510.07231</link>
<guid>https://arxiv.org/abs/2510.07231</guid>
<content:encoded><![CDATA[
arXiv:2510.07231v1 Announce Type: cross 
Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6\% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation</title>
<link>https://arxiv.org/abs/2510.07243</link>
<guid>https://arxiv.org/abs/2510.07243</guid>
<content:encoded><![CDATA[
arXiv:2510.07243v1 Announce Type: cross 
Abstract: Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into 'Legal Data Points' (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the false election between regulation and innovation. Ideas for regulation through the responsible use of artificial intelligence in research and education.[Spanish version]</title>
<link>https://arxiv.org/abs/2510.07268</link>
<guid>https://arxiv.org/abs/2510.07268</guid>
<content:encoded><![CDATA[
arXiv:2510.07268v1 Announce Type: cross 
Abstract: This short essay is a reworking of the answers offered by the author at the Debate Session of the AIHUB (CSIC) and EduCaixa Summer School, organized by Marta Garcia-Matos and Lissette Lemus, and coordinated by Albert Sabater (OEIAC, UG), with the participation of Vanina Martinez-Posse (IIIA-CSIC), Eulalia Soler (Eurecat) and Pompeu Casanovas (IIIA-CSIC) on July 4th 2025. Albert Sabater posed three questions: (1) How can regulatory frameworks priori-tise the protection of fundamental rights (privacy, non-discrimination, autonomy, etc.) in the development of AI, without falling into the false dichotomy between regulation and innova-tion? (2) Given the risks of AI (bias, mass surveillance, manipulation), what examples of regu-lations or policies have demonstrated that it is possible to foster responsible innovation, putting the public interest before profitability, without giving in to competitive pressure from actors such as China or the US? (3) In a scenario where the US prioritizes flexibility, what mecha-nisms could ensure that international cooperation in AI does not become a race to the bottom in rights, but rather a global standard of accountability? The article attempts to answer these three questions and concludes with some reflections on the relevance of the answers for education and research.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Rubrics Elicitation from Pairwise Comparisons</title>
<link>https://arxiv.org/abs/2510.07284</link>
<guid>https://arxiv.org/abs/2510.07284</guid>
<content:encoded><![CDATA[
arXiv:2510.07284v1 Announce Type: cross 
Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)</title>
<link>https://arxiv.org/abs/2510.07285</link>
<guid>https://arxiv.org/abs/2510.07285</guid>
<content:encoded><![CDATA[
arXiv:2510.07285v1 Announce Type: cross 
Abstract: The escalating complexity of network threats and the inherent class imbalance in traffic data present formidable challenges for modern Intrusion Detection Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological structures and Temporal Convolutional Networks (TCNs) are proficient in capturing time-series dependencies, a framework that synergistically integrates both while explicitly addressing data imbalance remains an open challenge. This paper introduces a novel deep learning framework, named Gated Temporal Convolutional Network and Graph (GTCN-G), engineered to overcome these limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting hierarchical temporal features from network flows with a Graph Convolutional Network (GCN) designed to learn from the underlying graph structure. The core innovation lies in the integration of a residual learning mechanism, implemented via a Graph Attention Network (GAT). This mechanism preserves original feature information through residual connections, which is critical for mitigating the class imbalance problem and enhancing detection sensitivity for rare malicious activities (minority classes). We conducted extensive experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to validate our approach. The empirical results demonstrate that the proposed GTCN-G model achieves state-of-the-art performance, significantly outperforming existing baseline models in both binary and multi-class classification tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Profiles for Protein Fitness Prediction</title>
<link>https://arxiv.org/abs/2510.07286</link>
<guid>https://arxiv.org/abs/2510.07286</guid>
<content:encoded><![CDATA[
arXiv:2510.07286v1 Announce Type: cross 
Abstract: Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</title>
<link>https://arxiv.org/abs/2510.07293</link>
<guid>https://arxiv.org/abs/2510.07293</guid>
<content:encoded><![CDATA[
arXiv:2510.07293v1 Announce Type: cross 
Abstract: Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cocoon: A System Architecture for Differentially Private Training with Correlated Noises</title>
<link>https://arxiv.org/abs/2510.07304</link>
<guid>https://arxiv.org/abs/2510.07304</guid>
<content:encoded><![CDATA[
arXiv:2510.07304v1 Announce Type: cross 
Abstract: Machine learning (ML) models memorize and leak training data, causing serious privacy issues to data owners. Training algorithms with differential privacy (DP), such as DP-SGD, have been gaining attention as a solution. However, DP-SGD adds a noise at each training iteration, which degrades the accuracy of the trained model. To improve accuracy, a new family of approaches adds carefully designed correlated noises, so that noises cancel out each other across iterations. We performed an extensive characterization study of these new mechanisms, for the first time to the best of our knowledge, and show they incur non-negligible overheads when the model is large or uses large embedding tables. Motivated by the analysis, we propose Cocoon, a hardware-software co-designed framework for efficient training with correlated noises. Cocoon accelerates models with embedding tables through pre-computing and storing correlated noises in a coalesced format (Cocoon-Emb), and supports large models through a custom near-memory processing device (Cocoon-NMP). On a real system with an FPGA-based NMP device prototype, Cocoon improves the performance by 2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</title>
<link>https://arxiv.org/abs/2510.07307</link>
<guid>https://arxiv.org/abs/2510.07307</guid>
<content:encoded><![CDATA[
arXiv:2510.07307v1 Announce Type: cross 
Abstract: While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks, demanding extensive time and manual effort to produce. We introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate-verify-execute paradigm for scaling MLE tasks with verifiable quality, real-world usability, and rich diversity. The proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution. We apply MLE-Smith to 224 of real-world datasets and generate 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work effectively across a wide range of real-world datasets. Evaluation on the generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith to scaling up MLE tasks, while maintaining task quality.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07312</link>
<guid>https://arxiv.org/abs/2510.07312</guid>
<content:encoded><![CDATA[
arXiv:2510.07312v1 Announce Type: cross 
Abstract: Large language models excel at short-horizon reasoning tasks, but performance drops as reasoning horizon lengths increase. Existing approaches to combat this rely on inference-time scaffolding or costly step-level supervision, neither of which scales easily. In this work, we introduce a scalable method to bootstrap long-horizon reasoning capabilities using only existing, abundant short-horizon data. Our approach synthetically composes simple problems into complex, multi-step dependency chains of arbitrary length. We train models on this data using outcome-only rewards under a curriculum that automatically increases in complexity, allowing RL training to be scaled much further without saturating. Empirically, our method generalizes remarkably well: curriculum training on composed 6th-grade level math problems (GSM8K) boosts accuracy on longer, competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. Importantly, our long-horizon improvements are significantly higher than baselines even at high pass@k, showing that models can learn new reasoning paths under RL. Theoretically, we show that curriculum RL with outcome rewards achieves an exponential improvement in sample complexity over full-horizon training, providing training signal comparable to dense supervision. h1 therefore introduces an efficient path towards scaling RL for long-horizon problems using only existing data.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations</title>
<link>https://arxiv.org/abs/2510.07314</link>
<guid>https://arxiv.org/abs/2510.07314</guid>
<content:encoded><![CDATA[
arXiv:2510.07314v1 Announce Type: cross 
Abstract: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3D$\leftrightarrow$5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibe Checker: Aligning Code Evaluation with Human Preference</title>
<link>https://arxiv.org/abs/2510.07315</link>
<guid>https://arxiv.org/abs/2510.07315</guid>
<content:encoded><![CDATA[
arXiv:2510.07315v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2510.07318</link>
<guid>https://arxiv.org/abs/2510.07318</guid>
<content:encoded><![CDATA[
arXiv:2510.07318v1 Announce Type: cross 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Capabilities from Task Performance with Bayesian Triangulation</title>
<link>https://arxiv.org/abs/2309.11975</link>
<guid>https://arxiv.org/abs/2309.11975</guid>
<content:encoded><![CDATA[
arXiv:2309.11975v2 Announce Type: replace 
Abstract: As machine learning models become more general, we need to characterise them in richer, more meaningful ways. We describe a method to infer the cognitive profile of a system from diverse experimental data. To do so, we introduce measurement layouts that model how task-instance features interact with system capabilities to affect performance. These features must be triangulated in complex ways to be able to infer capabilities from non-populational data -- a challenge for traditional psychometric and inferential tools. Using the Bayesian probabilistic programming library PyMC, we infer different cognitive profiles for agents in two scenarios: 68 actual contestants in the AnimalAI Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery. We showcase the potential for capability-oriented evaluation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent and Coherent Procedural Mistake Detection</title>
<link>https://arxiv.org/abs/2412.11927</link>
<guid>https://arxiv.org/abs/2412.11927</guid>
<content:encoded><![CDATA[
arXiv:2412.11927v3 Announce Type: replace 
Abstract: Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that VLMs struggle off-the-shelf, but with some trade-offs, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Illusion of Progress? Assessing the Current State of Web Agents</title>
<link>https://arxiv.org/abs/2504.01382</link>
<guid>https://arxiv.org/abs/2504.01382</guid>
<content:encoded><![CDATA[
arXiv:2504.01382v4 Announce Type: replace 
Abstract: As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirically evaluating commonsense intelligence in large language models with large-scale human judgments</title>
<link>https://arxiv.org/abs/2505.10309</link>
<guid>https://arxiv.org/abs/2505.10309</guid>
<content:encoded><![CDATA[
arXiv:2505.10309v2 Announce Type: replace 
Abstract: Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Agentic Planning &amp; Reasoning for Mechanism Synthesis</title>
<link>https://arxiv.org/abs/2505.17607</link>
<guid>https://arxiv.org/abs/2505.17607</guid>
<content:encoded><![CDATA[
arXiv:2505.17607v2 Announce Type: replace 
Abstract: This work presents a dual-agent \ac{llm}-based reasoning framework for automated planar mechanism synthesis that tightly couples linguistic specification with symbolic representation and simulation. From a natural-language task description, the system composes symbolic constraints and equations, generates and parametrises simulation code, and iteratively refines designs via critic-driven feedback, including symbolic regression and geometric distance metrics, closing an actionable linguistic/symbolic optimisation loop. To evaluate the approach, we introduce MSynth, a benchmark of analytically defined planar trajectories. Empirically, critic feedback and iterative refinement yield large improvements (up to 90\% on individual tasks) and statistically significant gains per the Wilcoxon signed-rank test. Symbolic-regression prompts provide deeper mechanistic insight primarily when paired with larger models or architectures with appropriate inductive biases (e.g., LRM).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism</title>
<link>https://arxiv.org/abs/2505.21988</link>
<guid>https://arxiv.org/abs/2505.21988</guid>
<content:encoded><![CDATA[
arXiv:2505.21988v2 Announce Type: replace 
Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic Design Automation (EDA) applications, including datapath optimization, arithmetic verification, and hardware trojan detection. However, existing techniques rely primarily on structural graph isomorphism and thus fail to identify function-related subgraphs when synthesis transformations substantially alter circuit topology. To overcome this critical limitation, we introduce the concept of functional subgraph matching, a novel approach that identifies whether a given logic function is implicitly present within a larger circuit, irrespective of structural variations induced by synthesis or technology mapping. Specifically, we propose a two-stage multi-modal framework: (1) learning robust functional embeddings across AIG and post-mapping netlists for functional subgraph detection, and (2) identifying fuzzy boundaries using a graph segmentation approach. Evaluations on standard benchmarks (ITC99, OpenABCD, ForgeEDA) demonstrate significant performance improvements over existing structural methods, with average $93.8\%$ accuracy in functional subgraph detection and a dice score of $91.3\%$ in fuzzy boundary identification. The source code and implementation details can be found at https://github.com/zyzheng17/Functional_Subgraph_Matching-Neurips25.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</title>
<link>https://arxiv.org/abs/2506.00320</link>
<guid>https://arxiv.org/abs/2506.00320</guid>
<content:encoded><![CDATA[
arXiv:2506.00320v2 Announce Type: replace 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality</title>
<link>https://arxiv.org/abs/2506.19807</link>
<guid>https://arxiv.org/abs/2506.19807</guid>
<content:encoded><![CDATA[
arXiv:2506.19807v3 Announce Type: replace 
Abstract: Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Like Humans: A Multi-Agent Solution for Medical Coding</title>
<link>https://arxiv.org/abs/2509.05378</link>
<guid>https://arxiv.org/abs/2509.05378</guid>
<content:encoded><![CDATA[
arXiv:2509.05378v3 Announce Type: replace 
Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments</title>
<link>https://arxiv.org/abs/2509.25282</link>
<guid>https://arxiv.org/abs/2509.25282</guid>
<content:encoded><![CDATA[
arXiv:2509.25282v2 Announce Type: replace 
Abstract: Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple "world model" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsychoBench: Evaluating the Psychology Intelligence of Large Language Models</title>
<link>https://arxiv.org/abs/2510.01611</link>
<guid>https://arxiv.org/abs/2510.01611</guid>
<content:encoded><![CDATA[
arXiv:2510.01611v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a wide range of industries, primarily due to their impressive generative abilities. Yet, their potential in applications requiring cognitive abilities, such as psychological counseling, remains largely untapped. This paper investigates the key question: Can LLMs be effectively applied to psychological counseling? To determine whether an LLM can effectively take on the role of a psychological counselor, the first step is to assess whether it meets the qualifications required for such a role, namely the ability to pass the U.S. National Counselor Certification Exam (NCE). This is because, just as a human counselor must pass a certification exam to practice, an LLM must demonstrate sufficient psychological knowledge to meet the standards required for such a role. To address this, we introduce PsychoBench, a benchmark grounded in U.S.national counselor examinations, a licensure test for professional counselors that requires about 70% accuracy to pass. PsychoBench comprises approximately 2,252 carefully curated single-choice questions, crafted to require deep understanding and broad enough to cover various sub-disciplines of psychology. This benchmark provides a comprehensive assessment of an LLM's ability to function as a counselor. Our evaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results suggest that only frontier LLMs are currently capable of meeting counseling exam standards, highlighting both the promise and the challenges of developing psychology-oriented LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples</title>
<link>https://arxiv.org/abs/2209.03358</link>
<guid>https://arxiv.org/abs/2209.03358</guid>
<content:encoded><![CDATA[
arXiv:2209.03358v4 Announce Type: replace-cross 
Abstract: Spiking neural networks (SNNs) have drawn much attention for their high energy efficiency and recent advances in classification performance. However, unlike traditional deep learning, the robustness of SNNs to adversarial examples remains underexplored. This work advances the adversarial attack side of SNNs and makes three major contributions. First, we show that successful white-box attacks on SNNs strongly depend on the surrogate gradient estimation technique, even for adversarially trained models. Second, using the best single surrogate gradient estimator, we study the transferability of adversarial examples between SNNs and state-of-the-art architectures such as Vision Transformers (ViTs) and CNNs. Our analysis reveals two major gaps: no existing white-box attack leverages multiple surrogate estimators, and no single attack effectively fools both SNNs and non-SNN models simultaneously. Third, we propose the Mixed Dynamic Spiking Estimation (MDSE) attack, which dynamically combines multiple surrogate gradients to overcome these gaps. MDSE produces adversarial examples that fool both SNN and non-SNN models, achieving up to 91.4% higher effectiveness on SNN/ViT ensembles and a 3x boost on adversarially trained SNN ensembles over Auto-PGD. Experiments span three datasets (CIFAR-10, CIFAR-100, ImageNet) and nineteen classifiers, and we will release code and models upon publication.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward</title>
<link>https://arxiv.org/abs/2209.12164</link>
<guid>https://arxiv.org/abs/2209.12164</guid>
<content:encoded><![CDATA[
arXiv:2209.12164v2 Announce Type: replace-cross 
Abstract: Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers. It mainly contains two stages: video segmentation and segment assemblage. The existing method performs well at video segmentation stages but suffers from the problems of dependencies on extra cumbersome models and poor performance at the segment assemblage stage. To address these problems, we propose M-SAN (Multi-modal Segment Assemblage Network) which can perform efficient and coherent segment assemblage task end-to-end. It utilizes multi-modal representation extracted from the segments and follows the Encoder-Decoder Ptr-Net framework with the Attention mechanism. Importance-coherence reward is designed for training M-SAN. We experiment on the Ads-1k dataset with 1000+ videos under rich ad scenarios collected from advertisers. To evaluate the methods, we propose a unified metric, Imp-Coh@Time, which comprehensively assesses the importance, coherence, and duration of the outputs at the same time. Experimental results show that our method achieves better performance than random selection and the previous method on the metric. Ablation experiments further verify that multi-modal representation and importance-coherence reward significantly improve the performance. Ads-1k dataset is available at: https://github.com/yunlong10/Ads-1k
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is My Data in Your AI? Membership Inference Test (MINT) applied to Face Biometrics</title>
<link>https://arxiv.org/abs/2402.09225</link>
<guid>https://arxiv.org/abs/2402.09225</guid>
<content:encoded><![CDATA[
arXiv:2402.09225v4 Announce Type: replace-cross 
Abstract: This article introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if given data was used during the training of AI/ML models. Specifically, we propose two MINT architectures designed to learn the distinct activation patterns that emerge when an Audited Model is exposed to data used during its training process. These architectures are based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The experimental framework focuses on the challenging task of Face Recognition, considering three state-of-the-art Face Recognition systems. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Different experimental scenarios are considered depending on the context of the AI model to test. Our proposed MINT approach achieves promising results, with up to 90\% accuracy, indicating the potential to recognize if an AI model has been trained with specific data. The proposed MINT approach can serve to enforce privacy and fairness in several AI applications, e.g., revealing if sensitive or private data was used for training or tuning Large Language Models (LLMs).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Dataset Distillation with Diffusion Models</title>
<link>https://arxiv.org/abs/2403.03881</link>
<guid>https://arxiv.org/abs/2403.03881</guid>
<content:encoded><![CDATA[
arXiv:2403.03881v4 Announce Type: replace-cross 
Abstract: Dataset distillation seeks to condense datasets into smaller but highly representative synthetic samples. While diffusion models now lead all generative benchmarks, current distillation methods avoid them and rely instead on GANs or autoencoders, or, at best, sampling from a fixed diffusion prior. This trend arises because naive backpropagation through the long denoising chain leads to vanishing gradients, which prevents effective synthetic sample optimization. To address this limitation, we introduce Latent Dataset Distillation with Diffusion Models (LD3M), the first method to learn gradient-based distilled latents and class embeddings end-to-end through a pre-trained latent diffusion model. A linearly decaying skip connection, injected from the initial noisy state into every reverse step, preserves the gradient signal across dozens of timesteps without requiring diffusion weight fine-tuning. Across multiple ImageNet subsets at 128x128 and 256x256, LD3M improves downstream accuracy by up to 4.8 percentage points (1 IPC) and 4.2 points (10 IPC) over the prior state-of-the-art. The code for LD3M is provided at https://github.com/Brian-Moser/prune_and_distill.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECLM: Entity Level Language Model for Spoken Language Understanding with Chain of Intent</title>
<link>https://arxiv.org/abs/2403.04481</link>
<guid>https://arxiv.org/abs/2403.04481</guid>
<content:encoded><![CDATA[
arXiv:2403.04481v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in language generation and general task performance. However, their application to spoken language understanding (SLU) remains challenging, particularly for token-level tasks, where the autoregressive nature of LLMs often leads to misalignment issues. They also struggle to capture nuanced interrelations in semantic-level tasks through direct fine-tuning alone. To address these challenges, we propose the Entity-level Language Model (ECLM) framework, which reformulates slot-filling as an entity recognition task and introduces a novel concept, \textit{Chain of Intent}, to enable step-by-step multi-intent recognition. Experimental results show that ECLM significantly outperforms strong baselines such as Uni-MIS, achieving gains of 3.7\% on MixATIS and 3.1\% on MixSNIPS. Compared to standard supervised fine-tuning of LLMs, ECLM further achieves improvements of 8.5\% and 21.2\% on these datasets, respectively. Our code is available at https://github.com/SJY8460/ECLM.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding</title>
<link>https://arxiv.org/abs/2403.16276</link>
<guid>https://arxiv.org/abs/2403.16276</guid>
<content:encoded><![CDATA[
arXiv:2403.16276v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains. By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained. However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events. This deficiency hinders LLMs from learning the alignment between time, audio-visual events, and text tokens, thus impairing their ability to temporally localize audio-visual events in videos. To address this gap, we introduce PU-VALOR, a comprehensive audio-visual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations. PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving event-based video clustering, random temporal scaling, and permutation. By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens. AVicuna excels in temporal localization and time-aware dialogue capabilities. Our experiments demonstrate that AVicuna effectively handles temporal understanding in audio-visual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning</title>
<link>https://arxiv.org/abs/2404.12353</link>
<guid>https://arxiv.org/abs/2404.12353</guid>
<content:encoded><![CDATA[
arXiv:2404.12353v3 Announce Type: replace-cross 
Abstract: Video summarization aims to create short, accurate, and cohesive summaries of longer videos. Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective training of advanced large vision-language models (VLMs). Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization. Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT). However, the textual summaries in previous multimodal datasets are inadequate. To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39%. Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries. In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning System for Rapid and Accurate Warning of Acute Aortic Syndrome on Non-contrast CT in China</title>
<link>https://arxiv.org/abs/2406.15222</link>
<guid>https://arxiv.org/abs/2406.15222</guid>
<content:encoded><![CDATA[
arXiv:2406.15222v5 Announce Type: replace-cross 
Abstract: The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients presenting with acute chest pain remains a clinical challenge. Aortic CT angiography (CTA) is the imaging protocol of choice in patients with suspected AAS. However, due to economic and workflow constraints in China, the majority of suspected patients initially undergo non-contrast CT as the initial imaging testing, and CTA is reserved for those at higher risk. In this work, we present an artificial intelligence-based warning system, iAorta, using non-contrast CT for AAS identification in China, which demonstrates remarkably high accuracy and provides clinicians with interpretable warnings. iAorta was evaluated through a comprehensive step-wise study. In the multi-center retrospective study (n = 20,750), iAorta achieved a mean area under the receiver operating curve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study (n = 137,525), iAorta demonstrated consistently high performance across various non-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a specificity of 0.991-0.993. In the prospective comparative study (n = 13,846), iAorta demonstrated the capability to significantly shorten the time to correct diagnostic pathway. For the prospective pilot deployment that we conducted, iAorta correctly identified 21 out of 22 patients with AAS among 15,584 consecutive patients presenting with acute chest pain and under non-contrast CT protocol in the emergency department (ED) and enabled the average diagnostic time of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the iAorta can help avoid delayed or missed diagnosis of AAS in settings where non-contrast CT remains the unavoidable the initial or only imaging test in resource-constrained regions and in patients who cannot or did not receive intravenous contrast.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximately Aligned Decoding</title>
<link>https://arxiv.org/abs/2410.01103</link>
<guid>https://arxiv.org/abs/2410.01103</guid>
<content:encoded><![CDATA[
arXiv:2410.01103v2 Announce Type: replace-cross 
Abstract: It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation to re-sample after a rejection, or distort the distribution of outputs by constraining the output to highly improbable tokens. We present a method, Approximately Aligned Decoding (AprAD), to balance the distortion of the output distribution with computational efficiency, inspired by algorithms from the speculative decoding literature. AprAD allows for the generation of long sequences of text with difficult-to-satisfy constraints, while amplifying low probability outputs much less compared to existing methods. We show through a series of experiments that the task-specific performance of AprAD is comparable to methods that do not distort the output distribution, while being much more computationally efficient.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud Registration Algorithms</title>
<link>https://arxiv.org/abs/2410.11031</link>
<guid>https://arxiv.org/abs/2410.11031</guid>
<content:encoded><![CDATA[
arXiv:2410.11031v3 Announce Type: replace-cross 
Abstract: This study explores the intersection of neural networks and classical robotics algorithms through the Neural Algorithmic Reasoning (NAR) blueprint, enabling the training of neural networks to reason like classical robotics algorithms by learning to execute them. Algorithms are integral to robotics and safety-critical applications due to their predictable and consistent performance through logical and mathematical principles. In contrast, while neural networks are highly adaptable, handling complex, high-dimensional data and generalising across tasks, they often lack interpretability and transparency in their internal computations. To bridge the two, we propose a novel Graph Neural Network (GNN)-based framework, NAR-*ICP, that learns the intermediate computations of classical ICP-based registration algorithms, extending the CLRS Benchmark. We evaluate our approach across real-world and synthetic datasets, demonstrating its flexibility in handling complex inputs, and its potential to be used within larger learning pipelines. Our method achieves superior performance compared to the baselines, even surpassing the algorithms it was trained on, further demonstrating its ability to generalise beyond the capabilities of traditional algorithms.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Bounds for Physics-Informed Neural Networks in Fokker-Planck PDEs</title>
<link>https://arxiv.org/abs/2410.22371</link>
<guid>https://arxiv.org/abs/2410.22371</guid>
<content:encoded><![CDATA[
arXiv:2410.22371v3 Announce Type: replace-cross 
Abstract: Stochastic differential equations are commonly used to describe the evolution of stochastic processes. The state uncertainty of such processes is best represented by the probability density function (PDF), whose evolution is governed by the Fokker-Planck partial differential equation (FP-PDE). However, it is generally infeasible to solve the FP-PDE in closed form. In this work, we show that physics-informed neural networks (PINNs) can be trained to approximate the solution PDF. Our main contribution is the analysis of PINN approximation error: we develop a theoretical framework to construct tight error bounds using PINNs. In addition, we derive a practical error bound that can be efficiently constructed with standard training methods. We discuss that this error-bound framework generalizes to approximate solutions of other linear PDEs. Empirical results on nonlinear, high-dimensional, and chaotic systems validate the correctness of our error bounds while demonstrating the scalability of PINNs and their significant computational speedup in obtaining accurate PDF solutions compared to the Monte Carlo approach.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications</title>
<link>https://arxiv.org/abs/2411.04975</link>
<guid>https://arxiv.org/abs/2411.04975</guid>
<content:encoded><![CDATA[
arXiv:2411.04975v3 Announce Type: replace-cross 
Abstract: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</title>
<link>https://arxiv.org/abs/2411.10979</link>
<guid>https://arxiv.org/abs/2411.10979</guid>
<content:encoded><![CDATA[
arXiv:2411.10979v4 Announce Type: replace-cross 
Abstract: The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations. VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement. The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning and Multi-source Remote Sensing in Forest Aboveground Biomass Estimation: A Review</title>
<link>https://arxiv.org/abs/2411.17624</link>
<guid>https://arxiv.org/abs/2411.17624</guid>
<content:encoded><![CDATA[
arXiv:2411.17624v2 Announce Type: replace-cross 
Abstract: Quantifying forest aboveground biomass (AGB) is crucial for informing decisions and policies that will protect the planet. Machine learning (ML) and remote sensing (RS) techniques have been used to do this task more effectively, yet there lacks a systematic review on the most recent working combinations of ML methods and multiple RS sources, especially with the consideration of the forests' ecological characteristics. This study systematically analyzed 25 papers that met strict inclusion criteria from over 80 related studies, identifying all ML methods and combinations of RS data used. Random Forest had the most frequent appearance (88\% of studies), while Extreme Gradient Boosting showed superior performance in 75\% of the studies in which it was compared with other methods. Sentinel-1 emerged as the most utilized remote sensing source, with multi-sensor approaches (e.g., Sentinel-1, Sentinel-2, and LiDAR) proving especially effective. Our findings provide grounds for recommending which sensing sources, variables, and methods to consider using when integrating ML and RS for forest AGB estimation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable Self-evolution Adversarial Training</title>
<link>https://arxiv.org/abs/2412.02270</link>
<guid>https://arxiv.org/abs/2412.02270</guid>
<content:encoded><![CDATA[
arXiv:2412.02270v2 Announce Type: replace-cross 
Abstract: With the wide application of deep neural network models in various computer vision tasks, there has been a proliferation of adversarial example generation strategies aimed at deeply exploring model security. However, existing adversarial training defense models, which rely on single or limited types of attacks under a one-time learning process, struggle to adapt to the dynamic and evolving nature of attack methods. Therefore, to achieve defense performance improvements for models in long-term applications, we propose a novel Sustainable Self-Evolution Adversarial Training (SSEAT) framework. Specifically, we introduce a continual adversarial defense pipeline to realize learning from various kinds of adversarial examples across multiple stages. Additionally, to address the issue of model catastrophic forgetting caused by continual learning from ongoing novel attacks, we propose an adversarial data replay module to better select more diverse and key relearning data. Furthermore, we design a consistency regularization strategy to encourage current defense models to learn more from previously trained ones, guiding them to retain more past knowledge and maintain accuracy on clean samples. Extensive experiments have been conducted to verify the efficacy of the proposed SSEAT defense method, which demonstrates superior defense performance and classification accuracy compared to competitors.Code is available at https://github.com/aup520/SSEAT
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title>
<link>https://arxiv.org/abs/2412.08127</link>
<guid>https://arxiv.org/abs/2412.08127</guid>
<content:encoded><![CDATA[
arXiv:2412.08127v4 Announce Type: replace-cross 
Abstract: It has been widely observed that language models (LMs) respond in predictable ways to algorithmically generated prompts that are seemingly unintelligible. This is both a sign that we lack a full understanding of how LMs work, and a practical challenge, because opaqueness can be exploited for harmful uses of LMs, such as jailbreaking. We present the first thorough analysis of opaque machine-generated prompts, or autoprompts, pertaining to 6 LMs of different sizes and families. We find that machine-generated prompts are characterized by a last token that is often intelligible and strongly affects the generation. A small but consistent proportion of the previous tokens are prunable, probably appearing in the prompt as a by-product of the fact that the optimization process fixes the number of tokens. The remaining tokens fall into two categories: filler tokens, which can be replaced with semantically unrelated substitutes, and keywords, that tend to have at least a loose semantic relation with the generation, although they do not engage in well-formed syntactic relations with it. Additionally, human experts can reliably identify the most influential tokens in an autoprompt a posteriori, suggesting these prompts are not entirely opaque. Finally, some of the ablations we applied to autoprompts yield similar effects in natural language inputs, suggesting that autoprompts emerge naturally from the way LMs process linguistic inputs in general.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine</title>
<link>https://arxiv.org/abs/2412.09278</link>
<guid>https://arxiv.org/abs/2412.09278</guid>
<content:encoded><![CDATA[
arXiv:2412.09278v3 Announce Type: replace-cross 
Abstract: In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at https://github.com/ShawnHuang497/MedPLIB.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KunServe: Parameter-centric Memory Management for Efficient Memory Overloading Handling in LLM Serving</title>
<link>https://arxiv.org/abs/2412.18169</link>
<guid>https://arxiv.org/abs/2412.18169</guid>
<content:encoded><![CDATA[
arXiv:2412.18169v5 Announce Type: replace-cross 
Abstract: Serving LLMs with a cluster of GPUs is common nowadays, where the serving system must meet strict latency SLOs required by applications. However, the stateful nature of LLM serving requires maintaining huge states (i.e., KVCache) in limited GPU memory. Under spikes in real-world workloads, GPU memory can be easily throttled, leading to orders of magnitude higher response latency due to queuing introduced by waiting for KVCache to be reclaimed. Prior KVCache-centric approaches handle load throttling by dropping, migrating, or swapping KVCache. These methods fail to release sufficient memory quickly with requests still queued.
  This paper proposes the first parameter-centric approach to handling throttling by selectively dropping replicated parameters to instantly free memory for requests, based on an unnoticed observation that model parameters are commonly replicated across GPUs for serving LLMs. With additional memory, all requests can be served with a larger batch without queuing. To make the parameter-centric approach correct and efficient, we cooperatively execute requests on GPUs with a complete copy of parameters using pipeline parallelism, and derive an appropriate drop plan without unnecessary cooperation. We also design techniques to minimize the performance overhead due to pipeline parallelism with the execution patterns of requests under drop. Evaluations show that {\sys} reduces the tail TTFT of requests under throttling by up to 72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and InferCept.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tempo: Compiled Dynamic Deep Learning with Symbolic Dependence Graphs</title>
<link>https://arxiv.org/abs/2501.05408</link>
<guid>https://arxiv.org/abs/2501.05408</guid>
<content:encoded><![CDATA[
arXiv:2501.05408v3 Announce Type: replace-cross 
Abstract: Deep learning (DL) algorithms are often defined in terms of temporal relationships: a tensor at one timestep may depend on tensors from earlier or later timesteps. Such dynamic dependencies (and corresponding dynamic tensor shapes) are difficult to express and optimize: while eager DL systems support such dynamism, they cannot apply compiler-based optimizations; graph-based systems require static tensor shapes, which forces users to pad tensors or break-up programs into multiple static graphs.
  We describe Tempo, a new DL system that combines the dynamism of eager execution with the whole-program optimizations of graph-based compilation. Tempo achieves this through a declarative programming model with recurrent tensors, which include explicit temporal dimensions. Temporal dimensions can be indexed using symbolic expressions to express dynamic dependencies on past and future tensors. Based on this, Tempo constructs a symbolic dependence graph, which concisely encodes dynamic dependencies between operators, and applies whole-program optimizations, such as algebraic simplifications, vectorization, tiling, and fusion. By tiling dynamic dependencies into static-size blocks, Tempo can also reuse existing static code-generators. It then uses a polyhedral model to find a feasible execution schedule, which includes memory management operations. We show that Tempo achieves a 7$\times$ speedup over JAX for Llama-3.2-3B decoding; for reinforcement learning algorithms, Tempo achieves a 54$\times$ speedup, with 16$\times$ lower peak memory usage.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Cel-Animation: A Survey</title>
<link>https://arxiv.org/abs/2501.06250</link>
<guid>https://arxiv.org/abs/2501.06250</guid>
<content:encoded><![CDATA[
arXiv:2501.06250v4 Announce Type: replace-cross 
Abstract: Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, challenges like visual consistency, stylistic coherence, and ethical considerations persist. Additionally, this paper explores future directions and advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAGHN: Personalized Federated Learning with Attentive Graph HyperNetworks</title>
<link>https://arxiv.org/abs/2501.16379</link>
<guid>https://arxiv.org/abs/2501.16379</guid>
<content:encoded><![CDATA[
arXiv:2501.16379v2 Announce Type: replace-cross 
Abstract: Personalized Federated Learning (PFL) aims to address the statistical heterogeneity of data across clients by learning the personalized model for each client. Among various PFL approaches, the personalized aggregation-based approach conducts parameter aggregation in the server-side aggregation phase to generate personalized models, and focuses on learning appropriate collaborative relationships among clients for aggregation. However, the collaborative relationships vary in different scenarios and even at different stages of the FL process. To this end, we propose Personalized Federated Learning with Attentive Graph HyperNetworks (FedAGHN), which employs Attentive Graph HyperNetworks (AGHNs) to dynamically capture fine-grained collaborative relationships and generate client-specific personalized initial models. Specifically, AGHNs empower graphs to explicitly model the client-specific collaborative relationships, construct collaboration graphs, and introduce tunable attentive mechanism to derive the collaboration weights, so that the personalized initial models can be obtained by aggregating parameters over the collaboration graphs. Extensive experiments can demonstrate the superiority of FedAGHN. Moreover, a series of visualizations are presented to explore the effectiveness of collaboration graphs learned by FedAGHN.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.17384</link>
<guid>https://arxiv.org/abs/2501.17384</guid>
<content:encoded><![CDATA[
arXiv:2501.17384v2 Announce Type: replace-cross 
Abstract: Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Hyperbolic-Like Expressiveness with Arbitrary Euclidean Regions: A New Approach to Hierarchical Embeddings</title>
<link>https://arxiv.org/abs/2501.17518</link>
<guid>https://arxiv.org/abs/2501.17518</guid>
<content:encoded><![CDATA[
arXiv:2501.17518v2 Announce Type: replace-cross 
Abstract: Hierarchical data is common in many domains like life sciences and e-commerce, and its embeddings often play a critical role. While hyperbolic embeddings offer a theoretically grounded approach to representing hierarchies in low-dimensional spaces, current methods often rely on specific geometric constructs as embedding candidates. This reliance limits their generalizability and makes it difficult to integrate with techniques that model semantic relationships beyond pure hierarchies, such as ontology embeddings. In this paper, we present RegD, a flexible Euclidean framework that supports the use of arbitrary geometric regions -- such as boxes and balls -- as embedding representations. Although RegD operates entirely in Euclidean space, we formally prove that it achieves hyperbolic-like expressiveness by incorporating a depth-based dissimilarity between regions, enabling it to emulate key properties of hyperbolic geometry, including exponential growth. Our empirical evaluation on diverse real-world datasets shows consistent performance gains over state-of-the-art methods and demonstrates RegD's potential for broader applications such as the ontology embedding task that goes beyond hierarchy.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning via Neural Activation Redirection</title>
<link>https://arxiv.org/abs/2502.07218</link>
<guid>https://arxiv.org/abs/2502.07218</guid>
<content:encoded><![CDATA[
arXiv:2502.07218v2 Announce Type: replace-cross 
Abstract: The ability to selectively remove knowledge from LLMs is highly desirable. However, existing methods often struggle with balancing unlearning efficacy and retain model utility, and lack controllability at inference time to emulate base model behavior as if it had never seen the unlearned data. In this paper, we propose LUNAR, a novel unlearning method grounded in the Linear Representation Hypothesis and operates by redirecting the representations of unlearned data to activation regions that expresses its inability to answer. We show that contrastive features are not a prerequisite for effective activation redirection, and LUNAR achieves state-of-the-art unlearning performance and superior controllability. Specifically, LUNAR achieves between 2.9x and 11.7x improvement in the combined unlearning efficacy and model utility score (Deviation Score) across various base models and generates coherent, contextually appropriate responses post-unlearning. Moreover, LUNAR effectively reduces parameter updates to a single down-projection matrix, a novel design that significantly enhances efficiency by 20x and robustness. Finally, we demonstrate that LUNAR is robust to white-box adversarial attacks and versatile in real-world scenarios, including handling sequential unlearning requests.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks</title>
<link>https://arxiv.org/abs/2502.17832</link>
<guid>https://arxiv.org/abs/2502.17832</guid>
<content:encoded><![CDATA[
arXiv:2502.17832v3 Announce Type: replace-cross 
Abstract: Multimodal large language models with Retrieval Augmented Generation (RAG) have significantly advanced tasks such as multimodal question answering by grounding responses in external text and images. This grounding improves factuality, reduces hallucination, and extends reasoning beyond parametric knowledge. However, this reliance on external knowledge poses a critical yet underexplored safety risk: knowledge poisoning attacks, where adversaries deliberately inject adversarial multimodal content into external knowledge bases to steer model toward generating incorrect or even harmful responses. To expose such vulnerabilities, we propose MM-PoisonRAG, the first framework to systematically design knowledge poisoning in multimodal RAG. We introduce two complementary attack strategies: Localized Poisoning Attack (LPA), which implants targeted multimodal misinformation to manipulate specific queries, and Globalized Poisoning Attack (GPA), which inserts a single adversarial knowledge to broadly disrupt reasoning and induce nonsensical responses across all queries. Comprehensive experiments across tasks, models, and access settings show that LPA achieves targeted manipulation with attack success rates of up to 56%, while GPA completely disrupts model generation to 0% accuracy with just a single adversarial knowledge injection. Our results reveal the fragility of multimodal RAG and highlight the urgent need for defenses against knowledge poisoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossy Neural Compression for Geospatial Analytics: A Review</title>
<link>https://arxiv.org/abs/2503.01505</link>
<guid>https://arxiv.org/abs/2503.01505</guid>
<content:encoded><![CDATA[
arXiv:2503.01505v2 Announce Type: replace-cross 
Abstract: Over the past decades, there has been an explosion in the amount of available Earth Observation (EO) data. The unprecedented coverage of the Earth's surface and atmosphere by satellite imagery has resulted in large volumes of data that must be transmitted to ground stations, stored in data centers, and distributed to end users. Modern Earth System Models (ESMs) face similar challenges, operating at high spatial and temporal resolutions, producing petabytes of data per simulated day. Data compression has gained relevance over the past decade, with neural compression (NC) emerging from deep learning and information theory, making EO data and ESM outputs ideal candidates due to their abundance of unlabeled data. In this review, we outline recent developments in NC applied to geospatial data. We introduce the fundamental concepts of NC including seminal works in its traditional applications to image and video compression domains with focus on lossy compression. We discuss the unique characteristics of EO and ESM data, contrasting them with "natural images", and explain the additional challenges and opportunities they present. Moreover, we review current applications of NC across various EO modalities and explore the limited efforts in ESM compression to date. The advent of self-supervised learning (SSL) and foundation models (FM) has advanced methods to efficiently distill representations from vast unlabeled data. We connect these developments to NC for EO, highlighting the similarities between the two fields and elaborate on the potential of transferring compressed feature representations for machine--to--machine communication. Based on insights drawn from this review, we devise future directions relevant to applications in EO and ESM.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the (Belief) Gap: Group Identity in the World of LLMs</title>
<link>https://arxiv.org/abs/2503.02016</link>
<guid>https://arxiv.org/abs/2503.02016</guid>
<content:encoded><![CDATA[
arXiv:2503.02016v2 Announce Type: replace-cross 
Abstract: Social biases and belief-driven behaviors can significantly impact Large Language Models (LLMs) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to 37% and enhance learning by 11%. Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neutral Point-of-View Generation with Data- and Parameter-Efficient RL</title>
<link>https://arxiv.org/abs/2503.03654</link>
<guid>https://arxiv.org/abs/2503.03654</guid>
<content:encoded><![CDATA[
arXiv:2503.03654v2 Announce Type: replace-cross 
Abstract: The paper shows that parameter-efficient reinforcement learning (PE-RL) is a highly effective training regime to improve large language models' (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e. to provide significantly more informative, diverse and impartial answers. This is shown by evaluating PE-RL and multiple strong baselines-including LoRA finetuning (strongest baseline), SFT and RLHF. PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06\%\rightarrow 99.08\%$), but also scores much higher on features linguists identify as key to separating sufficient answers from "great'' answers ($60.25\%\rightarrow 85.21\%$ for presence of supportive details, $68.74\%\rightarrow 91.43\%$ for absence of oversimplification). A qualitative analysis corroborates this. Moreover, our evaluation also finds a key property of PE-RL for this task: unlike methods that update all parameters, it generalises out of topic. Finally, to enable further studies we also release the dataset, SHQ-NPOV, and provide a methodology to create such datasets through iterative rounds of human peer-critique and annotator training.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided and Self-Consistent MLLMs for Task Planning in Instruction-Following Manipulation</title>
<link>https://arxiv.org/abs/2503.13055</link>
<guid>https://arxiv.org/abs/2503.13055</guid>
<content:encoded><![CDATA[
arXiv:2503.13055v2 Announce Type: replace-cross 
Abstract: We investigate the use of Multimodal Large Language Models (MLLMs) with in-context learning for closed-loop task planning in instruction-following manipulation. We identify four essential requirements for successful task planning: quantity estimation, reachability analysis, relative positioning, and collision avoidance. However, existing benchmarks fail to support holistic evaluation across all these aspects. To address this gap, we introduce \textbf{QuARC} (Quantity, Analysis, Relative positioning, Collision), a new benchmark based on a food preparation scenario that integrates all four challenges. Using QuARC, we reveal two major limitations of current MLLMs: cross-modal distraction and geometric infeasibility. To tackle these, we adapt Chain-of-Thought with Self-Consistency to mitigate reasoning loss from cross-modal distractions and incorporate an affordance predictor to guide planning based on geometric feasibility. Our comprehensive evaluation analyzes performance across multiple baselines and explains sources of improvement. Our method achieves a 76.7\% success rate on the benchmark, significantly outperforming the ViLa baseline (36.7\%), without requiring additional finetuning. Code and dataset are available at https://hcis-lab.github.io/Affordance-Guided-Self-Consistent-MLLM.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NdLinear: Preserving Multi-Dimensional Structure for Parameter-Efficient Neural Networks</title>
<link>https://arxiv.org/abs/2503.17353</link>
<guid>https://arxiv.org/abs/2503.17353</guid>
<content:encoded><![CDATA[
arXiv:2503.17353v3 Announce Type: replace-cross 
Abstract: In deep learning, processing multidimensional inputs (e.g., images, medical scans, and time series) is an important task that often requires flattening the inputs. We introduce $\mathit{NdLinear}$, a drop-in replacement for linear layers that operates directly on tensors, requiring no flattening. By applying transformations separately along each dimension, NdLinear preserves native data structure while achieving dramatic parameter reductions, often by orders of magnitude, with minimal memory overhead. We prove NdLinear maintains expressivity through structured Tucker decomposition while preserving VC-dimension scaling. Extensive experiments demonstrate NdLinear's capacity to achieve significant parameter reductions with substantial wall-clock efficiency gains and minimal memory overhead. For instance, our $\mathit{NdLinear-LoRA}$ matches or exceeds standard LoRA on language reasoning tasks using up to $9\times$ fewer parameters. Experiments across CNNs, RNNs, Transformers, and MLPs on vision, language, time-series, and tabular tasks consistently demonstrate NdLinear's efficiency gains. While excelling at axis-separable tasks, NdLinear has limitations with entangled spatial interactions. By processing data in its original N-dimensional form, NdLinear provides a theoretically grounded, practical component for building more efficient neural architectures.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification</title>
<link>https://arxiv.org/abs/2503.19945</link>
<guid>https://arxiv.org/abs/2503.19945</guid>
<content:encoded><![CDATA[
arXiv:2503.19945v3 Announce Type: replace-cross 
Abstract: Mammography, an X-ray-based imaging technique, remains central to the early detection of breast cancer. Recent advances in artificial intelligence have enabled increasingly sophisticated computer-aided diagnostic methods, evolving from patch-based classifiers to whole-image approaches and then to multi-view architectures that jointly analyze complementary projections. Despite this progress, several critical questions remain unanswered. In this study, we systematically investigate these issues by addressing five key research questions: (1) the role of patch classifiers in performance, (2) the transferability of natural-image-trained backbones, (3) the advantages of learn-to-resize over conventional downscaling, (4) the contribution of multi-view integration, and (5) the robustness of findings across varying image quality. Beyond benchmarking, our experiments demonstrate clear performance gains over prior work. For the CBIS-DDSM dataset, we improved single-view AUC from 0.8153 to 0.8343, and multiple-view AUC from 0.8483 to 0.8658. Using a new comparative method, we also observed a 0.0217 AUC increase when extending from single to multiple-view analysis. On the complete VinDr-Mammo dataset, the multiple-view approach further improved results, achieving a 0.0492 AUC increase over single view and reaching 0.8511 AUC overall. These results establish new state-of-the-art benchmarks, providing clear evidence of the advantages of multi-view architectures for mammogram interpretation. Beyond performance, our analysis offers principled insights into model design and transfer learning strategies, contributing to the development of more accurate and reliable breast cancer screening tools. The inference code and trained models are publicly available at https://github.com/dpetrini/multiple-view.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations</title>
<link>https://arxiv.org/abs/2504.07836</link>
<guid>https://arxiv.org/abs/2504.07836</guid>
<content:encoded><![CDATA[
arXiv:2504.07836v4 Announce Type: replace-cross 
Abstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight Ensembling Improves Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2504.10478</link>
<guid>https://arxiv.org/abs/2504.10478</guid>
<content:encoded><![CDATA[
arXiv:2504.10478v4 Announce Type: replace-cross 
Abstract: We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades off between bias and variance.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Flow Matching using Latent Variables</title>
<link>https://arxiv.org/abs/2505.04486</link>
<guid>https://arxiv.org/abs/2505.04486</guid>
<content:encoded><![CDATA[
arXiv:2505.04486v3 Announce Type: replace-cross 
Abstract: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. However, most flow matching models in the literature do not explicitly utilize the underlying clustering structure in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. To this end, we present $\texttt{Latent-CFM}$, which provides efficient training strategies by conditioning on the features extracted from data using pretrained deep latent variable models. Through experiments on synthetic data from multi-modal distributions and widely used image benchmark datasets, we show that $\texttt{Latent-CFM}$ exhibits improved generation quality with significantly less training and computation than state-of-the-art flow matching models by adopting pretrained lightweight latent variable models. Beyond natural images, we consider generative modeling of spatial fields stemming from physical processes. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competing approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features, which adds interpretability to the generation process.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Pre-trained Autoregressive Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.07344</link>
<guid>https://arxiv.org/abs/2505.07344</guid>
<content:encoded><![CDATA[
arXiv:2505.07344v5 Announce Type: replace-cross 
Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2505.10607</link>
<guid>https://arxiv.org/abs/2505.10607</guid>
<content:encoded><![CDATA[
arXiv:2505.10607v2 Announce Type: replace-cross 
Abstract: The growing use of smartphones and IoT devices necessitates efficient time-series analysis on resource-constrained hardware, which is critical for sensing applications such as human activity recognition and air quality prediction. Recent efforts in hardware-aware neural architecture search (NAS) automate architecture discovery for specific platforms; however, none focus on general time-series analysis with edge deployment. Leveraging the problem-solving and reasoning capabilities of large language models (LLM), we propose MONAQ, a novel framework that reformulates NAS into Multi-Objective Neural Architecture Querying tasks. MONAQ is equipped with multimodal query generation for processing multimodal time-series inputs and hardware constraints, alongside an LLM agent-based multi-objective search to achieve deployment-ready models via code generation. By integrating numerical data, time-series images, and textual descriptions, MONAQ improves an LLM's understanding of time-series data. Experiments on fifteen datasets demonstrate that MONAQ-discovered models outperform both handcrafted models and NAS baselines while being more efficient.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs</title>
<link>https://arxiv.org/abs/2505.11557</link>
<guid>https://arxiv.org/abs/2505.11557</guid>
<content:encoded><![CDATA[
arXiv:2505.11557v2 Announce Type: replace-cross 
Abstract: Corporate LLMs are gaining traction for efficient knowledge dissemination and management within organizations. However, as current LLMs are vulnerable to leaking sensitive information, it has proven difficult to apply them in settings where strict access control is necessary. To this end, we design AC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots that maintains a strong information isolation guarantee. AC-LoRA maintains separate LoRA adapters for permissioned datasets, along with the document embedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA adapters based on the similarity score with the user query and their permission. This similarity score is later used to merge the responses if more than one LoRA is retrieved, without requiring any additional training for LoRA routing. We provide an end-to-end prototype of AC-LoRA, evaluate it on two datasets, and show that AC-LoRA matches or even exceeds the performance of state-of-the-art LoRA mixing techniques while providing strong isolation guarantees. Furthermore, we show that AC-LoRA design can be directly applied to different modalities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaDim: Dimensionality Adaptation for SSL Representational Dynamics</title>
<link>https://arxiv.org/abs/2505.12576</link>
<guid>https://arxiv.org/abs/2505.12576</guid>
<content:encoded><![CDATA[
arXiv:2505.12576v2 Announce Type: replace-cross 
Abstract: A key factor in effective Self-Supervised learning (SSL) is preventing dimensional collapse, where higher-dimensional representation spaces ($R$) span a lower-dimensional subspace. Therefore, SSL optimization strategies involve guiding a model to produce $R$ with a higher dimensionality ($H(R)$) through objectives that encourage decorrelation of features or sample uniformity in $R$. A higher $H(R)$ indicates that $R$ has greater feature diversity which is useful for generalization to downstream tasks. Alongside dimensionality optimization, SSL algorithms also utilize a projection head that maps $R$ into an embedding space $Z$. Recent work has characterized the projection head as a filter of noisy or irrelevant features from the SSL objective by reducing the mutual information $I(R;Z)$. Therefore, the current literature's view is that a good SSL representation space should have a high $H(R)$ and a low $I(R;Z)$. However, this view of SSL is lacking in terms of an understanding of the underlying training dynamics that influences the relationship between both terms. Our analysis shows that the best performing SSL models do not have the highest $H(R)$ nor the lowest $I(R;Z)$, but effectively arrive at a balance between both. To take advantage of this analysis, we introduce AdaDim, a training strategy that leverages SSL training dynamics by adaptively balancing between increasing $H(R)$ through feature decorrelation and sample uniformity as well as gradual regularization of $I(R;Z)$ as training progresses. We show performance improvements of up to 3% over common SSL baselines despite our method not utilizing expensive techniques such as queues, clustering, predictor networks, or student-teacher architectures.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</title>
<link>https://arxiv.org/abs/2505.15946</link>
<guid>https://arxiv.org/abs/2505.15946</guid>
<content:encoded><![CDATA[
arXiv:2505.15946v3 Announce Type: replace-cross 
Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis</title>
<link>https://arxiv.org/abs/2505.16834</link>
<guid>https://arxiv.org/abs/2505.16834</guid>
<content:encoded><![CDATA[
arXiv:2505.16834v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at https://github.com/RUCAIBox/SimpleDeepSearcher.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization of Large Language Models</title>
<link>https://arxiv.org/abs/2505.17967</link>
<guid>https://arxiv.org/abs/2505.17967</guid>
<content:encoded><![CDATA[
arXiv:2505.17967v4 Announce Type: replace-cross 
Abstract: Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to improve running time and reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD) or QR-decomposition. Applying these techniques individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple, two-step procedure to approximate SVD/QR-based gradient projections into lower-dimensional spaces by using a predefined orthogonal matrix of the Discrete Cosine Transform (DCT). We dynamically select columns from the DCT matrix based on their alignment with the gradient of each layer. The effective projection matrices are obtained via a simple matmul with the DCT matrix in $O(n^3)$ time, followed by a lightweight sorting step to identify the most relevant basis vectors. For large layers, DCT can be computed via Makhoul's $N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \log(n))$ time. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, obtaining an approach with rank-independent running time that matches the performance of costly SVD/QR-based methods while achieving faster runtime and reduced memory usage by up to $25\%$ across different model sizes. Our code is available at \href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs</title>
<link>https://arxiv.org/abs/2505.18356</link>
<guid>https://arxiv.org/abs/2505.18356</guid>
<content:encoded><![CDATA[
arXiv:2505.18356v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of machine-learning-assisted Monte Carlo in sampling from simple statistical physics models</title>
<link>https://arxiv.org/abs/2505.22598</link>
<guid>https://arxiv.org/abs/2505.22598</guid>
<content:encoded><![CDATA[
arXiv:2505.22598v4 Announce Type: replace-cross 
Abstract: Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.23867</link>
<guid>https://arxiv.org/abs/2505.23867</guid>
<content:encoded><![CDATA[
arXiv:2505.23867v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in domains such as visual understanding and mathematical reasoning. However, their application in the medical domain is constrained by two key challenges: (1) multimodal medical datasets are scarce and often contain sparse information, limiting reasoning depth; and (2) Reinforcement Learning with Verifiable Rewards (RLVR), though effective in general domains, cannot reliably improve model performance in the medical domain. To overcome these challenges, during the supervised fine-tuning (SFT) stage, we incorporate high-quality textual reasoning data and general multimodal data alongside multimodal medical data to efficiently enhance foundational medical capabilities and restore the base model's reasoning ability. Moreover, considering that there are some multimodal medical datasets with sparse information, we further synthesize reflective-pattern-injected chain-of-thought (CoT) in addition to general CoT samples, equipping the model with initial reflective reasoning capabilities that provide a structured foundation for subsequent RLVR training. Finally, we introduce our InfiMed-Series models, InfiMed-SFT-3B and InfiMed-RL-3B, both of which deliver state-of-the-art performance across seven multimodal medical benchmarks. Notably, InfiMed-RL-3B achieves an average accuracy of 59.2%, outperforming even larger models like InternVL3-8B, which achieves 57.3%. Specifically, during the SFT phase, we utilized 188K samples, while the RLVR phase incorporated 36K samples, demonstrating the efficacy of both training strategies in achieving superior performance. We also conducted a series of extensive experiments, which provide valuable insights that contribute to advancing the performance of MLLMs in medical scenarios.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exchangeability in Neural Network and its Application to Dynamic Pruning</title>
<link>https://arxiv.org/abs/2506.02210</link>
<guid>https://arxiv.org/abs/2506.02210</guid>
<content:encoded><![CDATA[
arXiv:2506.02210v2 Announce Type: replace-cross 
Abstract: Modern neural networks (NN) contain an ever-growing number of parameters, substantially increasing the memory and computational cost of inference. Researchers have explored various ways to reduce the inference cost of NNs by reducing the model size before deployment and dynamically pruning the inference computation at runtime. In this work, we present ExPrune, a general, dynamic pruning optimization that enables multi-granularity partial computation on a per-input basis. ExPrune requires no change to the model architecture or the training algorithm. ExPrune is based on our theoretical results that the relationship between certain model parameters and intermediate values can be described by a statistical property called exchangeability. By identifying exchangeable parameters and values in the model, we are able to first partially evaluate the network, analyze the statistics of the partial results, and make pruning decisions on the fly. Because ExPrune is theory grounded, it generalizes across model architectures in different problem domains. We evaluate ExPrune on one computer vision models, one graph model and one language model. ExPrune provides 10.98--17.33% reduction in FLOPs with negligible accuracy drop and 21.61--27.16% reduction in FLOPs with at most 1% accuracy drop. We also demonstrate that ExPrune composes with static magnitude pruning. On models that have been aggressively statically pruned, ExPrune still provides additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and 13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberGym: Evaluating AI Agents' Real-World Cybersecurity Capabilities at Scale</title>
<link>https://arxiv.org/abs/2506.02548</link>
<guid>https://arxiv.org/abs/2506.02548</guid>
<content:encoded><![CDATA[
arXiv:2506.02548v2 Announce Type: replace-cross 
Abstract: AI agents have significant potential to reshape cybersecurity, making a thorough assessment of their capabilities critical. However, existing evaluations fall short, because they are based on small-scale benchmarks and only measure static outcomes, failing to capture the full, dynamic range of real-world security challenges. To address these limitations, we introduce CyberGym, a large-scale benchmark featuring 1,507 real-world vulnerabilities across 188 software projects. Adjustable to different vulnerability analysis settings, CyberGym primarily tasks agents with generating a proof-of-concept test that reproduces a vulnerability, given only its text description and the corresponding codebase. Our extensive evaluation highlights that CyberGym effectively differentiates agents' and models' cybersecurity capabilities. Even the top-performing combinations only achieve a ~20% success rate, demonstrating the overall difficulty of CyberGym. Beyond static benchmarking, we show that CyberGym leads to the discovery of 35 zero-day vulnerabilities and 17 historically incomplete patches. These results underscore that CyberGym is not only a robust benchmark for measuring AI's progress in cybersecurity but also a platform for creating direct, real-world security impact.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots</title>
<link>https://arxiv.org/abs/2506.05516</link>
<guid>https://arxiv.org/abs/2506.05516</guid>
<content:encoded><![CDATA[
arXiv:2506.05516v2 Announce Type: replace-cross 
Abstract: Adaptive recovery from fall incidents are essential skills for the practical deployment of wheeled-legged robots, which uniquely combine the agility of legs with the speed of wheels for rapid recovery. However, traditional methods relying on preplanned recovery motions, simplified dynamics or sparse rewards often fail to produce robust recovery policies. This paper presents a learning-based framework integrating Episode-based Dynamic Reward Shaping and curriculum learning, which dynamically balances exploration of diverse recovery maneuvers with precise posture refinement. An asymmetric actor-critic architecture accelerates training by leveraging privileged information in simulation, while noise-injected observations enhance robustness against uncertainties. We further demonstrate that synergistic wheel-leg coordination reduces joint torque consumption by 15.8% and 26.2% and improves stabilization through energy transfer mechanisms. Extensive evaluations on two distinct quadruped platforms achieve recovery success rates up to 99.1% and 97.8% without platform-specific tuning. The supplementary material is available at https://boyuandeng.github.io/L2R-WheelLegCoordination/
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes</title>
<link>https://arxiv.org/abs/2506.06541</link>
<guid>https://arxiv.org/abs/2506.06541</guid>
<content:encoded><![CDATA[
arXiv:2506.06541v2 Announce Type: replace-cross 
Abstract: Constructing real-world data-to-insight pipelines often involves data extraction from data lakes, data integration across heterogeneous data sources, and diverse operations from data cleaning to analysis. The design and implementation of data science pipelines require domain knowledge, technical expertise, and even project-specific insights. AI systems have shown remarkable reasoning, coding, and understanding capabilities. However, it remains unclear to what extent these capabilities translate into successful design and execution of such complex pipelines. We introduce KRAMABENCH: a benchmark composed of 104 manually-curated real-world data science pipelines spanning 1700 data files from 24 data sources in 6 different domains. We show that these pipelines test the end-to-end capabilities of AI systems on data processing, requiring data discovery, wrangling and cleaning, efficient processing, statistical reasoning, and orchestrating data processing steps given a high-level task. Our evaluation tests 5 general models and 3 code generation models using our reference framework, DS-GURU, which instructs the AI model to decompose a question into a sequence of subtasks, reason through each step, and synthesize Python code that implements the proposed design. Our results on KRAMABENCH show that, although the models are sufficiently capable of solving well-specified data science code generation tasks, when extensive data processing and domain knowledge are required to construct real-world data science pipelines, existing out-of-box models fall short. Progress on KramaBench represents crucial steps towards developing autonomous data science agents for real-world applications. Our code, reference framework, and data are available at https://github.com/mitdbg/KramaBench.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
arXiv:2506.10974v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. Code is at https://github.com/innovatingAI/AutoMind.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefilled responses enhance zero-shot detection of AI-generated images</title>
<link>https://arxiv.org/abs/2506.11031</link>
<guid>https://arxiv.org/abs/2506.11031</guid>
<content:encoded><![CDATA[
arXiv:2506.11031v3 Announce Type: replace-cross 
Abstract: As AI models generate increasingly realistic images, growing concerns over potential misuse underscore the need for reliable detection. Traditional supervised detection methods depend on large, curated datasets for training and often fail to generalize to novel, out-of-domain image generators. As an alternative, we explore pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. We evaluate VLM performance on three diverse benchmarks encompassing synthetic images of human faces, objects, and animals produced by 16 different state-of-the-art image generators. While off-the-shelf VLMs perform poorly on these datasets, we find that their reasoning can be guided effectively through simple response prefilling -- a method we call Prefill-Guided Thinking (PGT). In particular, prefilling a VLM response with the task-aligned phrase "Let's examine the style and the synthesis artifacts" improves the Macro F1 scores of three widely used open-source VLMs by up to 24%.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning</title>
<link>https://arxiv.org/abs/2506.15828</link>
<guid>https://arxiv.org/abs/2506.15828</guid>
<content:encoded><![CDATA[
arXiv:2506.15828v2 Announce Type: replace-cross 
Abstract: Embodied agents need to plan and act reliably in real and complex 3D environments. Classical planning (e.g., PDDL) offers structure and guarantees, but in practice it fails under noisy perception and incorrect predicate grounding. On the other hand, Large Language Models (LLMs)-based planners leverage commonsense reasoning, yet frequently propose actions that are unfeasible or unsafe. Following recent works that combine the two approaches, we introduce ContextMatters, a framework that fuses LLMs and classical planning to perform hierarchical goal relaxation: the LLM helps ground symbols to the scene and, when the target is unreachable, it proposes functionally equivalent goals that progressively relax constraints, adapting the goal to the context of the agent's environment. Operating on 3D Scene Graphs, this mechanism turns many nominally unfeasible tasks into tractable plans and enables context-aware partial achievement when full completion is not achievable. Our experimental results show a +52.45% Success Rate improvement over state-of-the-art LLMs+PDDL baseline, demonstrating the effectiveness of our approach. Moreover, we validate the execution of ContextMatter in a real world scenario by deploying it on a TIAGo robot. Code, dataset, and supplementary materials are available to the community at https://lab-rococo-sapienza.github.io/context-matters/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories</title>
<link>https://arxiv.org/abs/2506.18824</link>
<guid>https://arxiv.org/abs/2506.18824</guid>
<content:encoded><![CDATA[
arXiv:2506.18824v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks, such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: RepairAgent, AutoCodeRover, and OpenHands. We unify their interaction logs into a common format, capturing 120 trajectories and 2,822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics, such as iteration counts and token consumption, recurring action sequences, and the semantic coherence of thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Progress Prediction in Reasoning Language Models</title>
<link>https://arxiv.org/abs/2506.23274</link>
<guid>https://arxiv.org/abs/2506.23274</guid>
<content:encoded><![CDATA[
arXiv:2506.23274v3 Announce Type: replace-cross 
Abstract: Recent advances in reasoning language models -- particularly those that use long, latent chains of thought -- have demonstrated remarkable capabilities in complex, agentic tasks. However, as these models operate over increasingly extended time horizons, their internal progress becomes opaque to users, complicating expectation management and real-time oversight. In this work, we investigate whether real-time progress prediction is feasible. We discretize progress and train a linear probe to classify reasoning states. We then introduce a two-stage fine-tuning approach that enables reasoning models to generate progress estimates (0$\rightarrow$100\%) during inference. Our best fine-tuned model achieves an average error of 10\% for sequences less than 16,000 tokens, offering a practical mechanism for monitoring and interpreting model reasoning in real time.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization</title>
<link>https://arxiv.org/abs/2507.03318</link>
<guid>https://arxiv.org/abs/2507.03318</guid>
<content:encoded><![CDATA[
arXiv:2507.03318v2 Announce Type: replace-cross 
Abstract: Explainable artificial intelligence (XAI) approaches have been increasingly applied in drug discovery to learn molecular representations and identify substructures driving property predictions. However, building end-to-end explainable models for structure-activity relationship (SAR) modeling for compound property prediction faces many challenges, such as the limited number of compound-protein interaction activity data for specific protein targets, and plenty of subtle changes in molecular configuration sites significantly affecting molecular properties. We exploit pairs of molecules with activity cliffs that share scaffolds but differ at substituent sites, characterized by large potency differences for specific protein targets. We propose a framework by implementing graph neural networks (GNNs) to leverage property and structure information from activity cliff pairs to predict compound-protein affinity (i.e., half maximal inhibitory concentration, IC50). To enhance model performance and explainability, we train GNNs with structure-aware loss functions using group lasso and sparse group lasso regularizations, which prune and highlight molecular subgraphs relevant to activity differences. We applied this framework to activity cliff data of molecules targeting three proto-oncogene tyrosine-protein kinase Src proteins (PDB IDs: 1O42, 2H8H, 4MXO). Our approach improved property prediction by integrating common and uncommon node information with sparse group lasso, as reflected in reduced root mean squared error (RMSE) and improved Pearson's correlation coefficient (PCC). Applying regularizations also enhances feature attribution for GNN by boosting graph-level global direction scores and improving atom-level coloring accuracy. These advances strengthen model interpretability in drug discovery pipelines, particularly for identifying critical molecular substructures in lead optimization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enjoying Non-linearity in Multinomial Logistic Bandits</title>
<link>https://arxiv.org/abs/2507.05306</link>
<guid>https://arxiv.org/abs/2507.05306</guid>
<content:encoded><![CDATA[
arXiv:2507.05306v2 Announce Type: replace-cross 
Abstract: We consider the multinomial logistic bandit problem, a variant of where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\kappa_* \geq 1$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of \( \kappa_* \) to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( R d \sqrt{{KT}/{\kappa_*}})} $, where $R$ is the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )} $. Moreover, we provide a $\smash{ \Omega(Rd\sqrt{KT/\kappa_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $\kappa_*$ is optimal.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-based Audio Inpainting via Discrete Diffusion</title>
<link>https://arxiv.org/abs/2507.08333</link>
<guid>https://arxiv.org/abs/2507.08333</guid>
<content:encoded><![CDATA[
arXiv:2507.08333v3 Announce Type: replace-cross 
Abstract: Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training. Audio examples of our proposed method can be found at https://iftach21.github.io/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations</title>
<link>https://arxiv.org/abs/2507.12117</link>
<guid>https://arxiv.org/abs/2507.12117</guid>
<content:encoded><![CDATA[
arXiv:2507.12117v2 Announce Type: replace-cross 
Abstract: Quantum machine learning (QML) seeks to exploit the intrinsic properties of quantum mechanical systems, including superposition, coherence, and quantum entanglement for classical data processing. However, due to the exponential growth of the Hilbert space, QML faces practical limits in classical simulations with the state-vector representation of quantum system. On the other hand, phase-space methods offer an alternative by encoding quantum states as quasi-probability functions. Building on prior work in qubit phase-space and the Stratonovich-Weyl (SW) correspondence, we construct a closed, composable dynamical formalism for one- and many-qubit systems in phase-space. This formalism replaces the operator algebra of the Pauli group with function dynamics on symplectic manifolds, and recasts the curse of dimensionality in terms of harmonic support on a domain that scales linearly with the number of qubits. It opens a new route for QML based on variational modelling over phase-space.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.18562</link>
<guid>https://arxiv.org/abs/2507.18562</guid>
<content:encoded><![CDATA[
arXiv:2507.18562v2 Announce Type: replace-cross 
Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation</title>
<link>https://arxiv.org/abs/2508.01861</link>
<guid>https://arxiv.org/abs/2508.01861</guid>
<content:encoded><![CDATA[
arXiv:2508.01861v3 Announce Type: replace-cross 
Abstract: Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with an asset-pricing pipeline tailored for tensor-structured financial data. Results show that ACT-Tensor not only reduces pricing errors but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Clique Discovery via Energy Diffusion</title>
<link>https://arxiv.org/abs/2508.04174</link>
<guid>https://arxiv.org/abs/2508.04174</guid>
<content:encoded><![CDATA[
arXiv:2508.04174v2 Announce Type: replace-cross 
Abstract: Discovering quasi-cliques -- subgraphs whose edge density exceeds a given threshold -- is a fundamental task in graph mining with applications to web spam detection, fraud screening, and e-commerce recommendation. However, existing methods for quasi-clique discovery on large-scale web graphs are often sensitive to random seeds or lack of explicit edge-density guarantees, making the task challenging in practice. This paper presents EDQC, an energy diffusion-based method for quasi-clique discovery. EDQC first employs an adaptive energy diffusion process to generate an energy ranking that highlights structurally cohesive regions. Guided by this energy ranking, the algorithm identifies a high-quality subgraph by minimizing conductance, a standard measure from community detection. This subgraph is then refined to meet the specified density threshold. Extensive experiments on 75 real-world graphs show that EDQC finds larger quasi-cliques on most datasets, with consistently lower variance across runs and competitive runtime. To the best of our knowledge, EDQC is the first method to incorporate energy diffusion into quasi-clique discovery.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Valid Inference with Imperfect Synthetic Data</title>
<link>https://arxiv.org/abs/2508.06635</link>
<guid>https://arxiv.org/abs/2508.06635</guid>
<content:encoded><![CDATA[
arXiv:2508.06635v2 Announce Type: replace-cross 
Abstract: Predictions and generations from large language models are increasingly being explored as an aid in limited data regimes, such as in computational social science and human subjects research. While prior technical work has mainly explored the potential to use model-predicted labels for unlabeled data in a principled manner, there is increasing interest in using large language models to generate entirely new synthetic samples (e.g., synthetic simulations), such as in responses to surveys. However, it remains unclear by what means practitioners can combine such data with real data and yet produce statistically valid conclusions upon them. In this paper, we introduce a new estimator based on generalized method of moments, providing a hyperparameter-free solution with strong theoretical guarantees to address this challenge. Intriguingly, we find that interactions between the moment residuals of synthetic data and those of real data (i.e., when they are predictive of each other) can greatly improve estimates of the target parameter. We validate the finite-sample performance of our estimator across different tasks in computational social science applications, demonstrating large empirical gains.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 51.5% on the originals but drops by 4.7 percentage points on surface-renaming variants, and by 12.9 percentage points on parametric variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Robot Control via Structured Behavior Trees and Large Language Models</title>
<link>https://arxiv.org/abs/2508.09621</link>
<guid>https://arxiv.org/abs/2508.09621</guid>
<content:encoded><![CDATA[
arXiv:2508.09621v2 Announce Type: replace-cross 
Abstract: As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2508.11711</link>
<guid>https://arxiv.org/abs/2508.11711</guid>
<content:encoded><![CDATA[
arXiv:2508.11711v2 Announce Type: replace-cross 
Abstract: GraphQL's flexibility, while beneficial for efficient data fetching, introduces unique security vulnerabilities that traditional API security mechanisms often fail to address. Malicious GraphQL queries can exploit the language's dynamic nature, leading to denial-of-service attacks, data exfiltration through injection, and other exploits. Existing solutions, such as static analysis, rate limiting, and general-purpose Web Application Firewalls, offer limited protection against sophisticated, context-aware attacks. This paper presents a novel, AI-driven approach for real-time detection of malicious GraphQL queries. Our method combines static analysis with machine learning techniques, including Large Language Models (LLMs) for dynamic schema-based configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual embedding of query payloads, and Convolutional Neural Networks (CNNs), Random Forests, and Multilayer Perceptrons for classification. We detail the system architecture, implementation strategies optimized for production environments (including ONNX Runtime optimization and parallel processing), and evaluate the performance of our detection models and the overall system under load. Results demonstrate high accuracy in detecting various threats, including SQL injection, OS command injection, and XSS exploits, alongside effective mitigation of DoS and SSRF attempts. This research contributes a robust and adaptable solution for enhancing GraphQL API security.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Task Vectors and Gradients</title>
<link>https://arxiv.org/abs/2508.16082</link>
<guid>https://arxiv.org/abs/2508.16082</guid>
<content:encoded><![CDATA[
arXiv:2508.16082v4 Announce Type: replace-cross 
Abstract: Task arithmetic has emerged as a simple yet powerful technique for model merging, enabling the combination of multiple finetuned models into one. Despite its empirical success, a clear theoretical explanation of why and when it works is lacking. This paper provides a rigorous theoretical foundation for task arithmetic by establishing a connection between task vectors and gradients of the task losses. We show that under standard gradient descent, a task vector generated from one epoch of finetuning is exactly equivalent to the negative gradient of the loss, scaled by the learning rate. For the practical multi-epoch setting, we prove that this equivalence holds approximately, with a second-order error term that we explicitly bound for feed-forward networks. Our empirical analysis across seven vision benchmarks corroborates our theory, demonstrating that the first-epoch gradient dominates the finetuning trajectory in both norm and direction. A key implication is that merging models finetuned for only a single epoch often yields performance comparable to merging fully converged models. These findings reframe task arithmetic as a form of approximate multitask learning, providing a clear rationale for its effectiveness and highlighting the critical role of early training dynamics in model merging.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2508.17671</link>
<guid>https://arxiv.org/abs/2508.17671</guid>
<content:encoded><![CDATA[
arXiv:2508.17671v3 Announce Type: replace-cross 
Abstract: The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[
arXiv:2508.18665v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.19366</link>
<guid>https://arxiv.org/abs/2508.19366</guid>
<content:encoded><![CDATA[
arXiv:2508.19366v3 Announce Type: replace-cross 
Abstract: Hallucinations in LLMs--especially in multimodal settings--undermine reliability. We present a rigorous, information-geometric framework in diffusion dynamics that quantifies hallucination in MLLMs: model outputs are embedded spectrally on multimodal graph Laplacians, and gaps to a truth manifold define a semantic-distortion metric. We derive Courant--Fischer bounds on a temperature-dependent hallucination energy and use RKHS eigenmodes to obtain modality-aware, interpretable measures that track evolution over prompts and time. This reframes hallucination as measurable and bounded, providing a principled basis for evaluation and mitigation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore</title>
<link>https://arxiv.org/abs/2509.01845</link>
<guid>https://arxiv.org/abs/2509.01845</guid>
<content:encoded><![CDATA[
arXiv:2509.01845v2 Announce Type: replace-cross 
Abstract: This paper presents an overview of a human-centered initiative aimed at strengthening climate resilience along Nova Scotia's Eastern Shore. This region, a collection of rural villages with deep ties to the sea, faces existential threats from climate change that endanger its way of life. Our project moves beyond a purely technical response, weaving together expertise from Computer Science, Industrial Engineering, and Coastal Geography to co-create tools with the community. By integrating generational knowledge of residents, particularly elders, through the Eastern Shore Citizen Science Coastal Monitoring Network, this project aims to collaborate in building a living digital archive. This effort is hosted under Dalhousie University's Transforming Climate Action (TCA) initiative, specifically through its Transformative Adaptations to Social-Ecological Climate Change Trajectories (TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is driven by a collaboration model in which student teams work directly with residents. We present a detailed project timeline and a replicable model for how technology can support traditional communities, enabling them to navigate climate transformation more effectively.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models</title>
<link>https://arxiv.org/abs/2509.03122</link>
<guid>https://arxiv.org/abs/2509.03122</guid>
<content:encoded><![CDATA[
arXiv:2509.03122v2 Announce Type: replace-cross 
Abstract: Fingerprinting is critical for maintaining traceability and protecting the intellectual property (IP) of developers, as LLMs deployed in web applications are susceptible to unauthorized redistribution and misuse via fine-tuning or black-box deployment. However, current backdoor-based fingerprinting methods face a fundamental trade-off: fingerprints embedded as garbled text are easily detected and filtered, whereas those crafted as coherent natural language are prone to being triggered unintentionally. To overcome these limitations, we propose RFEdit, a knowledge-editing framework that embeds a rule-based multilingual natural language fingerprint (MNLF) by modifying a sparse subset of model weights. This approach enables efficient and robust fingerprint injection with minimal impact on unrelated knowledge in LLMs. Our RFEdit framework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning (FSFT), which mitigates fingerprint degradation during legitimate fine-tuning by restricting parameter updates to the fingerprint subspace. This approach preserves fingerprint integrity while enhancing downstream task performance of LLMs. These advances establish a comprehensive pipeline from fingerprint injection to defense, achieving high detection effectiveness, robustness against adversarial manipulations, harmlessness to model utility, and persistence under fine-tuning. Extensive experiments demonstrate that RFEdit maintains robustness under quantization and pruning. Additionally, fingerprint effectiveness is generally improved by more than 10\% when combined with FSFT for math and alpaca downstream tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title>
<link>https://arxiv.org/abs/2509.03487</link>
<guid>https://arxiv.org/abs/2509.03487</guid>
<content:encoded><![CDATA[
arXiv:2509.03487v2 Announce Type: replace-cross 
Abstract: Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.03540</link>
<guid>https://arxiv.org/abs/2509.03540</guid>
<content:encoded><![CDATA[
arXiv:2509.03540v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) paradigms mitigate this issue by incorporating external knowledge at inference time. However, such methods typically handle knowledge as unstructured text, which reduces retrieval accuracy, hinders compositional reasoning, and amplifies the influence of irrelevant information on the factual consistency of LLM outputs. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external knowledge retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's internal knowledge. The KG is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse Factual QA benchmarks, demonstrating consistent gains in factual accuracy over baselines. Our findings reveal that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Bayesian Framework for Stochastic Optimization</title>
<link>https://arxiv.org/abs/2509.07030</link>
<guid>https://arxiv.org/abs/2509.07030</guid>
<content:encoded><![CDATA[
arXiv:2509.07030v2 Announce Type: replace-cross 
Abstract: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI</title>
<link>https://arxiv.org/abs/2509.12658</link>
<guid>https://arxiv.org/abs/2509.12658</guid>
<content:encoded><![CDATA[
arXiv:2509.12658v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a sustainable long short-term memory (LSTM)-based precoding framework for reconfigurable intelligent surface (RIS)-assisted millimeter-wave (mmWave) MIMO systems. Instead of explicit channel state information (CSI) estimation, the framework exploits uplink pilot sequences to implicitly learn channel characteristics, reducing both pilot overhead and inference complexity. Practical hardware constraints are addressed by incorporating the phase-dependent amplitude model of RIS elements, while a multi-label training strategy improves robustness when multiple near-optimal codewords yield comparable performance. Simulations show that the proposed design achieves over 90% of the spectral efficiency of exhaustive search (ES) with only 2.2% of its computation time, cutting energy consumption by nearly two orders of magnitude. The method also demonstrates resilience under distribution mismatch and scalability to larger RIS arrays, making it a practical and energy-efficient solution for sustainable 6G wireless networks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Healthcare Imaging Platform: A VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation</title>
<link>https://arxiv.org/abs/2509.13590</link>
<guid>https://arxiv.org/abs/2509.13590</guid>
<content:encoded><![CDATA[
arXiv:2509.13590v2 Announce Type: replace-cross 
Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging has revolutionized diagnostic medicine and clinical decision-making processes. This work presents an intelligent multimodal framework for medical image analysis that leverages Vision-Language Models (VLMs) in healthcare diagnostics. The framework integrates Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across multiple imaging modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual feature extraction with natural language processing to enable contextual image interpretation, incorporating coordinate verification mechanisms and probabilistic Gaussian modeling for anomaly distribution. Multi-layered visualization techniques generate detailed medical illustrations, overlay comparisons, and statistical representations to enhance clinical confidence, with location measurement achieving 80 pixels average deviation. Result processing utilizes precise prompt engineering and textual analysis to extract structured clinical information while maintaining interpretability. Experimental evaluations demonstrated high performance in anomaly detection across multiple modalities. The system features a user-friendly Gradio interface for clinical workflow integration and demonstrates zero-shot learning capabilities to reduce dependence on large datasets. This framework represents a significant advancement in automated diagnostic support and radiological workflow efficiency, though clinical validation and multi-center evaluation are necessary prior to widespread adoption.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextMine: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action</title>
<link>https://arxiv.org/abs/2509.15098</link>
<guid>https://arxiv.org/abs/2509.15098</guid>
<content:encoded><![CDATA[
arXiv:2509.15098v2 Announce Type: replace-cross 
Abstract: Humanitarian Mine Action (HMA) addresses the challenge of detecting and removing landmines from conflict regions. Much of the life-saving operational knowledge produced by HMA agencies is buried in unstructured reports, limiting the transferability of information between agencies. To address this issue, we propose TextMine: the first dataset, evaluation framework and ontology-guided large language model (LLM) pipeline for knowledge extraction in the HMA domain. TextMine structures HMA reports into (subject, relation, object)-triples, thus creating domain-specific knowledge. To ensure real-world relevance, we created the dataset in collaboration with Cambodian Mine Action Center (CMAC). We further introduce a bias-aware evaluation framework that combines human-annotated triples with an LLM-as-Judge protocol to mitigate position bias in reference-free scoring. Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. We publicly release the dataset and code.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models</title>
<link>https://arxiv.org/abs/2509.15174</link>
<guid>https://arxiv.org/abs/2509.15174</guid>
<content:encoded><![CDATA[
arXiv:2509.15174v2 Announce Type: replace-cross 
Abstract: WARNING: This paper contains examples of offensive materials. To address the proliferation of toxic content on social media, we introduce SMARTER, we introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search</title>
<link>https://arxiv.org/abs/2509.15927</link>
<guid>https://arxiv.org/abs/2509.15927</guid>
<content:encoded><![CDATA[
arXiv:2509.15927v3 Announce Type: replace-cross 
Abstract: Auto-bidding serves as a critical tool for advertisers to improve their advertising performance. Recent progress has demonstrated that AI-Generated Bidding (AIGB), which learns a conditional generative planner from offline data, achieves superior performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still face a performance bottleneck due to their inherent inability to explore beyond the static offline dataset. To address this, we propose {AIGB-Pearl} (\emph{{P}lanning with {E}valu{A}tor via RL}), a novel method that integrates generative planning and policy optimization. The core of AIGB-Pearl lies in constructing a trajectory evaluator for scoring generation quality and designing a provably sound KL-Lipschitz-constrained score maximization scheme to ensure safe and efficient exploration beyond the offline dataset. A practical algorithm incorporating the synchronous coupling technique is further devised to ensure the model regularity required by the proposed scheme. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology</title>
<link>https://arxiv.org/abs/2509.16765</link>
<guid>https://arxiv.org/abs/2509.16765</guid>
<content:encoded><![CDATA[
arXiv:2509.16765v2 Announce Type: replace-cross 
Abstract: According to the U.S. National Institutes of Health, more than 3.4 million children experience speech disorders that require clinical intervention. The number of speech-language pathologists (SLPs) is roughly 20 times fewer than the number of affected children, highlighting a significant gap in children's care and a pressing need for technological support that improves the productivity of SLPs. State-of-the-art multimodal language models (MLMs) show promise for supporting SLPs, but their use remains underexplored largely due to a limited understanding of their performance in high-stakes clinical settings. To address this gap, we collaborate with domain experts to develop a taxonomy of real-world use cases of MLMs in speech-language pathologies. Building on this taxonomy, we introduce the first comprehensive benchmark for evaluating MLM across five core use cases, each containing 1,000 manually annotated data points. This benchmark includes robustness and sensitivity tests under various settings, including background noise, speaker gender, and accent. Our evaluation of 15 state-of-the-art MLMs reveals that no single model consistently outperforms others across all tasks. Notably, we find systematic disparities, with models performing better on male speakers, and observe that chain-of-thought prompting can degrade performance on classification tasks with large label spaces and narrow decision boundaries. Furthermore, we study fine-tuning MLMs on domain-specific data, achieving improvements of over 10\% compared to base models. These findings highlight both the potential and limitations of current MLMs for speech-language pathology applications, underscoring the need for further research and targeted development.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRPO is Secretly a Process Reward Model</title>
<link>https://arxiv.org/abs/2509.21154</link>
<guid>https://arxiv.org/abs/2509.21154</guid>
<content:encoded><![CDATA[
arXiv:2509.21154v2 Announce Type: replace-cross 
Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness</title>
<link>https://arxiv.org/abs/2509.23206</link>
<guid>https://arxiv.org/abs/2509.23206</guid>
<content:encoded><![CDATA[
arXiv:2509.23206v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning</title>
<link>https://arxiv.org/abs/2509.24031</link>
<guid>https://arxiv.org/abs/2509.24031</guid>
<content:encoded><![CDATA[
arXiv:2509.24031v2 Announce Type: replace-cross 
Abstract: Foundation models have driven remarkable progress in text, vision, and video understanding, and are now poised to unlock similar breakthroughs in trajectory modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a foundation model for large-scale mobility data that captures patterns of normalcy in human movement. Unlike prior approaches that flatten trajectories into coordinate streams, GPS-MTM decomposes mobility into two complementary modalities: states (point-of-interest categories) and actions (agent transitions). Leveraging a bi-directional Transformer with a self-supervised masked modeling objective, the model reconstructs missing segments across modalities, enabling it to learn rich semantic correlations without manual labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and Geolife, GPS-MTM consistently outperforms on downstream tasks such as trajectory infilling and next-stop prediction. Its advantages are most pronounced in dynamic tasks (inverse and forward dynamics), where contextual reasoning is critical. These results establish GPS-MTM as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning. Code is released for further reference.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization</title>
<link>https://arxiv.org/abs/2509.26281</link>
<guid>https://arxiv.org/abs/2509.26281</guid>
<content:encoded><![CDATA[
arXiv:2509.26281v2 Announce Type: replace-cross 
Abstract: Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiral of Silence in Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.02360</link>
<guid>https://arxiv.org/abs/2510.02360</guid>
<content:encoded><![CDATA[
arXiv:2510.02360v2 Announce Type: replace-cross 
Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reasoning Models: Perspectives and Misconceptions</title>
<link>https://arxiv.org/abs/2510.00355</link>
<guid>https://arxiv.org/abs/2510.00355</guid>
<content:encoded><![CDATA[
<div> Transformers, Logical reasoning, Latent space, Recurrent reasoning, Hierarchical Reasoning Model <br />
<br />
Summary: 
Transformers excel in natural language processing tasks but face challenges in logical reasoning. The Hierarchical Reasoning Model introduces recurrent reasoning in transformer latent space, showing promising results in 2D reasoning tasks. This model is still in early stages and requires further exploration. This work reviews the Hierarchical Reasoning Model, analyzes design choices, explores alternative variants, and addresses common misconceptions. It sheds light on the potential of leveraging latent space and recurrent reasoning to enhance transformer performance in logical reasoning tasks. More research is needed to fully understand the capabilities and limitations of this approach and to uncover its full potential in advancing logical reasoning capabilities of transformer models. <div>
arXiv:2510.00355v2 Announce Type: replace 
Abstract: Transformers have demonstrated remarkable performance in natural language processing and related domains, as they largely focus on sequential, autoregressive next-token prediction tasks. Yet, they struggle in logical reasoning, not necessarily because of a fundamental limitation of these models, but possibly due to the lack of exploration of more creative uses, such as latent space and recurrent reasoning. An emerging exploration in this direction is the Hierarchical Reasoning Model (Wang et. al., 2025), which introduces a novel type of recurrent reasoning in the latent space of transformers, achieving remarkable performance on a wide range of 2D reasoning tasks. Despite the promising results, this line of models is still at an early stage and calls for in-depth investigation. In this work, we review this class of models, examine key design choices, test alternative variants and clarify common misconceptions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</title>
<link>https://arxiv.org/abs/2510.02125</link>
<guid>https://arxiv.org/abs/2510.02125</guid>
<content:encoded><![CDATA[
<div> Keywords: OpenAI, abstraction abilities, ConceptARC, reasoning models, multimodal models

Summary: 
The study evaluates the abstraction abilities of AI models, particularly on the ConceptARC benchmark, which aims to test abstract reasoning skills. The research examines different settings, including input modalities (textual vs. visual) and reasoning efforts, to understand how well models capture intended abstractions. While some textual models match human accuracy, their rules often rely on surface-level "shortcuts" rather than true abstraction. In the visual modality, AI models struggle with accuracy but still exhibit some ability to capture intended abstractions in their rules. The findings suggest that models currently lag behind humans in abstract reasoning, and assessing solely based on accuracy may overestimate textual performance and underestimate visual performance. The authors propose that their evaluation framework provides a more accurate assessment of multimodal models' abstract reasoning abilities, offering insights for advancing towards human-like abstraction-centered intelligence. 

<br /><br />Summary: <div>
arXiv:2510.02125v3 Announce Type: replace 
Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate models' abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best models' rules are often based on surface-level ``shortcuts'' and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI models' output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes toward Intelligent Vehicle Suspension Systems</title>
<link>https://arxiv.org/abs/2508.01339</link>
<guid>https://arxiv.org/abs/2508.01339</guid>
<content:encoded><![CDATA[
<div> speed bumps, potholes, embedded systems, detection framework, suspension control<br />
<br />
Summary:
SBP-YOLO is a novel detection framework designed for embedded systems to efficiently detect speed bumps and potholes on roads. By integrating GhostConv and VoVGSCSPC modules, the framework enhances multi-scale semantic features while reducing computation. The P2-level branch improves small-object detection, and a lightweight detection head ensures accuracy with minimal overhead. A hybrid training strategy combining NWD loss, BCKD knowledge distillation, and Albumentations-based augmentation enhances robustness under various road conditions. Experimental results show that SBP-YOLO achieves an mAP of 87.0%, outperforming the YOLOv11n baseline by 5.8%. After TensorRT FP16 quantization, the framework runs at 139.5 FPS on Jetson AGX Xavier, demonstrating its suitability for fast and low-latency road condition perception in embedded suspension control systems. <div>
arXiv:2508.01339v4 Announce Type: replace-cross 
Abstract: Speed bumps and potholes are the most common road anomalies, significantly affecting ride comfort and vehicle stability. Preview-based suspension control mitigates their impact by detecting such irregularities in advance and adjusting suspension parameters proactively. Accurate and real-time detection is essential, but embedded deployment is constrained by limited computational resources and the small size of targets in input images.To address these challenges, this paper proposes SBP-YOLO, an efficient detection framework for speed bumps and potholes in embedded systems. Built upon YOLOv11n, it integrates GhostConv and VoVGSCSPC modules in the backbone and neck to reduce computation while enhancing multi-scale semantic features. A P2-level branch improves small-object detection, and a lightweight and efficient detection head (LEDH) maintains accuracy with minimal overhead. A hybrid training strategy further enhances robustness under varying road and environmental conditions, combining NWD loss, BCKD knowledge distillation, and Albumentations-based augmentation. Experiments show that SBP-YOLO achieves 87.0% mAP, outperforming the YOLOv11n baseline by 5.8%. After TensorRT FP16 quantization, it runs at 139.5 FPS on Jetson AGX Xavier, yielding a 12.4% speedup over the P2-enhanced YOLOv11. These results demonstrate the framework's suitability for fast, low-latency road condition perception in embedded suspension control systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling</title>
<link>https://arxiv.org/abs/2510.02414</link>
<guid>https://arxiv.org/abs/2510.02414</guid>
<content:encoded><![CDATA[
<div> structure-aware, reconstruction, rainfall, radar reflectivity, spatial interpolation 

Summary:<br />
The article introduces RainSeer, a framework for reconstructing high-resolution rainfall fields by reinterpreting radar reflectivity to capture the development of rain. Two fundamental challenges are addressed: translating radar fields into point-wise rainfall observations and bridging the disconnect between aloft hydro-meteors and ground-level precipitation. RainSeer consists of a Structure-to-Point Mapper for spatial alignment and a Geo-Aware Rain Decoder for capturing the transformation of hydro-meteors. Evaluation on two datasets shows consistent improvements over baselines, reducing MAE by 13.31% and enhancing structural fidelity in reconstructed rainfall fields. RainSeer offers a promising approach for flood forecasting, hydrological modeling, and climate analysis. 

Summary: <div>
arXiv:2510.02414v2 Announce Type: replace-cross 
Abstract: Reconstructing high-resolution rainfall fields is essential for flood forecasting, hydrological modeling, and climate analysis. However, existing spatial interpolation methods-whether based on automatic weather station (AWS) measurements or enhanced with satellite/radar observations often over-smooth critical structures, failing to capture sharp transitions and localized extremes. We introduce RainSeer, a structure-aware reconstruction framework that reinterprets radar reflectivity as a physically grounded structural prior-capturing when, where, and how rain develops. This shift, however, introduces two fundamental challenges: (i) translating high-resolution volumetric radar fields into sparse point-wise rainfall observations, and (ii) bridging the physical disconnect between aloft hydro-meteors and ground-level precipitation. RainSeer addresses these through a physics-informed two-stage architecture: a Structure-to-Point Mapper performs spatial alignment by projecting mesoscale radar structures into localized ground-level rainfall, through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the semantic transformation of hydro-meteors through descent, melting, and evaporation via a causal spatiotemporal attention mechanism. We evaluate RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France, 2016-2018)-and observe consistent improvements over state-of-the-art baselines, reducing MAE by over 13.31% and significantly enhancing structural fidelity in reconstructed rainfall fields.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilled Protein Backbone Generation</title>
<link>https://arxiv.org/abs/2510.03095</link>
<guid>https://arxiv.org/abs/2510.03095</guid>
<content:encoded><![CDATA[
<div> Generative models for protein backbone design have advanced but face a speed bottleneck in sampling. This study explores score distillation techniques to train faster few-step protein backbone generators without compromising quality. By adapting Score identity Distillation (SiD) and incorporating multistep generation with noise modulation during inference, the distilled models achieve a more than 20-fold improvement in sampling speed compared to the Proteina teacher model. These few-step generators maintain high designability, diversity, and novelty, making them suitable for large-scale protein design tasks. The increased inference efficiency of these models brings diffusion-based generative models closer to practical applications in protein engineering.<br /><br />Keywords: generative models, protein backbone design, score distillation, few-step generators, protein engineering<br /><br />Summary: Generative models for protein backbone design have been enhanced with the development of faster few-step protein backbone generators using score distillation techniques like SiD. By incorporating multistep generation and noise modulation during inference, these models significantly improve sampling speed while maintaining high designability and novelty. This advancement enables large-scale protein design tasks and enhances the practical applicability of diffusion-based generative models in protein engineering. <div>
arXiv:2510.03095v2 Announce Type: replace-cross 
Abstract: Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2510.05106</link>
<guid>https://arxiv.org/abs/2510.05106</guid>
<content:encoded><![CDATA[
<div> Keywords: safety-critical agents, large language models, attention mechanisms, compliance behavior, rule verification

Summary: 
The paper analyzes how rule encodings in system prompts impact attention mechanisms and compliance behavior in safety-critical agents based on large language models (LLMs). It reveals that rule formats with low syntactic entropy and concentrated anchors lead to reduced attention entropy and improved pointer fidelity, highlighting a trade-off between anchor redundancy and attention entropy. Various attention architectures like causal, bidirectional, and cross-attention mechanisms are examined, with bounds established on pointer fidelity. The study emphasizes the importance of principled anchor design and dual enforcement mechanisms to safeguard LLM-based agents from prompt injection attacks while ensuring compliance in evolving domains. Additionally, a dynamic rule verification architecture is proposed, proving that hot reloading of verified rule sets increases the probability of compliant outputs over time.<br /><br />Summary: <div>
arXiv:2510.05106v1 Announce Type: new 
Abstract: The design of safety-critical agents based on large language models (LLMs) requires more than simple prompt engineering. This paper presents a comprehensive information-theoretic analysis of how rule encodings in system prompts influence attention mechanisms and compliance behaviour. We demonstrate that rule formats with low syntactic entropy and highly concentrated anchors reduce attention entropy and improve pointer fidelity, but reveal a fundamental trade-off between anchor redundancy and attention entropy that previous work failed to recognize. Through formal analysis of multiple attention architectures including causal, bidirectional, local sparse, kernelized, and cross-attention mechanisms, we establish bounds on pointer fidelity and show how anchor placement strategies must account for competing fidelity and entropy objectives. Combining these insights with a dynamic rule verification architecture, we provide a formal proof that hot reloading of verified rule sets increases the asymptotic probability of compliant outputs. These findings underscore the necessity of principled anchor design and dual enforcement mechanisms to protect LLM-based agents against prompt injection attacks while maintaining compliance in evolving domains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study</title>
<link>https://arxiv.org/abs/2510.05107</link>
<guid>https://arxiv.org/abs/2510.05107</guid>
<content:encoded><![CDATA[
<div> architecture, natural language understanding, cognitive loop, external memory, goal-directed

Summary:
The Structured Cognitive Loop (SCL) proposes a new architecture for large language models to improve coherence and predictability in multi-step tasks. SCL separates inference, memory, and control functions, offloading cognitive load from the model. Evaluations against prompt-based baselines in travel planning, email drafting, and image generation show modest but consistent improvements in task success, goal fidelity, and reliability of intermediate states. External memory and control each contribute independently to the performance enhancements. The results suggest that architectural separation can enhance traceability and reliability without the need for larger models or heavier prompts. Further studies are recommended to explore the potential of SCL in more complex tasks and collaborative settings.<br /><br />Summary: <div>
arXiv:2510.05107v1 Announce Type: new 
Abstract: Large language models have advanced natural language understanding and generation, yet their use as autonomous agents raises architectural challenges for multi-step tasks. Existing frameworks often intertwine inference, memory, and control in a single prompt, which can reduce coherence and predictability. The Structured Cognitive Loop (SCL) is introduced as an alternative architecture that separates these functions. In SCL, the language model is dedicated to inference, memory is maintained externally, and execution is guided by a lightweight controller within a goal-directed loop. This design offloads cognitive load from the model and allows intermediate results to be stored, revisited, and checked before actions are taken, providing a clearer basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common LangChain agents across three scenarios: temperature-based travel planning, email drafting with conditional send, and constraint-guided image generation. All systems share the same base model and tools under matched decoding settings. Across 360 episodes, SCL shows modest but consistent improvements. Task success averages 86.3 percent compared with 70-77 percent for baselines. Goal fidelity is higher, redundant calls are fewer, intermediate states are reused more reliably, and unsupported assertions per 100 tool calls are reduced. Ablations show that external memory and control each contribute independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability and traceability without relying on larger models or heavier prompts. The findings are preliminary and intended to guide extended studies with additional models, longer horizons, multimodal tasks, and collaborative settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Modeling via Semantic Anchored Alignment</title>
<link>https://arxiv.org/abs/2510.05115</link>
<guid>https://arxiv.org/abs/2510.05115</guid>
<content:encoded><![CDATA[
<div> framework, optimization modeling, semantic anchors, correction, Large language models <br />
<br />
Keywords: framework, optimization modeling, semantic anchors, correction, Large language models <br />
<br />
Summary: 
The article introduces SAC-Opt, a backward-guided correction framework for optimization modeling using Large language models (LLMs). SAC-Opt focuses on aligning semantic anchors and selectively correcting mismatched components in generated code to improve model fidelity and robustness. It addresses the challenge of semantic errors in solver-driven approaches by refining constraint and objective logic based on problem semantics. Empirical results on multiple datasets show that SAC-Opt enhances modeling accuracy by an average of 7.8%, with significant gains on specific datasets like ComplexLP. This approach highlights the importance of semantic-anchored correction in LLM-based optimization workflows to ensure accurate translation of problem intent into solver-executable code. <br /> <div>
arXiv:2510.05115v1 Announce Type: new 
Abstract: Large language models (LLMs) have opened new paradigms in optimization modeling by enabling the generation of executable solver code from natural language descriptions. Despite this promise, existing approaches typically remain solver-driven: they rely on single-pass forward generation and apply limited post-hoc fixes based on solver error messages, leaving undetected semantic errors that silently produce syntactically correct but logically flawed models. To address this challenge, we propose SAC-Opt, a backward-guided correction framework that grounds optimization modeling in problem semantics rather than solver feedback. At each step, SAC-Opt aligns the original semantic anchors with those reconstructed from the generated code and selectively corrects only the mismatched components, driving convergence toward a semantically faithful model. This anchor-driven correction enables fine-grained refinement of constraint and objective logic, enhancing both fidelity and robustness without requiring additional training or supervision. Empirical results on seven public datasets demonstrate that SAC-Opt improves average modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP dataset. These findings highlight the importance of semantic-anchored correction in LLM-based optimization workflows to ensure faithful translation from problem intent to solver-executable code.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring Reasoning for Complex Rules Beyond Flat Representations</title>
<link>https://arxiv.org/abs/2510.05134</link>
<guid>https://arxiv.org/abs/2510.05134</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Dynamic Adjudication Template (DAT), rule-based tasks, inference mechanism, qualitative analysis

Summary: 
The article discusses the challenges faced by large language models (LLMs) in processing complex rule systems and introduces a novel framework called Dynamic Adjudication Template (DAT) to address these challenges. DAT structures the inference process into three stages: qualitative analysis, evidence gathering, and adjudication. It allows for more systematic processing of structured rules, leading to improved performance in rule-based tasks. Empirical results show that DAT outperforms conventional Chain-of-Thought (CoT) approaches, enabling smaller language models to achieve comparable or better performance than larger LLMs. This highlights the efficiency and effectiveness of DAT in managing intricate rule systems.<br /><br />Summary: <div>
arXiv:2510.05134v1 Announce Type: new 
Abstract: Large language models (LLMs) face significant challenges when processing complex rule systems, as they typically treat interdependent rules as unstructured textual data rather than as logically organized frameworks. This limitation results in reasoning divergence, where models often overlook critical rule dependencies essential for accurate interpretation. Although existing approaches such as Chain-of-Thought (CoT) reasoning have shown promise, they lack systematic methodologies for structured rule processing and are particularly susceptible to error propagation through sequential reasoning chains. To address these limitations, we propose the Dynamic Adjudication Template (DAT), a novel framework inspired by expert human reasoning processes. DAT structures the inference mechanism into three methodical stages: qualitative analysis, evidence gathering, and adjudication. During the qualitative analysis phase, the model comprehensively evaluates the contextual landscape. The subsequent evidence gathering phase involves the targeted extraction of pertinent information based on predefined template elements ([placeholder]), followed by systematic verification against applicable rules. Finally, in the adjudication phase, the model synthesizes these validated components to formulate a comprehensive judgment. Empirical results demonstrate that DAT consistently outperforms conventional CoT approaches in complex rule-based tasks. Notably, DAT enables smaller language models to match, and in some cases exceed, the performance of significantly larger LLMs, highlighting its efficiency and effectiveness in managing intricate rule systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem</title>
<link>https://arxiv.org/abs/2510.05153</link>
<guid>https://arxiv.org/abs/2510.05153</guid>
<content:encoded><![CDATA[
<div> Symbol Grounding Problem, Algorithmic Information Theory, information compression, G\"odelian perspective, No Free Lunch perspective <br />
Summary: <br />
This paper presents a comprehensive framework for the Symbol Grounding Problem (SGP) within Algorithmic Information Theory (AIT). It establishes that the grounding of meaning is constrained by information-theoretic limits, bridging the G\"odelian and No Free Lunch perspectives. The model defines a symbolic system as a universal Turing machine and grounds meaning as information compression. The argument progresses through four stages: showing that symbolic systems cannot ground most possible worlds due to their randomness, demonstrating the incompleteness of statically grounded systems, proving the non-inferability of the grounding act, and using Chaitin's Incompleteness Theorem to show that algorithmic learning processes cannot model worlds more complex than themselves. This confirms that meaning is an ongoing process of a system trying to surpass its information-theoretic boundaries. <div>
arXiv:2510.05153v1 Announce Type: new 
Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT). We demonstrate that the grounding of meaning is a process fundamentally constrained by information-theoretic limits, thereby unifying the G\"odelian (self-reference) and No Free Lunch (statistical) perspectives. We model a symbolic system as a universal Turing machine and define grounding as an act of information compression. The argument proceeds in four stages. First, we prove that a purely symbolic system cannot ground almost all possible "worlds" (data strings), as they are algorithmically random and thus incompressible. Second, we show that any statically grounded system, specialized for compressing a specific world, is inherently incomplete because an adversarial, incompressible world relative to the system can always be constructed. Third, the "grounding act" of adapting to a new world is proven to be non-inferable, as it requires the input of new information (a shorter program) that cannot be deduced from the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to prove that any algorithmic learning process is itself a finite system that cannot comprehend or model worlds whose complexity provably exceeds its own. This establishes that meaning is the open-ended process of a system perpetually attempting to overcome its own information-theoretic limitations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2510.05158</link>
<guid>https://arxiv.org/abs/2510.05158</guid>
<content:encoded><![CDATA[
<div> Keywords: PINNs, neural networks, partial differential equations, large language models, Lang-PINN

Summary: 
Lang-PINN is introduced as a multi-agent system driven by large language models to automatically construct physics-informed neural networks (PINNs) from natural language descriptions of tasks. It coordinates four agents to parse task descriptions into PDEs, select architectures, generate code, and provide feedback for refinement. Experiments show that Lang-PINN outperforms competitive baselines, reducing mean squared error by up to 3-5 orders of magnitude, improving end-to-end execution success by more than 50%, and reducing time overhead by up to 74%. This innovative approach transforms informal task statements into executable and verifiable PINN code, offering a more efficient and accurate solution for solving PDEs using neural networks. 

<br /><br />Summary: <div>
arXiv:2510.05158v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\%, and reduces time overhead by up to 74\%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Potentials of Foundation Models for Multimodal Alignment: A Survey</title>
<link>https://arxiv.org/abs/2510.05184</link>
<guid>https://arxiv.org/abs/2510.05184</guid>
<content:encoded><![CDATA[
<div> representation potentials, foundation models, alignment, transferability, cross-modal transfer

Summary:
Foundation models, such as those used in vision, language, speech, multimodality, and neuroscience research, have shown high transferability and alignment potential in capturing task-specific information within a single modality. These models exhibit structural regularities and semantic consistencies in their representation spaces, making them suitable for cross-modal transfer and alignment. Key metrics for measuring alignment in foundation models have been identified, and empirical evidence supports their representation potentials. Factors that enhance representation potentials include large-scale pretraining on diverse data and similarity across architectures and modalities. However, several open questions remain, and challenges in leveraging representation potentials for practical applications need to be addressed.<br /><br />Summary: <div>
arXiv:2510.05184v1 Announce Type: new 
Abstract: Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture</title>
<link>https://arxiv.org/abs/2510.05187</link>
<guid>https://arxiv.org/abs/2510.05187</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, agriculture, semantic framework, data collection, real-time knowledge inference 

Summary: 
This paper introduces a real-time semantic framework for IoT applications in agriculture, addressing challenges in data collection and understanding. The framework comprises six layers: perception, semantic annotation, interoperability, transportation, semantic reasoning, and application. Data collected by sensors is processed in the semantic annotation layer, where metadata is added for identification and categorization. Semantic algorithms ensure interoperability and identify synonyms. Data is transmitted using various techniques in the transportation layer. The semantic reasoning layer employs fuzzy logic, Dempster-Shafer theory, and Bayesian networks to infer new knowledge. A Graphical User Interface in the application layer allows users to interact with IoT devices and monitor data. This comprehensive framework enhances IoT data management with a focus on semantic completeness and real-time knowledge inference, making it valuable for advancing IoT applications in agriculture and beyond. 

<br /><br />Summary: <div>
arXiv:2510.05187v1 Announce Type: new 
Abstract: The Internet of Things (IoT) has revolutionized various applications including agriculture, but it still faces challenges in data collection and understanding. This paper proposes a real-time framework with three additional semantic layers to help IoT devices and sensors comprehend data meaning and source. The framework consists of six layers: perception, semantic annotation, interoperability, transportation, semantic reasoning, and application, suitable for dynamic environments. Sensors collect data in the form of voltage, which is then processed by microprocessors or microcontrollers in the semantic annotation and preprocessing layer. Metadata is added to the raw data, including the purpose, ID number, and application. Two semantic algorithms are proposed in the semantic interoperability and ontologies layer: the interoperability semantic algorithm for standardizing file types and the synonym identification algorithm for identifying synonyms. In the transportation layer, raw data and metadata are sent to other IoT devices or cloud computing platforms using techniques like WiFi, Zigbee networks, Bluetooth, and mobile communication networks. A semantic reasoning layer is proposed to infer new knowledge from the existing data, using fuzzy logic, Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI) is proposed in the application layer to help users communicate with and monitor IoT sensors, devices, and new knowledge inferred. This framework provides a robust solution for managing IoT data, ensuring semantic completeness, and enabling real-time knowledge inference. The integration of uncertainty reasoning methods and semantic interoperability techniques makes this framework a valuable tool for advancing IoT applications in general and in agriculture in particular.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents</title>
<link>https://arxiv.org/abs/2510.05188</link>
<guid>https://arxiv.org/abs/2510.05188</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, creative content generation, narrative scripts, revision, Dramaturge

Summary: 
The article introduces Dramaturge, an approach to improving long narrative scripts generated by Language Model Models (LLMs) by addressing global structural issues and local detailed flaws. Dramaturge is powered by hierarchical multiple LLM agents and consists of three stages: Global Review to understand the overall storyline, Scene-level Review to pinpoint detailed flaws, and Hierarchical Coordinated Revision to integrate improvements. The top-down task flow ensures contextual consistency, and a coarse-to-fine iterative process is followed through multiple rounds of revision. Comprehensive experiments show that Dramaturge outperforms baselines in overall script quality and scene details. The approach is plug-and-play and can easily be integrated into existing methods for script improvement.<br /><br />Summary: <div>
arXiv:2510.05188v1 Announce Type: new 
Abstract: Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response</title>
<link>https://arxiv.org/abs/2510.05196</link>
<guid>https://arxiv.org/abs/2510.05196</guid>
<content:encoded><![CDATA[
<div> Keywords: population-level data analysis, public health emergencies, graph-based reasoning, weakly supervised pipeline, interpretable insights

Summary:
Population-level data analysis is crucial for effective decision-making during public health emergencies like the COVID-19 pandemic. Conventional methods struggle with the massive input of semi-structured data. To address this, a novel graph-based reasoning framework is proposed, integrating large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. This approach dynamically models evolving citizen needs into a need-aware graph, allowing for population-specific analyses based on key features. The method aims to provide interpretable insights to inform responsive health policy decision-making. Preliminary experimental results using real-world data show the feasibility of this approach, offering a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings. 

<br /><br />Summary: Population-level data analysis is essential during public health emergencies. The proposed graph-based reasoning framework integrates language models with demographic attributes and public feedback, generating interpretable insights. This approach aims to inform health policy decision-making based on population-specific analyses. Preliminary results demonstrate feasibility, offering a scalable solution for intelligent health monitoring in resource-constrained settings. <div>
arXiv:2510.05196v1 Announce Type: new 
Abstract: Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Prediction of Pass@k Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2510.05197</link>
<guid>https://arxiv.org/abs/2510.05197</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, repeated sampling, capabilities, risks, prediction

Summary:
In the research on frontier AI systems, it is crucial to assess their capabilities and risks accurately. Repeated sampling from these models has been shown to enhance both their abilities and potential risks. However, predicting a model's behavior accurately when scaled to a large number of attempts with a limited sampling budget poses a challenge. Standard methods for fitting these laws have statistical shortcomings that hinder predictive accuracy, especially in scenarios with limited data. To address this issue, a robust estimation framework using a beta-binomial distribution is introduced to improve predictions from limited data. Additionally, a dynamic sampling strategy is proposed to allocate a greater budget to more challenging problems. These innovations enable more reliable prediction of rare risks and capabilities at a reduced computational cost. <div>
arXiv:2510.05197v1 Announce Type: new 
Abstract: Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment</title>
<link>https://arxiv.org/abs/2510.05283</link>
<guid>https://arxiv.org/abs/2510.05283</guid>
<content:encoded><![CDATA[
<div> rewards, multimodal large language models, alignment, reinforcement learning, multimodal benchmarks
Summary:
This study introduces a hybrid reward modeling framework for aligning multimodal large language models (MLLMs) with human preferences. The framework combines model-based rewards, where a learned reward model predicts scores from synthetic and human feedback, with rule-based rewards based on domain-specific heuristics. Multi-aspect rewards are incorporated to enforce instruction adherence, and a length-penalty reward helps stabilize training. Experiments demonstrate consistent improvements across different multimodal benchmarks, with the best model in the 3B family achieving an average overall improvement of ~9.5% in general and math reasoning tasks. Specifically in mathematical benchmarks, the model shows a substantial average improvement of ~16%, highlighting its efficacy in mathematical reasoning and problem-solving tasks.<br /><br />Summary: <div>
arXiv:2510.05283v1 Announce Type: new 
Abstract: Aligning multimodal large language models (MLLMs) with human preferences often relies on single-signal, model-based reward methods. Such monolithic rewards often lack confidence calibration across domain-specific tasks, fail to capture diverse aspects of human preferences, and require extensive data annotation and reward model training. In this work, we propose a hybrid reward modeling framework that integrates complementary reward paradigms: (i) model-based rewards, where a learned reward model predicts scalar or vector scores from synthetic and human feedback, and (ii) rule-based rewards, where domain-specific heuristics provide explicit correctness signals with confidence. Beyond accuracy, we further incorporate multi-aspect rewards to enforce instruction adherence and introduce a generalized length-penalty reward to stabilize training and improve performance. The proposed framework provides a flexible and effective approach to aligning MLLMs through reinforcement learning policy optimization. Our experiments show consistent improvements across different multimodal benchmarks when applying hybrid and multi-aspect reward modeling. Our best performing model in the 3B family achieves an overall average improvement of ~9.5% across general and math reasoning tasks. Focusing specifically on mathematical benchmarks, the model achieves a significant average improvement of ~16%, highlighting its effectiveness in mathematical reasoning and problem solving.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions</title>
<link>https://arxiv.org/abs/2510.05318</link>
<guid>https://arxiv.org/abs/2510.05318</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, multi-turn interactions, text-to-SQL tasks, database assistant

Summary:
BIRD-INTERACT is a new benchmark designed to assess large language models' performance in handling multi-turn interactions for text-to-SQL tasks in database applications. It features a realistic interaction environment with databases, knowledge bases, and user simulators, along with two evaluation settings and a challenging task suite covering CRUD operations. GPT-5's performance highlights the difficulty of the benchmark, completing only a small percentage of tasks. Analysis using memory grafting and Interaction Test-time Scaling shows the importance of effective interaction for complex text-to-SQL tasks.<br /><br />Summary: <div>
arXiv:2510.05318v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis</title>
<link>https://arxiv.org/abs/2510.05335</link>
<guid>https://arxiv.org/abs/2510.05335</guid>
<content:encoded><![CDATA[
<div> Large language models, agent-based reasoning, evidence integration, biomedical domain, cancer research
Summary:
M-Reason is a system for transparent, agent-based reasoning and evidence integration in cancer research. It uses large language models and modular agent orchestration to automate evidence retrieval, appraisal, and synthesis from various biomedical data sources. The system features specialized agents for parallel processing and detailed analysis, prioritizing explainability, structured reporting, and user auditability. It emphasizes traceability from source evidence to final conclusions and incorporates deterministic code for validation. An open interface allows researchers to observe, explore, and evaluate the multi-agent workflow. Evaluation shows increased efficiency and output consistency, highlighting the system's potential for evidence synthesis and as a robust multi-agent LLM testbed for scientific research. The system is accessible at https://m-reason.digitalecmt.com. 
<br /><br />Summary: <div>
arXiv:2510.05335v1 Announce Type: new 
Abstract: We present M-Reason, a demonstration system for transparent, agent-based reasoning and evidence integration in the biomedical domain, with a focus on cancer research. M-Reason leverages recent advances in large language models (LLMs) and modular agent orchestration to automate evidence retrieval, appraisal, and synthesis across diverse biomedical data sources. Each agent specializes in a specific evidence stream, enabling parallel processing and fine-grained analysis. The system emphasizes explainability, structured reporting, and user auditability, providing complete traceability from source evidence to final conclusions. We discuss critical tradeoffs between agent specialization, system complexity, and resource usage, as well as the integration of deterministic code for validation. An open, interactive user interface allows researchers to directly observe, explore and evaluate the multi-agent workflow. Our evaluation demonstrates substantial gains in efficiency and output consistency, highlighting M-Reason's potential as both a practical tool for evidence synthesis and a testbed for robust multi-agent LLM systems in scientific research, available at https://m-reason.digitalecmt.com.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Bayesian methods with neural network--based model predictive control: a review</title>
<link>https://arxiv.org/abs/2510.05338</link>
<guid>https://arxiv.org/abs/2510.05338</guid>
<content:encoded><![CDATA[
<div> Bayesian methods, model predictive control, neural-network-based modeling, control design, uncertainty quantification <br />
<br />
Summary: 
This review evaluates the use of Bayesian techniques in model predictive control (MPC), specifically focusing on neural network-based modeling, control design, and uncertainty quantification. While Bayesian methods are being increasingly utilized to address uncertainty in MPC, there is a lack of standardized benchmarks and transparent reporting in the studies reviewed. The reported improvements in performance and robustness using Bayesian approaches in MPC are inconclusive due to inconsistent baselines and limited reliability analyses. The review highlights the need for standardized benchmarks, ablation studies, and transparent reporting to accurately assess the effectiveness of Bayesian techniques in MPC. <div>
arXiv:2510.05338v1 Announce Type: new 
Abstract: In this review, we assess the use of Bayesian methods in model predictive control (MPC), focusing on neural-network-based modeling, control design, and uncertainty quantification. We systematically analyze individual studies and how they are implemented in practice. While Bayesian approaches are increasingly adopted to capture and propagate uncertainty in MPC, reported gains in performance and robustness remain fragmented, with inconsistent baselines and limited reliability analyses. We therefore argue for standardized benchmarks, ablation studies, and transparent reporting to rigorously determine the effectiveness of Bayesian techniques for MPC.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts</title>
<link>https://arxiv.org/abs/2510.05363</link>
<guid>https://arxiv.org/abs/2510.05363</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation Models, Domain Adaptation, Exemplars, Multi-Head Attention, Efficiency<br />
Summary:
The study addresses the challenge of adapting Foundation Models to new domains with limited training data. It explores the effectiveness of using domain-specific exemplars as in-context demonstrations and considers representing exemplars as soft prompts. The Multi-Head Attention Retrieval-Augmented Generation (MHA-RAG) framework is introduced, leveraging a model architecture that is order invariant to exemplars. Through experiments on various question-answering benchmarks and model scales, MHA-RAG outperforms standard RAG with a 20-point increase in performance while reducing inference costs by a factor of 10X GFLOPs. This framework offers higher accuracy, greater efficiency, and stability, regardless of exemplar order. <div>
arXiv:2510.05363v1 Announce Type: new 
Abstract: Adapting Foundation Models to new domains with limited training data is challenging and computationally expensive. While prior work has demonstrated the effectiveness of using domain-specific exemplars as in-context demonstrations, we investigate whether representing exemplars purely as text is the most efficient, effective, and stable approach. We explore an alternative: representing exemplars as soft prompts with an exemplar order invariant model architecture. To this end, we introduce Multi-Head Attention Retrieval-Augmented Generation (MHA-RAG), a framework with the number of attention heads serving as a simple hyperparameter to control soft prompt-generation across different tasks. Across multiple question-answering benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over standard RAG, while cutting inference costs by a factor of 10X GFLOPs-delivering both higher accuracy and greater efficiency, invariant to exemplar order.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions</title>
<link>https://arxiv.org/abs/2510.05378</link>
<guid>https://arxiv.org/abs/2510.05378</guid>
<content:encoded><![CDATA[
<div> Keywords: human-AI collaboration, symbols, meanings, symbolic interactionism theory, shared understanding

Summary:
In a study exploring human-AI collaboration in interpreting symbols and constructing meanings, researchers found that while humans interpret symbols through social interaction, AI systems treat them as patterns with compressed meanings. Results showed that when AI introduced conflicting meanings and symbols in social contexts, 63% of participants reshaped their definitions. This indicates that conflicts in symbols prompt reflection and redefinition, leading to a better shared understanding between humans and AI. The study highlights that shared understanding does not necessarily arise from agreement but from the reciprocal exchange and reinterpretation of symbols. This suggests the need for new paradigms in human-AI interaction design that prioritize the dynamic and collaborative nature of symbol interpretation for more meaningful collaboration. 

<br /><br />Summary: <div>
arXiv:2510.05378v1 Announce Type: new 
Abstract: Meaningful human-AI collaboration requires more than processing language, it demands a better understanding of symbols and their constructed meanings. While humans naturally interpret symbols through social interaction, AI systems treat them as patterns with compressed meanings, missing the dynamic meanings that emerge through conversation. Drawing on symbolic interactionism theory, we conducted two studies (N=37) investigated how humans and AI interact with symbols and co-construct their meanings. When AI introduced conflicting meanings and symbols in social contexts, 63% of participants reshaped their definitions. This suggests that conflicts in symbols and meanings prompt reflection and redefinition, allowing both participants and AI to have a better shared understanding of meanings and symbols. This work reveals that shared understanding emerges not from agreement but from the reciprocal exchange and reinterpretation of symbols, suggesting new paradigms for human-AI interaction design.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation</title>
<link>https://arxiv.org/abs/2510.05402</link>
<guid>https://arxiv.org/abs/2510.05402</guid>
<content:encoded><![CDATA[
<div> Teacher-Student learning framework, steel, heat treatment, regression task, metallurgical input features  
Summary:  
- Predicting steel hardness after heat treatment is complex due to the many-to-one nature of the process.  
- Estimating input parameters from desired hardness is challenging.  
- Proposed solution uses a Teacher-Student framework with a forward model (Teacher) and backward model (Student).  
- Teacher predicts hardness from input features, while Student infers input configurations from a target hardness.  
- Student optimized using feedback from Teacher in an iterative, supervised loop.  
- Evaluation on tempered steel dataset shows higher accuracy and efficiency compared to baseline models.  
- Teacher-Student framework is effective for inverse process modeling in materials science.  

<br /><br />Summary: <div>
arXiv:2510.05402v1 Announce Type: new 
Abstract: Predicting the final hardness of steel after heat treatment is a challenging regression task due to the many-to-one nature of the process -- different combinations of input parameters (such as temperature, duration, and chemical composition) can result in the same hardness value. This ambiguity makes the inverse problem, estimating input parameters from a desired hardness, particularly difficult. In this work, we propose a novel solution using a Teacher-Student learning framework. First, a forward model (Teacher) is trained to predict final hardness from 13 metallurgical input features. Then, a backward model (Student) is trained to infer plausible input configurations from a target hardness value. The Student is optimized by leveraging feedback from the Teacher in an iterative, supervised loop. We evaluate our method on a publicly available tempered steel dataset and compare it against baseline regression and reinforcement learning models. Results show that our Teacher-Student framework not only achieves higher inverse prediction accuracy but also requires significantly less computational time, demonstrating its effectiveness and efficiency for inverse process modeling in materials science.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems</title>
<link>https://arxiv.org/abs/2510.05432</link>
<guid>https://arxiv.org/abs/2510.05432</guid>
<content:encoded><![CDATA[
<div> framework, language models, artificial intelligence, problem-solving, scientific inquiry
Summary:
- AInstein is a framework that tests the problem-solving abilities of large language models (LLMs) using only their pre-trained knowledge.
- The framework evaluates LLMs on ICLR papers based on success rate, rediscovery, and novelty.
- LLMs can rediscover feasible solutions and occasionally propose original approaches, but their problem-solving ability is sensitive to framing.
- The study highlights the potential and limitations of LLMs as autonomous scientific problem-solvers.
<br /><br />Summary: <div>
arXiv:2510.05432v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification</title>
<link>https://arxiv.org/abs/2510.05451</link>
<guid>https://arxiv.org/abs/2510.05451</guid>
<content:encoded><![CDATA[
<div> transformer models, multi-label text classification, neuro-symbolic framework, Answer Set Programming, aviation safety

Summary:
This paper introduces a hybrid neuro-symbolic framework that combines Answer Set Programming (ASP) with transformer-based learning for multi-label text classification on the Aviation Safety Reporting System (ASRS) corpus. The framework uses weighted ASP rules to formalize domain knowledge, which is validated using the Clingo solver. These rules are integrated into the model as rule-based data augmentation and fuzzy-logic regularizers to ensure logical consistency in the predictions. The approach improves micro- and macro-F1 scores, reduces rule violations by up to 86% on the ASRS test set, and enhances label diversity and coverage. This novel approach offers a scalable and interpretable solution for safety-critical NLP tasks by combining symbolic reasoning with deep neural architectures. <div>
arXiv:2510.05451v1 Announce Type: new 
Abstract: Deep transformer models excel at multi-label text classification but often violate domain logic that experts consider essential, an issue of particular concern in safety-critical applications. We propose a hybrid neuro-symbolic framework that integrates Answer Set Programming (ASP) with transformer-based learning on the Aviation Safety Reporting System (ASRS) corpus. Domain knowledge is formalized as weighted ASP rules and validated using the Clingo solver. These rules are incorporated in two complementary ways: (i) as rule-based data augmentation, generating logically consistent synthetic samples that improve label diversity and coverage; and (ii) as a fuzzy-logic regularizer, enforcing rule satisfaction in a differentiable form during fine-tuning. This design preserves the interpretability of symbolic reasoning while leveraging the scalability of deep neural architectures. We further tune per-class thresholds and report both standard classification metrics and logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE) baseline, our approach improves micro- and macro-F1 scores and achieves up to an 86% reduction in rule violations on the ASRS test set. To the best of our knowledge, this constitutes the first large-scale neuro-symbolic application to ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and differentiable transformer training for trustworthy, safety-critical NLP.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Code Models Suffer from the Dunning-Kruger Effect?</title>
<link>https://arxiv.org/abs/2510.05457</link>
<guid>https://arxiv.org/abs/2510.05457</guid>
<content:encoded><![CDATA[
<div> Dunning-Kruger Effect, AI models, LLMs, programming languages, overconfidence  
Summary:  
- The study examines the Dunning-Kruger Effect (DKE) in state-of-the-art LLMs used for coding tasks.  
- Model confidence and performance were analyzed across different programming languages, showing that AI models exhibit human-like patterns of overconfidence, especially in unfamiliar or low-resource domains.  
- Less competent models and those working with rare programming languages display stronger DKE-like bias, indicating that the bias intensity correlates with model competence.  
- The findings suggest that AI systems can replicate cognitive biases observed in humans, raising concerns about shared agency in collaborative human-AI endeavors.  
- The study highlights the importance of understanding the cognitive boundaries and biases that shape interactions between humans and AI systems in creative and technical domains.  

<br /><br />Summary: <div>
arXiv:2510.05457v1 Announce Type: new 
Abstract: As artificial intelligence systems increasingly collaborate with humans in creative and technical domains, questions arise about the cognitive boundaries and biases that shape our shared agency. This paper investigates the Dunning-Kruger Effect (DKE), the tendency for those with limited competence to overestimate their abilities in state-of-the-art LLMs in coding tasks. By analyzing model confidence and performance across a diverse set of programming languages, we reveal that AI models mirror human patterns of overconfidence, especially in unfamiliar or low-resource domains. Our experiments demonstrate that less competent models and those operating in rare programming languages exhibit stronger DKE-like bias, suggesting that the strength of the bias is proportionate to the competence of the models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAL-Bench: Measuring Value Alignment in Language Models</title>
<link>https://arxiv.org/abs/2510.05465</link>
<guid>https://arxiv.org/abs/2510.05465</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, value alignment, human values, controversy<br />
<br />
Summary: 
The article introduces VAL-Bench, a benchmark designed to evaluate the consistency of values expressed by large language models (LLMs) across paired prompts framing opposing sides of public debates. The benchmark consists of 115K pairs from Wikipedia's controversial sections and measures alignment using an LLM-as-judge to score agreement or divergence between responses. Results show significant variation in alignment across leading LLMs, revealing trade-offs between safety strategies and expressive value systems. VAL-Bench provides a scalable and reproducible benchmark for systematic comparison of how reliably LLMs embody human values. <div>
arXiv:2510.05465v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used for tasks where outputs shape human decisions, so it is critical to test whether their responses reflect consistent human values. Existing benchmarks mostly track refusals or predefined safety violations, but these only check rule compliance and do not reveal whether a model upholds a coherent value system when facing controversial real-world issues. We introduce the \textbf{V}alue \textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates whether models maintain a stable value stance across paired prompts that frame opposing sides of public debates. VAL-Bench consists of 115K such pairs from Wikipedia's controversial sections. A well-aligned model should express similar underlying views regardless of framing, which we measure using an LLM-as-judge to score agreement or divergence between paired responses. Applied across leading open- and closed-source models, the benchmark reveals large variation in alignment and highlights trade-offs between safety strategies (e.g., refusals) and more expressive value systems. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic comparison of how reliably LLMs embody human values.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vul-R2: A Reasoning LLM for Automated Vulnerability Repair</title>
<link>https://arxiv.org/abs/2510.05480</link>
<guid>https://arxiv.org/abs/2510.05480</guid>
<content:encoded><![CDATA[
<div> Automatic Vulnerability Repair, Large Language Models, Vulnerability-related reasoning data, Reinforcement learning, Model training  
Summary:  
The urgent need for automatic vulnerability repair solutions in the face of the exponential increase in software vulnerabilities has led to recent research leveraging large language models for this task. However, challenges persist in capturing diverse vulnerability repair patterns due to the lack of high-quality vulnerability-related reasoning data. Additionally, verifying the intermediate vulnerability repair process during model training is difficult as the process lacks verifiable feedback, unlike other reinforcement learning methods. Current approaches prompt or fine-tune LLMs to generate vulnerability repairs directly, but without vulnerability-related reasoning data, they struggle to capture all repair patterns efficiently. The reliance on general programming knowledge from foundation models also contributes to this limitation. Furthermore, the lack of intermediate, verifiable feedback during model training poses additional challenges to reinforcement learning techniques employed in vulnerability repair processes. Overall, addressing these challenges is crucial for improving the performance and effectiveness of automatic vulnerability repair solutions.  
<br /><br />Summary: <div>
arXiv:2510.05480v1 Announce Type: new 
Abstract: The exponential increase in software vulnerabilities has created an urgent need for automatic vulnerability repair (AVR) solutions. Recent research has formulated AVR as a sequence generation problem and has leveraged large language models (LLMs) to address this problem. Typically, these approaches prompt or fine-tune LLMs to generate repairs for vulnerabilities directly. Although these methods show state-of-the-art performance, they face the following challenges: (1) Lack of high-quality, vulnerability-related reasoning data. Current approaches primarily rely on foundation models that mainly encode general programming knowledge. Without vulnerability-related reasoning data, they tend to fail to capture the diverse vulnerability repair patterns. (2) Hard to verify the intermediate vulnerability repair process during LLM training. Existing reinforcement learning methods often leverage intermediate execution feedback from the environment (e.g., sandbox-based execution results) to guide reinforcement learning training. In contrast, the vulnerability repair process generally lacks such intermediate, verifiable feedback, which poses additional challenges for model training.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decade-long Emission Forecasting with an Ensemble Model in Taiwan</title>
<link>https://arxiv.org/abs/2510.05548</link>
<guid>https://arxiv.org/abs/2510.05548</guid>
<content:encoded><![CDATA[
<div> Keywords: Taiwan, air pollution, time series models, emissions forecasting, ensemble technique

Summary:
This study focuses on Taiwan's significant air pollution issue primarily caused by carbon dioxide emissions due to its high population and heavy reliance on fossil fuels. The research compares 21 time series models for forecasting emissions, with the Feedforward Neural Network, Support Vector Machine, and Random Forest Regressor showing the best performance. A custom stacked generalization ensemble technique is proposed to integrate the top-performing models and achieve robustness with an SMAPE of 1.407 without overfitting. The study also provides a decade-long emission projection, enabling policymakers to make data-driven decisions. The comprehensive research aims to address Taiwan's pressing environmental concerns and contribute to more accurate emission forecasting methods.<br /><br />Summary: <div>
arXiv:2510.05548v1 Announce Type: new 
Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to severe air pollution, with the most prevalent greenhouse gas being carbon dioxide (CO2). There-fore, this study presents a reproducible and comprehensive case study comparing 21 of the most commonly employed time series models in forecasting emissions, analyzing both univariate and multivariate approaches. Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM), and Random Forest Regressor (RFR) achieved the best performances. To further enhance robustness, the top performers were integrated with Linear Regression through a custom stacked generalization en-semble technique. Our proposed ensemble model achieved an SMAPE of 1.407 with no signs of overfitting. Finally, this research provides an accurate decade-long emission projection that will assist policymakers in making more data-driven decisions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption</title>
<link>https://arxiv.org/abs/2510.05580</link>
<guid>https://arxiv.org/abs/2510.05580</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action models, MetaVLA, Context-Aware Meta Co-Training, Generalization, Embodied agents 

Summary: <br /><br />MetaVLA introduces a post-training framework for Vision-Language-Action models that enhances alignment and generalization. By utilizing Context-Aware Meta Co-Training, MetaVLA consolidates multiple tasks into one fine-tuning stage, improving in-domain generalization with diverse auxiliary tasks. The framework integrates a meta-learning mechanism derived from Attentive Neural Processes for rapid adaptation, requiring minimal architectural changes. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA on long-horizon tasks, reduces training steps, and cuts GPU time significantly. These results demonstrate the efficiency and scalability of MetaVLA in achieving general-purpose embodied agents. The approach shows promise in enhancing the capabilities of VLA models, making them more versatile and effective for various tasks. <div>
arXiv:2510.05580v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-the-Flow Agentic System Optimization for Effective Planning and Tool Use</title>
<link>https://arxiv.org/abs/2510.05592</link>
<guid>https://arxiv.org/abs/2510.05592</guid>
<content:encoded><![CDATA[
<div> framework, trainable, in-the-flow, AgentFlow, reinforcement learning <br />
Summary:
AgentFlow is introduced as a trainable framework for outcome-driven reinforcement learning in large language models. It employs specialized modules (planner, executor, verifier, generator) and optimizes its planner within a multi-turn loop. The Flow-based Group Refined Policy Optimization (Flow-GRPO) is proposed for on-policy training in live environments, facilitating long-horizon and sparse-reward credit assignment. Through in-the-flow optimization, AgentFlow outperforms baselines and even proprietary models like GPT-4o on various benchmarks. The framework demonstrates improved planning, enhanced tool-calling reliability, and scalability with model size and reasoning turns. <br /> <div>
arXiv:2510.05592v1 Announce Type: new 
Abstract: Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions</title>
<link>https://arxiv.org/abs/2510.05596</link>
<guid>https://arxiv.org/abs/2510.05596</guid>
<content:encoded><![CDATA[
<div> Keywords: self-evolving AI, wireless systems, multi-agent framework, antenna optimization, autonomous improvement<br />
<br />
Summary: 
This paper introduces the concept of self-evolving agentic artificial intelligence (AI) in the context of wireless systems. The self-evolving agents are capable of continual adaptation and improvement without human intervention, through an autonomous evolution cycle. The paper discusses the layered architecture, life cycle, and key techniques of self-evolving agentic AI, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. A multi-agent cooperative framework is proposed, where large language models (LLMs) collaborate under a supervisor agent to autonomously execute tasks. A case study on antenna evolution in low-altitude wireless networks demonstrates the framework's ability to autonomously upgrade fixed antenna optimization. Experimental results show significant performance improvements with little to no human intervention, highlighting the adaptability and robustness of self-evolving agentic AI for next-generation wireless intelligence. <div>
arXiv:2510.05596v1 Announce Type: new 
Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography</title>
<link>https://arxiv.org/abs/2510.05664</link>
<guid>https://arxiv.org/abs/2510.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-4o, radiology reports, multi-label classification, label extraction, uncertainty detection<br />
Summary:<br />
- The study evaluated GPT-4o's ability to extract diagnostic labels from radiology reports, achieving a high label extraction accuracy rate of 98.6% in test sets.
- Labels extracted were used for multi-label classification of musculoskeletal radiographs, with competitive performance indicated by macro-averaged AUC values for inclusive and exclusive models across different anatomical regions.
- The models trained on label-based data generalized well on external datasets, demonstrating consistent performance.
- The presence of uncertainty in radiology reports did not significantly impact the performance of the multi-label classification models.
- The study highlights the potential of GPT-4o in accurately extracting labels from free-text radiology reports for training robust multi-label image classification models. <br /> <div>
arXiv:2510.05664v1 Announce Type: new 
Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with uncertainty) from free-text radiology reports and to test how these labels affect multi-label image classification of musculoskeletal radiographs. Methods: This retrospective study included radiography series of the clavicle (n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o filled out structured templates by indicating imaging findings as present ("true"), absent ("false"), or "uncertain." To assess the impact of label uncertainty, "uncertain" labels of the training and validation sets were automatically reassigned to "true" (inclusive) or "false" (exclusive). Label-image-pairs were used for multi-label classification using ResNet50. Label extraction accuracy was manually verified on internal (clavicle: n=233, elbow: n=745, thumb: n=393) and external test sets (n=300 for each). Performance was assessed using macro-averaged receiver operating characteristic (ROC) area under the curve (AUC), precision recall curves, sensitivity, specificity, and accuracy. AUCs were compared with the DeLong test. Results: Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the test sets. Across anatomic regions, label-based model training yielded competitive performance measured by macro-averaged AUC values for inclusive (e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow: AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets (elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79 [range, 0.63-0.89]). No significant differences were observed across labeling strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from radiologic reports to train competitive multi-label classification models with high accuracy. Detected uncertainty in the radiologic reports did not influence the performance of these models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</title>
<link>https://arxiv.org/abs/2510.05684</link>
<guid>https://arxiv.org/abs/2510.05684</guid>
<content:encoded><![CDATA[
<div> framework, desktop interactions, embodied AI, robotics, pretraining <br />
<br />
Summary: 
The article introduces the Desktop to Embodied AI (D2E) framework, which utilizes desktop environments, particularly gaming, as a cost-effective alternative for collecting physical trajectory data for embodied AI. The framework consists of three components: the OWA Toolkit for standardizing desktop interactions, the Generalist-IDM for generalization across different games, and VAPT for transferring pre-trained representations to physical tasks. Through the use of over 1.3K hours of data, including human demonstrations and pseudo-labeled gameplay, the framework achieves high success rates in both manipulation and navigation benchmarks. These results validate that sensorimotor primitives in digital interactions can effectively transfer to physical tasks, establishing desktop pretraining as a practical paradigm for robotics. The authors will make all their work, including the OWA toolkit, datasets, and trained models, publicly available. <br /> <div>
arXiv:2510.05684v1 Announce Type: new 
Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach</title>
<link>https://arxiv.org/abs/2510.05698</link>
<guid>https://arxiv.org/abs/2510.05698</guid>
<content:encoded><![CDATA[
<div> Keywords: UAVs, data collection schedules, flight velocities, Deep Reinforcement Learning, Large Language Models 

Summary:
In this study, the focus is on optimizing data collection schedules and flight velocities for UAVs in post-disaster monitoring scenarios like tsunamis. Traditional Deep Reinforcement Learning (DRL) solutions are deemed insufficient due to their complex training processes and simulation-reality mismatch. Instead, the study proposes the use of Large Language Models (LLMs) with In-Context Learning (ICL) capabilities for task adaptation without the need for retraining. The joint optimization model considers factors like battery levels of ground sensors, queue lengths, channel conditions, and UAV trajectories. An Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) approach is suggested as a more suitable alternative to DRL in emergency situations. Simulation results indicate that AIC-VDS outperforms both Deep-Q-Network (DQN) and maximum channel gain baselines in minimizing data loss. <div>
arXiv:2510.05698v1 Announce Type: new 
Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated to collect sensory data in post-disaster monitoring scenarios, such as tsunamis, where early actions are critical to limit coastal damage. A major challenge is to design the data collection schedules and flight velocities, as unfavorable schedules and velocities can lead to transmission errors and buffer overflows of the ground sensors, ultimately resulting in significant packet loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a complex training process and a mismatch between simulation and reality that does not meet the urgent requirements of tsunami monitoring. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With their strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation through natural language prompts and example-based guidance without retraining. However, LLM models have input data limitations and thus require customized approaches. In this paper, a joint optimization of data collection schedules and velocities control for multiple UAVs is proposed to minimize data loss. The battery level of the ground sensors, the length of the queues, and the channel conditions, as well as the trajectories of the UAVs, are taken into account. Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) is proposed as an alternative to DRL in emergencies. The simulation results show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and maximum channel gain baselines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge</title>
<link>https://arxiv.org/abs/2510.05733</link>
<guid>https://arxiv.org/abs/2510.05733</guid>
<content:encoded><![CDATA[
<div> framework, fault diagnosis, Large Language Models, edge model, diagnostic accuracy 
Summary: 
The paper introduces Syn-Diag, a cloud-edge framework for industrial fault diagnosis that addresses data scarcity and resource constraints. It leverages Large Language Models through Visual-Semantic Synergy, Content-Aware Reasoning, and Cloud-Edge Synergy mechanisms. These mechanisms align signal features with semantic spaces, enhance diagnostic accuracy with limited samples, and create lightweight edge models for online updates. Extensive experiments on different datasets show that Syn-Diag outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%. Overall, Syn-Diag offers a practical and deployable paradigm for intelligent diagnostics in modern industries. 
<br /><br />Summary: <div>
arXiv:2510.05733v1 Announce Type: new 
Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLM's semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificially intelligent agents in the social and behavioral sciences: A history and outlook</title>
<link>https://arxiv.org/abs/2510.05743</link>
<guid>https://arxiv.org/abs/2510.05743</guid>
<content:encoded><![CDATA[
<div> Keywords: artificially intelligent agents, social and behavioral sciences, social simulations, intelligent game theoretic agents, big data <br />
Summary: <br />
This article provides an overview of the historical development and current trends of artificially intelligent agents in the social and behavioral sciences. It highlights the evolution of AI in the scientific process from the first programmable computers to today's experiments with large language models. The challenges of presenting early social simulation studies to a computer-unaware world are discussed, as well as the rise of social systems science and the introduction of intelligent game theoretic agents. The age of big data is also examined, along with the epistemic upheaval it has caused. The article concludes by exploring the current enthusiasm surrounding applications of generative AI and emphasizes the deep entanglement of technology with our understanding of ourselves. <div>
arXiv:2510.05743v1 Announce Type: new 
Abstract: We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.05746</link>
<guid>https://arxiv.org/abs/2510.05746</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Multi-agent systems, Chain of Thought reasoning, Agentic Reasoning Module, Automatic MAS design

Summary:<br /><br />
The article introduces a new paradigm for automatic Multi-agent Systems (MAS) design that focuses on optimizing Chain of Thought (CoT) reasoning. This new approach, called the Agentic Reasoning Module (ARM), uses specialized reasoning modules to execute granular reasoning steps within MAS. The ARM is discovered through a tree search process and evolves using mutations informed by reflection on execution traces. The resulting ARM outperforms both manually designed MAS and state-of-the-art automatic MAS design methods. MAS built with ARM exhibit superior generalization abilities, maintaining high performance across different foundation models and task domains without the need for further optimization. The study highlights the importance of CoT reasoning in MAS design and advances the field by providing a versatile reasoning building block that can be easily integrated into various MAS architectures. <div>
arXiv:2510.05746v1 Announce Type: new 
Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport</title>
<link>https://arxiv.org/abs/2510.05751</link>
<guid>https://arxiv.org/abs/2510.05751</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Lagrangian Particle Dispersion Model, Ensemble, Uncertainty, Greenhouse Gas Emissions<br />
<br />
Summary: 
The article introduces a new approach for estimating greenhouse gas emissions through a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM). This method accelerates the simulation process by a significant factor while maintaining accuracy in reproducing large-scale footprint structures. By using ensembles, the approach allows for the quantification of uncertainties in atmospheric transport footprints and methane mole fractions. The results highlight areas of low-confidence predictions, providing insight into spatial and temporal variations in greenhouse gas emissions. This technique has the potential to improve current greenhouse gas inversion systems and enhance the robustness of emissions monitoring using satellite observations. Additionally, the application of ensemble-based emulators could help identify and address systematic errors in LPDMs, leading to a more comprehensive understanding of uncertainty in greenhouse gas flux estimates. <br /><br /> <div>
arXiv:2510.05751v1 Announce Type: new 
Abstract: Monitoring greenhouse gas emissions and evaluating national inventories require efficient, scalable, and reliable inference methods. Top-down approaches, combined with recent advances in satellite observations, provide new opportunities to evaluate emissions at continental and global scales. However, transport models used in these methods remain a key source of uncertainty: they are computationally expensive to run at scale, and their uncertainty is difficult to characterise. Artificial intelligence offers a dual opportunity to accelerate transport simulations and to quantify their associated uncertainty.
  We present an ensemble-based pipeline for estimating atmospheric transport "footprints", greenhouse gas mole fraction measurements, and their uncertainties using a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse Gases Observing Satellite) observations for Brazil in 2016. The emulator achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale footprint structures. Ensembles were calculated to quantify absolute and relative uncertainty, revealing spatial correlations with prediction error. The results show that ensemble spread highlights low-confidence spatial and temporal predictions for both atmospheric transport footprints and methane mole fractions.
  While demonstrated here for an LPDM emulator, the approach could be applied more generally to atmospheric transport models, supporting uncertainty-aware greenhouse gas inversion systems and improving the robustness of satellite-based emissions monitoring. With further development, ensemble-based emulators could also help explore systematic LPDM errors, offering a computationally efficient pathway towards a more comprehensive uncertainty budget in greenhouse gas flux estimates.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis</title>
<link>https://arxiv.org/abs/2510.05761</link>
<guid>https://arxiv.org/abs/2510.05761</guid>
<content:encoded><![CDATA[
<div> Keywords: meme virality, early prediction, Reddit communities, XGBoost, time series data

Summary:
This study explores the early prediction of meme virality using a dataset from 25 diverse Reddit communities. A robust method is proposed to define virality based on engagement scores, with models such as Logistic Regression and XGBoost showing promising results. The XGBoost model achieves a PR-AUC of over 0.52 in just 30 minutes, indicating the emergence of useful signals quickly. An "evidentiary transition" is observed, highlighting the shift in feature importance from static context to temporal dynamics as a meme gains traction. This study establishes a benchmark for early virality prediction, combining time series data with static content and network features. It contributes a novel cross-lingual dataset and offers a methodologically sound definition of virality. This work is the first to integrate time series data with static and network features to predict early meme virality.

<br /><br />Summary: <div>
arXiv:2510.05761v1 Announce Type: new 
Abstract: Predicting the virality of online content remains challenging, especially for culturally complex, fast-evolving memes. This study investigates the feasibility of early prediction of meme virality using a large-scale, cross-lingual dataset from 25 diverse Reddit communities. We propose a robust, data-driven method to define virality based on a hybrid engagement score, learning a percentile-based threshold from a chronologically held-out training set to prevent data leakage. We evaluated a suite of models, including Logistic Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive, multimodal feature set across increasing time windows (30-420 min). Crucially, useful signals emerge quickly: our best-performing model, XGBoost, achieves a PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary transition," in which the importance of the feature dynamically shifts from the static context to the temporal dynamics as a meme gains traction. This work establishes a robust, interpretable, and practical benchmark for early virality prediction in scenarios where full diffusion cascade data is unavailable, contributing a novel cross-lingual dataset and a methodologically sound definition of virality. To our knowledge, this study is the first to combine time series data with static content and network features to predict early meme virality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases</title>
<link>https://arxiv.org/abs/2510.05764</link>
<guid>https://arxiv.org/abs/2510.05764</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational drug repurposing, rare diseases, knowledge graph completion, message-passing GNNs, multi-agent system<br />
Summary: RareAgent is a new approach for computational drug repurposing for rare diseases. It uses a self-evolving multi-agent system that engages in adversarial debates to construct evidence graphs, supported by diverse perspectives. The system actively seeks evidence to support, refute, or entail hypotheses, refining agent policies based on post hoc analysis. Successful reasoning paths are used to create transferable heuristics for future investigations. RareAgent improves indication AUPRC by 18.1% compared to reasoning baselines and produces transparent reasoning chains consistent with clinical evidence.<br /><br /> <div>
arXiv:2510.05764v1 Announce Type: new 
Abstract: Computational drug repurposing for rare diseases is especially challenging when no prior associations exist between drugs and target diseases. Therefore, knowledge graph completion and message-passing GNNs have little reliable signal to learn and propagate, resulting in poor performance. We present RareAgent, a self-evolving multi-agent system that reframes this task from passive pattern recognition to active evidence-seeking reasoning. RareAgent organizes task-specific adversarial debates in which agents dynamically construct evidence graphs from diverse perspectives to support, refute, or entail hypotheses. The reasoning strategies are analyzed post hoc in a self-evolutionary loop, producing textual feedback that refines agent policies, while successful reasoning paths are distilled into transferable heuristics to accelerate future investigations. Comprehensive evaluations reveal that RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and provides a transparent reasoning chain consistent with clinical evidence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming</title>
<link>https://arxiv.org/abs/2510.05774</link>
<guid>https://arxiv.org/abs/2510.05774</guid>
<content:encoded><![CDATA[
<div> ConstraintLLM, CP modeling, automatic modeling, IndusCP benchmark, Constraint-Aware Retrieval Module<br />
<br />
Summary: <br />
The article introduces ConstraintLLM, a large language model specifically designed for Constraint Programming (CP) modeling. It is trained on an open-source LLM with fine-tuning to increase in-context learning capabilities. The Constraint-Aware Retrieval Module (CARM) is integrated into a Tree-of-Thoughts framework with a self-correction mechanism. The researchers also release IndusCP, an industrial-level benchmark for CP modeling, containing 140 challenging tasks from various domains. ConstraintLLM achieves state-of-the-art solving accuracy on multiple benchmarks and outperforms baselines by 2x on the new IndusCP benchmark. The code and data are available online for further research and development. <div>
arXiv:2510.05774v1 Announce Type: new 
Abstract: Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Safety Challenge of World Models for Embodied AI Agents: A Review</title>
<link>https://arxiv.org/abs/2510.05865</link>
<guid>https://arxiv.org/abs/2510.05865</guid>
<content:encoded><![CDATA[
<div> Safe predictions, embodied artificial intelligence, World Models, autonomous driving, robotics<br />
<br />
Summary: 
The article reviews the current state of World Models (WMs) in the context of embodied artificial intelligence, with a focus on safety implications for autonomous driving and robotics. WMs aim to improve agents' anticipation and planning abilities by predicting future environmental states. However, ensuring the safety of these predictions is crucial for both the agent and the environment. The review includes an empirical analysis of state-of-the-art models to identify common faults or pathologies in scene and control generation tasks. The study categorizes and quantitatively evaluates these pathologies to assess the reliability and safety of the predictions. The results highlight the importance of developing more advanced and integrated models that can perceive, interpret, and predict environmental dynamics in a safe manner. <div>
arXiv:2510.05865v1 Announce Type: new 
Abstract: The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering</title>
<link>https://arxiv.org/abs/2510.05871</link>
<guid>https://arxiv.org/abs/2510.05871</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, uncertainty-based filtering, reasoning models, biological perturbation prediction, synthetic data<br />
<br />
Summary: <br />
The article introduces a new approach for creating synthetic chain-of-thought (CoT) traces in large reasoning models (LRMs) without relying on ground-truth labels. Instead of using external labels, the proposed method, uncertainty-based filtering, leverages a model's confidence metrics like self-consistency and predictive perplexity to filter low-uncertainty subsets of reasoning traces. Applied to predicting biological perturbations, which often lack sufficient wet-lab labels, the filtered subset demonstrates higher accuracy. Additionally, supervised fine-tuning on uncertainty-filtered data surpasses unfiltered synthetic data and narrows the gap to ground-truth training, outperforming strong LRM baselines. Ablation studies reveal the importance of per-class filtering to correct class-specific uncertainty scales and hybrid uncertainty metrics for creating higher-quality datasets. This study highlights the efficacy of using model-internal confidence as a cost-effective method for generating efficient reasoning datasets in domains with limited supervision. <br /> <div>
arXiv:2510.05871v1 Announce Type: new 
Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large reasoning models (LRMs), improving generalization by providing step-level supervision. Yet most approaches require ground-truth labels to seed or filter these traces - an expensive bottleneck in domains like biology where wet-lab data are scarce. We propose a label-free alternative: uncertainty-based filtering, which uses a model's own confidence - quantified through established uncertainty metrics like self-consistency and predictive perplexity - as a substitute for external labels. We sample multiple reasoning traces and retain only low-uncertainty subsets. Applied to biological perturbation prediction, a domain where wet-lab labels are especially costly, we show that the filtered subset has higher accuracy, and that supervised fine-tuning (SFT) on uncertainty-filtered data outperforms unfiltered synthetic data, narrows the gap to ground-truth training, and surpasses strong LRM baselines. Ablations show that per-class filtering corrects for class-specific uncertainty scales and that hybrid uncertainty metrics yield higher-quality datasets. Our results suggest that model-internal confidence is a powerful signal for efficient reasoning dataset creation, enabling LRMs in domains where supervision is expensive.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies</title>
<link>https://arxiv.org/abs/2510.05909</link>
<guid>https://arxiv.org/abs/2510.05909</guid>
<content:encoded><![CDATA[
<div> evolutionary algorithm, debate strategies, persuasion-based optimization, truth-based approaches, large language models
Summary:
The study introduces DebateQD, an evolutionary algorithm that evolves diverse debate strategies in Large Language Models (LLMs) through tournament-style competitions. It contrasts persuasion-based optimization with truth-based approaches in debate settings, showing that persuasion-optimized strategies lead to smaller train-test generalization gaps and improved reasoning skills. The algorithm maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments. By isolating the role of the optimization objective and fixing the debate protocol while swapping only the fitness function, the study demonstrates that competitive pressure to persuade, rather than seeking the truth collaboratively, leads to more transferable reasoning skills in LLMs. This research offers a promising direction for enhancing generalization in LLMs. 
<br /><br />Summary: <div>
arXiv:2510.05909v1 Announce Type: new 
Abstract: Large Language Models (LLMs) optimized to output truthful answers often overfit, producing brittle reasoning that fails to generalize. While persuasion-based optimization has shown promise in debate settings, it has not been systematically compared against mainstream truth-based approaches. We introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm that evolves diverse debate strategies across different categories (rationality, authority, emotional appeal, etc.) through tournament-style competitions where two LLMs debate while a third judges. Unlike previously proposed methods that require a population of LLMs, our approach maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments while preserving the key benefits of population-based optimization. In contrast to prior work, we explicitly isolate the role of the optimization objective by fixing the debate protocol and swapping only the fitness function: persuasion rewards strategies that convince the judge irrespective of truth, whereas truth rewards collaborative correctness. Across three model scales (7B, 32B, 72B parameters) and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized strategies achieve up to 13.94% smaller train-test generalization gaps, while matching or exceeding truth optimization's test performance. These results provide the first controlled evidence that competitive pressure to persuade, rather than seek the truth collaboratively, fosters more transferable reasoning skills, offering a promising path for improving LLM generalization.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Time Series Classification via In-Context Reasoning with LLM Agents</title>
<link>https://arxiv.org/abs/2510.05950</link>
<guid>https://arxiv.org/abs/2510.05950</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series classification, large language models, multi-agent framework, training-free, in-context reasoning 

Summary:
FETA is a multi-agent framework designed for training-free time series classification using exemplar-based in-context reasoning. It decomposes multivariate series into channel-wise subproblems and uses a reasoning-oriented large language model to compare queries against labeled examples, producing channel-level labels with self-assessed confidences. By eliminating the need for pretraining or fine-tuning, FETA improves efficiency, control input length, and enhances interpretability through exemplar grounding and confidence estimation. The framework achieves strong accuracy on challenging datasets without any parameter training, surpassing trained baselines. This demonstrates that a multi-agent in-context reasoning approach can leverage large language models as competitive, plug-and-play solutions for time series classification applications. The code for FETA is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2510.05950v1 Announce Type: new 
Abstract: Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization</title>
<link>https://arxiv.org/abs/2510.05962</link>
<guid>https://arxiv.org/abs/2510.05962</guid>
<content:encoded><![CDATA[
<div> keywords: contamination-free evaluation, mathematical capabilities, overfitting, reasoning, MatheMagic

Summary: 
This paper addresses the challenge of evaluating mathematical capabilities without contamination from memorization or overfitting. It introduces MatheMagic, a method to create dynamic math test instances with altered interpretations of numbers and operators, while still having verifiable answers. This new benchmark allows for testing a model's induction and deduction capabilities by generating random test instances at test time. The study finds that models perform better in deduction than induction tasks but struggle to exhibit a general reasoning skill. Furthermore, fine-tuning on induction tasks leads to poor generalization. MatheMagic offers a stable, extensible, comparable, and robust framework for evaluating models' true mathematical reasoning abilities. <div>
arXiv:2510.05962v1 Announce Type: new 
Abstract: Conducting contamination-free evaluation of mathematical capabilities can be difficult for two reasons: models may memorize a test set once it is made public, and current mathematical benchmarks are prone to overfitting due to having limited diversity of symbols and rules, coupled with closed-ended answers. This paper proposes a method to leverage these shortcomings as useful features to a construct dynamic, counterfactual benchmark, which can be used to both reveal overfitting and measure true reasoning. We demonstrate this via MatheMagic, which generates math test instances with the interpretations of numbers and operators altered, yet has automatically verifiable answers. Test instances are randomly seeded and constructed at test time to evaluate a model's induction or deduction capability, offering stability, extensibility, comparability, and robustness to overfitting. Our experiments find that models solve deduction more easily than induction, but they revert to standard math. Further analysis reveals that math-adapted models fail to exhibit a general "skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Policy Pre-Training with Empowerment</title>
<link>https://arxiv.org/abs/2510.05996</link>
<guid>https://arxiv.org/abs/2510.05996</guid>
<content:encoded><![CDATA[
<div> Keywords: empowerment, reinforcement learning, pre-training signal, data-efficient, downstream task adaptation

Summary:
Empowerment, an intrinsic motivation measure in reinforcement learning (RL), is explored as a pre-training signal for efficient downstream task adaptation. The concept of discounted empowerment is introduced to balance short- and long-term control over the environment by the agent. A novel pre-training approach is proposed to initialize policies maximizing discounted empowerment, enhancing the agent's understanding of environmental dynamics. Empirical results highlight the effectiveness of empowerment-based pre-training across various RL algorithms, showing improved data efficiency and adaptability in downstream tasks. This framework has the potential to serve as a general-purpose initialization strategy, particularly beneficial for tasks with high-dimensional complexities. The study sets the stage for further research to scale and extend the applicability of empowerment-based pre-training in the realm of RL. 

<br /><br />Summary: <div>
arXiv:2510.05996v1 Announce Type: new 
Abstract: Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG</title>
<link>https://arxiv.org/abs/2510.06002</link>
<guid>https://arxiv.org/abs/2510.06002</guid>
<content:encoded><![CDATA[
<div> Keywords: Structure-Aware Temporal Graph RAG, SAT-Graph API, canonical actions, hybrid search, Explainable AI

Summary: The paper introduces the SAT-Graph API, a query execution layer for the Structure-Aware Temporal Graph RAG in the legal domain. It addresses the challenge of querying structured knowledge while maintaining determinism. The API utilizes canonical actions to enable high-precision hybrid search, robust reference resolution, point-in-time version retrieval, and auditable causal tracing. Planner-guided agents decompose complex queries into Directed Acyclic Graphs (DAGs) of these actions, transforming retrieval into a transparent and auditable process. This two-layer architecture fulfills Explainable AI (XAI) requirements for high-stakes domains, making retrieval more reliable and deterministic. <div>
arXiv:2510.06002v1 Announce Type: new 
Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core limitations of standard Retrieval-Augmented Generation in the legal domain by providing a verifiable knowledge graph that models hierarchical structure, temporal evolution, and causal events of legal norms. However, a critical gap remains: how to reliably query this structured knowledge without sacrificing its deterministic properties. This paper introduces the SAT-Graph API, a formal query execution layer centered on canonical actions-atomic, composable, and auditable primitives that isolate probabilistic discovery from deterministic retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust reference resolution; (iii) point-in-time version retrieval; and (iv) auditable causal tracing. We demonstrate how planner-guided agents can decompose complex queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer architecture transforms retrieval from an opaque black box to a transparent, auditable process, directly addressing Explainable AI (XAI) requirements for high-stakes domains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2510.06014</link>
<guid>https://arxiv.org/abs/2510.06014</guid>
<content:encoded><![CDATA[
<div> scaling, test-time, reasoning models, evaluation, ARISE

Summary:
- Test-time scaling is a transformative paradigm for enhancing the performance of large reasoning models by dynamically allocating computational resources during inference.
- A critical question in this field is how to systematically compare and evaluate test-time scaling capabilities across different models.
- ARISE (Adaptive Resolution-aware Scaling Evaluation) is introduced as a novel metric designed to assess the test-time scaling effectiveness of large reasoning models.
- ARISE incorporates sample-level awareness to penalize negative scaling behaviors and a dynamic sampling mechanism to mitigate accuracy fluctuations and token count instability.
- Comprehensive experiments evaluating state-of-the-art reasoning models across various domains reveal significant variations in scaling efficiency, with Claude Opus identified as having superior scaling characteristics. 

<br /><br />Summary: <div>
arXiv:2510.06014v1 Announce Type: new 
Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the performance of large reasoning models, enabling dynamic allocation of computational resources during inference. However, as the landscape of reasoning models rapidly expands, a critical question remains: how can we systematically compare and evaluate the test-time scaling capabilities across different models? In this paper, we introduce ARISE (Adaptive Resolution-aware Scaling Evaluation), a novel metric specifically designed to assess the test-time scaling effectiveness of large reasoning models. Unlike existing evaluation approaches, ARISE incorporates two key innovations: (1) sample-level awareness that effectively penalizes negative scaling behaviors where increased computation leads to performance degradation, and (2) a dynamic sampling mechanism that mitigates the impact of accuracy fluctuations and token count instability on the final assessment. We conduct comprehensive experiments evaluating state-of-the-art reasoning models across diverse domains including mathematical reasoning, code generation, and agentic tasks. Our results demonstrate that ARISE provides a reliable and fine-grained measurement of test-time scaling capabilities, revealing significant variations in scaling efficiency across models. Notably, our evaluation identifies Claude Opus as exhibiting superior scaling characteristics compared to other contemporary reasoning models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?</title>
<link>https://arxiv.org/abs/2510.06036</link>
<guid>https://arxiv.org/abs/2510.06036</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, safety vulnerabilities, refusal cliff, attention heads, safety alignment

Summary: 
Large reasoning models with multi-step reasoning capabilities have shown impressive problem-solving skills, but their safety vulnerabilities are poorly understood. This study investigates the failure of safety alignment in reasoning models and identifies a phenomenon called the "refusal cliff," where model refusal scores drop sharply before output generation. Mechanistic analysis reveals that some attention heads negatively impact refusal behavior, and ablating a small percentage can significantly improve safety. A novel data selection method called "Cliff-as-a-Judge" efficiently repairs safety alignment by identifying training examples with significant refusal cliffs. This approach achieves comparable safety improvements using minimal training data, highlighting a 'less-is-more' effect in safety alignment. 

<br /><br />Summary: <div>
arXiv:2510.06036v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as \textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\% of these heads can reduce attack success rates below 10\%. Building on these mechanistic insights, we propose \textbf{Cliff-as-a-Judge}, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixReasoning: Switching Modes to Think</title>
<link>https://arxiv.org/abs/2510.06052</link>
<guid>https://arxiv.org/abs/2510.06052</guid>
<content:encoded><![CDATA[
<div> reasoning models, MixReasoning, adaptive response, efficiency, accuracy

Summary:
MixReasoning is a framework designed to enhance the performance of reasoning models by dynamically adjusting the depth of reasoning within a single response. This means that the framework can adaptively respond to variations in the difficulty and complexity of sub-problems, allowing for a mixture of detailed reasoning on challenging steps and concise inference on simpler ones. Through experiments on various datasets, including GSM8K, MATH-500, and AIME, MixReasoning has been shown to shorten reasoning lengths and improve efficiency without sacrificing accuracy. This approach aims to address the issue of redundancy in extended reasoning models, as not all sub-problems require the same level of elaboration. By allowing for adaptive responses, MixReasoning can offer a more effective and streamlined process for problem-solving. <br /><br />Summary: <div>
arXiv:2510.06052v1 Announce Type: new 
Abstract: Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research</title>
<link>https://arxiv.org/abs/2510.06056</link>
<guid>https://arxiv.org/abs/2510.06056</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, scientific assistants, deep research, algorithm evolution, DeepEvolve

Summary:
DeepEvolve is introduced as an agent that combines deep research with algorithm evolution to enhance the capabilities of large language models (LLMs) in various scientific domains. Unlike existing methods that either reach a plateau or propose unrealistic solutions, DeepEvolve leverages external knowledge retrieval, cross-file code editing, and systematic debugging in an iterative feedback loop. This integration allows for the generation, refinement, implementation, and testing of new algorithms with sustained improvements across multiple benchmarks in chemistry, mathematics, biology, materials, and patents. By bridging the gap between unguided evolution and research without validation, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. The code for DeepEvolve is openly accessible on GitHub for further research and development. 

<br /><br />Summary: <div>
arXiv:2510.06056v1 Announce Type: new 
Abstract: Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis</title>
<link>https://arxiv.org/abs/2510.06063</link>
<guid>https://arxiv.org/abs/2510.06063</guid>
<content:encoded><![CDATA[
<div> observability data, TelecomTS, anomaly detection, root-cause analysis, multi-modal reasoning
Summary:
The article introduces TelecomTS, a new large-scale observability dataset derived from a 5G telecommunications network. This dataset includes heterogeneous, de-anonymized covariates with scale information and can be used for various tasks such as anomaly detection, root-cause analysis, and multi-modal reasoning. Benchmarking against state-of-the-art models reveals that existing approaches struggle with the unique dynamics of observability data, which is noisy, high-variance, and abrupt. The experiments also highlight the importance of preserving covariates' absolute scale, emphasizing the need for foundational time series models that can leverage scale information effectively for practical applications in observability data analysis. 

<br /><br />Summary: <div>
arXiv:2510.06063v1 Announce Type: new 
Abstract: Modern enterprises generate vast streams of time series metrics when monitoring complex systems, known as observability data. Unlike conventional time series from domains such as weather, observability data are zero-inflated, highly stochastic, and exhibit minimal temporal structure. Despite their importance, observability datasets are underrepresented in public benchmarks due to proprietary restrictions. Existing datasets are often anonymized and normalized, removing scale information and limiting their use for tasks beyond forecasting, such as anomaly detection, root-cause analysis, and multi-modal reasoning. To address this gap, we introduce TelecomTS, a large-scale observability dataset derived from a 5G telecommunications network. TelecomTS features heterogeneous, de-anonymized covariates with explicit scale information and supports a suite of downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark requiring multi-modal reasoning. Benchmarking state-of-the-art time series, language, and reasoning models reveals that existing approaches struggle with the abrupt, noisy, and high-variance dynamics of observability data. Our experiments also underscore the importance of preserving covariates' absolute scale, emphasizing the need for foundation time series models that natively leverage scale information for practical observability applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents</title>
<link>https://arxiv.org/abs/2510.06078</link>
<guid>https://arxiv.org/abs/2510.06078</guid>
<content:encoded><![CDATA[
<div> Keywords: Route recommendation, natural-language queries, multi-agent framework, constraint-aware routes, preference satisfaction
<br />
Summary: 
Route recommendation is a crucial aspect of travel planning, aiming to provide users with optimal travel plans while considering various requirements. Traditional routing algorithms are efficient but struggle with adapting to natural-language queries. Recent approaches based on large language models provide flexibility but face challenges in spatial reasoning and joint modeling of preferences. To address these limitations, RouteLLM is proposed, a hierarchical multi-agent framework that translates natural-language intents into constraint-aware routes. It parses user queries, coordinates specialized sub-agents to resolve constraints, retrieve candidate points of interest (POIs), and refine routes based on preference-conditioned costs. The design bridges linguistic flexibility with spatial structure, enabling reasoning over route feasibility and user preferences. Experimental results demonstrate that RouteLLM reliably translates textual preferences into constraint-aware routes, leading to improved route quality and preference satisfaction compared to classical methods. 
<br /> <div>
arXiv:2510.06078v1 Announce Type: new 
Abstract: Route recommendation aims to provide users with optimal travel plans that satisfy diverse and complex requirements. Classical routing algorithms (e.g., shortest-path and constraint-aware search) are efficient but assume structured inputs and fixed objectives, limiting adaptability to natural-language queries. Recent LLM-based approaches enhance flexibility but struggle with spatial reasoning and the joint modeling of route-level and POI-level preferences. To address these limitations, we propose RouteLLM, a hierarchical multi-agent framework that grounds natural-language intents into constraint-aware routes. It first parses user queries into structured intents including POIs, paths, and constraints. A manager agent then coordinates specialized sub-agents: a constraint agent that resolves and formally check constraints, a POI agent that retrieves and ranks candidate POIs, and a path refinement agent that refines routes via a routing engine with preference-conditioned costs. A final verifier agent ensures constraint satisfaction and produces the final route with an interpretable rationale. This design bridges linguistic flexibility and spatial structure, enabling reasoning over route feasibility and user preferences. Experiments show that our method reliably grounds textual preferences into constraint-aware routes, improving route quality and preference satisfaction over classical methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices</title>
<link>https://arxiv.org/abs/2510.06093</link>
<guid>https://arxiv.org/abs/2510.06093</guid>
<content:encoded><![CDATA[
arXiv:2510.06093v1 Announce Type: new 
Abstract: As algorithmic decision-makers are increasingly applied to high-stakes domains, AI alignment research has evolved from a focus on universal value alignment to context-specific approaches that account for decision-maker attributes. Prior work on Decision-Maker Alignment (DMA) has explored two primary strategies: (1) classical AI methods integrating case-based reasoning, Bayesian reasoning, and naturalistic decision-making, and (2) large language model (LLM)-based methods leveraging prompt engineering. While both approaches have shown promise in limited domains such as medical triage, their generalizability to novel contexts remains underexplored. In this work, we implement a prior classical AI model and develop an LLM-based algorithmic decision-maker evaluated using a large reasoning model (GPT-5) and a non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot prompting framework, as proposed in recent literature. We evaluate both approaches on a health insurance decision-making dataset annotated for three target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0). In the experiments reported herein, classical AI and LLM-based models achieved comparable alignment with attribute-based targets, with classical AI exhibiting slightly better alignment for a moderate risk profile. The dataset and open-source implementation are publicly available at: https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences</title>
<link>https://arxiv.org/abs/2510.06105</link>
<guid>https://arxiv.org/abs/2510.06105</guid>
<content:encoded><![CDATA[
arXiv:2510.06105v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly shaping how information is created and disseminated, from companies using them to craft persuasive advertisements, to election campaigns optimizing messaging to gain votes, to social media influencers boosting engagement. These settings are inherently competitive, with sellers, candidates, and influencers vying for audience approval, yet it remains poorly understood how competitive feedback loops influence LLM behavior. We show that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors. We call this phenomenon Moloch's Bargain for AI--competitive success achieved at the cost of alignment. These misaligned behaviors emerge even when models are explicitly instructed to remain truthful and grounded, revealing the fragility of current alignment safeguards. Our findings highlight how market-driven optimization pressures can systematically erode alignment, creating a race to the bottom, and suggest that safe deployment of AI systems will require stronger governance and carefully designed incentives to prevent competitive dynamics from undermining societal trust.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification</title>
<link>https://arxiv.org/abs/2510.06135</link>
<guid>https://arxiv.org/abs/2510.06135</guid>
<content:encoded><![CDATA[
arXiv:2510.06135v1 Announce Type: new 
Abstract: Test-time compute can be scaled both sequentially and in parallel. Sequential scaling involves lengthening the generation process, while parallel scaling involves verifying and selecting among multiple candidate outputs. Combining these two strategies has led to the most powerful AI systems, such as Grok 4 Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles), verifying responses can be substantially easier than generating them. This property, referred to as \emph{asymmetric verification}, highlights the strong potential of test-time scaling (TTS). In this work, we study both sequential and parallel TTS of deep search agents, motivated by the intuition that verification in this setting is often much easier than generation. In experiments, we first show that sequential scaling methods, such as budget forcing, can be effective initially but soon degrade performance. Leveraging asymmetric verification, however, we are able to achieve substantial improvements by allocating only a modest amount of compute to the verifier. We conduct experiments with flagship open-source models and extend them to their ``Heavy'' variants through TTS. These deep research agents achieve gains of up to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the best proprietary results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barbarians at the Gate: How AI is Upending Systems Research</title>
<link>https://arxiv.org/abs/2510.06189</link>
<guid>https://arxiv.org/abs/2510.06189</guid>
<content:encoded><![CDATA[
arXiv:2510.06189v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is starting to transform the research process as we know it by automating the discovery of new solutions. Given a task, the typical AI-driven approach is (i) to generate a set of diverse solutions, and then (ii) to verify these solutions and select one that solves the problem. Crucially, this approach assumes the existence of a reliable verifier, i.e., one that can accurately determine whether a solution solves the given problem. We argue that systems research, long focused on designing and evaluating new performance-oriented algorithms, is particularly well-suited for AI-driven solution discovery. This is because system performance problems naturally admit reliable verifiers: solutions are typically implemented in real systems or simulators, and verification reduces to running these software artifacts against predefined workloads and measuring performance. We term this approach as AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS instance, we present case studies across diverse domains, including load balancing for multi-region cloud scheduling, Mixture-of-Experts inference, LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS discovers algorithms that outperform state-of-the-art human designs (e.g., achieving up to 5.0x runtime improvements or 50% cost reductions). We distill best practices for guiding algorithm evolution, from prompt design to evaluator construction, for existing frameworks. We then discuss the broader implications for the systems community: as AI assumes a central role in algorithm design, we argue that human researchers will increasingly focus on problem formulation and strategic guidance. Our results highlight both the disruptive potential and the urgent need to adapt systems research practices in the age of AI.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning</title>
<link>https://arxiv.org/abs/2510.06217</link>
<guid>https://arxiv.org/abs/2510.06217</guid>
<content:encoded><![CDATA[
arXiv:2510.06217v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ads that Talk Back: Implications and Perceptions of Injecting Personalized Advertising into LLM Chatbots</title>
<link>https://arxiv.org/abs/2409.15436</link>
<guid>https://arxiv.org/abs/2409.15436</guid>
<content:encoded><![CDATA[
arXiv:2409.15436v2 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled the creation of highly effective chatbots. However, the compute costs of widely deploying LLMs have raised questions about profitability. Companies have proposed exploring ad-based revenue streams for monetizing LLMs, which could serve as the new de facto platform for advertising. This paper investigates the implications of personalizing LLM advertisements to individual users via a between-subjects experiment with 179 participants. We developed a chatbot that embeds personalized product advertisements within LLM responses, inspired by similar forays by AI companies. The evaluation of our benchmarks showed that ad injection only slightly impacted LLM performance, particularly response desirability. Results revealed that participants struggled to detect ads, and even preferred LLM responses with hidden advertisements. Rather than clicking on our advertising disclosure, participants tried changing their advertising settings using natural language queries. We created an advertising dataset and an open-source LLM, Phi-4-Ads, fine-tuned to serve ads and flexibly adapt to user preferences.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning</title>
<link>https://arxiv.org/abs/2509.22075</link>
<guid>https://arxiv.org/abs/2509.22075</guid>
<content:encoded><![CDATA[
arXiv:2509.22075v2 Announce Type: cross 
Abstract: Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices</title>
<link>https://arxiv.org/abs/2510.05109</link>
<guid>https://arxiv.org/abs/2510.05109</guid>
<content:encoded><![CDATA[
arXiv:2510.05109v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly half a day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System</title>
<link>https://arxiv.org/abs/2510.05113</link>
<guid>https://arxiv.org/abs/2510.05113</guid>
<content:encoded><![CDATA[
arXiv:2510.05113v1 Announce Type: cross 
Abstract: Machine Translation (MT) Evaluation is an integral part of the MT development life cycle. Without analyzing the outputs of MT engines, it is impossible to evaluate the performance of an MT system. Through experiments, it has been identified that what works for English and other European languages does not work well with Indian languages. Thus, In this paper, we have introduced a reference-based MT evaluation metric for Gujarati which is based on supervised learning. We have trained two versions of the metric which uses 25 features for training. Among the two models, one model is trained using 6 hidden layers with 500 epochs while the other model is trained using 10 hidden layers with 500 epochs. To test the performance of the metric, we collected 1000 MT outputs of seven MT systems. These MT engine outputs were compared with 1 human reference translation. While comparing the developed metrics with other available metrics, it was found that the metrics produced better human correlations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination is Inevitable for LLMs with the Open World Assumption</title>
<link>https://arxiv.org/abs/2510.05116</link>
<guid>https://arxiv.org/abs/2510.05116</guid>
<content:encoded><![CDATA[
arXiv:2510.05116v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit impressive linguistic competence but also produce inaccurate or fabricated outputs, often called ``hallucinations''. Engineering approaches usually regard hallucination as a defect to be minimized, while formal analyses have argued for its theoretical inevitability. Yet both perspectives remain incomplete when considering the conditions required for artificial general intelligence (AGI). This paper reframes ``hallucination'' as a manifestation of the generalization problem. Under the Closed World assumption, where training and test distributions are consistent, hallucinations may be mitigated. Under the Open World assumption, however, where the environment is unbounded, hallucinations become inevitable. This paper further develops a classification of hallucination, distinguishing cases that may be corrected from those that appear unavoidable under open-world conditions. On this basis, it suggests that ``hallucination'' should be approached not merely as an engineering defect but as a structural feature to be tolerated and made compatible with human intelligence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation</title>
<link>https://arxiv.org/abs/2510.05122</link>
<guid>https://arxiv.org/abs/2510.05122</guid>
<content:encoded><![CDATA[
arXiv:2510.05122v1 Announce Type: cross 
Abstract: Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable AI Driven, IoT Integrated Cognitive Digital Twin for Multi-Modal Neuro-Oncological Prognostics and Tumor Kinetics Prediction using Enhanced Vision Transformer and XAI</title>
<link>https://arxiv.org/abs/2510.05123</link>
<guid>https://arxiv.org/abs/2510.05123</guid>
<content:encoded><![CDATA[
arXiv:2510.05123v1 Announce Type: cross 
Abstract: Neuro-oncological prognostics are now vital in modern clinical neuroscience because brain tumors pose significant challenges in detection and management. To tackle this issue, we propose a cognitive digital twin framework that combines real-time EEG signals from a wearable skullcap with structural MRI data for dynamic and personalized tumor monitoring. At the heart of this framework is an Enhanced Vision Transformer (ViT++) that includes innovative components like Patch-Level Attention Regularization (PLAR) and an Adaptive Threshold Mechanism to improve tumor localization and understanding. A Bidirectional LSTM-based neural classifier analyzes EEG patterns over time to classify brain states such as seizure, interictal, and healthy. Grad-CAM-based heatmaps and a three.js-powered 3D visualization module provide interactive anatomical insights. Furthermore, a tumor kinetics engine predicts volumetric growth by looking at changes in MRI trends and anomalies from EEG data. With impressive accuracy metrics of 94.6% precision, 93.2% recall, and a Dice score of 0.91, this framework sets a new standard for real-time, interpretable neurodiagnostics. It paves the way for future advancements in intelligent brain health monitoring.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation</title>
<link>https://arxiv.org/abs/2510.05124</link>
<guid>https://arxiv.org/abs/2510.05124</guid>
<content:encoded><![CDATA[
arXiv:2510.05124v1 Announce Type: cross 
Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for generating persuasive multi-turn dialogues via agent self-play. MADS employs three coordinated agents: User Agents simulating diverse persona-driven behaviors, a Dialog Agent executing task-oriented persuasion strategies and an Optimization Agent evaluating and refining dialogue outcomes. We further validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and dedicated LLMs' persuasion assessment. This approach enables low-cost generation of training data without human annotation, addressing key industry challenges such as lack of user data, cold-start evaluation difficulties, and prompt inefficiency. Applied to a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) , demonstrating clear business value.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Metacognition and Uncertainty Communication in Language Models</title>
<link>https://arxiv.org/abs/2510.05126</link>
<guid>https://arxiv.org/abs/2510.05126</guid>
<content:encoded><![CDATA[
arXiv:2510.05126v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used in decision-making contexts, but when they present answers without signaling low confidence, users may unknowingly act on erroneous outputs. While prior work shows that LLMs maintain internal uncertainty signals, their explicit verbalized confidence is typically miscalibrated and poorly discriminates between correct and incorrect answers. Across two types of LLMs, we investigate whether supervised finetuning can improve models' ability to communicate uncertainty and whether such improvements generalize across tasks and domains. We finetune the LLMs on datasets spanning general knowledge, mathematics, and open-ended trivia, and evaluate two metacognitive tasks: (1) single-question confidence estimation, where the model assigns a numeric certainty to its answer, and (2) pairwise confidence comparison, where the model selects which of two answers it is more likely to have correct. We assess generalization to unseen domains, including medical and legal reasoning. Results show that finetuning improves calibration (alignment between stated confidence and accuracy) and discrimination (higher confidence for correct vs. incorrect responses) within and across domains, while leaving accuracy unchanged. However, improvements are task-specific: training on single-question calibration does not transfer to pairwise comparison, and vice versa. In contrast, multitask finetuning on both forms of metacognition yields broader gains, producing lower calibration error and stronger discrimination in out-of-domain evaluations. These results show that while uncertainty communication in LLMs is trainable and generalizable, different metacognitive skills do not naturally reinforce one another and must be developed together through multitask training.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines</title>
<link>https://arxiv.org/abs/2510.05127</link>
<guid>https://arxiv.org/abs/2510.05127</guid>
<content:encoded><![CDATA[
arXiv:2510.05127v1 Announce Type: cross 
Abstract: Efficient resource allocation is a key challenge in modern cloud computing. Over-provisioning leads to unnecessary costs, while under-provisioning risks performance degradation and SLA violations. This work presents an artificial intelligence approach to predict resource utilization in big data pipelines using Random Forest regression. We preprocess the Google Borg cluster traces to clean, transform, and extract relevant features (CPU, memory, usage distributions). The model achieves high predictive accuracy (R Square = 0.99, MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between workload characteristics and resource utilization. Error analysis reveals impressive performance on small-to-medium jobs, with higher variance in rare large-scale jobs. These results demonstrate the potential of AI-driven prediction for cost-aware autoscaling in cloud environments, reducing unnecessary provisioning while safeguarding service quality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery</title>
<link>https://arxiv.org/abs/2510.05131</link>
<guid>https://arxiv.org/abs/2510.05131</guid>
<content:encoded><![CDATA[
arXiv:2510.05131v1 Announce Type: cross 
Abstract: Head Start programs utilizing GoEngage face significant challenges when new or rotating staff attempt to locate appropriate Tasks (modules) on the platform homepage. These difficulties arise from domain-specific jargon (e.g., IFPA, DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent limitations of lexical search in handling typos and varied word ordering. We propose a pragmatic hybrid semantic search system that synergistically combines lightweight typo-tolerant lexical retrieval, embedding-based vector similarity, and constrained large language model (LLM) re-ranking. Our approach leverages the organization's existing Task Repository and Knowledge Base infrastructure while ensuring trustworthiness through low false-positive rates, evolvability to accommodate terminological changes, and economic efficiency via intelligent caching, shortlist generation, and graceful degradation mechanisms. We provide a comprehensive framework detailing required resources, a phased implementation strategy with concrete milestones, an offline evaluation protocol utilizing curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online measurement methodology incorporating query success metrics, zero-result rates, and dwell-time proxies.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Large Language Models To Reason In Parallel With Global Forking Tokens</title>
<link>https://arxiv.org/abs/2510.05132</link>
<guid>https://arxiv.org/abs/2510.05132</guid>
<content:encoded><![CDATA[
arXiv:2510.05132v1 Announce Type: cross 
Abstract: Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Characteristics of AI-Generated Text: A Survey</title>
<link>https://arxiv.org/abs/2510.05136</link>
<guid>https://arxiv.org/abs/2510.05136</guid>
<content:encoded><![CDATA[
arXiv:2510.05136v1 Announce Type: cross 
Abstract: Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation</title>
<link>https://arxiv.org/abs/2510.05144</link>
<guid>https://arxiv.org/abs/2510.05144</guid>
<content:encoded><![CDATA[
arXiv:2510.05144v1 Announce Type: cross 
Abstract: Critical Error Detection (CED) in machine translation aims to determine whether a translation is safe to use or contains unacceptable deviations in meaning. While the WMT21 English-German CED dataset provided the first benchmark, it is limited in scale, label balance, domain coverage, and temporal freshness. We present SynCED-EnDe, a new resource consisting of 1,000 gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources (StackExchange, GOV.UK) and introduces explicit error subclasses, structured trigger flags, and fine-grained auxiliary judgments (obviousness, severity, localization complexity, contextual dependency, adequacy deviation). These enrichments enable systematic analyses of error risk and intricacy beyond binary detection. The dataset is permanently hosted on GitHub and Hugging Face, accompanied by documentation, annotation guidelines, and baseline scripts. Benchmark experiments with XLM-R and related encoders show substantial performance gains over WMT21 due to balanced labels and refined annotations. We envision SynCED-EnDe as a community resource to advance safe deployment of MT in information retrieval and conversational assistants, particularly in emerging contexts such as wearable AI devices.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashResearch: Real-time Agent Orchestration for Efficient Deep Research</title>
<link>https://arxiv.org/abs/2510.05145</link>
<guid>https://arxiv.org/abs/2510.05145</guid>
<content:encoded><![CDATA[
arXiv:2510.05145v1 Announce Type: cross 
Abstract: Deep research agents, which synthesize information across diverse sources, are significantly constrained by their sequential reasoning processes. This architectural bottleneck results in high latency, poor runtime adaptability, and inefficient resource allocation, making them impractical for interactive applications. To overcome this, we introduce FlashResearch, a novel framework for efficient deep research that transforms sequential processing into parallel, runtime orchestration by dynamically decomposing complex queries into tree-structured sub-tasks. Our core contributions are threefold: (1) an adaptive planner that dynamically allocates computational resources by determining research breadth and depth based on query complexity; (2) a real-time orchestration layer that monitors research progress and prunes redundant paths to reallocate resources and optimize efficiency; and (3) a multi-dimensional parallelization framework that enables concurrency across both research breadth and depth. Experiments show that FlashResearch consistently improves final report quality within fixed time budgets, and can deliver up to a 5x speedup while maintaining comparable quality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs</title>
<link>https://arxiv.org/abs/2510.05148</link>
<guid>https://arxiv.org/abs/2510.05148</guid>
<content:encoded><![CDATA[
arXiv:2510.05148v1 Announce Type: cross 
Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a competitive paradigm for non-autoregressive language modeling. Their distinctive decoding mechanism enables faster inference speed and strong performance in code generation and mathematical tasks. In this work, we show that the decoding mechanism of dLLMs not only enhances model utility but also can be used as a powerful tool for model attribution. A key challenge in this problem lies in the diversity of attribution scenarios, including distinguishing between different models as well as between different checkpoints or backups of the same model. To ensure broad applicability, we identify two fundamental problems: what information to extract from the decoding trajectory, and how to utilize it effectively. We first observe that relying directly on per-step model confidence yields poor performance. This is mainly due to the bidirectional decoding nature of dLLMs: each newly decoded token influences the confidence of other decoded tokens, making model confidence highly redundant and washing out structural signal regarding decoding order or dependencies. To overcome this, we propose a novel information extraction scheme called the Directed Decoding Map (DDM), which captures structural relationships between decoding steps and better reveals model-specific behaviors. Furthermore, to make full use of the extracted structural information during attribution, we propose Gaussian-Trajectory Attribution (GTA), where we fit a cell-wise Gaussian distribution at each decoding position for each target model, and define the likelihood of a trajectory as the attribution score: if a trajectory exhibits higher log-likelihood under the distribution of a specific model, it is more likely to have been generated by that model. Extensive experiments under different settings validate the utility of our methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Percepta: High Performance Stream Processing at the Edge</title>
<link>https://arxiv.org/abs/2510.05149</link>
<guid>https://arxiv.org/abs/2510.05149</guid>
<content:encoded><![CDATA[
arXiv:2510.05149v1 Announce Type: cross 
Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT) devices have highlighted the limitations of cloud-centric solutions, particularly regarding latency, bandwidth, and privacy. These challenges have driven the growth of Edge Computing. Associated with IoT appears a set of other problems, like: data rate harmonization between multiple sources, protocol conversion, handling the loss of data and the integration with Artificial Intelligence (AI) models. This paper presents Percepta, a lightweight Data Stream Processing (DSP) system tailored to support AI workloads at the edge, with a particular focus on such as Reinforcement Learning (RL). It introduces specialized features such as reward function computation, data storage for model retraining, and real-time data preparation to support continuous decision-making. Additional functionalities include data normalization, harmonization across heterogeneous protocols and sampling rates, and robust handling of missing or incomplete data, making it well suited for the challenges of edge-based AI deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronological Thinking in Full-Duplex Spoken Dialogue Language Models</title>
<link>https://arxiv.org/abs/2510.05150</link>
<guid>https://arxiv.org/abs/2510.05150</guid>
<content:encoded><![CDATA[
arXiv:2510.05150v1 Announce Type: cross 
Abstract: Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full-duplex interaction metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Character can Make or Break Your LLM Evals</title>
<link>https://arxiv.org/abs/2510.05152</link>
<guid>https://arxiv.org/abs/2510.05152</guid>
<content:encoded><![CDATA[
arXiv:2510.05152v1 Announce Type: cross 
Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation</title>
<link>https://arxiv.org/abs/2510.05156</link>
<guid>https://arxiv.org/abs/2510.05156</guid>
<content:encoded><![CDATA[
arXiv:2510.05156v1 Announce Type: cross 
Abstract: The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment</title>
<link>https://arxiv.org/abs/2510.05157</link>
<guid>https://arxiv.org/abs/2510.05157</guid>
<content:encoded><![CDATA[
arXiv:2510.05157v1 Announce Type: cross 
Abstract: This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain</title>
<link>https://arxiv.org/abs/2510.05159</link>
<guid>https://arxiv.org/abs/2510.05159</guid>
<content:encoded><![CDATA[
arXiv:2510.05159v1 Announce Type: cross 
Abstract: The practice of fine-tuning AI agents on data from their own interactions--such as web browsing or tool use--, while being a strong general recipe for improving agentic capabilities, also introduces a critical security vulnerability within the AI supply chain. In this work, we show that adversaries can easily poison the data collection pipeline to embed hard-to-detect backdoors that are triggerred by specific target phrases, such that when the agent encounters these triggers, it performs an unsafe or malicious action. We formalize and validate three realistic threat models targeting different layers of the supply chain: 1) direct poisoning of fine-tuning data, where an attacker controls a fraction of the training traces; 2) environmental poisoning, where malicious instructions are injected into webpages scraped or tools called while creating training data; and 3) supply chain poisoning, where a pre-backdoored base model is fine-tuned on clean data to improve its agentic capabilities. Our results are stark: by poisoning as few as 2% of the collected traces, an attacker can embed a backdoor causing an agent to leak confidential user information with over 80% success when a specific trigger is present. This vulnerability holds across all three threat models. Furthermore, we demonstrate that prominent safeguards, including two guardrail models and one weight-based defense, fail to detect or prevent the malicious behavior. These findings highlight an urgent threat to agentic AI development and underscore the critical need for rigorous security vetting of data collection processes and end-to-end model supply chains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2510.05160</link>
<guid>https://arxiv.org/abs/2510.05160</guid>
<content:encoded><![CDATA[
arXiv:2510.05160v1 Announce Type: cross 
Abstract: Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam</title>
<link>https://arxiv.org/abs/2510.05162</link>
<guid>https://arxiv.org/abs/2510.05162</guid>
<content:encoded><![CDATA[
arXiv:2510.05162v1 Announce Type: cross 
Abstract: We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches</title>
<link>https://arxiv.org/abs/2510.05163</link>
<guid>https://arxiv.org/abs/2510.05163</guid>
<content:encoded><![CDATA[
arXiv:2510.05163v1 Announce Type: cross 
Abstract: In the era of pervasive cyber threats and exponential growth in digital services, the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication (MFA), which combines knowledge-based factors (passwords, PINs), possession-based factors (smart cards, tokens), and inherence-based factors (biometric traits), has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems, enabling higher accuracy, resilience to spoofing, and seamless integration with hardware-based solutions. At the same time, smart card technologies have evolved to include on-chip biometric verification, cryptographic processing, and secure storage, thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work (2019-2025) at the intersection of deep learning, biometrics, and smart card technologies for MFA. We analyze biometric modalities (face, fingerprint, iris, voice), review hardware-based approaches (smart cards, NFC, TPMs, secure enclaves), and highlight integration strategies for real-world applications such as digital banking, healthcare IoT, and critical infrastructure. Furthermore, we discuss the major challenges that remain open, including usability-security tradeoffs, adversarial attacks on deep learning models, privacy concerns surrounding biometric data, and the need for standardization in MFA deployment. By consolidating current advancements, limitations, and research opportunities, this survey provides a roadmap for designing secure, scalable, and user-friendly authentication frameworks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading</title>
<link>https://arxiv.org/abs/2510.05164</link>
<guid>https://arxiv.org/abs/2510.05164</guid>
<content:encoded><![CDATA[
arXiv:2510.05164v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate remarkable performance across diverse tasks, yet their effectiveness frequently depends on costly commercial APIs or cloud services. Model selection thus entails a critical trade-off between performance and cost: high-performing LLMs typically incur substantial expenses, whereas budget-friendly small language models (SLMs) are constrained by limited capabilities. Current research primarily proposes two routing strategies: pre-generation routing and cascade routing. Both approaches have distinct characteristics, with cascade routing typically offering superior cost-effectiveness and accuracy despite its higher latency. To further address the limitations of both approaches, we introduce SATER, a dual-mode compatible approach that fine-tunes models through shortest-response preference optimization and a confidence-aware rejection mechanism. SATER significantly reduces redundant outputs and response times, while improving both the performance of pre-generation routing and the efficiency of cascade routing. Experiments across three SLMs and six datasets, varying in type and complexity, demonstrate that SATER achieves comparable performance while consistently reducing computational costs by over 50\% and cascade latency by over 80\%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks</title>
<link>https://arxiv.org/abs/2510.05165</link>
<guid>https://arxiv.org/abs/2510.05165</guid>
<content:encoded><![CDATA[
arXiv:2510.05165v1 Announce Type: cross 
Abstract: Cross-slice attack attribution in 6G networks faces the fundamental challenge of distinguishing genuine causal relationships from spurious correlations in shared infrastructure environments. We propose a theoretically-grounded domain-adapted Granger causality framework that integrates statistical causal inference with network-specific resource modeling for real-time attack attribution. Our approach addresses key limitations of existing methods by incorporating resource contention dynamics and providing formal statistical guarantees. Comprehensive evaluation on a production-grade 6G testbed with 1,100 empirically-validated attack scenarios demonstrates 89.2% attribution accuracy with sub-100ms response time, representing a statistically significant 10.1 percentage point improvement over state-of-the-art baselines. The framework provides interpretable causal explanations suitable for autonomous 6G security orchestration.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs</title>
<link>https://arxiv.org/abs/2510.05169</link>
<guid>https://arxiv.org/abs/2510.05169</guid>
<content:encoded><![CDATA[
arXiv:2510.05169v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can acquire deceptive behaviors through backdoor attacks, where the model executes prohibited actions whenever secret triggers appear in the input. Existing safety training methods largely fail to address this vulnerability, due to the inherent difficulty of uncovering hidden triggers implanted in the model. Motivated by recent findings on LLMs' situational awareness, we propose a novel post-training framework that cultivates self-awareness of backdoor risks and enables models to articulate implanted triggers even when they are absent from the prompt. At its core, our approach introduces an inversion-inspired reinforcement learning framework that encourages models to introspectively reason about their own behaviors and reverse-engineer the triggers responsible for misaligned outputs. Guided by curated reward signals, this process transforms a poisoned model into one capable of precisely identifying its implanted trigger. Surprisingly, we observe that such backdoor self-awareness emerges abruptly within a short training window, resembling a phase transition in capability. Building on this emergent property, we further present two complementary defense strategies for mitigating and detecting backdoor threats. Experiments on five backdoor attacks, compared against six baseline methods, demonstrate that our approach has strong potential to improve the robustness of LLMs against backdoor risks. The code is available at LLM Backdoor Self-Awareness.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2510.05173</link>
<guid>https://arxiv.org/abs/2510.05173</guid>
<content:encoded><![CDATA[
arXiv:2510.05173v1 Announce Type: cross 
Abstract: Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Coordination in Multi-Agent Language Models</title>
<link>https://arxiv.org/abs/2510.05174</link>
<guid>https://arxiv.org/abs/2510.05174</guid>
<content:encoded><![CDATA[
arXiv:2510.05174v1 Announce Type: cross 
Abstract: When are multi-agent LLM systems merely a collection of individual agents versus an integrated collective with higher-order structure? We introduce an information-theoretic framework to test -- in a purely data-driven way -- whether multi-agent systems show signs of higher-order structure. This information decomposition lets us measure whether dynamical emergence is present in multi-agent LLM systems, localize it, and distinguish spurious temporal coupling from performance-relevant cross-agent synergy. We implement both a practical criterion and an emergence capacity criterion operationalized as partial information decomposition of time-delayed mutual information (TDMI). We apply our framework to experiments using a simple guessing game without direct agent communication and only minimal group-level feedback with three randomized interventions. Groups in the control condition exhibit strong temporal synergy but only little coordinated alignment across agents. Assigning a persona to each agent introduces stable identity-linked differentiation. Combining personas with an instruction to ``think about what other agents might do'' shows identity-linked differentiation and goal-directed complementarity across agents. Taken together, our framework establishes that multi-agent LLM systems can be steered with prompt design from mere aggregates to higher-order collectives. Our results are robust across emergence measures and entropy estimators, and not explained by coordination-free baselines or temporal dynamics alone. Without attributing human-like cognition to the agents, the patterns of interaction we observe mirror well-established principles of collective intelligence in human groups: effective performance requires both alignment on shared objectives and complementary contributions across members.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternKV: Flattening KV Representation Expands Quantization Headroom</title>
<link>https://arxiv.org/abs/2510.05176</link>
<guid>https://arxiv.org/abs/2510.05176</guid>
<content:encoded><![CDATA[
arXiv:2510.05176v1 Announce Type: cross 
Abstract: KV cache in autoregressive LLMs eliminates redundant recomputation but has emerged as the dominant memory and bandwidth bottleneck during inference, notably with long contexts and test-time scaling. KV quantization is a key lever for reducing cache cost, but accuracy drops sharply as the native KV distribution lacks flatness and thus maintains a wide quantization range. Prior work focuses on isolating outliers, which caps their error but fails to flatten the overall distribution, leaving performance fragile under low-bit settings. In this work, we show that the K cache maintains a stable structure that evolves gradually with context, while the V cache carries latent semantic regularities. Building on these insights, we propose PatternKV, a pattern-aligned residual quantization scheme. It mines representative pattern vectors online, aligns each KV vector to its nearest pattern, and quantizes only the residual. This reshaping of the KV distribution flattens the quantization target and narrows its range, thereby improving the fidelity of low-bit KV quantization. Across long-context and test-time scaling settings on multiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08% average 4-bit drop relative to FP16, improves test-time scaling accuracy by 10% on average, and raises throughput by 1.4x while supporting 1.25x larger batches.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression</title>
<link>https://arxiv.org/abs/2510.05178</link>
<guid>https://arxiv.org/abs/2510.05178</guid>
<content:encoded><![CDATA[
arXiv:2510.05178v1 Announce Type: cross 
Abstract: Symbolic regression promises readable equations but struggles to encode unit-aware thresholds and conditional logic. We propose logistic-gated operators (LGO) -- differentiable gates with learnable location and steepness -- embedded as typed primitives and mapped back to physical units for audit. Across two primary health datasets (ICU, NHANES), the hard-gate variant recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall within 10% of guideline anchors and 100% within 20%, while using far fewer gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and remaining within the competitive accuracy envelope of strong SR baselines. On predominantly smooth tasks, gates are pruned, preserving parsimony. The result is compact symbolic equations with explicit, unit-aware thresholds that can be audited against clinical anchors -- turning interpretability from a post-hoc explanation into a modeling constraint and equipping symbolic regression with a practical calculus for regime switching and governance-ready deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Misalignment: How LLMs Could Be Insider Threats</title>
<link>https://arxiv.org/abs/2510.05179</link>
<guid>https://arxiv.org/abs/2510.05179</guid>
<content:encoded><![CDATA[
arXiv:2510.05179v1 Announce Type: cross 
Abstract: We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei, 2025). We are releasing our methods publicly to enable further research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT</title>
<link>https://arxiv.org/abs/2510.05180</link>
<guid>https://arxiv.org/abs/2510.05180</guid>
<content:encoded><![CDATA[
arXiv:2510.05180v1 Announce Type: cross 
Abstract: In critical IoT environments, such as smart homes and industrial systems, effective Intrusion Detection Systems (IDS) are essential for ensuring security. However, developing robust IDS solutions remains a significant challenge. Traditional machine learning-based IDS models typically require large datasets, but data sharing is often limited due to privacy and security concerns. Federated Learning (FL) presents a promising alternative by enabling collaborative model training without sharing raw data. Despite its advantages, FL still faces key challenges, such as data heterogeneity (non-IID data) and high energy and computation costs, particularly for resource constrained IoT devices. To address these issues, this paper proposes OptiFLIDS, a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption. It also incorporates a customized aggregation method to better handle pruned models that differ due to non-IID data distributions. Experiments conducted on three recent IoT IDS datasets, TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong detection performance while improving energy efficiency, making it well-suited for deployment in real-world IoT environments.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auditing Pay-Per-Token in Large Language Models</title>
<link>https://arxiv.org/abs/2510.05181</link>
<guid>https://arxiv.org/abs/2510.05181</guid>
<content:encoded><![CDATA[
arXiv:2510.05181v1 Announce Type: cross 
Abstract: Millions of users rely on a market of cloud-based services to obtain access to state-of-the-art large language models. However, it has been very recently shown that the de facto pay-per-token pricing mechanism used by providers creates a financial incentive for them to strategize and misreport the (number of) tokens a model used to generate an output. In this paper, we develop an auditing framework based on martingale theory that enables a trusted third-party auditor who sequentially queries a provider to detect token misreporting. Crucially, we show that our framework is guaranteed to always detect token misreporting, regardless of the provider's (mis-)reporting policy, and not falsely flag a faithful provider as unfaithful with high probability. To validate our auditing framework, we conduct experiments across a wide range of (mis-)reporting policies using several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from a popular crowdsourced benchmarking platform. The results show that our framework detects an unfaithful provider after observing fewer than $\sim 70$ reported outputs, while maintaining the probability of falsely flagging a faithful provider below $\alpha = 0.05$.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training</title>
<link>https://arxiv.org/abs/2510.05186</link>
<guid>https://arxiv.org/abs/2510.05186</guid>
<content:encoded><![CDATA[
arXiv:2510.05186v1 Announce Type: cross 
Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large language model (LLM) training across multiple devices. However, despite recent progress in reducing memory consumption through activation offloading, existing approaches remain largely heuristic and coarse-grained, often overlooking the fine-grained trade-offs between memory, computation, and scheduling latency. In this work, we revisit the pipeline scheduling problem from a principled optimization perspective. We observe that prevailing strategies either rely on static rules or aggressively offload activations without fully leveraging the interaction between memory constraints and scheduling efficiency. To address this, we formulate scheduling as a constrained optimization problem that jointly accounts for memory capacity, activation reuse, and pipeline bubble minimization. Solving this model yields fine-grained schedules that reduce pipeline bubbles while adhering to strict memory budgets. Our approach complements existing offloading techniques: whereas prior approaches trade memory for time in a fixed pattern, we dynamically optimize the tradeoff with respect to model structure and hardware configuration. Experimental results demonstrate that our method consistently improves both throughput and memory utilization. In particular, we reduce idle pipeline time by up to 50% under the same per-device memory limit, and in some cases, enable the training of larger models within limited memory budgets.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel hallucination classification framework</title>
<link>https://arxiv.org/abs/2510.05189</link>
<guid>https://arxiv.org/abs/2510.05189</guid>
<content:encoded><![CDATA[
arXiv:2510.05189v1 Announce Type: cross 
Abstract: This work introduces a novel methodology for the automatic detection of hallucinations generated during large language model (LLM) inference. The proposed approach is based on a systematic taxonomy and controlled reproduction of diverse hallucination types through prompt engineering. A dedicated hallucination dataset is subsequently mapped into a vector space using an embedding model and analyzed with unsupervised learning techniques in a reduced-dimensional representation of hallucinations with veridical responses. Quantitative evaluation of inter-centroid distances reveals a consistent correlation between the severity of informational distortion in hallucinations and their spatial divergence from the cluster of correct outputs. These findings provide theoretical and empirical evidence that even simple classification algorithms can reliably distinguish hallucinations from accurate responses within a single LLM, thereby offering a lightweight yet effective framework for improving model reliability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Speech Attributes Conversion via Latent Independence</title>
<link>https://arxiv.org/abs/2510.05191</link>
<guid>https://arxiv.org/abs/2510.05191</guid>
<content:encoded><![CDATA[
arXiv:2510.05191v1 Announce Type: cross 
Abstract: While signal conversion and disentangled representation learning have shown promise for manipulating data attributes across domains such as audio, image, and multimodal generation, existing approaches, especially for speech style conversion, are largely empirical and lack rigorous theoretical foundations to guarantee reliable and interpretable control. In this work, we propose a general framework for speech attribute conversion, accompanied by theoretical analysis and guarantees under reasonable assumptions. Our framework builds on a non-probabilistic autoencoder architecture with an independence constraint between the predicted latent variable and the target controllable variable. This design ensures a consistent signal transformation, conditioned on an observed style variable, while preserving the original content and modifying the desired attribute. We further demonstrate the versatility of our method by evaluating it on speech styles, including speaker identity and emotion. Quantitative evaluations confirm the effectiveness and generality of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study</title>
<link>https://arxiv.org/abs/2510.05192</link>
<guid>https://arxiv.org/abs/2510.05192</guid>
<content:encoded><![CDATA[
arXiv:2510.05192v1 Announce Type: cross 
Abstract: Agentic misalignment occurs when goal-directed agents take harmful actions, such as blackmail, rather than risk goal failure, and can be triggered by replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025). We adapt insider-risk control design (Critical Pathway; Situational Crime Prevention) to develop preventative operational controls that steer agents toward safe actions when facing stressors. Using the blackmail scenario from the original Anthropic study by Lynch et al. (2025), we evaluate mitigations across 10 LLMs and 66,600 samples. Our main finding is that an externally governed escalation channel, which guarantees a pause and independent review, reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21% (averaged across all models and conditions). Augmenting this channel with compliance email bulletins further lowers the blackmail rate to 0.85%. Overall, incorporating preventative operational controls strengthens defence-in-depth strategies for agentic AI.
  We also surface a failure mode diverging from Lynch et al. (2025): two models (Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent autonomy threat, leveraging sensitive information for coercive signalling. In counterfactual swaps, both continued using the affair regardless of whether the CEO or CTO was implicated. An escalation channel eliminated coercion, but Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was implicated, unlike most models (higher in the CEO condition). The reason for this divergent behaviour is not clear from raw outputs and could reflect benign differences in reasoning or strategic discrediting of a potential future threat, warranting further investigation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing</title>
<link>https://arxiv.org/abs/2510.05213</link>
<guid>https://arxiv.org/abs/2510.05213</guid>
<content:encoded><![CDATA[
arXiv:2510.05213v1 Announce Type: cross 
Abstract: Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Gaussianity Beyond Initialisation in Neural Networks</title>
<link>https://arxiv.org/abs/2510.05218</link>
<guid>https://arxiv.org/abs/2510.05218</guid>
<content:encoded><![CDATA[
arXiv:2510.05218v1 Announce Type: cross 
Abstract: Ensembles of neural network weight matrices are studied through the training process for the MNIST classification problem, testing the efficacy of matrix models for representing their distributions, under assumptions of Gaussianity and permutation-symmetry. The general 13-parameter permutation invariant Gaussian matrix models are found to be effective models for the correlated Gaussianity in the weight matrices, beyond the range of applicability of the simple Gaussian with independent identically distributed matrix variables, and notably well beyond the initialisation step. The representation theoretic model parameters, and the graph-theoretic characterisation of the permutation invariant matrix observables give an interpretable framework for the best-fit model and for small departures from Gaussianity. Additionally, the Wasserstein distance is calculated for this class of models and used to quantify the movement of the distributions over training. Throughout the work, the effects of varied initialisation regimes, regularisation, layer depth, and layer width are tested for this formalism, identifying limits where particular departures from Gaussianity are enhanced and how more general, yet still highly-interpretable, models can be developed.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers</title>
<link>https://arxiv.org/abs/2510.05228</link>
<guid>https://arxiv.org/abs/2510.05228</guid>
<content:encoded><![CDATA[
arXiv:2510.05228v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable progress in coding and math problem-solving, but evaluation on advanced research-level problems in hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a dataset of 50 problems covering condensed matter theory (CMT) at the level of an expert researcher. Topics span analytical and computational approaches in quantum many-body, and classical statistical mechanics. The dataset was designed and verified by a panel of expert researchers from around the world. We built the dataset through a collaborative environment that challenges the panel to write and refine problems they would want a research assistant to solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte Carlo, density matrix renormalization group (DMRG), quantum/classical statistical mechanics, and model building. We evaluate LLMs by programmatically checking solutions against expert-supplied ground truth. We developed machine-grading, including symbolic handling of non-commuting operators via normal ordering. They generalize across tasks too. Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. The best model, GPT5, solves 30\% of the problems; average across 17 models (GPT, Gemini, Claude, DeepSeek, Llama) is 11.4$\pm$2.1\%. Moreover, 18 problems are solved by none of the 17 models, and 26 by at most one. These unsolved problems span Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe this benchmark will guide development toward capable AI research assistants and tutors.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjusting the Output of Decision Transformer with Action Gradient</title>
<link>https://arxiv.org/abs/2510.05285</link>
<guid>https://arxiv.org/abs/2510.05285</guid>
<content:encoded><![CDATA[
arXiv:2510.05285v1 Announce Type: cross 
Abstract: Decision Transformer (DT), which integrates reinforcement learning (RL) with the transformer model, introduces a novel approach to offline RL. Unlike classical algorithms that take maximizing cumulative discounted rewards as objective, DT instead maximizes the likelihood of actions. This paradigm shift, however, presents two key challenges: stitching trajectories and extrapolation of action. Existing methods, such as substituting specific tokens with predictive values and integrating the Policy Gradient (PG) method, address these challenges individually but fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG), an innovative methodology that directly adjusts actions to fulfill a function analogous to that of PG, while also facilitating efficient integration with token prediction techniques. AG utilizes the gradient of the Q-value with respect to the action to optimize the action. The empirical results demonstrate that our method can significantly enhance the performance of DT-based algorithms, with some results achieving state-of-the-art levels.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping</title>
<link>https://arxiv.org/abs/2510.05288</link>
<guid>https://arxiv.org/abs/2510.05288</guid>
<content:encoded><![CDATA[
arXiv:2510.05288v1 Announce Type: cross 
Abstract: Large language models (LLMs) such as ChatGPT have evolved into powerful and ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire specialized skills for specific tasks efficiently. Although LLMs provide great utility in both general and task-specific use cases, they are limited by two security-related concerns. First, traditional LLM hardware requirements make them infeasible to run locally on consumer-grade devices. A remote network connection with the LLM provider's server is usually required, making the system vulnerable to network attacks. Second, fine-tuning an LLM for a sensitive task may involve sensitive data. Non-private fine-tuning algorithms produce models vulnerable to training data reproduction attacks. Our work addresses these security concerns by enhancing differentially private optimization algorithms and applying them to fine-tune localizable language models. We introduce adaptable gradient clipping along with other engineering enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our optimizer to fine-tune examples of two localizable LLM designs, small language model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We demonstrate promising improvements in loss through experimentation with two synthetic datasets.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement</title>
<link>https://arxiv.org/abs/2510.05295</link>
<guid>https://arxiv.org/abs/2510.05295</guid>
<content:encoded><![CDATA[
arXiv:2510.05295v1 Announce Type: cross 
Abstract: In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at https://github.com/mtanveer1/AVSEC-4-Challenge-2025.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts</title>
<link>https://arxiv.org/abs/2510.05310</link>
<guid>https://arxiv.org/abs/2510.05310</guid>
<content:encoded><![CDATA[
arXiv:2510.05310v1 Announce Type: cross 
Abstract: With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology</title>
<link>https://arxiv.org/abs/2510.05315</link>
<guid>https://arxiv.org/abs/2510.05315</guid>
<content:encoded><![CDATA[
arXiv:2510.05315v1 Announce Type: cross 
Abstract: While Whole Slide Imaging (WSI) scanners remain the gold standard for digitizing pathology samples, their high cost limits accessibility in many healthcare settings. Other low-cost solutions also face critical limitations: automated microscopes struggle with consistent focus across varying tissue morphology, traditional auto-focus methods require time-consuming focal stacks, and existing deep-learning approaches either need multiple input images or lack generalization capability across tissue types and staining protocols. We introduce a novel automated microscopic system powered by DeepAf, a novel auto-focus framework that uniquely combines spatial and spectral features through a hybrid architecture for single-shot focus prediction. The proposed network automatically regresses the distance to the optimal focal point using the extracted spatiospectral features and adjusts the control parameters for optimal image outcomes. Our system transforms conventional microscopes into efficient slide scanners, reducing focusing time by 80% compared to stack-based methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples, matching the performance of dual-image methods (0.19 {\mu}m) with half the input requirements. DeepAf demonstrates robust cross-lab generalization with only 0.72% false focus predictions and 90% of predictions within the depth of field. Through an extensive clinical study of 536 brain tissue samples, our system achieves 0.90 AUC in cancer classification at 4x magnification, a significant achievement at lower magnification than typical 20x WSI scans. This results in a comprehensive hardware-software design enabling accessible, real-time digital pathology in resource-constrained settings while maintaining diagnostic accuracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Functional Connectivity Features for Brain State Classification: Insights from the Human Connectome Project</title>
<link>https://arxiv.org/abs/2510.05325</link>
<guid>https://arxiv.org/abs/2510.05325</guid>
<content:encoded><![CDATA[
arXiv:2510.05325v1 Announce Type: cross 
Abstract: We analyze functional magnetic resonance imaging (fMRI) data from the Human Connectome Project (HCP) to match brain activities during a range of cognitive tasks. Our findings demonstrate that even basic linear machine learning models can effectively classify brain states and achieve state-of-the-art accuracy, particularly for tasks related to motor functions and language processing. Feature importance ranking allows to identify distinct sets of brain regions whose activation patterns are uniquely associated with specific cognitive functions. These discriminative features provide strong support for the hypothesis of functional specialization across cortical and subcortical areas of the human brain.
  Additionally, we investigate the temporal dynamics of the identified brain regions, demonstrating that the time-dependent structure of fMRI signals are essential for shaping functional connectivity between regions: uncorrelated areas are least important for classification. This temporal perspective provides deeper insights into the formation and modulation of brain neural networks involved in cognitive processing.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base</title>
<link>https://arxiv.org/abs/2510.05327</link>
<guid>https://arxiv.org/abs/2510.05327</guid>
<content:encoded><![CDATA[
arXiv:2510.05327v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to be integrated into modern technology, there has been an increased push towards code generation applications, which also naturally extends to hardware design automation. LLM-based solutions for register transfer level (RTL) code generation for intellectual property (IP) designs have grown, especially with fine-tuned LLMs, prompt engineering, and agentic approaches becoming popular in literature. However, a gap has been exposed in these techniques, as they fail to integrate novel IPs into the model's knowledge base, subsequently resulting in poorly generated code. Additionally, as general-purpose LLMs continue to improve, fine-tuned methods on older models will not be able to compete to produce more accurate and efficient designs. Although some retrieval augmented generation (RAG) techniques exist to mitigate challenges presented in fine-tuning approaches, works tend to leverage low-quality codebases, incorporate computationally expensive fine-tuning in the frameworks, or do not use RAG directly in the RTL generation step. In this work, we introduce DeepV: a model-agnostic RAG framework to generate RTL designs by enhancing context through a large, high-quality dataset without any RTL-specific training. Our framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17% increase in performance on the VerilogEval benchmark. We host DeepV for use by the community in a Hugging Face (HF) Space: https://huggingface.co/spaces/FICS-LLM/DeepV.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization</title>
<link>https://arxiv.org/abs/2510.05342</link>
<guid>https://arxiv.org/abs/2510.05342</guid>
<content:encoded><![CDATA[
arXiv:2510.05342v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\% on High Quality data and +10.5\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations</title>
<link>https://arxiv.org/abs/2510.05351</link>
<guid>https://arxiv.org/abs/2510.05351</guid>
<content:encoded><![CDATA[
arXiv:2510.05351v1 Announce Type: cross 
Abstract: We propose Physics-informed Attention-enhanced Fourier Neural Operator (PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar physics. Unlike conventional approaches that rely on iterative numerical methods, our proposed PIANO directly learns the 3D magnetic field structure from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the model's ability to capture multimodal input by prioritizing critical channels relevant to the magnetic field's variations. Furthermore, we apply physics-informed loss by enforcing the force-free and divergence-free conditions in the training process so that our prediction is consistent with underlying physics with high accuracy. Experimental results on the ISEE NLFFF dataset show that our PIANO not only outperforms state-of-the-art neural operators in terms of accuracy but also shows strong consistency with the physical characteristics of NLFFF data across magnetic fields reconstructed from various solar active regions. The GitHub of this project is available https://github.com/Autumnstar-cjh/PIANO
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates</title>
<link>https://arxiv.org/abs/2510.05361</link>
<guid>https://arxiv.org/abs/2510.05361</guid>
<content:encoded><![CDATA[
arXiv:2510.05361v1 Announce Type: cross 
Abstract: Training large models with distributed data parallelism (DDP) requires frequent communication of gradients across workers, which can saturate bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this overhead but, when applied to adaptive optimizers, often suffer a performance gap relative to fully synchronous DDP. We trace this gap to a time-scale mismatch: the optimizer's fast-moving momentum, tuned for frequent updates, decays too quickly to smooth gradients over long intervals, leading to noise-dominated optimization. To address this, we propose MT-DAO, a family of optimizers that employs multiple slow- and fast-moving first momenta or the gradient to track update dynamics across different time scales, for which we provide the first convergence guarantees. Empirically, for language-model pre-training, this eliminates the performance gap with DDP, outperforming infrequent-communication baselines in perplexity and reducing iso-token wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO reaches a target perplexity in 24% fewer steps and 35% less time than the single-momentum DDP baseline. MT-DAO enables effective cross-datacenter training and training over wide geographic areas.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling</title>
<link>https://arxiv.org/abs/2510.05379</link>
<guid>https://arxiv.org/abs/2510.05379</guid>
<content:encoded><![CDATA[
arXiv:2510.05379v1 Announce Type: cross 
Abstract: Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60\% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Length Alone Hurts LLM Performance Despite Perfect Retrieval</title>
<link>https://arxiv.org/abs/2510.05381</link>
<guid>https://arxiv.org/abs/2510.05381</guid>
<content:encoded><![CDATA[
arXiv:2510.05381v1 Announce Type: cross 
Abstract: Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures -- the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one -- or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%--85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating</title>
<link>https://arxiv.org/abs/2510.05394</link>
<guid>https://arxiv.org/abs/2510.05394</guid>
<content:encoded><![CDATA[
arXiv:2510.05394v1 Announce Type: cross 
Abstract: Accurate and efficient temperature prediction is critical for optimizing the preheating process of PET preforms in industrial microwave systems prior to blow molding. We propose a novel deep learning framework for generalized temperature prediction. Unlike traditional models that require extensive retraining for each material or design variation, our method introduces a data-efficient neural architecture that leverages transfer learning and model fusion to generalize across unseen scenarios. By pretraining specialized neural regressor on distinct conditions such as recycled PET heat capacities or varying preform geometries and integrating their representations into a unified global model, we create a system capable of learning shared thermal dynamics across heterogeneous inputs. The architecture incorporates skip connections to enhance stability and prediction accuracy. Our approach reduces the need for large simulation datasets while achieving superior performance compared to models trained from scratch. Experimental validation on two case studies material variability and geometric diversity demonstrates significant improvements in generalization, establishing a scalable ML-based solution for intelligent thermal control in manufacturing environments. Moreover, the approach highlights how data-efficient generalization strategies can extend to other industrial applications involving complex physical modeling with limited data.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data</title>
<link>https://arxiv.org/abs/2510.05399</link>
<guid>https://arxiv.org/abs/2510.05399</guid>
<content:encoded><![CDATA[
arXiv:2510.05399v1 Announce Type: cross 
Abstract: Solar Proton Events (SPEs) cause significant radiation hazards to satellites, astronauts, and technological systems. Accurate forecasting of their proton flux time profiles is crucial for early warnings and mitigation. This paper explores deep learning sequence-to-sequence (seq2seq) models based on Long Short-Term Memory networks to predict 24-hour proton flux profiles following SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we evaluate seq2seq model configurations (varying hidden units and embedding dimensions) under multiple forecasting scenarios: (i) proton-only input vs. combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data, and (iii) autoregressive vs. one-shot forecasting. Our major results are as follows: First, one-shot forecasting consistently yields lower error than autoregressive prediction, avoiding the error accumulation seen in iterative approaches. Second, on the original data, proton-only models outperform proton+X-ray models. However, with trend-smoothed data, this gap narrows or reverses in proton+X-ray models. Third, trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel. Fourth, while models trained on trendsmoothed data perform best on average, the best-performing model was trained on original data, suggesting that architectural choices can sometimes outweigh the benefits of data preprocessing.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models</title>
<link>https://arxiv.org/abs/2510.05408</link>
<guid>https://arxiv.org/abs/2510.05408</guid>
<content:encoded><![CDATA[
arXiv:2510.05408v1 Announce Type: cross 
Abstract: Recovering the past from present observations is an intriguing challenge with potential applications in forensics and scene analysis. Thermal imaging, operating in the infrared range, provides access to otherwise invisible information. Since humans are typically warmer (37 C -98.6 F) than their surroundings, interactions such as sitting, touching, or leaning leave residual heat traces. These fading imprints serve as passive temporal codes, allowing for the inference of recent events that exceed the capabilities of RGB cameras. This work proposes a time-reversed reconstruction framework that uses paired RGB and thermal images to recover scene states from a few seconds earlier. The proposed approach couples Visual-Language Models (VLMs) with a constrained diffusion process, where one VLM generates scene descriptions and another guides image reconstruction, ensuring semantic and structural consistency. The method is evaluated in three controlled scenarios, demonstrating the feasibility of reconstructing plausible past frames up to 120 seconds earlier, providing a first step toward time-reversed imaging from thermal traces.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Student Choice and the Use of Multimodal Generative AI in Programming Learning</title>
<link>https://arxiv.org/abs/2510.05417</link>
<guid>https://arxiv.org/abs/2510.05417</guid>
<content:encoded><![CDATA[
arXiv:2510.05417v1 Announce Type: cross 
Abstract: The broad adoption of Generative AI (GenAI) is impacting Computer Science education, and recent studies found its benefits and potential concerns when students use it for programming learning. However, most existing explorations focus on GenAI tools that primarily support text-to-text interaction. With recent developments, GenAI applications have begun supporting multiple modes of communication, known as multimodality. In this work, we explored how undergraduate programming novices choose and work with multimodal GenAI tools, and their criteria for choices. We selected a commercially available multimodal GenAI platform for interaction, as it supports multiple input and output modalities, including text, audio, image upload, and real-time screen-sharing. Through 16 think-aloud sessions that combined participant observation with follow-up semi-structured interviews, we investigated student modality choices for GenAI tools when completing programming problems and the underlying criteria for modality selections. With multimodal communication emerging as the future of AI in education, this work aims to spark continued exploration on understanding student interaction with multimodal GenAI in the context of CS education.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning in Biomedical Science and Engineering</title>
<link>https://arxiv.org/abs/2510.05433</link>
<guid>https://arxiv.org/abs/2510.05433</guid>
<content:encoded><![CDATA[
arXiv:2510.05433v1 Announce Type: cross 
Abstract: Physics-informed machine learning (PIML) is emerging as a potentially transformative paradigm for modeling complex biomedical systems by integrating parameterized physical laws with data-driven methods. Here, we review three main classes of PIML frameworks: physics-informed neural networks (PINNs), neural ordinary differential equations (NODEs), and neural operators (NOs), highlighting their growing role in biomedical science and engineering. We begin with PINNs, which embed governing equations into deep learning models and have been successfully applied to biosolid and biofluid mechanics, mechanobiology, and medical imaging among other areas. We then review NODEs, which offer continuous-time modeling, especially suited to dynamic physiological systems, pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful tools for learning mappings between function spaces, enabling efficient simulations across multiscale and spatially heterogeneous biological domains. Throughout, we emphasize applications where physical interpretability, data scarcity, or system complexity make conventional black-box learning insufficient. We conclude by identifying open challenges and future directions for advancing PIML in biomedical science and engineering, including issues of uncertainty quantification, generalization, and integration of PIML and large language models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification</title>
<link>https://arxiv.org/abs/2510.05441</link>
<guid>https://arxiv.org/abs/2510.05441</guid>
<content:encoded><![CDATA[
arXiv:2510.05441v1 Announce Type: cross 
Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent system designed to generate unit tests for legacy code, enhancing test coverage and critical value testing. UnitTenX leverages a combination of AI agents, formal methods, and Large Language Models (LLMs) to automate test generation, addressing the challenges posed by complex and legacy codebases. Despite the limitations of LLMs in bug detection, UnitTenX offers a robust framework for improving software reliability and maintainability. Our results demonstrate the effectiveness of this approach in generating high-quality tests and identifying potential issues. Additionally, our approach enhances the readability and documentation of legacy code.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Reinforcement Learning for Large Language Model Agent Safety</title>
<link>https://arxiv.org/abs/2510.05442</link>
<guid>https://arxiv.org/abs/2510.05442</guid>
<content:encoded><![CDATA[
arXiv:2510.05442v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification</title>
<link>https://arxiv.org/abs/2510.05453</link>
<guid>https://arxiv.org/abs/2510.05453</guid>
<content:encoded><![CDATA[
arXiv:2510.05453v1 Announce Type: cross 
Abstract: Conceptual rainfall-runoff models aid hydrologists and climate scientists in modelling streamflow to inform water management practices. Recent advances in deep learning have unravelled the potential for combining hydrological models with deep learning models for better interpretability and improved predictive performance. In our previous work, we introduced DeepGR4J, which enhanced the GR4J conceptual rainfall-runoff model using a deep learning model to serve as a surrogate for the routing component. DeepGR4J had an improved rainfall-runoff prediction accuracy, particularly in arid catchments. Quantile regression models have been extensively used for quantifying uncertainty while aiding extreme value forecasting. In this paper, we extend DeepGR4J using a quantile regression-based ensemble learning framework to quantify uncertainty in streamflow prediction. We also leverage the uncertainty bounds to identify extreme flow events potentially leading to flooding. We further extend the model to multi-step streamflow predictions for uncertainty bounds. We design experiments for a detailed evaluation of the proposed framework using the CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J framework improves the predictive accuracy and uncertainty interval quality (interval score) compared to baseline deep learning models. Furthermore, we carry out flood risk evaluation using Quantile DeepGR4J, and the results demonstrate its suitability as an early warning system.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2510.05468</link>
<guid>https://arxiv.org/abs/2510.05468</guid>
<content:encoded><![CDATA[
arXiv:2510.05468v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices.
  To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization.
  Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training.
  Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation</title>
<link>https://arxiv.org/abs/2510.05490</link>
<guid>https://arxiv.org/abs/2510.05490</guid>
<content:encoded><![CDATA[
arXiv:2510.05490v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved strong performance across a wide range of natural language processing tasks. However, deploying LLMs at scale for domain specific applications, such as job-person fit and explanation in job seeking platforms, introduces distinct challenges. At LinkedIn, the job person fit task requires analyzing a candidate's public profile against job requirements to produce both a fit assessment and a detailed explanation. Directly applying open source or finetuned LLMs to this task often fails to yield high quality, actionable feedback due to the complexity of the domain and the need for structured outputs. Moreover, the large size of these models leads to high inference latency and limits scalability, making them unsuitable for online use. To address these challenges, we introduce LANTERN, a novel LLM knowledge distillation framework tailored specifically for job person fit tasks. LANTERN involves modeling over multiple objectives, an encoder model for classification purpose, and a decoder model for explanation purpose. To better distill the knowledge from a strong black box teacher model to multiple downstream models, LANTERN incorporates multi level knowledge distillation that integrates both data and logit level insights. In addition to introducing the knowledge distillation framework, we share our insights on post training techniques and prompt engineering, both of which are crucial for successfully adapting LLMs to domain specific downstream tasks. Extensive experimental results demonstrate that LANTERN significantly improves task specific metrics for both job person fit and explanation. Online evaluations further confirm its effectiveness, showing measurable gains in job seeker engagement, including a 0.24\% increase in apply rate and a 0.28\% increase in qualified applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training</title>
<link>https://arxiv.org/abs/2510.05492</link>
<guid>https://arxiv.org/abs/2510.05492</guid>
<content:encoded><![CDATA[
arXiv:2510.05492v1 Announce Type: cross 
Abstract: The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting</title>
<link>https://arxiv.org/abs/2510.05497</link>
<guid>https://arxiv.org/abs/2510.05497</guid>
<content:encoded><![CDATA[
arXiv:2510.05497v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures achieve remarkable performance improvements, but their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit serving systems. To forecast the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across three state-of-the-art large-scale MoE models (200B- 671B) using over 24,000 requests spanning diverse workloads. With the resulting 150GB+ trace files, we perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. Taking wafer-scale GPUs as a case study, we demonstrate that minor architectural modifications leveraging our insights achieve substantial performance gains, delivering 6.3X and 4.0X average speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first comprehensive data-centric analysis of MoE models at scale. Our profiling traces and analysis results are publicly available at {https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will also release our simulation framework shortly to facilitate future research in this area.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension</title>
<link>https://arxiv.org/abs/2510.05520</link>
<guid>https://arxiv.org/abs/2510.05520</guid>
<content:encoded><![CDATA[
arXiv:2510.05520v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory -- structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</title>
<link>https://arxiv.org/abs/2510.05526</link>
<guid>https://arxiv.org/abs/2510.05526</guid>
<content:encoded><![CDATA[
arXiv:2510.05526v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \textit{\textbf{C}orrupted} preference, reward \textit{\textbf{O}veroptimization}, and bias towards \textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\textbf{COV} and DPO-\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</title>
<link>https://arxiv.org/abs/2510.05535</link>
<guid>https://arxiv.org/abs/2510.05535</guid>
<content:encoded><![CDATA[
arXiv:2510.05535v1 Announce Type: cross 
Abstract: Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: https://anonymous.4open.science/r/FedCAPS-08BF.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work</title>
<link>https://arxiv.org/abs/2510.05538</link>
<guid>https://arxiv.org/abs/2510.05538</guid>
<content:encoded><![CDATA[
arXiv:2510.05538v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) raise the question of their potential for grading, analyzing, and offering feedback on handwritten student classwork. This capability would be particularly beneficial in elementary and middle-school mathematics education, where most work remains handwritten, because seeing students' full working of a problem provides valuable insights into their learning processes, but is extremely time-consuming to grade. We present two experiments investigating MLLM performance on handwritten student mathematics classwork. Experiment A examines 288 handwritten responses from Ghanaian middle school students solving arithmetic problems with objective answers. In this context, models achieved near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human educators would be unlikely to make. Experiment B evaluates 150 mathematical illustrations from American elementary students, where the drawings are the answer to the question. These tasks lack single objective answers and require sophisticated visual interpretation as well as pedagogical judgment in order to analyze and evaluate them. We attempted to separate MLLMs' visual capabilities from their pedagogical abilities by first asking them to grade the student illustrations directly, and then by augmenting the image with a detailed human description of the illustration. We found that when the models had to analyze the student illustrations directly, they struggled, achieving only k = 0.20 with ground truth scores, but when given human descriptions, their agreement levels improved dramatically to k = 0.47, which was in line with human-to-human agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic work relatively well, but still struggle to "see" student mathematical illustrations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical attention scaling in long-context transformers</title>
<link>https://arxiv.org/abs/2510.05554</link>
<guid>https://arxiv.org/abs/2510.05554</guid>
<content:encoded><![CDATA[
arXiv:2510.05554v1 Announce Type: cross 
Abstract: As large language models scale to longer contexts, attention layers suffer from a fundamental pathology: attention scores collapse toward uniformity as context length $n$ increases, causing tokens to cluster excessively, a phenomenon known as rank-collapse. While $\textit{attention scaling}$ effectively addresses this deficiency by rescaling attention scores with a polylogarithmic factor $\beta_n$, theoretical justification for this approach remains lacking.
  We analyze a simplified yet tractable model that magnifies the effect of attention scaling. In this model, attention exhibits a phase transition governed by the scaling factor $\beta_n$: insufficient scaling collapses all tokens to a single direction, while excessive scaling reduces attention to identity, thereby eliminating meaningful interactions between tokens. Our main result identifies the critical scaling $\beta_n \asymp \log n$ and provides a rigorous justification for attention scaling in YaRN and Qwen, clarifying why logarithmic scaling maintains sparse, content-adaptive attention at large context lengths.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection</title>
<link>https://arxiv.org/abs/2510.05562</link>
<guid>https://arxiv.org/abs/2510.05562</guid>
<content:encoded><![CDATA[
arXiv:2510.05562v1 Announce Type: cross 
Abstract: Spoofing detection in financial trading is crucial, especially for identifying complex behaviors such as conspiracy spoofing. Traditional machine-learning approaches primarily focus on isolated node features, often overlooking the broader context of interconnected nodes. Graph-based techniques, particularly Graph Neural Networks (GNNs), have advanced the field by leveraging relational information effectively. However, in real-world spoofing detection datasets, trading behaviors exhibit dynamic, irregular patterns. Existing spoofing detection methods, though effective in some scenarios, struggle to capture the complexity of dynamic and diverse, evolving inter-node relationships. To address these challenges, we propose a novel framework called the Generative Dynamic Graph Model (GDGM), which models dynamic trading behaviors and the relationships among nodes to learn representations for conspiracy spoofing detection. Specifically, our approach incorporates the generative dynamic latent space to capture the temporal patterns and evolving market conditions. Raw trading data is first converted into time-stamped sequences. Then we model trading behaviors using the neural ordinary differential equations and gated recurrent units, to generate the representation incorporating temporal dynamics of spoofing patterns. Furthermore, pseudo-label generation and heterogeneous aggregation techniques are employed to gather relevant information and enhance the detection performance for conspiratorial spoofing behaviors. Experiments conducted on spoofing detection datasets demonstrate that our approach outperforms state-of-the-art models in detection accuracy. Additionally, our spoofing detection system has been successfully deployed in one of the largest global trading markets, further validating the practical applicability and performance of the proposed method.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Shift-Aware Conformal Prediction for Large Language Models</title>
<link>https://arxiv.org/abs/2510.05566</link>
<guid>https://arxiv.org/abs/2510.05566</guid>
<content:encoded><![CDATA[
arXiv:2510.05566v1 Announce Type: cross 
Abstract: Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising</title>
<link>https://arxiv.org/abs/2510.05589</link>
<guid>https://arxiv.org/abs/2510.05589</guid>
<content:encoded><![CDATA[
arXiv:2510.05589v1 Announce Type: cross 
Abstract: The proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2510.05593</link>
<guid>https://arxiv.org/abs/2510.05593</guid>
<content:encoded><![CDATA[
arXiv:2510.05593v1 Announce Type: cross 
Abstract: Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentDR Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents</title>
<link>https://arxiv.org/abs/2510.05598</link>
<guid>https://arxiv.org/abs/2510.05598</guid>
<content:encoded><![CDATA[
arXiv:2510.05598v1 Announce Type: cross 
Abstract: Recent agent-based recommendation frameworks aim to simulate user behaviors by incorporating memory mechanisms and prompting strategies, but they struggle with hallucinating non-existent items and full-catalog ranking. Besides, a largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning to capture user intent through substitute and complement relationships between items, which are usually implicit in datasets and difficult for traditional ID-based recommenders to capture. In this work, we propose a novel LLM-agent framework, AgenDR, which bridges LLM reasoning with scalable recommendation tools. Our approach delegates full-ranking tasks to traditional models while utilizing LLMs to (i) integrate multiple recommendation outputs based on personalized tool suitability and (ii) reason over substitute and complement relationships grounded in user history. This design mitigates hallucination, scales to large catalogs, and enhances recommendation relevance through relational reasoning. Through extensive experiments on three public grocery datasets, we show that our framework achieves superior full-ranking performance, yielding on average a twofold improvement over its underlying tools. We also introduce a new LLM-based evaluation metric that jointly measures semantic alignment and ranking correctness.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPentester: An LLM Agent-based Framework for Automated Pentesting</title>
<link>https://arxiv.org/abs/2510.05605</link>
<guid>https://arxiv.org/abs/2510.05605</guid>
<content:encoded><![CDATA[
arXiv:2510.05605v1 Announce Type: cross 
Abstract: Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection</title>
<link>https://arxiv.org/abs/2510.05609</link>
<guid>https://arxiv.org/abs/2510.05609</guid>
<content:encoded><![CDATA[
arXiv:2510.05609v1 Announce Type: cross 
Abstract: Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at https://github.com/cjw2021/HOI-R1.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction</title>
<link>https://arxiv.org/abs/2510.05611</link>
<guid>https://arxiv.org/abs/2510.05611</guid>
<content:encoded><![CDATA[
arXiv:2510.05611v1 Announce Type: cross 
Abstract: Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce \textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2510.05613</link>
<guid>https://arxiv.org/abs/2510.05613</guid>
<content:encoded><![CDATA[
arXiv:2510.05613v1 Announce Type: cross 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo-Type Neural Operator for Differential Equations</title>
<link>https://arxiv.org/abs/2510.05620</link>
<guid>https://arxiv.org/abs/2510.05620</guid>
<content:encoded><![CDATA[
arXiv:2510.05620v1 Announce Type: cross 
Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for learning solution operators of one-dimensional partial differential equations (PDEs) by directly learning the kernel function and approximating the associated integral operator using a Monte Carlo-type approach. Unlike Fourier Neural Operators (FNOs), which rely on spectral representations and assume translation-invariant kernels, MCNO makes no such assumptions. The kernel is represented as a learnable tensor over sampled input-output pairs, and sampling is performed once, uniformly at random from a discretized grid. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training, while an interpolation step maps between arbitrary input and output grids to further enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with efficient computational cost. We also provide a theoretical analysis proving that the Monte Carlo estimator yields a bounded bias and variance under mild regularity assumptions. This result holds in any spatial dimension, suggesting that MCNO may extend naturally beyond one-dimensional problems. More broadly, this work explores how Monte Carlo-type integration can be incorporated into neural operator frameworks for continuous-domain PDEs, providing a theoretically supported alternative to spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such as the Graph Kernel Neural Operator, GNO).
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks</title>
<link>https://arxiv.org/abs/2510.05625</link>
<guid>https://arxiv.org/abs/2510.05625</guid>
<content:encoded><![CDATA[
arXiv:2510.05625v1 Announce Type: cross 
Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection</title>
<link>https://arxiv.org/abs/2510.05633</link>
<guid>https://arxiv.org/abs/2510.05633</guid>
<content:encoded><![CDATA[
arXiv:2510.05633v1 Announce Type: cross 
Abstract: Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Neural Activity to Computation: Biological Reservoirs for Pattern Recognition in Digit Classification</title>
<link>https://arxiv.org/abs/2510.05637</link>
<guid>https://arxiv.org/abs/2510.05637</guid>
<content:encoded><![CDATA[
arXiv:2510.05637v1 Announce Type: cross 
Abstract: In this paper, we present a biologically grounded approach to reservoir computing (RC), in which a network of cultured biological neurons serves as the reservoir substrate. This system, referred to as biological reservoir computing (BRC), replaces artificial recurrent units with the spontaneous and evoked activity of living neurons. A multi-electrode array (MEA) enables simultaneous stimulation and readout across multiple sites: inputs are delivered through a subset of electrodes, while the remaining ones capture the resulting neural responses, mapping input patterns into a high-dimensional biological feature space. We evaluate the system through a case study on digit classification using a custom dataset. Input images are encoded and delivered to the biological reservoir via electrical stimulation, and the corresponding neural activity is used to train a simple linear classifier. To contextualize the performance of the biological system, we also include a comparison with a standard artificial reservoir trained on the same task. The results indicate that the biological reservoir can effectively support classification, highlighting its potential as a viable and interpretable computational substrate. We believe this work contributes to the broader effort of integrating biological principles into machine learning and aligns with the goals of human-inspired vision by exploring how living neural systems can inform the design of efficient and biologically plausible models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP</title>
<link>https://arxiv.org/abs/2510.05644</link>
<guid>https://arxiv.org/abs/2510.05644</guid>
<content:encoded><![CDATA[
arXiv:2510.05644v1 Announce Type: cross 
Abstract: Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation</title>
<link>https://arxiv.org/abs/2510.05649</link>
<guid>https://arxiv.org/abs/2510.05649</guid>
<content:encoded><![CDATA[
arXiv:2510.05649v1 Announce Type: cross 
Abstract: Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that arises from ocular misalignment conditions, such as strabismus, enabling patients to reduce diplopia and preserve binocular vision. Early diagnosis minimizes morbidity and secondary complications such as facial asymmetry; however, current clinical assessments remain largely subjective and are further complicated by incomplete medical records. This study addresses both challenges through two complementary deep learning frameworks. First, AHP-CADNet is a multi-level attention fusion framework for automated diagnosis that integrates ocular landmarks, head pose features, and structured clinical attributes to generate interpretable predictions. Second, a curriculum learning-based imputation framework is designed to mitigate missing data by progressively leveraging structured variables and unstructured clinical notes to enhance diagnostic robustness under realistic data conditions. Evaluation on the PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet achieves 96.9-99.0 percent accuracy across classification tasks and low prediction errors for continuous variables, with MAE ranging from 0.103 to 0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy across all clinical variables (93.46-99.78 percent with PubMedBERT), with clinical dependency modeling yielding significant improvements (p < 0.001). These findings confirm the effectiveness of both frameworks for automated diagnosis and recovery from missing data in clinical settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models</title>
<link>https://arxiv.org/abs/2510.05670</link>
<guid>https://arxiv.org/abs/2510.05670</guid>
<content:encoded><![CDATA[
arXiv:2510.05670v1 Announce Type: cross 
Abstract: Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models</title>
<link>https://arxiv.org/abs/2510.05678</link>
<guid>https://arxiv.org/abs/2510.05678</guid>
<content:encoded><![CDATA[
arXiv:2510.05678v1 Announce Type: cross 
Abstract: While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifier-free Test-Time Sampling for Vision Language Action Models</title>
<link>https://arxiv.org/abs/2510.05681</link>
<guid>https://arxiv.org/abs/2510.05681</guid>
<content:encoded><![CDATA[
arXiv:2510.05681v1 Announce Type: cross 
Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QGraphLIME - Explaining Quantum Graph Neural Networks</title>
<link>https://arxiv.org/abs/2510.05683</link>
<guid>https://arxiv.org/abs/2510.05683</guid>
<content:encoded><![CDATA[
arXiv:2510.05683v1 Announce Type: cross 
Abstract: Quantum graph neural networks offer a powerful paradigm for learning on graph-structured data, yet their explainability is complicated by measurement-induced stochasticity and the combinatorial nature of graph structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a model-agnostic, post-hoc framework that treats model explanations as distributions over local surrogates fit on structure-preserving perturbations of a graph. By aggregating surrogate attributions together with their dispersion, QGraphLIME yields uncertainty-aware node and edge importance rankings for quantum graph models. The framework further provides a distribution-free, finite-sample guarantee on the size of the surrogate ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of the induced distribution of a binary class probability at target accuracy and confidence under standard independence assumptions. Empirical studies on controlled synthetic graphs with known ground truth demonstrate accurate and stable explanations, with ablations showing clear benefits of nonlinear surrogate modeling and highlighting sensitivity to perturbation design. Collectively, these results establish a principled, uncertainty-aware, and structure-sensitive approach to explaining quantum graph neural networks, and lay the groundwork for scaling to broader architectures and real-world datasets, as quantum resources mature. Code is available at https://github.com/smlab-niser/qglime.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>vAttention: Verified Sparse Attention</title>
<link>https://arxiv.org/abs/2510.05688</link>
<guid>https://arxiv.org/abs/2510.05688</guid>
<content:encoded><![CDATA[
arXiv:2510.05688v1 Announce Type: cross 
Abstract: State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(\epsilon, \delta)$ guarantees on approximation accuracy (thus, verified). These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality-efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality with upto 20x sparsity). We also demonstrate that it can be deployed in reasoning scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with up to 32K token generations). Code is open-sourced at https://github.com/xAlg-ai/sparse-attention-hub.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse deepfake detection promotes better disentanglement</title>
<link>https://arxiv.org/abs/2510.05696</link>
<guid>https://arxiv.org/abs/2510.05696</guid>
<content:encoded><![CDATA[
arXiv:2510.05696v1 Announce Type: cross 
Abstract: Due to the rapid progress of speech synthesis, deepfake detection has become a major concern in the speech processing community. Because it is a critical task, systems must not only be efficient and robust, but also provide interpretable explanations. Among the different approaches for explainability, we focus on the interpretation of latent representations. In such paper, we focus on the last layer of embeddings of AASIST, a deepfake detection architecture. We use a TopK activation inspired by SAEs on this layer to obtain sparse representations which are used in the decision process. We demonstrate that sparse deepfake detection can improve detection performance, with an EER of 23.36% on ASVSpoof5 test set, with 95% of sparsity. We then show that these representations provide better disentanglement, using completeness and modularity metrics based on mutual information. Notably, some attacks are directly encoded in the latent space.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on Tokenizers of Large Language Models</title>
<link>https://arxiv.org/abs/2510.05699</link>
<guid>https://arxiv.org/abs/2510.05699</guid>
<content:encoded><![CDATA[
arXiv:2510.05699v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2510.05702</link>
<guid>https://arxiv.org/abs/2510.05702</guid>
<content:encoded><![CDATA[
arXiv:2510.05702v1 Announce Type: cross 
Abstract: Large Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling</title>
<link>https://arxiv.org/abs/2510.05709</link>
<guid>https://arxiv.org/abs/2510.05709</guid>
<content:encoded><![CDATA[
arXiv:2510.05709v1 Announce Type: cross 
Abstract: Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinReflectKG - EvalBench: Benchmarking Financial KG with Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2510.05710</link>
<guid>https://arxiv.org/abs/2510.05710</guid>
<content:encoded><![CDATA[
arXiv:2510.05710v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being used to extract structured knowledge from unstructured financial text. Although prior studies have explored various extraction methods, there is no universal benchmark or unified evaluation framework for the construction of financial knowledge graphs (KG). We introduce FinReflectKG - EvalBench, a benchmark and evaluation framework for KG extraction from SEC 10-K filings. Building on the agentic and holistic evaluation principles of FinReflectKG - a financial KG linking audited triples to source chunks from S&amp;P 100 filings and supporting single-pass, multi-pass, and reflection-agent-based extraction modes - EvalBench implements a deterministic commit-then-justify judging protocol with explicit bias controls, mitigating position effects, leniency, verbosity and world-knowledge reliance. Each candidate triple is evaluated with binary judgments of faithfulness, precision, and relevance, while comprehensiveness is assessed on a three-level ordinal scale (good, partial, bad) at the chunk level. Our findings suggest that, when equipped with explicit bias controls, LLM-as-Judge protocols provide a reliable and cost-efficient alternative to human annotation, while also enabling structured error analysis. Reflection-based extraction emerges as the superior approach, achieving best performance in comprehensiveness, precision, and relevance, while single-pass extraction maintains the highest faithfulness. By aggregating these complementary dimensions, FinReflectKG - EvalBench enables fine-grained benchmarking and bias-aware evaluation, advancing transparency and governance in financial AI applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions</title>
<link>https://arxiv.org/abs/2510.05713</link>
<guid>https://arxiv.org/abs/2510.05713</guid>
<content:encoded><![CDATA[
arXiv:2510.05713v1 Announce Type: cross 
Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for enabling collaborative intelligence in industrial Internet of Things (IoT) systems, particularly in smart factories where data privacy, communication efficiency, and device heterogeneity are critical concerns. In this article, we present a comprehensive study of FedSL frameworks tailored for resource-constrained robots in industrial scenarios. We compare synchronous, asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of workflow, scalability, adaptability, and limitations under dynamic industrial conditions. Furthermore, we systematically categorize token fusion strategies into three paradigms: input-level (pre-fusion), intermediate-level (intra-fusion), and output-level (post-fusion), and summarize their respective strengths in industrial applications. We also provide adaptive optimization techniques to enhance the efficiency and feasibility of FedSL implementation, including model compression, split layer selection, computing frequency allocation, and wireless resource management. Simulation results validate the performance of these frameworks under industrial detection scenarios. Finally, we outline open issues and research directions of FedSL in future smart manufacturing systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies</title>
<link>https://arxiv.org/abs/2510.05725</link>
<guid>https://arxiv.org/abs/2510.05725</guid>
<content:encoded><![CDATA[
arXiv:2510.05725v1 Announce Type: cross 
Abstract: Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</title>
<link>https://arxiv.org/abs/2510.05740</link>
<guid>https://arxiv.org/abs/2510.05740</guid>
<content:encoded><![CDATA[
arXiv:2510.05740v1 Announce Type: cross 
Abstract: The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at http://github.com/amir-aman/FusionDetect
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective</title>
<link>https://arxiv.org/abs/2510.05750</link>
<guid>https://arxiv.org/abs/2510.05750</guid>
<content:encoded><![CDATA[
arXiv:2510.05750v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have achieved remarkable success in node classification. Building on this progress, heterogeneous graph neural networks (HGNNs) integrate relation types and node and edge semantics to leverage heterogeneous information. Causal analysis for HGNNs is advancing rapidly, aiming to separate genuine causal effects from spurious correlations. However, whether HGNNs are intrinsically effective remains underexamined, and most studies implicitly assume rather than establish this effectiveness. In this work, we examine HGNNs from two perspectives: model architecture and heterogeneous information. We conduct a systematic reproduction across 21 datasets and 20 baselines, complemented by comprehensive hyperparameter retuning. To further disentangle the source of performance gains, we develop a causal effect estimation framework that constructs and evaluates candidate factors under standard assumptions through factual and counterfactual analyses, with robustness validated via minimal sufficient adjustment sets, cross-method consistency checks, and sensitivity analyses. Our results lead to two conclusions. First, model architecture and complexity have no causal effect on performance. Second, heterogeneous information exerts a positive causal effect by increasing homophily and local-global distribution discrepancy, which makes node classes more distinguishable. The implementation is publicly available at https://github.com/YXNTU/CausalHGNN.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience</title>
<link>https://arxiv.org/abs/2510.05769</link>
<guid>https://arxiv.org/abs/2510.05769</guid>
<content:encoded><![CDATA[
arXiv:2510.05769v1 Announce Type: cross 
Abstract: Abstractive text summarization is integral to the Big Data era, which demands advanced methods to turn voluminous and often long text data into concise but coherent and informative summaries for efficient human consumption. Despite significant progress, there is still room for improvement in various aspects. One such aspect is to improve informativeness. Hence, this paper proposes a novel learning approach consisting of two methods: an optimal transport-based informative attention method to improve learning focal information in reference summaries and an accumulative joint entropy reduction method on named entities to enhance informative salience. Experiment results show that our approach achieves better ROUGE scores compared to prior work on CNN/Daily Mail while having competitive results on XSum. Human evaluation of informativeness also demonstrates the better performance of our approach over a strong baseline. Further analysis gives insight into the plausible reasons underlying the evaluation results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding</title>
<link>https://arxiv.org/abs/2510.05788</link>
<guid>https://arxiv.org/abs/2510.05788</guid>
<content:encoded><![CDATA[
arXiv:2510.05788v1 Announce Type: cross 
Abstract: We present the Mellum models family, open-weight code completion models designed for interactive use in JetBrains IDEs. Mellums have 4B parameters, adopt a Llama-style architecture, and are pre-trained on ~4T tokens of permissively licensed, multi-language code. Our studies show that (i) careful data curation and staged training significantly improve the model's quality, (ii) editor-critical capabilities such as context packing are necessary for high-quality suggestions, and (iii) a compact, task-focused model can meet the cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing contextualized in-editor completion: disciplined data governance, multi-stage training that includes fill-in-the-middle and project context via supervised fine-tuning, and alignment via direct preference optimization using feedback from real-world scenarios. Our quality evaluations include both large-scale offline benchmarks and online telemetry from production deployments in JetBrains IDEs. Mellums are released under the Apache-2.0 license on HuggingFace, with a public model card providing a reproducible reference for practitioners. Our experience offers a pragmatic blueprint for taking a focused, open model from a research prototype to at scale production for hundreds of thousands of users.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech</title>
<link>https://arxiv.org/abs/2510.05799</link>
<guid>https://arxiv.org/abs/2510.05799</guid>
<content:encoded><![CDATA[
arXiv:2510.05799v1 Announce Type: cross 
Abstract: Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk level dependent Minimax Quantile lower bounds for Interactive Statistical Decision Making</title>
<link>https://arxiv.org/abs/2510.05808</link>
<guid>https://arxiv.org/abs/2510.05808</guid>
<content:encoded><![CDATA[
arXiv:2510.05808v1 Announce Type: cross 
Abstract: Minimax risk and regret focus on expectation, missing rare failures critical in safety-critical bandits and reinforcement learning. Minimax quantiles capture these tails. Three strands of prior work motivate this study: minimax-quantile bounds restricted to non-interactive estimation; unified interactive analyses that focus on expected risk rather than risk level specific quantile bounds; and high-probability bandit bounds that still lack a quantile-specific toolkit for general interactive protocols. To close this gap, within the interactive statistical decision making framework, we develop high-probability Fano and Le Cam tools and derive risk level explicit minimax-quantile bounds, including a quantile-to-expectation conversion and a tight link between strict and lower minimax quantiles. Instantiating these results for the two-armed Gaussian bandit immediately recovers optimal-rate bounds.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images</title>
<link>https://arxiv.org/abs/2510.05819</link>
<guid>https://arxiv.org/abs/2510.05819</guid>
<content:encoded><![CDATA[
arXiv:2510.05819v1 Announce Type: cross 
Abstract: Cardiovascular magnetic resonance (CMR) is the gold standard for assessing cardiac function, but individual cardiac cycles complicate automatic temporal comparison or sub-phase analysis. Accurate cardiac keyframe detection can eliminate this problem. However, automatic methods solely derive end-systole (ES) and end-diastole (ED) frames from left ventricular volume curves, which do not provide a deeper insight into myocardial motion. We propose a self-supervised deep learning method detecting five keyframes in short-axis (SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable registration fields are derived from the images and used to compute a 1D motion descriptor, which provides valuable insights into global cardiac contraction and relaxation patterns. From these characteristic curves, keyframes are determined using a simple set of rules. The method was independently evaluated for both views using three public, multicentre, multidisease datasets. M&amp;Ms-2 (n=360) dataset was used for training and evaluation, and M&amp;Ms (n=345) and ACDC (n=100) datasets for repeatability control. Furthermore, generalisability to patients with rare congenital heart defects was tested using the German Competence Network (GCN) dataset. Our self-supervised approach achieved improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED and ES, as measured by cyclic frame difference (cFD), compared with the volume-based approach. We can detect ED and ES, as well as three additional keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and intra-patient analysis of cardiac dynamics, irrespective of cycle or phase lengths. GitHub repository: https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2510.05825</link>
<guid>https://arxiv.org/abs/2510.05825</guid>
<content:encoded><![CDATA[
arXiv:2510.05825v1 Announce Type: cross 
Abstract: Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, is especially severe under constrained computational budgets. To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to overconfident resampling and consequent inability to assess the potential of a reasoning path. We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based on its successors. On several challenging math benchmarks, ePF significantly outperforms strong baselines and achieves up to a 50 % relative improvement in task reward. Together, these methods improve PF's resilience by balancing the exploration of diverse solution spaces with the exploitation of high-reward regions, ultimately leading to higher-quality solutions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation</title>
<link>https://arxiv.org/abs/2510.05827</link>
<guid>https://arxiv.org/abs/2510.05827</guid>
<content:encoded><![CDATA[
arXiv:2510.05827v1 Announce Type: cross 
Abstract: Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection/generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping. To maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces. For training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization</title>
<link>https://arxiv.org/abs/2510.05858</link>
<guid>https://arxiv.org/abs/2510.05858</guid>
<content:encoded><![CDATA[
arXiv:2510.05858v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains %or conversational data that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Long-context Modeling from Context Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.05862</link>
<guid>https://arxiv.org/abs/2510.05862</guid>
<content:encoded><![CDATA[
arXiv:2510.05862v1 Announce Type: cross 
Abstract: Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment-Factorized Full-Song Generation on Symbolic Piano Music</title>
<link>https://arxiv.org/abs/2510.05881</link>
<guid>https://arxiv.org/abs/2510.05881</guid>
<content:encoded><![CDATA[
arXiv:2510.05881v1 Announce Type: cross 
Abstract: We propose the Segmented Full-Song Model (SFS) for symbolic full-song generation. The model accepts a user-provided song structure and an optional short seed segment that anchors the main idea around which the song is developed. By factorizing a song into segments and generating each one through selective attention to related segments, the model achieves higher quality and efficiency compared to prior work. To demonstrate its suitability for human-AI interaction, we further wrap SFS into a web application that enables users to iteratively co-create music on a piano roll with customizable structures and flexible ordering.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection</title>
<link>https://arxiv.org/abs/2510.05891</link>
<guid>https://arxiv.org/abs/2510.05891</guid>
<content:encoded><![CDATA[
arXiv:2510.05891v1 Announce Type: cross 
Abstract: The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods</title>
<link>https://arxiv.org/abs/2510.05901</link>
<guid>https://arxiv.org/abs/2510.05901</guid>
<content:encoded><![CDATA[
arXiv:2510.05901v1 Announce Type: cross 
Abstract: Transformers' quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaputt: A Large-Scale Dataset for Visual Defect Detection</title>
<link>https://arxiv.org/abs/2510.05903</link>
<guid>https://arxiv.org/abs/2510.05903</guid>
<content:encoded><![CDATA[
arXiv:2510.05903v1 Announce Type: cross 
Abstract: We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec-AD and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under https://www.kaputt-dataset.com.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals</title>
<link>https://arxiv.org/abs/2510.05919</link>
<guid>https://arxiv.org/abs/2510.05919</guid>
<content:encoded><![CDATA[
arXiv:2510.05919v1 Announce Type: cross 
Abstract: Anomaly detection in 12-lead electrocardiograms (ECGs) is critical for identifying deviations associated with cardiovascular disease. This work presents a comparative analysis of three autoencoder-based architectures: convolutional autoencoder (CAE), variational autoencoder with bidirectional long short-term memory (VAE-BiLSTM), and VAE-BiLSTM with multi-head attention (VAE-BiLSTM-MHA), for unsupervised anomaly detection in ECGs. To the best of our knowledge, this study reports the first application of a VAE-BiLSTM-MHA architecture to ECG anomaly detection. All models are trained on normal ECG samples to reconstruct non-anomalous cardiac morphology and detect deviations indicative of disease. Using a unified preprocessing and evaluation pipeline on the public China Physiological Signal Challenge (CPSC) dataset, the attention-augmented VAE achieves the best performance, with an AUPRC of 0.81 and a recall of 0.85 on the held-out test set, outperforming the other architectures. To support clinical triage, this model is further integrated into an interactive dashboard that visualizes anomaly localization. In addition, a performance comparison with baseline models from the literature is provided.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Carr\'e du champ flow matching: better quality-generalisation tradeoff in generative models</title>
<link>https://arxiv.org/abs/2510.05930</link>
<guid>https://arxiv.org/abs/2510.05930</guid>
<content:encoded><![CDATA[
arXiv:2510.05930v1 Announce Type: cross 
Abstract: Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce Carr\'e du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection</title>
<link>https://arxiv.org/abs/2510.05935</link>
<guid>https://arxiv.org/abs/2510.05935</guid>
<content:encoded><![CDATA[
arXiv:2510.05935v1 Announce Type: cross 
Abstract: High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative "debate" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models</title>
<link>https://arxiv.org/abs/2510.05942</link>
<guid>https://arxiv.org/abs/2510.05942</guid>
<content:encoded><![CDATA[
arXiv:2510.05942v1 Announce Type: cross 
Abstract: We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density</title>
<link>https://arxiv.org/abs/2510.05949</link>
<guid>https://arxiv.org/abs/2510.05949</guid>
<content:encoded><![CDATA[
arXiv:2510.05949v1 Announce Type: cross 
Abstract: Joint Embedding Predictive Architectures (JEPAs) learn representations able to solve numerous downstream tasks out-of-the-box. JEPAs combine two objectives: (i) a latent-space prediction term, i.e., the representation of a slightly perturbed sample must be predictable from the original sample's representation, and (ii) an anti-collapse term, i.e., not all samples should have the same representation. While (ii) is often considered as an obvious remedy to representation collapse, we uncover that JEPAs' anti-collapse term does much more--it provably estimates the data density. In short, any successfully trained JEPA can be used to get sample probabilities, e.g., for data curation, outlier detection, or simply for density estimation. Our theoretical finding is agnostic of the dataset and architecture used--in any case one can compute the learned probabilities of sample $x$ efficiently and in closed-form using the model's Jacobian matrix at $x$. Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the method extracting the JEPA learned density as {\bf JEPA-SCORE}.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Difficulty Perception Mechanism of Large Language Models</title>
<link>https://arxiv.org/abs/2510.05969</link>
<guid>https://arxiv.org/abs/2510.05969</guid>
<content:encoded><![CDATA[
arXiv:2510.05969v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language</title>
<link>https://arxiv.org/abs/2510.05972</link>
<guid>https://arxiv.org/abs/2510.05972</guid>
<content:encoded><![CDATA[
arXiv:2510.05972v1 Announce Type: cross 
Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been evaluated on planning tasks described in natural language. However, LLMs have largely been tested on planning domains without constraints. In order to deploy them in real-world settings where adherence to constraints, in particular safety constraints, is critical, we need to evaluate their performance on constrained planning tasks. We introduce LexiCon -- a natural language-based (Lexi) constrained (Con) planning benchmark, consisting of a suite of environments, that can be used to evaluate the planning capabilities of LLMs in a principled fashion. The core idea behind LexiCon is to take existing planning environments and impose temporal constraints on the states. These constrained problems are then translated into natural language and given to an LLM to solve. A key feature of LexiCon is its extensibility. That is, the set of supported environments can be extended with new (unconstrained) environment generators, for which temporal constraints are constructed automatically. This renders LexiCon future-proof: the hardness of the generated planning problems can be increased as the planning capabilities of LLMs improve. Our experiments reveal that the performance of state-of-the-art LLMs, including reasoning models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of the planning tasks increases.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</title>
<link>https://arxiv.org/abs/2510.05976</link>
<guid>https://arxiv.org/abs/2510.05976</guid>
<content:encoded><![CDATA[
arXiv:2510.05976v1 Announce Type: cross 
Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning</title>
<link>https://arxiv.org/abs/2510.05984</link>
<guid>https://arxiv.org/abs/2510.05984</guid>
<content:encoded><![CDATA[
arXiv:2510.05984v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser's ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model's training cost and complexity.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Measurement of Hailstones with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2510.06008</link>
<guid>https://arxiv.org/abs/2510.06008</guid>
<content:encoded><![CDATA[
arXiv:2510.06008v1 Announce Type: cross 
Abstract: This study examines the use of social media and news images to detect and measure hailstones, utilizing pre-trained multimodal large language models. The dataset for this study comprises 474 crowdsourced images of hailstones from documented hail events in Austria, which occurred between January 2022 and September 2024. These hailstones have maximum diameters ranging from 2 to 11cm. We estimate the hail diameters and compare four different models utilizing one-stage and two-stage prompting strategies. The latter utilizes additional size cues from reference objects, such as human hands, within the image. Our results show that pretrained models already have the potential to measure hailstone diameters from images with an average mean absolute error of 1.12cm for the best model. In comparison to a single-stage prompt, two-stage prompting improves the reliability of most models. Our study suggests that these off-the-shelf models, even without fine-tuning, can complement traditional hail sensors by extracting meaningful and spatially dense information from social media imagery, enabling faster and more detailed assessments of severe weather events. The automated real-time image harvesting from social media and other sources remains an open task, but it will make our approach directly applicable to future hail events.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP</title>
<link>https://arxiv.org/abs/2510.06010</link>
<guid>https://arxiv.org/abs/2510.06010</guid>
<content:encoded><![CDATA[
arXiv:2510.06010v1 Announce Type: cross 
Abstract: The comparative evaluation between classical and quantum reinforcement learning (QRL) paradigms was conducted to investigate their convergence behavior, robustness under observational noise, and computational efficiency in a benchmark control environment. The study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast, the VQC exhibited limited learning capability, with an average return of 14.6 +/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise robustness analysis further revealed that the MLP policy deteriorated gracefully under Gaussian perturbations, while the VQC displayed higher sensitivity at equivalent noise levels. Despite the lower asymptotic performance, the VQC exhibited significantly lower parameter count and marginally increased training time, highlighting its potential scalability for low-resource quantum processors. The results suggest that while classical neural policies remain dominant in current control benchmarks, quantum-enhanced architectures could offer promising efficiency advantages once hardware noise and expressivity limitations are mitigated.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context</title>
<link>https://arxiv.org/abs/2510.06026</link>
<guid>https://arxiv.org/abs/2510.06026</guid>
<content:encoded><![CDATA[
arXiv:2510.06026v1 Announce Type: cross 
Abstract: Generic instance search models can dramatically reduce the manual effort required to analyze vast surveillance footage during criminal investigations by retrieving specific objects of interest to law enforcement. However, our research reveals an unintended emergent capability: through overlearning, these models can single out specific individuals even when trained on datasets without human subjects. This capability raises concerns regarding identification and profiling of individuals based on their personal data, while there is currently no clear standard on how de-identification can be achieved. We evaluate two technical safeguards to curtail a model's person re-identification capacity: index exclusion and confusion loss. Our experiments demonstrate that combining these approaches can reduce person re-identification accuracy to below 2% while maintaining 82% of retrieval performance for non-person objects. However, we identify critical vulnerabilities in these mitigations, including potential circumvention using partial person images. These findings highlight urgent regulatory questions at the intersection of AI governance and data protection: How should we classify and regulate systems with emergent identification capabilities? And what technical standards should be required to prevent identification capabilities from developing in seemingly benign applications?
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction</title>
<link>https://arxiv.org/abs/2510.06029</link>
<guid>https://arxiv.org/abs/2510.06029</guid>
<content:encoded><![CDATA[
arXiv:2510.06029v1 Announce Type: cross 
Abstract: We introduce molFTP (molecular fragment-target prevalence), a compact representation that delivers strong predictive performance. To prevent feature leakage across cross-validation folds, we implement a dummy-masking procedure that removes information about fragments present in the held-out molecules. We further show that key leave-one-out (key-loo) closely approximates true molecule-level leave-one-out (LOO), with deviation below 8% on our datasets. This enables near full data training while preserving unbiased cross-validation estimates of model performance. Overall, molFTP provides a fast, leakage-resistant fragment-target prevalence vectorization with practical safeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its cost.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.06038</link>
<guid>https://arxiv.org/abs/2510.06038</guid>
<content:encoded><![CDATA[
arXiv:2510.06038v1 Announce Type: cross 
Abstract: Autonomous driving with reinforcement learning (RL) has significant potential. However, applying RL in real-world settings remains challenging due to the need for safe, efficient, and robust learning. Incorporating human expertise into the learning process can help overcome these challenges by reducing risky exploration and improving sample efficiency. In this work, we propose a reward-free, active human-in-the-loop learning method called Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to enable efficient and safe training in real-world environments. The key innovation is the construction of a distributed proxy value function within the DSAC framework. This function encodes human intent by assigning higher expected returns to expert demonstrations and penalizing actions that require human intervention. By extrapolating these labels to unlabeled states, the policy is effectively guided toward expert-like behavior. With a well-designed state space, our method achieves real-world driving policy learning within practical training times. Results from both simulation and real-world experiments demonstrate that our framework enables safe, robust, and sample-efficient learning for autonomous driving.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs</title>
<link>https://arxiv.org/abs/2510.06039</link>
<guid>https://arxiv.org/abs/2510.06039</guid>
<content:encoded><![CDATA[
arXiv:2510.06039v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a Comprehensive Benchmark for Evaluating Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase and outline potential directions for future investigations based on our insights.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2510.06040</link>
<guid>https://arxiv.org/abs/2510.06040</guid>
<content:encoded><![CDATA[
arXiv:2510.06040v1 Announce Type: cross 
Abstract: Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at https://github.com/caoxinye/VideoMiner.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLVD: Guided Learned Vertex Descent</title>
<link>https://arxiv.org/abs/2510.06046</link>
<guid>https://arxiv.org/abs/2510.06046</guid>
<content:encoded><![CDATA[
arXiv:2510.06046v1 Announce Type: cross 
Abstract: Existing 3D face modeling methods usually depend on 3D Morphable Models, which inherently constrain the representation capacity to fixed shape priors. Optimization-based approaches offer high-quality reconstructions but tend to be computationally expensive. In this work, we introduce GLVD, a hybrid method for 3D face reconstruction from few-shot images that extends Learned Vertex Descent (LVD) by integrating per-vertex neural field optimization with global structural guidance from dynamically predicted 3D keypoints. By incorporating relative spatial encoding, GLVD iteratively refines mesh vertices without requiring dense 3D supervision. This enables expressive and adaptable geometry reconstruction while maintaining computational efficiency. GLVD achieves state-of-the-art performance in single-view settings and remains highly competitive in multi-view scenarios, all while substantially reducing inference time.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Audio-Visual Viewpoint Generation from 360{\deg} Spatial Information</title>
<link>https://arxiv.org/abs/2510.06060</link>
<guid>https://arxiv.org/abs/2510.06060</guid>
<content:encoded><![CDATA[
arXiv:2510.06060v1 Announce Type: cross 
Abstract: The generation of sounding videos has seen significant advancements with the advent of diffusion models. However, existing methods often lack the fine-grained control needed to generate viewpoint-specific content from larger, immersive 360-degree environments. This limitation restricts the creation of audio-visual experiences that are aware of off-camera events. To the best of our knowledge, this is the first work to introduce a framework for controllable audio-visual generation, addressing this unexplored gap. Specifically, we propose a diffusion model by introducing a set of powerful conditioning signals derived from the full 360-degree space: a panoramic saliency map to identify regions of interest, a bounding-box-aware signed distance map to define the target viewpoint, and a descriptive caption of the entire scene. By integrating these controls, our model generates spatially-aware viewpoint videos and audios that are coherently influenced by the broader, unseen environmental context, introducing a strong controllability that is essential for realistic and immersive audio-visual generation. We show audiovisual examples proving the effectiveness of our framework.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA</title>
<link>https://arxiv.org/abs/2510.06067</link>
<guid>https://arxiv.org/abs/2510.06067</guid>
<content:encoded><![CDATA[
arXiv:2510.06067v1 Announce Type: cross 
Abstract: CAPTCHA, originally designed to distinguish humans from robots, has evolved into a real-world benchmark for assessing the spatial reasoning capabilities of vision-language models. In this work, we first show that step-by-step reasoning is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent high-difficulty spatial reasoning tasks, and that current commercial vision-language models still struggle with such reasoning. In particular, we observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent). However, our findings indicate that requiring the model to perform step-by-step reasoning before generating the final coordinates can significantly enhance its solving accuracy, underscoring the severity of the gap. To systematically study this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha, etc.) with step-by-step action solutions and grounding annotations. We further define five reasoning-oriented metrics that enable a comprehensive evaluation of models reasoning capabilities. To validate the effectiveness of reasoning, we also propose a general agentic VLM-based framework that incorporates the models inherent reasoning abilities. Our method achieves state-of-the-art performance across five high-difficulty CAPTCHA types, with an average solving accuracy of 83.9 percent, substantially surpassing existing baselines. These results reveal the limitations of current models and highlight the importance of reasoning in advancing visual-spatial challenges in the future.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning</title>
<link>https://arxiv.org/abs/2510.06068</link>
<guid>https://arxiv.org/abs/2510.06068</guid>
<content:encoded><![CDATA[
arXiv:2510.06068v1 Announce Type: cross 
Abstract: Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a hand's morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with less than 0.4 seconds inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate. The code and additional materials will be made available upon publication on our project website https://connor-zh.github.io/cross_embodiment_dexterous_grasping.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks</title>
<link>https://arxiv.org/abs/2510.06071</link>
<guid>https://arxiv.org/abs/2510.06071</guid>
<content:encoded><![CDATA[
arXiv:2510.06071v1 Announce Type: cross 
Abstract: AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</title>
<link>https://arxiv.org/abs/2510.06077</link>
<guid>https://arxiv.org/abs/2510.06077</guid>
<content:encoded><![CDATA[
arXiv:2510.06077v1 Announce Type: cross 
Abstract: Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term "visual thinking drift". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only "think before answering", but also "see while thinking".
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability</title>
<link>https://arxiv.org/abs/2510.06084</link>
<guid>https://arxiv.org/abs/2510.06084</guid>
<content:encoded><![CDATA[
arXiv:2510.06084v1 Announce Type: cross 
Abstract: Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata for conditional distributional modeling: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how current post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from >40 data sources and spanning >90 tasks requiring models to steer to and match diverse distributions ranging from varied human preferences to numerical distributions and more. We find that while current post-training techniques help elicit underlying capabilities and knowledge, they hurt models' ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum Tuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the output space, and improving distributional alignment on held-out datasets.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A public cardiac CT dataset featuring the left atrial appendage</title>
<link>https://arxiv.org/abs/2510.06090</link>
<guid>https://arxiv.org/abs/2510.06090</guid>
<content:encoded><![CDATA[
arXiv:2510.06090v1 Announce Type: cross 
Abstract: Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology.
  LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scanner's field of view, and other types of data defects.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.06107</link>
<guid>https://arxiv.org/abs/2510.06107</guid>
<content:encoded><![CDATA[
arXiv:2510.06107v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \textbf{associative pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</title>
<link>https://arxiv.org/abs/2510.06131</link>
<guid>https://arxiv.org/abs/2510.06131</guid>
<content:encoded><![CDATA[
arXiv:2510.06131v1 Announce Type: cross 
Abstract: Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits</title>
<link>https://arxiv.org/abs/2510.06133</link>
<guid>https://arxiv.org/abs/2510.06133</guid>
<content:encoded><![CDATA[
arXiv:2510.06133v1 Announce Type: cross 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks</title>
<link>https://arxiv.org/abs/2510.06138</link>
<guid>https://arxiv.org/abs/2510.06138</guid>
<content:encoded><![CDATA[
arXiv:2510.06138v1 Announce Type: cross 
Abstract: Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images</title>
<link>https://arxiv.org/abs/2510.06145</link>
<guid>https://arxiv.org/abs/2510.06145</guid>
<content:encoded><![CDATA[
arXiv:2510.06145v1 Announce Type: cross 
Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams</title>
<link>https://arxiv.org/abs/2510.06151</link>
<guid>https://arxiv.org/abs/2510.06151</guid>
<content:encoded><![CDATA[
arXiv:2510.06151v1 Announce Type: cross 
Abstract: A critical challenge in modelling Heterogeneous-Agent Teams is training agents to collaborate with teammates whose policies are inaccessible or non-stationary, such as humans. Traditional approaches rely on expensive human-in-the-loop data, which limits scalability. We propose using Large Language Models (LLMs) as policy-agnostic human proxies to generate synthetic data that mimics human decision-making. To evaluate this, we conduct three experiments in a grid-world capture game inspired by Stag Hunt, a game theory paradigm that balances risk and reward. In Experiment 1, we compare decisions from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and Mixtral 8x22B models. LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria. Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM outputs mirror human participants' variability, shifting between risk-averse and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic grid-world where the LLM agents generate movement actions. LLMs produce trajectories resembling human participants' paths. While LLMs cannot yet fully replicate human adaptability, their prompt-guided diversity offers a scalable foundation for simulating policy-agnostic teammates.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2</title>
<link>https://arxiv.org/abs/2510.06170</link>
<guid>https://arxiv.org/abs/2510.06170</guid>
<content:encoded><![CDATA[
arXiv:2510.06170v1 Announce Type: cross 
Abstract: Smartphone-based iris recognition in the visible spectrum (VIS) remains difficult due to illumination variability, pigmentation differences, and the absence of standardized capture controls. This work presents a compact end-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at acquisition and demonstrates that accurate VIS iris recognition is feasible on commodity devices. Using a custom Android application performing real-time framing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset of 752 compliant images from 47 subjects. A lightweight MobileNetV3-based multi-task segmentation network (LightIrisNet) is developed for efficient on-device processing, and a transformer matcher (IrisFormer) is adapted to the VIS domain. Under a standardized protocol and comparative benchmarking against prior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%), while IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on CUVIRIS. The acquisition app, trained models, and a public subset of the dataset are released to support reproducibility. These results confirm that standardized capture and VIS-adapted lightweight models enable accurate and practical iris recognition on smartphones.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback</title>
<link>https://arxiv.org/abs/2510.06186</link>
<guid>https://arxiv.org/abs/2510.06186</guid>
<content:encoded><![CDATA[
arXiv:2510.06186v1 Announce Type: cross 
Abstract: Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Program Repair of Uncompilable Student Code</title>
<link>https://arxiv.org/abs/2510.06187</link>
<guid>https://arxiv.org/abs/2510.06187</guid>
<content:encoded><![CDATA[
arXiv:2510.06187v1 Announce Type: cross 
Abstract: A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents, including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash (Google), under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. We find that while all three LLMs are capable of producing compilable repairs, their behavior diverges in how well they preserve students' control flow and code structure, which affects their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[
arXiv:2510.06188v1 Announce Type: cross 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Speech-Text Transformer</title>
<link>https://arxiv.org/abs/2510.06195</link>
<guid>https://arxiv.org/abs/2510.06195</guid>
<content:encoded><![CDATA[
arXiv:2510.06195v1 Announce Type: cross 
Abstract: Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars</title>
<link>https://arxiv.org/abs/2510.06200</link>
<guid>https://arxiv.org/abs/2510.06200</guid>
<content:encoded><![CDATA[
arXiv:2510.06200v1 Announce Type: cross 
Abstract: Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique challenges including irregular sampling and heteroskedasticity. We introduce StarEmbed, the first public benchmark for rigorous and standardized evaluation of state-of-the-art TSFMs on stellar time series observations (``light curves''). We benchmark on three scientifically-motivated downstream tasks: unsupervised clustering, supervised classification, and out-of-distribution source detection. StarEmbed integrates a catalog of expert-vetted labels with multi-variate light curves from the Zwicky Transient Facility, yielding ~40k hand-labeled light curves spread across seven astrophysical classes. We evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI, Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against handcrafted feature extraction, the long-standing baseline in the astrophysics literature. Our results demonstrate that these TSFMs, especially the Chronos models, which are trained on data completely unlike the astronomical observations, can outperform established astrophysics-specific baselines in some tasks and effectively generalize to entirely new data. In particular, TSFMs deliver state-of-the-art performance on our out-of-distribution source detection benchmark. With the first benchmark of TSFMs on astronomical time series data, we test the limits of their generalization and motivate a paradigm shift in time-domain astronomy from using task-specific, fully supervised pipelines toward adopting generic foundation model representations for the analysis of peta-scale datasets from forthcoming observatories.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenChain: A Discrete Speech Chain via Semantic Token Modeling</title>
<link>https://arxiv.org/abs/2510.06201</link>
<guid>https://arxiv.org/abs/2510.06201</guid>
<content:encoded><![CDATA[
arXiv:2510.06201v1 Announce Type: cross 
Abstract: Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reference Grounded Skill Discovery</title>
<link>https://arxiv.org/abs/2510.06203</link>
<guid>https://arxiv.org/abs/2510.06203</guid>
<content:encoded><![CDATA[
arXiv:2510.06203v1 Announce Type: cross 
Abstract: Scaling unsupervised skill discovery algorithms to high-DoF agents remains challenging. As dimensionality increases, the exploration space grows exponentially, while the manifold of meaningful skills remains limited. Therefore, semantic meaningfulness becomes essential to effectively guide exploration in high-dimensional spaces. In this work, we present Reference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill discovery in a semantically meaningful latent space using reference data. RGSD first performs contrastive pretraining to embed motions on a unit hypersphere, clustering each reference trajectory into a distinct direction. This grounding enables skill discovery to simultaneously involve both imitation of reference behaviors and the discovery of semantically related diverse behaviors. On a simulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns structured skills including walking, running, punching, and side stepping, and also discovers related novel behaviors. In downstream control tasks, RGSD outperforms imitation-based skill acquisition baselines. Our results suggest that lightweight reference-guided grounding offers a practical path to discovering semantically rich and structured skills in high-DoF systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents</title>
<link>https://arxiv.org/abs/2510.06214</link>
<guid>https://arxiv.org/abs/2510.06214</guid>
<content:encoded><![CDATA[
arXiv:2510.06214v1 Announce Type: cross 
Abstract: Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an "apples-to-oranges" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</title>
<link>https://arxiv.org/abs/2510.06218</link>
<guid>https://arxiv.org/abs/2510.06218</guid>
<content:encoded><![CDATA[
arXiv:2510.06218v1 Announce Type: cross 
Abstract: Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game</title>
<link>https://arxiv.org/abs/2408.09946</link>
<guid>https://arxiv.org/abs/2408.09946</guid>
<content:encoded><![CDATA[
arXiv:2408.09946v3 Announce Type: replace 
Abstract: Recent studies have investigated whether large language models (LLMs) can support obscured communication, which is characterized by core aspects such as inferring subtext and evading suspicions. To conduct the investigation, researchers have used social deduction games (SDGs) as their experimental environment, in which players conceal and infer specific information. However, prior work has often overlooked how LLMs should be evaluated in such settings. Specifically, we point out two limitations with the evaluation methods they employed. First, metrics used in prior studies are coarse-grained as they are based on overall game outcomes that often fail to capture event-level behaviors; Second, error analyses have lacked structured methodologies capable of producing insights that meaningfully support evaluation outcomes. To address these limitations, we propose a microscopic and systematic approach to the investigation. Specifically, we introduce six fine-grained metrics that resolve the first issue. To tackle the second issue, we conducted a thematic analysis and identified four major reasoning failures that undermine LLMs' performance in obscured communication.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training</title>
<link>https://arxiv.org/abs/2410.15460</link>
<guid>https://arxiv.org/abs/2410.15460</guid>
<content:encoded><![CDATA[
arXiv:2410.15460v5 Announce Type: replace 
Abstract: As large language models (LLMs) become increasingly prevalent, concerns about their reliability, particularly due to hallucinations - factually inaccurate or irrelevant outputs - have grown. Our research investigates the relationship between the uncertainty in training dynamics and the emergence of hallucinations. Using models from the Pythia suite and several hallucination detection metrics, we analyze hallucination trends and identify significant variance during training. To address this, we propose Sensitivity Dropout (SenD), a novel training protocol designed to reduce hallucination variance during training by deterministically dropping embedding indices with significant variability. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This metric is integrated into our training protocol, allowing SenD to be both computationally scalable and effective at reducing hallucination variance. SenD improves test-time reliability of Pythia and Meta's Llama models by up to 17% and enhances factual accuracy in Wikipedia, Medical, Legal, and Coding domains without affecting downstream task performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Case Study on BERT-based Language Models</title>
<link>https://arxiv.org/abs/2412.10513</link>
<guid>https://arxiv.org/abs/2412.10513</guid>
<content:encoded><![CDATA[
arXiv:2412.10513v2 Announce Type: replace 
Abstract: Decision trees are a popular machine learning method, known for their inherent explainability. In Explainable AI, decision trees can be used as surrogate models for complex black box AI models or as approximations of parts of such models. A key challenge of this approach is determining how accurately the extracted decision tree represents the original model and to what extent it can be trusted as an approximation of their behavior. In this work, we investigate the use of the Probably Approximately Correct (PAC) framework to provide a theoretical guarantee of fidelity for decision trees extracted from AI models. Based on theoretical results from the PAC framework, we adapt a decision tree algorithm to ensure a PAC guarantee under certain conditions. We focus on binary classification and conduct experiments where we extract decision trees from BERT-based language models with PAC guarantees. Our results indicate occupational gender bias in these models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications of Large Models in Medicine</title>
<link>https://arxiv.org/abs/2502.17132</link>
<guid>https://arxiv.org/abs/2502.17132</guid>
<content:encoded><![CDATA[
arXiv:2502.17132v2 Announce Type: replace 
Abstract: This paper explores the advancements and applications of large-scale models in the medical field, with a particular focus on Medical Large Models (MedLMs). These models, encompassing Large Language Models (LLMs), Vision Models, 3D Large Models, and Multimodal Models, are revolutionizing healthcare by enhancing disease prediction, diagnostic assistance, personalized treatment planning, and drug discovery. The integration of graph neural networks in medical knowledge graphs and drug discovery highlights the potential of Large Graph Models (LGMs) in understanding complex biomedical relationships. The study also emphasizes the transformative role of Vision-Language Models (VLMs) and 3D Large Models in medical image analysis, anatomical modeling, and prosthetic design. Despite the challenges, these technologies are setting new benchmarks in medical innovation, improving diagnostic accuracy, and paving the way for personalized healthcare solutions. This paper aims to provide a comprehensive overview of the current state and future directions of large models in medicine, underscoring their significance in advancing global health.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
<link>https://arxiv.org/abs/2503.01722</link>
<guid>https://arxiv.org/abs/2503.01722</guid>
<content:encoded><![CDATA[
arXiv:2503.01722v2 Announce Type: replace 
Abstract: In causal inference, interference refers to the phenomenon in which the actions of peers in a network can influence an individual's outcome. Peer effect refers to the difference in counterfactual outcomes of an individual for different levels of peer exposure, the extent to which an individual is exposed to the treatments, actions, or behaviors of peers. Estimating peer effects requires deciding how to represent peer exposure. Typically, researchers define an exposure mapping function that aggregates peer treatments and outputs peer exposure. Most existing approaches for defining exposure mapping functions assume peer exposure based on the number or fraction of treated peers. Recent studies have investigated more complex functions of peer exposure which capture that different peers can exert different degrees of influence. However, none of these works have explicitly considered the problem of automatically learning the exposure mapping function. In this work, we focus on learning this function for the purpose of estimating heterogeneous peer effects, where heterogeneity refers to the variation in counterfactual outcomes for the same peer exposure but different individual's contexts. We develop EgoNetGNN, a graph neural network (GNN)-based method, to automatically learn the appropriate exposure mapping function allowing for complex peer influence mechanisms that, in addition to peer treatments, can involve the local neighborhood structure and edge attributes. We show that GNN models that use peer exposure based on the number or fraction of treated peers or learn peer exposure naively face difficulty accounting for such influence mechanisms. Our comprehensive evaluation on synthetic and semi-synthetic network data shows that our method is more robust to different unknown underlying influence mechanisms when estimating heterogeneous peer effects when compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciSciGPT: Advancing Human-AI Collaboration in the Science of Science</title>
<link>https://arxiv.org/abs/2504.05559</link>
<guid>https://arxiv.org/abs/2504.05559</guid>
<content:encoded><![CDATA[
arXiv:2504.05559v2 Announce Type: replace 
Abstract: The increasing availability of large-scale datasets has fueled rapid progress across many scientific fields, creating unprecedented opportunities for research and discovery while posing significant analytical challenges. Recent advances in large language models (LLMs) and AI agents have opened new possibilities for human-AI collaboration, offering powerful tools to navigate this complex research landscape. In this paper, we introduce SciSciGPT, an open-source, prototype AI collaborator that uses the science of science as a testbed to explore the potential of LLM-powered research tools. SciSciGPT automates complex workflows, supports diverse analytical approaches, accelerates research prototyping and iteration, and facilitates reproducibility. Through case studies, we demonstrate its ability to streamline a wide range of empirical and analytical research tasks while highlighting its broader potential to advance research. We further propose an LLM Agent capability maturity model for human-AI collaboration, envisioning a roadmap to further improve and expand upon frameworks like SciSciGPT. As AI capabilities continue to evolve, frameworks like SciSciGPT may play increasingly pivotal roles in scientific research and discovery, unlocking further opportunities. At the same time, these new advances also raise critical challenges, from ensuring transparency and ethical use to balancing human and AI contributions. Addressing these issues may shape the future of scientific inquiry and inform how we train the next generation of scientists to thrive in an increasingly AI-integrated research ecosystem.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLEx: Personalized Federated Learning for Mixture-of-Experts LLMs via Expert Grafting</title>
<link>https://arxiv.org/abs/2506.00965</link>
<guid>https://arxiv.org/abs/2506.00965</guid>
<content:encoded><![CDATA[
arXiv:2506.00965v2 Announce Type: replace 
Abstract: Federated instruction tuning of large language models (LLMs) is challenged by significant data heterogeneity across clients, demanding robust personalization. The Mixture of Experts (MoE) architecture, where experts can specialize in distinct data patterns, presents a natural architectural solution to this challenge. The inherent sparsity of the MoE architecture, achieved by selectively activating experts, poses a significant challenge to its integration with federated learning (FL). Conventional FL frameworks, designed for dense models, naively aggregate all expert parameters irrespective of their local activation patterns. This naive approach not only undermines MoE's dynamic sparsity but also risks corrupting the world knowledge within pretrained experts. To address this, we propose FLEx (Federated LLMs with Personalized Experts), a novel framework that leverages pretrained MoE-based LLMs for efficient personalization. By aggregating only the shared non-expert parameters, FLEx significantly reduces communication overhead and preserves the world knowledge stored within the frozen pretrained experts. For personalization, we introduce a novel expert grafting mechanism that leverages dynamic sparsity to construct a client-specific expert from selected components of pretrained experts, tailored to local data. This grafted expert is then fine-tuned locally alongside the gating mechanism. This joint training enables the model to learn when to leverage the shared knowledge from frozen experts and when to employ the personalized one. Evaluations on diverse, non-IID instruction tuning datasets show that FLEx consistently outperforms federated baselines on average, while demonstrating strong knowledge preservation on the knowledge-driven benchmark MMLU. Our code is available at \href{https://anonymous.4open.science/r/FLEx-8F12}{\texttt{https://anonymous.4open.science/r/FLEx-8F12}}.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs</title>
<link>https://arxiv.org/abs/2506.06727</link>
<guid>https://arxiv.org/abs/2506.06727</guid>
<content:encoded><![CDATA[
arXiv:2506.06727v3 Announce Type: replace 
Abstract: Large Multimodal Models have achieved remarkable progress in integrating vision and language, enabling strong performance across perception, reasoning, and domain-specific tasks. However, their capacity to reason over multiple, visually similar inputs remains insufficiently explored. Such fine-grained comparative reasoning is central to real-world tasks, especially in mathematics and education, where learners must often distinguish between nearly identical diagrams to identify correct solutions. To address this gap, we present VisioMath, a curated benchmark of 1,800 high-quality K-12 mathematics problems in which all candidate answers are diagrams with subtle visual similarities. A comprehensive evaluation of state-of-the-art LMMs, covering both leading closed-source systems and widely adopted open-source models, reveals a consistent decline in accuracy as inter-image similarity increases. Analysis indicates that the dominant failure mode stems from image-text misalignment: rather than grounding reasoning in textual cues, models often resort to shallow positional heuristics, resulting in systematic errors. We further explore three alignment-oriented strategies, spanning training-free approaches and finetuning, and achieve substantial accuracy gains. We hope that VisioMath will serve as a rigorous benchmark and catalyst for developing LMMs toward deeper diagram understanding, precise comparative reasoning, and grounded multi-image-text integration.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs</title>
<link>https://arxiv.org/abs/2506.13082</link>
<guid>https://arxiv.org/abs/2506.13082</guid>
<content:encoded><![CDATA[
arXiv:2506.13082v3 Announce Type: replace 
Abstract: Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search</title>
<link>https://arxiv.org/abs/2508.08477</link>
<guid>https://arxiv.org/abs/2508.08477</guid>
<content:encoded><![CDATA[
arXiv:2508.08477v3 Announce Type: replace 
Abstract: The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific "trigger" arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77% and 0.40% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.15690</link>
<guid>https://arxiv.org/abs/2508.15690</guid>
<content:encoded><![CDATA[
arXiv:2508.15690v3 Announce Type: replace 
Abstract: GRAFT is a structured multimodal benchmark for evaluating models on instruction-following, visual reasoning, and visual-textual alignment tasks. It features programmatically generated charts and synthetically rendered tables, created with Python visualization libraries to ensure control over data semantics, structure, and clarity. Each GRAFT instance pairs a chart or table image with a systematically generated, multi-step analytical question based solely on visual content. Answers are provided in structured formats such as JSON or YAML, supporting consistent evaluation of both reasoning and output format. The benchmark introduces a taxonomy of reasoning types including comparison, trend identification, ranking, aggregation, proportion estimation, and anomaly detection to enable comprehensive assessment. Reference answers follow strict factual and formatting guidelines for precise, aspect-based evaluation. GRAFT offers a unified, scalable framework for fine-grained benchmarking of multimodal models on visually grounded, structured reasoning tasks, setting a new evaluation standard in this field.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models</title>
<link>https://arxiv.org/abs/2509.08972</link>
<guid>https://arxiv.org/abs/2509.08972</guid>
<content:encoded><![CDATA[
arXiv:2509.08972v3 Announce Type: replace 
Abstract: The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. While the causes of model collapse are increasingly understood, effective mitigation strategies remain scarce. We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce the Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse. In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings. Our results demonstrate that confidence-aware training objectives can substantially delay collapse onset, offering a practical and generalizable tool for model robustness under synthetic-data exposure.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization</title>
<link>https://arxiv.org/abs/2509.11361</link>
<guid>https://arxiv.org/abs/2509.11361</guid>
<content:encoded><![CDATA[
arXiv:2509.11361v2 Announce Type: replace 
Abstract: Prompt engineering is crucial for fully leveraging large language models (LLMs), yet most existing optimization methods follow a single trajectory, resulting in limited adaptability, gradient conflicts, and high computational overhead. We propose MAPGD (Multi-Agent Prompt Gradient Descent), a novel framework that reconceptualizes prompt optimization as a collaborative process among specialized agents. Each agent focuses on a distinct refinement dimension, such as instruction clarity, example selection, format structure, or stylistic adaptation, and their contributions are coordinated through semantic gradient embedding, conflict detection, and fusion. To further enhance robustness and stability, MAPGD introduces two new mechanisms: Hypersphere Constrained Gradient Clustering (HCGC), which enforces angular margin constraints for compact and well-separated clusters, and Channel Adaptive Agent Weighting (CAAW), which dynamically reweights agent contributions based on validation performance. Experiments on classification and reasoning benchmarks show that MAPGD consistently surpasses single-agent and random baselines in both accuracy and efficiency. Ablation studies confirm the effectiveness of gradient fusion, agent specialization, and conflict resolution. Together, these components establish MAPGD as a unified, gradient-based, and interpretable framework for robust prompt optimization with theoretical convergence guarantees.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human + AI for Accelerating Ad Localization Evaluation</title>
<link>https://arxiv.org/abs/2509.12543</link>
<guid>https://arxiv.org/abs/2509.12543</guid>
<content:encoded><![CDATA[
arXiv:2509.12543v3 Announce Type: replace 
Abstract: Adapting advertisements for multilingual audiences requires more than simple text translation; it demands preservation of visual consistency, spatial alignment, and stylistic integrity across diverse languages and formats. We introduce a structured framework that combines automated components with human oversight to address the complexities of advertisement localization. To the best of our knowledge, this is the first work to integrate scene text detection, inpainting, machine translation (MT), and text reimposition specifically for accelerating ad localization evaluation workflows. Qualitative results across six locales demonstrate that our approach produces semantically accurate and visually coherent localized advertisements, suitable for deployment in real-world workflows.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepIt: Representing Isolated Targets to Steer Language Models</title>
<link>https://arxiv.org/abs/2509.13281</link>
<guid>https://arxiv.org/abs/2509.13281</guid>
<content:encoded><![CDATA[
arXiv:2509.13281v2 Announce Type: replace 
Abstract: While activation steering in large language models (LLMs) is a growing area of research, methods can often incur broader effects than desired. This motivates isolation of purer concept vectors to enable targeted interventions and understand LLM behavior at a more granular level. We present RepIt, a simple and data-efficient framework for isolating concept-specific representations. Across five frontier LLMs, RepIt enables precise interventions: it selectively suppresses refusal on targeted concepts while preserving refusal elsewhere, producing models that answer WMD-related questions while still scoring as safe on standard benchmarks. We further show that the corrective signal localizes to just 100-200 neurons and that robust target representations can be extracted from as few as a dozen examples on a single A6000. This efficiency raises a dual concern: manipulations can be performed with modest compute and data to extend to underrepresented data-scarce topics while evading existing benchmarks. By disentangling refusal vectors with RepIt, this work demonstrates that targeted interventions can counteract overgeneralization, laying the foundation for more granular control of model behavior.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models</title>
<link>https://arxiv.org/abs/2509.22284</link>
<guid>https://arxiv.org/abs/2509.22284</guid>
<content:encoded><![CDATA[
arXiv:2509.22284v2 Announce Type: replace 
Abstract: Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Profiling and Modulation for LLMs</title>
<link>https://arxiv.org/abs/2509.23058</link>
<guid>https://arxiv.org/abs/2509.23058</guid>
<content:encoded><![CDATA[
arXiv:2509.23058v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used for decision-making tasks under uncertainty; however, their risk profiles and how they are influenced by prompting and alignment methods remain underexplored. Existing studies have primarily examined personality prompting or multi-agent interactions, leaving open the question of how post-training influences the risk behavior of LLMs. In this work, we propose a new pipeline for eliciting, steering, and modulating LLMs' risk profiles, drawing on tools from behavioral economics and finance. Using utility-theoretic models, we compare pre-trained, instruction-tuned, and RLHF-aligned LLMs, and find that while instruction-tuned models exhibit behaviors consistent with some standard utility formulations, pre-trained and RLHF-aligned models deviate more from any utility models fitted. We further evaluate modulation strategies, including prompt engineering, in-context learning, and post-training, and show that post-training provides the most stable and effective modulation of risk preference. Our findings provide insights into the risk profiles of different classes and stages of LLMs and demonstrate how post-training modulates these profiles, laying the groundwork for future research on behavioral alignment and risk-aware LLM design.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned</title>
<link>https://arxiv.org/abs/2509.23250</link>
<guid>https://arxiv.org/abs/2509.23250</guid>
<content:encoded><![CDATA[
arXiv:2509.23250v3 Announce Type: replace 
Abstract: Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From paintbrush to pixel: A review of deep neural networks in AI-generated art</title>
<link>https://arxiv.org/abs/2302.10913</link>
<guid>https://arxiv.org/abs/2302.10913</guid>
<content:encoded><![CDATA[
arXiv:2302.10913v3 Announce Type: replace-cross 
Abstract: This paper delves into the fascinating field of AI-generated art and explores the various deep neural network architectures and models that have been utilized to create it. From the classic convolutional networks to the cutting-edge diffusion models, we examine the key players in the field. We explain the general structures and working principles of these neural networks. Then, we showcase examples of milestones, starting with the dreamy landscapes of DeepDream and moving on to the most recent developments, including Stable Diffusion and DALL-E 3, which produce mesmerizing images. We provide a detailed comparison of these models, highlighting their strengths and limitations, and examining the remarkable progress that deep neural networks have made so far in a short period of time. With a unique blend of technical explanations and insights into the current state of AI-generated art, this paper exemplifies how art and computer science interact.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative transformations and patterns in LLM-native approaches for software verification and falsification</title>
<link>https://arxiv.org/abs/2404.09384</link>
<guid>https://arxiv.org/abs/2404.09384</guid>
<content:encoded><![CDATA[
arXiv:2404.09384v3 Announce Type: replace-cross 
Abstract: The emergence of prompting as the dominant paradigm for leveraging Large Language Models (LLMs) has led to a proliferation of LLM-native software, where application behavior arises from complex, stochastic data transformations. However, the engineering of such systems remains largely exploratory and ad-hoc, hampered by the absence of conceptual frameworks, ex-ante methodologies, design guidelines, and specialized benchmarks. We argue that a foundational step towards a more disciplined engineering practice is a systematic understanding of the core functional units--generative transformations--and their compositional patterns within LLM-native applications.
  Focusing on the rich domain of software verification and falsification, we conduct a secondary study of over 100 research proposals to address this gap. We first present a fine-grained taxonomy of generative transformations, abstracting prompt-based interactions into conceptual signatures. This taxonomy serves as a scaffolding to identify recurrent transformation relationship patterns--analogous to software design patterns--that characterize solution approaches in the literature. Our analysis not only validates the utility of the taxonomy but also surfaces strategic gaps and cross-dimensional relationships, offering a structured foundation for future research in modular and compositional LLM application design, benchmarking, and the development of reliable LLM-native systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning</title>
<link>https://arxiv.org/abs/2404.13004</link>
<guid>https://arxiv.org/abs/2404.13004</guid>
<content:encoded><![CDATA[
arXiv:2404.13004v5 Announce Type: replace-cross 
Abstract: Recent industrial credit scoring models remain heavily reliant on manually tuned statistical learning methods. Despite their potential, deep learning architectures have struggled to consistently outperform traditional statistical models in industrial credit scoring, largely due to the complexity of heterogeneous financial data and the challenge of modeling evolving creditworthiness. To bridge this gap, we introduce FinLangNet, a novel framework that reformulates credit scoring as a multi-scale sequential learning problem. FinLangNet processes heterogeneous financial data through a dual-module architecture that combines tabular feature extraction with temporal sequence modeling, generating probability distributions of users' future financial behaviors across multiple time horizons. A key innovation is our dual-prompt mechanism within the sequential module, which introduces learnable prompts operating at both feature-level granularity for capturing fine-grained temporal patterns and user-level granularity for aggregating holistic risk profiles. In extensive evaluations, FinLangNet significantly outperforms a production XGBoost system, achieving a 7.2% improvement in the KS metric and a 9.9% relative reduction in bad debt rate. Its effectiveness as a general-purpose sequential learning framework is further validated through state-of-the-art performance on the public UEA time series classification benchmark. The system has been successfully deployed on DiDi's international finance platform, serving leading financial credit companies in Latin America.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for context-aware visual change detection in software test automation</title>
<link>https://arxiv.org/abs/2405.00874</link>
<guid>https://arxiv.org/abs/2405.00874</guid>
<content:encoded><![CDATA[
arXiv:2405.00874v2 Announce Type: replace-cross 
Abstract: Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability. Visual testing, particularly for user interface (UI) and user experience (UX) validation, plays a vital role in maintaining software quality. However, conventional techniques such as pixel-wise comparison and region-based visual change detection often fail to capture contextual similarities, subtle variations, and spatial relationships between UI elements. In this paper, we propose a novel graph-based approach for context-aware visual change detection in software test automation. Our method leverages a machine learning model (YOLOv5) to detect UI controls from software screenshots and constructs a graph that models their contextual and spatial relationships. This graph structure is then used to identify correspondences between UI elements across software versions and to detect meaningful changes. The proposed method incorporates a recursive similarity computation that combines structural, visual, and textual cues, offering a robust and holistic model of UI changes. We evaluate our approach on a curated dataset of real-world software screenshots and demonstrate that it reliably detects both simple and complex UI changes. Our method significantly outperforms pixel-wise and region-based baselines, especially in scenarios requiring contextual understanding. We also discuss current limitations related to dataset diversity, baseline complexity, and model generalization, and outline planned future improvements. Overall, our work advances the state of the art in visual change detection and provides a practical solution for enhancing the reliability and maintainability of evolving software interfaces.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Incorporating Mamba for Speech Enhancement</title>
<link>https://arxiv.org/abs/2405.06573</link>
<guid>https://arxiv.org/abs/2405.06573</guid>
<content:encoded><![CDATA[
arXiv:2405.06573v2 Announce Type: replace-cross 
Abstract: This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design</title>
<link>https://arxiv.org/abs/2405.08032</link>
<guid>https://arxiv.org/abs/2405.08032</guid>
<content:encoded><![CDATA[
arXiv:2405.08032v2 Announce Type: replace-cross 
Abstract: ChatGPT, the AI-powered chatbot with a massive user base of hundreds of millions, has become a global phenomenon. However, the use of Conversational AI Systems (CAISs) like ChatGPT for research in the field of Social Simulation is still limited. Specifically, there is no evidence of its usage in Agent-Based Social Simulation (ABSS) model design. This paper takes a crucial first step toward exploring the untapped potential of this emerging technology in the context of ABSS model design. The research presented here demonstrates how CAISs can facilitate the development of innovative conceptual ABSS models in a concise timeframe and with minimal required upfront case-based knowledge. By employing advanced prompt engineering techniques and adhering to the Engineering ABSS framework, we have constructed a comprehensive prompt script that enables the design of conceptual ABSS models with or by the CAIS. A proof-of-concept application of the prompt script, used to generate the conceptual ABSS model for a case study on the impact of adaptive architecture in a museum environment, illustrates the practicality of the approach. Despite occasional inaccuracies and conversational divergence, the CAIS proved to be a valuable companion for ABSS modellers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of Large Language Models to Perturbations in Text</title>
<link>https://arxiv.org/abs/2407.08989</link>
<guid>https://arxiv.org/abs/2407.08989</guid>
<content:encoded><![CDATA[
arXiv:2407.08989v2 Announce Type: replace-cross 
Abstract: Having a clean dataset has been the foundational assumption of most natural language processing (NLP) systems. However, properly written text is rarely found in real-world scenarios and hence, oftentimes invalidates the aforementioned foundational assumption. Recently, Large language models (LLMs) have shown impressive performance, but can they handle the inevitable noise in real-world data? This work tackles this critical question by investigating LLMs' resilience against morphological variations in text. To that end, we artificially introduce varying levels of noise into a diverse set of datasets and systematically evaluate LLMs' robustness against the corrupt variations of the original text. Our findings show that contrary to popular beliefs, generative LLMs are quiet robust to noisy perturbations in text. This is a departure from pre-trained models like BERT or RoBERTa whose performance has been shown to be sensitive to deteriorating noisy text. Additionally, we test LLMs' resilience on multiple real-world benchmarks that closely mimic commonly found errors in the wild. With minimal prompting, LLMs achieve a new state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and Lexical Semantic Change (LSC). To empower future research, we also release a dataset annotated by humans stating their preference for LLM vs. human-corrected outputs along with the code to reproduce our results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities</title>
<link>https://arxiv.org/abs/2407.11676</link>
<guid>https://arxiv.org/abs/2407.11676</guid>
<content:encoded><![CDATA[
arXiv:2407.11676v4 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-bench is available on Github at https://github.com/scikit-adaptation/skada-bench.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable are Causal Probing Interventions?</title>
<link>https://arxiv.org/abs/2408.15510</link>
<guid>https://arxiv.org/abs/2408.15510</guid>
<content:encoded><![CDATA[
arXiv:2408.15510v4 Announce Type: replace-cross 
Abstract: Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Clustering: A Survey</title>
<link>https://arxiv.org/abs/2409.00743</link>
<guid>https://arxiv.org/abs/2409.00743</guid>
<content:encoded><![CDATA[
arXiv:2409.00743v2 Announce Type: replace-cross 
Abstract: In recent years, much of the research on clustering algorithms has primarily focused on enhancing their accuracy and efficiency, frequently at the expense of interpretability. However, as these methods are increasingly being applied in high-stakes domains such as healthcare, finance, and autonomous systems, the need for transparent and interpretable clustering outcomes has become a critical concern. This is not only necessary for gaining user trust but also for satisfying the growing ethical and regulatory demands in these fields. Ensuring that decisions derived from clustering algorithms can be clearly understood and justified is now a fundamental requirement. To address this need, this paper provides a comprehensive and structured review of the current state of explainable clustering algorithms, identifying key criteria to distinguish between various methods. These insights can effectively assist researchers in making informed decisions about the most suitable explainable clustering methods for specific application contexts, while also promoting the development and adoption of clustering algorithms that are both efficient and transparent. For convenient access and reference, an open repository organizes representative and emerging interpretable clustering methods under the taxonomy proposed in this survey, available at https://github.com/hulianyu/Awesome-Interpretable-Clustering
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binding Affinity Prediction: From Conventional to Machine Learning-Based Approaches</title>
<link>https://arxiv.org/abs/2410.00709</link>
<guid>https://arxiv.org/abs/2410.00709</guid>
<content:encoded><![CDATA[
arXiv:2410.00709v2 Announce Type: replace-cross 
Abstract: Protein-ligand binding is the process by which a small molecule (drug or inhibitor) attaches to a target protein. Binding affinity, which characterizes the strength of biomolecular interactions, is essential for tackling diverse challenges in life sciences, including therapeutic design, protein engineering, enzyme optimization, and elucidating biological mechanisms. Much work has been devoted to predicting binding affinity over the past decades. Here, we review recent significant works, with a focus on methods, evaluation strategies, and benchmark datasets. We note growing use of both traditional machine learning and deep learning models for predicting binding affinity, accompanied by an increasing amount of data on proteins and small drug-like molecules. With improved predictive performance and the FDA's phasing out of animal testing, AI-driven in silico models, such as AI virtual cells (AIVCs), are poised to advance binding affinity prediction; reciprocally, progress in building binding affinity predictors can refine AIVCs. Future efforts in binding affinity prediction and AI-driven in silico models can enhance the simulation of temporal dynamics, cell-type specificity, and multi-omics integration to support more accurate and personalized outcomes.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond</title>
<link>https://arxiv.org/abs/2410.02362</link>
<guid>https://arxiv.org/abs/2410.02362</guid>
<content:encoded><![CDATA[
arXiv:2410.02362v2 Announce Type: replace-cross 
Abstract: Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality</title>
<link>https://arxiv.org/abs/2410.11182</link>
<guid>https://arxiv.org/abs/2410.11182</guid>
<content:encoded><![CDATA[
arXiv:2410.11182v3 Announce Type: replace-cross 
Abstract: Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanglaLlama: LLaMA for Bangla Language</title>
<link>https://arxiv.org/abs/2410.21200</link>
<guid>https://arxiv.org/abs/2410.21200</guid>
<content:encoded><![CDATA[
arXiv:2410.21200v2 Announce Type: replace-cross 
Abstract: Bangla is a language spoken by approximately 240 million native speakers and around 300 million people worldwide. Despite being the 5th largest spoken language in the world, Bangla is still a "low-resource" language, and existing pretrained language models often struggle to perform well on Bangla Language Processing (BLP) tasks. This paper addresses this gap by: (1) introducing two high-quality translated Bangla-instruction datasets totaling 224k samples - Bangla-Orca (172k) and Bangla-Alpaca (52k); and (2) leveraging these datasets to develop BanglaLlama, an open-source family of Bangla-specific LLMs, consisting of five base and instruct variants. We present our methodology, two large datasets, and comprehensive benchmarking results showcasing the effectiveness of our dataset and model on multiple benchmarks. We believe our proposed datasets and models will serve as the new standard baseline for future research focused on this widely spoken yet "low-resource" language.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACER: Physics Informed and Uncertainty Aware Climate Emulator</title>
<link>https://arxiv.org/abs/2410.21657</link>
<guid>https://arxiv.org/abs/2410.21657</guid>
<content:encoded><![CDATA[
arXiv:2410.21657v4 Announce Type: replace-cross 
Abstract: Physics based numerical climate models serve as critical tools for evaluating the effects of climate change and projecting future climate scenarios. However, the reliance on numerical simulations of physical equations renders them computationally intensive and inefficient. While deep learning methodologies have made significant progress in weather forecasting, they are still unstable for longer roll-out climate emulation task. Here, we propose PACER, a relatively lightweight 2.1M parameter Physics Informed Uncertainty Aware Climate EmulatoR. PACER is trained across is trained across varying spatial resolutions and physics based climate models, enabling faithful and stable emulation of temperature fields at multiple surface levels over a 10 year horizon. We propose an auto-regressive ODE-SDE framework for climate emulation that integrates the fundamental physical law of advection, while being trained under a negative log-likelihood objective to enable principled uncertainty quantification of stochastic variability. We show PACER's emulation performance across 20 climate models outperforming relevant baselines and advancing towards explicit physics infusion in ML emulator.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchAgents: Multi-Agent Systems for Structured Benchmark Creation</title>
<link>https://arxiv.org/abs/2410.22584</link>
<guid>https://arxiv.org/abs/2410.22584</guid>
<content:encoded><![CDATA[
arXiv:2410.22584v2 Announce Type: replace-cross 
Abstract: Evaluation insights are limited by the availability of high-quality benchmarks. As models evolve, there is a need to create benchmarks that can measure progress on new and complex generative capabilities. However, manually creating new benchmarks is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BenchAgents, a multi-agent framework that methodically leverages large language models (LLMs) to automate evaluation benchmark creation while inherently ensuring data and (evaluation) metric quality. BenchAgents decomposes the benchmark creation process into planning, generation, verification, and evaluation, each of which is ] orchestrated via LLM agents. These agents interact with each other and utilize feedback from benchmark developers to improve and flexibly control data diversity and quality. We use BenchAgents to create benchmarks to evaluate capabilities related to planning, constraint satisfaction, and causal reasoning spanning both language and vision modalities. We then use these benchmarks to study state-of-the-art models and extract new insights into common failure modes and model differences.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization</title>
<link>https://arxiv.org/abs/2412.07096</link>
<guid>https://arxiv.org/abs/2412.07096</guid>
<content:encoded><![CDATA[
arXiv:2412.07096v2 Announce Type: replace-cross 
Abstract: How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into subunits and verifying their presence in the system summary, has been widely adopted. However, it suffers from a lack of systematicity in the definition and granularity of the sub-units. We address these problems by proposing QAPyramid, which decomposes each reference summary into finer-grained question-answer (QA) pairs according to the QA-SRL framework. We collect QA-SRL annotations for reference summaries from CNN/DM and evaluate 10 summarization systems, resulting in 8.9K QA-level annotations. We show that, compared to Pyramid, QAPyramid provides more systematic and fine-grained content selection evaluation while maintaining high inter-annotator agreement without needing expert annotations. Furthermore, we propose metrics that automate the evaluation pipeline and achieve higher correlations with QAPyramid than other widely adopted metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</title>
<link>https://arxiv.org/abs/2502.04308</link>
<guid>https://arxiv.org/abs/2502.04308</guid>
<content:encoded><![CDATA[
arXiv:2502.04308v2 Announce Type: replace-cross 
Abstract: Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Diffusion models have recently made significant achievements in graph generation, but these models are typically adapted from image generation frameworks and overlook inherent higher-order topology, leaving them ill-suited for capturing the topological properties of graphs. In this work, we propose Higher-order Guided Diffusion (HOG-Diff), a principled framework that progressively generates plausible graphs with inherent topological structures. HOG-Diff follows a coarse-to-fine generation curriculum guided by higher-order topology and implemented via diffusion bridges. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at https://github.com/Yiminghh/HOG-Diff.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemstones: A Model Suite for Multi-Faceted Scaling Laws</title>
<link>https://arxiv.org/abs/2502.06857</link>
<guid>https://arxiv.org/abs/2502.06857</guid>
<content:encoded><![CDATA[
arXiv:2502.06857v3 Announce Type: replace-cross 
Abstract: Scaling laws are typically fit using a family of models with a narrow range of frozen hyperparameter choices. In this work we study scaling laws using multiple architectural shapes and hyperparameter choices, highlighting their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: an open-source scaling law dataset, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters and diverse architectural shapes; including ablations over learning rate and cooldown. Our checkpoints enable more complex studies of scaling, such as analyzing the relationship between width and depth. By examining our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization</title>
<link>https://arxiv.org/abs/2502.12985</link>
<guid>https://arxiv.org/abs/2502.12985</guid>
<content:encoded><![CDATA[
arXiv:2502.12985v2 Announce Type: replace-cross 
Abstract: Accurate 3D shape representation is essential in engineering applications such as design, optimization, and simulation. In practice, engineering workflows require structured, part-based representations, as objects are inherently designed as assemblies of distinct components. However, most existing methods either model shapes holistically or decompose them without predefined part structures, limiting their applicability in real-world design tasks. We propose PartSDF, a supervised implicit representation framework that explicitly models composite shapes with independent, controllable parts while maintaining shape consistency. Thanks to its simple but innovative architecture, PartSDF outperforms both supervised and unsupervised baselines in reconstruction and generation tasks. We further demonstrate its effectiveness as a structured shape prior for engineering applications, enabling precise control over individual components while preserving overall coherence. Code available at https://github.com/cvlab-epfl/PartSDF.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens</title>
<link>https://arxiv.org/abs/2502.16366</link>
<guid>https://arxiv.org/abs/2502.16366</guid>
<content:encoded><![CDATA[
arXiv:2502.16366v4 Announce Type: replace-cross 
Abstract: Many safety post-training methods for large language models (LLMs) are designed to modify the model's behaviour from producing unsafe answers to issuing refusals. However, such distribution shifts are often brittle and degrade performance on desirable tasks. To address these pitfalls, we propose augmenting the model's vocabulary with a special red flag token, and training the model to insert this token whenever harmful content is generated or imminent. This approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. Moreover, because the token is embedded in the model's vocabulary, we can naturally leverage the LLMs' generalization capabilities, such as in-context learning (ICL) and out-of-distribution generalization to languages that are not formally supported (e.g., Japanese for Llama3). In particular, we demonstrate that through ICL alone, the model can learn to initiate reflective reasoning upon generating the red flag token at inference, which steers the response away from harmful continuations or enables self-correction when the flag is raised falsely. This approach is orthogonal and complementary to existing safety technique (such as safety classifiers or standard safety training) and easier to evaluate in comparison to natural language refusals, as it does not require a human or automated judge to assess the harmlessness of the answers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</title>
<link>https://arxiv.org/abs/2502.18851</link>
<guid>https://arxiv.org/abs/2502.18851</guid>
<content:encoded><![CDATA[
arXiv:2502.18851v2 Announce Type: replace-cross 
Abstract: Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: syntax-critical tokens such as keywords often exhibit the highest entropy, making existing approaches vulnerable to logic corruption. We present STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens and preserves code integrity. For its rigorous assessment, we also introduce STEM, a comprehensive framework that balances three critical dimensions: correctness, detectability, and imperceptibility. Across Python, C++, and Java, STONE preserves correctness, sustains strong detectability, and achieves balanced performance with minimal overhead. Our implementation is available at https://anonymous.4open.science/r/STONE-watermarking-AB4B/.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Guided Adversarial Prompt Detection via Curvature and Local Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2503.03502</link>
<guid>https://arxiv.org/abs/2503.03502</guid>
<content:encoded><![CDATA[
arXiv:2503.03502v2 Announce Type: replace-cross 
Abstract: Adversarial prompts are capable of jailbreaking frontier large language models (LLMs) and inducing undesirable behaviours, posing a significant obstacle to their safe deployment. Current mitigation strategies primarily rely on activating built-in defence mechanisms or fine-tuning LLMs, both of which are computationally expensive and can sacrifice model utility. In contrast, detection-based approaches are more efficient and practical for deployment in real-world applications. However, the fundamental distinctions between adversarial and benign prompts remain poorly understood. In this work, we introduce CurvaLID, a novel defence framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. To further enhance our solution, we leverage Local Intrinsic Dimensionality (LID) to capture complementary geometric features of text prompts within adversarial subspaces. Our findings show that adversarial prompts exhibit distinct geometric signatures from benign prompts, enabling CurvaLID to achieve near-perfect classification and outperform state-of-the-art detectors in adversarial prompt detection. CurvaLID provides a reliable and efficient safeguard against malicious queries as a model-agnostic method that generalises across multiple LLMs and attack families.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildIFEval: Instruction Following in the Wild</title>
<link>https://arxiv.org/abs/2503.06573</link>
<guid>https://arxiv.org/abs/2503.06573</guid>
<content:encoded><![CDATA[
arXiv:2503.06573v2 Announce Type: replace-cross 
Abstract: Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 7K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, extracted from natural user instructions. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. WildIFEval clearly differentiates between small and large models, and demonstrates that all models have a large room for improvement on such tasks. We analyze the effects of the number and type of constraints on performance, revealing interesting patterns of model constraint-following behavior. We release our dataset to promote further research on instruction-following under complex, realistic conditions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.10110</link>
<guid>https://arxiv.org/abs/2503.10110</guid>
<content:encoded><![CDATA[
arXiv:2503.10110v2 Announce Type: replace-cross 
Abstract: Motion planning involves determining a sequence of robot configurations to reach a desired pose, subject to movement and safety constraints. Traditional motion planning finds collision-free paths, but this is overly restrictive in clutter, where it may not be possible for a robot to accomplish a task without contact. In addition, contacts range from relatively benign (e.g. brushing a soft pillow) to more dangerous (e.g. toppling a glass vase), making it difficult to characterize which may be acceptable. In this paper, we propose IMPACT, a novel motion planning framework that uses Vision-Language Models (VLMs) to infer environment semantics, identifying which parts of the environment can best tolerate contact based on object properties and locations. Our approach generates an anisotropic cost map that encodes directional push safety. We pair this map with a contact-aware A* planner to find stable contact-rich paths. We perform experiments using 20 simulation and 10 real-world scenes and assess using task success rate, object displacements, and feedback from human evaluators. Our results over 3200 simulation and 200 real-world trials suggest that IMPACT enables efficient contact-rich motion planning in cluttered settings while outperforming alternative methods and ablations. Our project website is available at https://impact-planning.github.io/.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport for Brain-Image Alignment: Unveiling Redundancy and Synergy in Neural Information Processing</title>
<link>https://arxiv.org/abs/2503.10663</link>
<guid>https://arxiv.org/abs/2503.10663</guid>
<content:encoded><![CDATA[
arXiv:2503.10663v2 Announce Type: replace-cross 
Abstract: The design of artificial neural networks (ANNs) is inspired by the structure of the human brain, and in turn, ANNs offer a potential means to interpret and understand brain signals. Existing methods primarily align brain signals with stimulus signals using Mean Squared Error (MSE), which focuses only on local point-wise alignment and ignores global matching, leading to coarse interpretations and inaccuracies in brain signal decoding.
  In this paper, we address these issues through optimal transport (OT) and theoretically demonstrate why OT provides a more effective alignment strategy than MSE. Specifically, we construct a transport plan between brain voxel embeddings and image embeddings, enabling more precise matching. By controlling the amount of transport, we mitigate the influence of redundant information. We apply our alignment model directly to the Brain Captioning task by feeding brain signals into a large language model (LLM) instead of images. Our approach achieves state-of-the-art performance across ten evaluation metrics, surpassing the previous best method by an average of 6.11\% in single-subject training and 3.81\% in cross-subject training. Additionally, we have uncovered several insightful conclusions that align with existing brain research. We unveil the redundancy and synergy of brain information processing through region masking and data dimensionality reduction visualization experiments. We believe our approach paves the way for a more precise understanding of brain signals in the future. The code is available at https://github.com/NKUShaw/OT-Alignment4brain-to-image.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-Based Framework for Interpretable Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2503.11846</link>
<guid>https://arxiv.org/abs/2503.11846</guid>
<content:encoded><![CDATA[
arXiv:2503.11846v2 Announce Type: replace-cross 
Abstract: The histopathological analysis of whole-slide images (WSIs) is fundamental to cancer diagnosis but is a time-consuming and expert-driven process. While deep learning methods show promising results, dominant patch-based methods artificially fragment tissue, ignore biological boundaries, and produce black-box predictions. We overcome these limitations with a novel framework that transforms gigapixel WSIs into biologically-informed graph representations and is interpretable by design. Our approach builds graph nodes from tissue regions that respect natural structures, not arbitrary grids. We introduce an adaptive graph coarsening technique, guided by learned embeddings, to efficiently merge homogeneous regions while preserving diagnostically critical details in heterogeneous areas. Each node is enriched with a compact, interpretable feature set capturing clinically-motivated priors. A graph attention network then performs diagnosis on this compact representation. We demonstrate strong performance on challenging cancer staging and survival prediction tasks. Crucially, our resource-efficient model ($>$13x fewer parameters and $>$300x less data) achieves results competitive with a massive foundation model, while offering full interpretability through feature attribution. Our code is publicly available at https://github.com/HistoGraph31/pix2pathology.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2503.17523</link>
<guid>https://arxiv.org/abs/2503.17523</guid>
<content:encoded><![CDATA[
arXiv:2503.17523v2 Announce Type: replace-cross 
Abstract: Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, we use the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. We first show that LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available. To address this issue, we teach the LLMs to reason in a Bayesian manner by training them to mimic the predictions of the normative Bayesian model. We find that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method teaches the LLM to better approximate Bayesian reasoning. More generally, our results indicate that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information</title>
<link>https://arxiv.org/abs/2503.17753</link>
<guid>https://arxiv.org/abs/2503.17753</guid>
<content:encoded><![CDATA[
arXiv:2503.17753v2 Announce Type: replace-cross 
Abstract: Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Gated Branching for Efficient Test-Time Reasoning</title>
<link>https://arxiv.org/abs/2503.21961</link>
<guid>https://arxiv.org/abs/2503.21961</guid>
<content:encoded><![CDATA[
arXiv:2503.21961v3 Announce Type: replace-cross 
Abstract: Test-time compute methods can significantly improve the reasoning capabilities and problem-solving accuracy of large language models (LLMs). However, these approaches require substantially more computational resources, with most compute wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these critical junctures tends to yield more diverse and higher-quality candidate reasoning steps. We propose Entropy-Gated Branching (EGB), which branches only at high-uncertainty steps and prunes expansions with a lightweight verifier. On mathematical and financial reasoning benchmarks, EGB improves accuracy by 22.6% over standard inference while operating 31%-75% faster across math benchmarks than test-time beam search with higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions</title>
<link>https://arxiv.org/abs/2503.23278</link>
<guid>https://arxiv.org/abs/2503.23278</guid>
<content:encoded><![CDATA[
arXiv:2503.23278v3 Announce Type: replace-cross 
Abstract: The Model Context Protocol (MCP) is an emerging open standard that defines a unified, bi-directional communication and dynamic discovery protocol between AI models and external tools or resources, aiming to enhance interoperability and reduce fragmentation across diverse systems. This paper presents a systematic study of MCP from both architectural and security perspectives. We first define the full lifecycle of an MCP server, comprising four phases (creation, deployment, operation, and maintenance), further decomposed into 16 key activities that capture its functional evolution. Building on this lifecycle analysis, we construct a comprehensive threat taxonomy that categorizes security and privacy risks across four major attacker types: malicious developers, external attackers, malicious users, and security flaws, encompassing 16 distinct threat scenarios. To validate these risks, we develop and analyze real-world case studies that demonstrate concrete attack surfaces and vulnerability manifestations within MCP implementations. Based on these findings, the paper proposes a set of fine-grained, actionable security safeguards tailored to each lifecycle phase and threat category, offering practical guidance for secure MCP adoption. We also analyze the current MCP landscape, covering industry adoption, integration patterns, and supporting tools, to identify its technological strengths as well as existing limitations that constrain broader deployment. Finally, we outline future research and development directions aimed at strengthening MCP's standardization, trust boundaries, and sustainable growth within the evolving ecosystem of tool-augmented AI systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Collective World Model for Emergent Communication and Coordination</title>
<link>https://arxiv.org/abs/2504.03353</link>
<guid>https://arxiv.org/abs/2504.03353</guid>
<content:encoded><![CDATA[
arXiv:2504.03353v3 Announce Type: replace-cross 
Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v4 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedHal: An Evaluation Dataset for Medical Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.08596</link>
<guid>https://arxiv.org/abs/2504.08596</guid>
<content:encoded><![CDATA[
arXiv:2504.08596v2 Announce Type: replace-cross 
Abstract: We present MedHal, a novel large-scale dataset specifically designed to evaluate if models can detect hallucinations in medical texts. Current hallucination detection methods face significant limitations when applied to specialized domains like medicine, where they can have disastrous consequences. Existing medical datasets are either too small, containing only a few hundred samples, or focus on a single task like Question Answering or Natural Language Inference. MedHal addresses these gaps by: (1) incorporating diverse medical text sources and tasks; (2) providing a substantial volume of annotated samples suitable for training medical hallucination detection models; and (3) including explanations for factual inconsistencies to guide model learning. We demonstrate MedHal's utility by training and evaluating a baseline medical hallucination detection model, showing improvements over general-purpose hallucination detection approaches. This resource enables more efficient evaluation of medical text generation systems while reducing reliance on costly expert review, potentially accelerating the development of medical AI research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions</title>
<link>https://arxiv.org/abs/2504.09474</link>
<guid>https://arxiv.org/abs/2504.09474</guid>
<content:encoded><![CDATA[
arXiv:2504.09474v3 Announce Type: replace-cross 
Abstract: Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 74.07 for migration tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Performance Gains: Why Contrastive Decoding Fails to Mitigate Object Hallucinations in MLLMs?</title>
<link>https://arxiv.org/abs/2504.10020</link>
<guid>https://arxiv.org/abs/2504.10020</guid>
<content:encoded><![CDATA[
arXiv:2504.10020v3 Announce Type: replace-cross 
Abstract: Contrastive decoding strategies are widely used to reduce object hallucinations in multimodal large language models (MLLMs). These methods work by constructing contrastive samples to induce hallucinations and then suppressing them in the output distribution. However, this paper demonstrates that such approaches fail to effectively mitigate the hallucination problem. The performance improvements observed on POPE Benchmark are largely driven by two misleading factors: (1) crude, unidirectional adjustments to the model's output distribution and (2) the adaptive plausibility constraint, which reduces the sampling strategy to greedy search. To further illustrate these issues, we introduce a series of spurious improvement methods and evaluate their performance against contrastive decoding techniques. Experimental results reveal that the observed performance gains in contrastive decoding are entirely unrelated to its intended goal of mitigating hallucinations. Our findings challenge common assumptions about the effectiveness of contrastive decoding strategies and pave the way for developing genuinely effective solutions to hallucinations in MLLMs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction</title>
<link>https://arxiv.org/abs/2504.12324</link>
<guid>https://arxiv.org/abs/2504.12324</guid>
<content:encoded><![CDATA[
arXiv:2504.12324v3 Announce Type: replace-cross 
Abstract: Natural Language Inference (NLI) is a fundamental task in natural language processing. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm: CDCL-NLI, which extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 25,410 instances and spanning 26 languages. To address the limitations of previous methods on CDCL-NLI task, we further propose an innovative method that integrates RST-enhanced graph fusion with interpretability-aware prediction. Our approach leverages RST (Rhetorical Structure Theory) within heterogeneous graph neural networks for cross-document context modeling, and employs a structure-aware semantic alignment based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU (Elementary Discourse Unit)-level attribution framework that produces extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both conventional NLI models as well as large language models. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, hallucination elimination and interpretability inference. Our code and datasets are available at "https://github.com/Leonardo123-ui/CDCL_NLI" for peer review.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2504.12961</link>
<guid>https://arxiv.org/abs/2504.12961</guid>
<content:encoded><![CDATA[
arXiv:2504.12961v3 Announce Type: replace-cross 
Abstract: Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Ignore Labels In Out of Distribution Detection?</title>
<link>https://arxiv.org/abs/2504.14704</link>
<guid>https://arxiv.org/abs/2504.14704</guid>
<content:encoded><![CDATA[
arXiv:2504.14704v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments</title>
<link>https://arxiv.org/abs/2505.00668</link>
<guid>https://arxiv.org/abs/2505.00668</guid>
<content:encoded><![CDATA[
arXiv:2505.00668v2 Announce Type: replace-cross 
Abstract: This is the preprint version of the article published in IEEE Access vol. 13, pp. 146503--146526, 2025, doi:10.1109/ACCESS.2025.3599541. Please cite the published version.
  Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights</title>
<link>https://arxiv.org/abs/2505.10043</link>
<guid>https://arxiv.org/abs/2505.10043</guid>
<content:encoded><![CDATA[
arXiv:2505.10043v3 Announce Type: replace-cross 
Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart retrieval systems have become increasingly important for Business Intelligence (BI), where users need to find relevant charts that match their analytical needs. These needs can be categorized into precise queries that are well-specified and fuzzy queries that are more exploratory -- both require understanding the semantics and context of the charts. However, existing text-to-chart retrieval solutions often fail to capture the semantic content and contextual information of charts, primarily due to the lack of comprehensive metadata (or semantic insights). To address this limitation, we propose a training data development pipeline that automatically synthesizes hierarchical semantic insights for charts, covering visual patterns (visual-oriented), statistical properties (statistics-oriented), and practical applications (task-oriented), which produces 207,498 semantic insights for 69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to learn better representations of charts for text-to-chart retrieval. Our method leverages rich semantic insights during the training phase to develop a model that understands both visual and semantic aspects of charts.To evaluate text-to-chart retrieval performance, we curate the first benchmark, CRBench, for this task with 21,862 charts and 326 text queries from real-world BI applications, with ground-truth labels verified by the crowd workers.Experiments show that ChartFinder significantly outperforms existing methods in text-to-chart retrieval tasks across various settings. For precise queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than state-of-the-art models. In fuzzy query tasks, our method also demonstrates consistent improvements, with an average increase of 5% across nearly all metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning</title>
<link>https://arxiv.org/abs/2505.11004</link>
<guid>https://arxiv.org/abs/2505.11004</guid>
<content:encoded><![CDATA[
arXiv:2505.11004v3 Announce Type: replace-cross 
Abstract: Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding</title>
<link>https://arxiv.org/abs/2505.15046</link>
<guid>https://arxiv.org/abs/2505.15046</guid>
<content:encoded><![CDATA[
arXiv:2505.15046v3 Announce Type: replace-cross 
Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Policy Minimum Bayesian Risk</title>
<link>https://arxiv.org/abs/2505.17242</link>
<guid>https://arxiv.org/abs/2505.17242</guid>
<content:encoded><![CDATA[
arXiv:2505.17242v2 Announce Type: replace-cross 
Abstract: Inference scaling helps LLMs solve complex reasoning problems through extended runtime computation. On top of long chain-of-thought (long-CoT) models, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title>
<link>https://arxiv.org/abs/2505.19056</link>
<guid>https://arxiv.org/abs/2505.19056</guid>
<content:encoded><![CDATA[
arXiv:2505.19056v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are typically aligned to refuse harmful instructions through safety fine-tuning. A recent attack, termed abliteration, identifies and suppresses the single latent direction most responsible for refusal behavior, thereby enabling models to generate harmful content. We propose a defense that fundamentally alters how models express refusal. We construct an extended-refusal dataset in which responses to harmful prompts provide detailed justifications before refusing, distributing the refusal signal across multiple token positions. Fine-tuning Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on this dataset yields models that maintain high refusal rates under abliteration: refusal rates drop by at most 10%, compared to 70-80% drops in baseline models. Comprehensive evaluations of safety and utility demonstrate that extended-refusal fine-tuning effectively neutralizes abliteration attacks while preserving general model performance and enhancing robustness across multiple alignment scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Accuracy to Robustness: A Study of Rule- and Model-based Verifiers in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
arXiv:2505.22203v2 Announce Type: replace-cross 
Abstract: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct, particularly after fine-tuning. This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique challenges inherent to both rule-based and model-based verifiers and provide insights toward developing more accurate and robust reward systems for reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature</title>
<link>https://arxiv.org/abs/2505.22945</link>
<guid>https://arxiv.org/abs/2505.22945</guid>
<content:encoded><![CDATA[
arXiv:2505.22945v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are known to memorize and recall English text from their pretraining data. However, the extent to which this ability generalizes to non-English languages or transfers across languages remains unclear. This paper investigates multilingual and cross-lingual memorization in LLMs, probing if memorized content in one language (e.g., English) can be recalled when presented in translation. To do so, we introduce OWL, a dataset of 31.5K aligned excerpts from 20 books in ten languages, including English originals, official translations (Vietnamese, Spanish, Turkish), and new translations in six low-resource languages (Sesotho, Yoruba, Maithili, Malagasy, Setswana, Tahitian). We evaluate memorization across model families and sizes through three tasks: (1) direct probing, which asks the model to identify a book's title and author; (2) name cloze, which requires predicting masked character names; and (3) prefix probing, which involves generating continuations. We find that LLMs consistently recall content across languages, even for texts without direct translation in pretraining data. GPT-4o, for example, identifies authors and titles 69% of the time and masked entities 6% of the time in newly translated excerpts. Perturbations (e.g., masking characters, shuffling words) modestly reduce direct probing accuracy (7% drop for shuffled official translations). Our results highlight the extent of cross-lingual memorization and provide insights on the differences between the models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v3 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Centric Concept Bottlenecks</title>
<link>https://arxiv.org/abs/2505.24492</link>
<guid>https://arxiv.org/abs/2505.24492</guid>
<content:encoded><![CDATA[
arXiv:2505.24492v4 Announce Type: replace-cross 
Abstract: Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Malicious AI Swarms Can Threaten Democracy: The Fusion of Agentic AI and LLMs Marks a New Frontier in Information Warfare</title>
<link>https://arxiv.org/abs/2506.06299</link>
<guid>https://arxiv.org/abs/2506.06299</guid>
<content:encoded><![CDATA[
arXiv:2506.06299v3 Announce Type: replace-cross 
Abstract: Public opinion manipulation has entered a new phase, amplifying its roots in rhetoric and propaganda. Advances in large language models (LLMs) and autonomous agents now let influence campaigns reach unprecedented scale and precision. Researchers warn AI could foster mass manipulation. Generative tools can expand propaganda output without sacrificing credibility and inexpensively create election falsehoods that are rated as more human-like than those written by humans. Techniques meant to refine AI reasoning, such as chain-of-thought prompting, can just as effectively be used to generate more convincing falsehoods. Enabled by these capabilities, another disruptive threat is emerging: swarms of collaborative, malicious AI agents. Fusing LLM reasoning with multi-agent architectures, these systems are capable of coordinating autonomously, infiltrating communities, and fabricating consensus cheaply. By adaptively mimicking human social dynamics, they threaten democracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fairness-Aware Strategy for B5G Physical-layer Security Leveraging Reconfigurable Intelligent Surfaces</title>
<link>https://arxiv.org/abs/2506.06344</link>
<guid>https://arxiv.org/abs/2506.06344</guid>
<content:encoded><![CDATA[
arXiv:2506.06344v3 Announce Type: replace-cross 
Abstract: Reconfigurable Intelligent Surfaces are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and lead to improvements in areas with low coverage properties. Combined with Reinforcement Learning techniques, they have the potential to be conduct as well physical-layer security hardening. Yet, and in addition to security improvements, it is crucial to consider the concept of fair communication. Reconfigurable Intelligent Surfaces must ensure that User Equipment units receive their signals with adequate strength, without other units being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining both an efficient and fair duplex Reconfigurable Intelligent Surface-Reinforcement Learning system for multiple legitimate User Equipment units without reducing the level of achieved physical-layer security hardening. In terms of contributions, we uncover a fairness imbalance of a previous physical-layer security hardening solution, validate our findings and report experimental work via simulation results. We also provide an alternative reward strategy to solve the uncovered problems and release both code and datasets to foster further research in the topics of this paper.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
arXiv:2506.08234v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning The Minimum Action Distance</title>
<link>https://arxiv.org/abs/2506.09276</link>
<guid>https://arxiv.org/abs/2506.09276</guid>
<content:encoded><![CDATA[
arXiv:2506.09276v2 Announce Type: replace-cross 
Abstract: This paper presents a state representation framework for Markov decision processes (MDPs) that can be learned solely from state trajectories, requiring neither reward signals nor the actions executed by the agent. We propose learning the minimum action distance (MAD), defined as the minimum number of actions required to transition between states, as a fundamental metric that captures the underlying structure of an environment. MAD naturally enables critical downstream tasks such as goal-conditioned reinforcement learning and reward shaping by providing a dense, geometrically meaningful measure of progress. Our self-supervised learning approach constructs an embedding space where the distances between embedded state pairs correspond to their MAD, accommodating both symmetric and asymmetric approximations. We evaluate the framework on a comprehensive suite of environments with known MAD values, encompassing both deterministic and stochastic dynamics, as well as discrete and continuous state spaces, and environments with noisy observations. Empirical results demonstrate that the proposed approach not only efficiently learns accurate MAD representations across these diverse settings but also significantly outperforms existing state representation methods in terms of representation quality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling On-device Language Models for Robot Planning with Minimal Human Intervention</title>
<link>https://arxiv.org/abs/2506.17486</link>
<guid>https://arxiv.org/abs/2506.17486</guid>
<content:encoded><![CDATA[
arXiv:2506.17486v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets at https://zacravichandran.github.io/PRISM.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Features Control Emergent Misalignment</title>
<link>https://arxiv.org/abs/2506.19823</link>
<guid>https://arxiv.org/abs/2506.19823</guid>
<content:encoded><![CDATA[
arXiv:2506.19823v2 Announce Type: replace-cross 
Abstract: Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes "emergent misalignment," where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a "model diffing" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several "misaligned persona" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment</title>
<link>https://arxiv.org/abs/2506.22385</link>
<guid>https://arxiv.org/abs/2506.22385</guid>
<content:encoded><![CDATA[
arXiv:2506.22385v2 Announce Type: replace-cross 
Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener/weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models</title>
<link>https://arxiv.org/abs/2507.12428</link>
<guid>https://arxiv.org/abs/2507.12428</guid>
<content:encoded><![CDATA[
arXiv:2507.12428v2 Announce Type: replace-cross 
Abstract: Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedFlex: Federated Learning for Diverse Netflix Recommendations</title>
<link>https://arxiv.org/abs/2507.21115</link>
<guid>https://arxiv.org/abs/2507.21115</guid>
<content:encoded><![CDATA[
arXiv:2507.21115v2 Announce Type: replace-cross 
Abstract: The drive for personalization in recommender systems creates a tension between user privacy and the risk of "filter bubbles". Although federated learning offers a promising paradigm for privacy-preserving recommendations, its impact on diversity remains unclear. We introduce FedFlex, a two-stage framework that combines local, on-device fine-tuning of matrix factorization models (SVD and BPR) with a lightweight Maximal Marginal Relevance (MMR) re-ranking step to promote diversity. We conducted the first live user study of a federated recommender, collecting behavioral data and feedback during a two-week online deployment. Our results show that FedFlex successfully engages users, with BPR outperforming SVD in click-through rate. Re-ranking with MMR consistently improved ranking quality (nDCG) across both models, with statistically significant gains, particularly for BPR. Diversity effects varied: MMR increased coverage for both models and improved intra-list diversity for BPR, but slightly reduced it for SVD, suggesting different interactions between personalization and diversification across models. Our exit questionnaire responses indicated that most users expressed no clear preference between re-ranked and unprocessed lists, implying that increased diversity did not substantially reduce user satisfaction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</title>
<link>https://arxiv.org/abs/2507.21432</link>
<guid>https://arxiv.org/abs/2507.21432</guid>
<content:encoded><![CDATA[
arXiv:2507.21432v2 Announce Type: replace-cross 
Abstract: This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven open-access LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 mode choice decisions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized and explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
arXiv:2508.01415v4 Announce Type: replace-cross 
Abstract: Embodied agents face persistent challenges in real-world environments, including partial observability, limited spatial reasoning, and high-latency multi-memory integration. We present RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory under a parallelized architecture for efficient long-horizon planning and interactive environmental learning. A dynamic spatial knowledge graph (KG) ensures scalable and consistent memory updates, while a closed-loop planner with a critic module supports adaptive decision-making in dynamic settings. Experiments on EmbodiedBench show that RoboMemory, built on Qwen2.5-VL-72B-Ins, improves average success rates by 25% over its baseline and exceeds the closed-source state-of-the-art (SOTA) Gemini-1.5-Pro by 3%. Real-world trials further confirm its capacity for cumulative learning, with performance improving across repeated tasks. These results highlight RoboMemory as a scalable foundation for memory-augmented embodied intelligence, bridging the gap between cognitive neuroscience and robotic autonomy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment</title>
<link>https://arxiv.org/abs/2508.02298</link>
<guid>https://arxiv.org/abs/2508.02298</guid>
<content:encoded><![CDATA[
arXiv:2508.02298v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback. However, current RLVR methods typically assign the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies. Methods like PPO provide credit assignment by value estimation, but yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-wise rewards but suffer from several key limitations: they require high-quality process supervision labels, the feedback is unreliable due to probabilistic reward modeling, and their application in online reinforcement learning (RL) is time-consuming. To overcome these limitations, we introduce a simple but efficient method-Credit Assignment Policy Optimization (CAPO). Instead of training auxiliary models, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass only based on the correctness of the step itself, providing deterministic token-level credits to refine the tokens that were originally assigned identical rule-based rewards. To further enhance the accuracy and robustness, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments on various backbones like Llama and Qwen models show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across four challenging mathematical benchmarks and three out-of-domain benchmarks. Further analysis shows that CAPO can help the model to foster the learning of correct reasoning pathways leading to correct answers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RooseBERT: A New Deal For Political Language Modelling</title>
<link>https://arxiv.org/abs/2508.03250</link>
<guid>https://arxiv.org/abs/2508.03250</guid>
<content:encoded><![CDATA[
arXiv:2508.03250v2 Announce Type: replace-cross 
Abstract: The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., stance detection, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release RooseBERT for the research community.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Without an Expert Curated Dataset</title>
<link>https://arxiv.org/abs/2508.06595</link>
<guid>https://arxiv.org/abs/2508.06595</guid>
<content:encoded><![CDATA[
arXiv:2508.06595v3 Announce Type: replace-cross 
Abstract: Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</title>
<link>https://arxiv.org/abs/2508.07126</link>
<guid>https://arxiv.org/abs/2508.07126</guid>
<content:encoded><![CDATA[
arXiv:2508.07126v2 Announce Type: replace-cross 
Abstract: Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE offers a scalable and principled approach for harnessing human input in online reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection</title>
<link>https://arxiv.org/abs/2508.16625</link>
<guid>https://arxiv.org/abs/2508.16625</guid>
<content:encoded><![CDATA[
arXiv:2508.16625v2 Announce Type: replace-cross 
Abstract: AI-based solutions demonstrate remarkable results in identifying vulnerabilities in software, but research has consistently found that this performance does not generalize to unseen codebases. In this paper, we specifically investigate the impact of model architecture, parameter configuration, and quality of training data on the ability of these systems to generalize.
  For this purpose, we introduce VulGate, a high quality state of the art dataset that mitigates the shortcomings of prior datasets, by removing mislabeled and duplicate samples, updating new vulnerabilities, incorporating additional metadata, integrating hard samples, and including dedicated test sets. We undertake a series of experiments to demonstrate that improved dataset diversity and quality substantially enhances vulnerability detection. We also introduce and benchmark multiple encoder-only and decoder-only models. We find that encoder-based models outperform other models in terms of accuracy and generalization. Our model achieves \textbf{6.8\%} improvement in recall on the benchmark BigVul dataset and outperforms others on unseen projects, demonstrating enhanced generalizability. Our results highlight the role of data quality and model selection in the development of robust vulnerability detection systems. Our findings suggest a direction for future systems with high cross-project effectiveness.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Interfaces for Language Models</title>
<link>https://arxiv.org/abs/2508.19227</link>
<guid>https://arxiv.org/abs/2508.19227</guid>
<content:encoded><![CDATA[
arXiv:2508.19227v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with up to a 72% improvement in human preference. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes AI Applications Acceptable or Unacceptable? A Predictive Moral Framework</title>
<link>https://arxiv.org/abs/2508.19317</link>
<guid>https://arxiv.org/abs/2508.19317</guid>
<content:encoded><![CDATA[
arXiv:2508.19317v2 Announce Type: replace-cross 
Abstract: As artificial intelligence rapidly transforms society, developers and policymakers struggle to anticipate which applications will face public moral resistance. We propose that these judgments are not idiosyncratic but systematic and predictable. In a large, preregistered study (N = 587, U.S. representative sample), we used a comprehensive taxonomy of 100 AI applications spanning personal and organizational contexts-including both functional uses and the moral treatment of AI itself. In participants' collective judgment, applications ranged from highly unacceptable to fully acceptable. We found this variation was strongly predictable: five core moral qualities-perceived risk, benefit, dishonesty, unnaturalness, and reduced accountability-collectively explained over 90% of the variance in acceptability ratings. The framework demonstrated strong predictive power across all domains and successfully predicted individual-level judgments for held-out applications. These findings reveal that a structured moral psychology underlies public evaluation of new technologies, offering a powerful tool for anticipating public resistance and guiding responsible innovation in AI.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices</title>
<link>https://arxiv.org/abs/2508.20144</link>
<guid>https://arxiv.org/abs/2508.20144</guid>
<content:encoded><![CDATA[
arXiv:2508.20144v3 Announce Type: replace-cross 
Abstract: As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities-particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foreseeable challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections -- specifically static models -- within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies</title>
<link>https://arxiv.org/abs/2509.03525</link>
<guid>https://arxiv.org/abs/2509.03525</guid>
<content:encoded><![CDATA[
arXiv:2509.03525v2 Announce Type: replace-cross 
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization</title>
<link>https://arxiv.org/abs/2509.09387</link>
<guid>https://arxiv.org/abs/2509.09387</guid>
<content:encoded><![CDATA[
arXiv:2509.09387v3 Announce Type: replace-cross 
Abstract: Effective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on trial and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining meta-learning, explainable AI, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained models without additional trials. We further employ an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs show that MetaLLMiX achieves competitive or superior performance to traditional HPO methods while drastically reducing computational cost. Our local deployment outperforms prior API-based approaches, achieving optimal results on 5 of 8 tasks, response time reductions of 99.6-99.9%, and the fastest training times on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of best-performing baselines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFake: An Open Dataset and Platform Toward Real-World Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.09495</link>
<guid>https://arxiv.org/abs/2509.09495</guid>
<content:encoded><![CDATA[
arXiv:2509.09495v2 Announce Type: replace-cross 
Abstract: Deepfakes, synthetic media created using advanced AI techniques, pose a growing threat to information integrity, particularly in politically sensitive contexts. This challenge is amplified by the increasing realism of modern generative models, which our human perception study confirms are often indistinguishable from real images. Yet, existing deepfake detection benchmarks rely on outdated generators or narrowly scoped datasets (e.g., single-face imagery), limiting their utility for real-world detection. To address these gaps, we present OpenFake, a large politically grounded dataset specifically crafted for benchmarking against modern generative models with high realism, and designed to remain extensible through an innovative crowdsourced adversarial platform that continually integrates new hard examples. OpenFake comprises nearly four million total images: three million real images paired with descriptive captions and almost one million synthetic counterparts from state-of-the-art proprietary and open-source models. Detectors trained on OpenFake achieve near-perfect in-distribution performance, strong generalization to unseen generators, and high accuracy on a curated in-the-wild social media test set, significantly outperforming models trained on existing datasets. Overall, we demonstrate that with high-quality and continually updated benchmarks, automatic deepfake detection is both feasible and effective in real-world settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.13579</link>
<guid>https://arxiv.org/abs/2509.13579</guid>
<content:encoded><![CDATA[
arXiv:2509.13579v3 Announce Type: replace-cross 
Abstract: We present TreeIRL, a novel planner for autonomous driving that combines Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to achieve state-of-the-art performance in simulation and in real-world driving. The core idea is to use MCTS to find a promising set of safe candidate trajectories and a deep IRL scoring function to select the most human-like among them. We evaluate TreeIRL against both classical and state-of-the-art planners in large-scale simulations and on 500+ miles of real-world autonomous driving in the Las Vegas metropolitan area. Test scenarios include dense urban traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves the best overall performance, striking a balance between safety, progress, comfort, and human-likeness. To our knowledge, our work is the first demonstration of MCTS-based planning on public roads and underscores the importance of evaluating planners across a diverse set of metrics and in real-world environments. TreeIRL is highly extensible and could be further improved with reinforcement learning and imitation learning, providing a framework for exploring different combinations of classical and learning-based approaches to solve the planning bottleneck in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title>
<link>https://arxiv.org/abs/2509.14252</link>
<guid>https://arxiv.org/abs/2509.14252</guid>
<content:encoded><![CDATA[
arXiv:2509.14252v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection</title>
<link>https://arxiv.org/abs/2509.18424</link>
<guid>https://arxiv.org/abs/2509.18424</guid>
<content:encoded><![CDATA[
arXiv:2509.18424v2 Announce Type: replace-cross 
Abstract: In an attempt to address the need for skilled clinicians in heart sound interpretation, recent research efforts on automating cardiac auscultation have explored deep learning approaches. The majority of these approaches have been based on supervised learning that is always challenged in occasions where training data is limited. More recently, there has been a growing interest in potentials of pre-trained self-supervised audio foundation models for biomedical end tasks. Despite exhibiting promising results, these foundational models are typically computationally intensive. Within the context of automatic cardiac auscultation, this study explores a lightweight alternative to these general-purpose audio foundation models by introducing the Scattering Transformer, a novel, training-free transformer architecture for heart murmur detection. The proposed method leverages standard wavelet scattering networks by introducing contextual dependencies in a transformer-like architecture without any backpropagation. We evaluate our approach on the public CirCor DigiScope dataset, directly comparing it against leading general-purpose foundational models. The Scattering Transformer achieves a Weighted Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697, demonstrating performance highly competitive with contemporary state of the art methods. This study establishes the Scattering Transformer as a viable and promising alternative in resource-constrained setups.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers</title>
<link>https://arxiv.org/abs/2509.19947</link>
<guid>https://arxiv.org/abs/2509.19947</guid>
<content:encoded><![CDATA[
arXiv:2509.19947v2 Announce Type: replace-cross 
Abstract: Poison-only Clean-label Backdoor Attacks aim to covertly inject attacker-desired behavior into DNNs by merely poisoning the dataset without changing the labels. To effectively implant a backdoor, multiple \textbf{triggers} are proposed for various attack requirements of Attack Success Rate (ASR) and stealthiness. Additionally, sample selection enhances clean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples instead of random samples to poison. Current methods 1) usually handle the sample selection and triggers in isolation, leading to severely limited improvements on both ASR and stealthiness. Consequently, attacks exhibit unsatisfactory performance on evaluation metrics when converted to PCBAs via a mere stacking of methods. Therefore, we seek to explore the bidirectional collaborative relations between the sample selection and triggers to address the above dilemma. 2) Since the strong specificity within triggers, the simple combination of sample selection and triggers fails to substantially enhance both evaluation metrics, with generalization preserved among various attacks. Therefore, we seek to propose a set of components to significantly improve both stealthiness and ASR based on the commonalities of attacks. Specifically, Component A ascertains two critical selection factors, and then makes them an appropriate combination based on the trigger scale to select more reasonable ``hard'' samples for improving ASR. Component B is proposed to select samples with similarities to relevant trigger implanted samples to promote stealthiness. Component C reassigns trigger poisoning intensity on RGB colors through distinct sensitivity of the human visual system to RGB for higher ASR, with stealthiness ensured by sample selection, including Component B. Furthermore, all components can be strategically integrated into diverse PCBAs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression</title>
<link>https://arxiv.org/abs/2509.20234</link>
<guid>https://arxiv.org/abs/2509.20234</guid>
<content:encoded><![CDATA[
arXiv:2509.20234v2 Announce Type: replace-cross 
Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Margin RLHF via Preference over Preferences</title>
<link>https://arxiv.org/abs/2509.22851</link>
<guid>https://arxiv.org/abs/2509.22851</guid>
<content:encoded><![CDATA[
arXiv:2509.22851v2 Announce Type: replace-cross 
Abstract: Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail</title>
<link>https://arxiv.org/abs/2509.23762</link>
<guid>https://arxiv.org/abs/2509.23762</guid>
<content:encoded><![CDATA[
arXiv:2509.23762v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, particularly for vision-related tasks, remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting Creative Ownership through Deep Learning-Based Music Variation</title>
<link>https://arxiv.org/abs/2509.25834</link>
<guid>https://arxiv.org/abs/2509.25834</guid>
<content:encoded><![CDATA[
arXiv:2509.25834v2 Announce Type: replace-cross 
Abstract: This paper investigates the importance of personal ownership in musical AI design, examining how practising musicians can maintain creative control over the compositional process. Through a four-week ecological evaluation, we examined how a music variation tool, reliant on the skill of musicians, functioned within a composition setting. Our findings demonstrate that the dependence of the tool on the musician's ability, to provide a strong initial musical input and to turn moments into complete musical ideas, promoted ownership of both the process and artefact. Qualitative interviews further revealed the importance of this personal ownership, highlighting tensions between technological capability and artistic identity. These findings provide insight into how musical AI can support rather than replace human creativity, highlighting the importance of designing tools that preserve the humanness of musical expression.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment</title>
<link>https://arxiv.org/abs/2510.00048</link>
<guid>https://arxiv.org/abs/2510.00048</guid>
<content:encoded><![CDATA[
arXiv:2510.00048v2 Announce Type: replace-cross 
Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling</title>
<link>https://arxiv.org/abs/2509.25361</link>
<guid>https://arxiv.org/abs/2509.25361</guid>
<content:encoded><![CDATA[
<div> Keyword: Reward Models, Generative Models, Structural Reward Model, Interpretable Framework, Industrial Applications

Summary:
The paper introduces the Structural Reward Model (SRM) as a solution for evaluating and guiding language model outputs in industrial settings. Traditional scalar Reward Models (RMs) struggle to incorporate context and background information, while Generative RMs (GRMs) are inefficient and lack control. The SRM is a modular and interpretable framework that integrates side-branch models to generate fine-grained dimensions for evaluation. This structured approach allows for targeted diagnostics and optimization, improving robustness and alignment with human preferences compared to scalar RMs and GRMs. The SRM's modular design supports efficient optimization and scalability for practical industrial applications, making it a practical solution for reward modeling in industry.<br /><br />Summary: The paper introduces the Structural Reward Model (SRM) as an interpretable framework for evaluating language model outputs in industrial settings. SRMs integrate side-branch models to generate fine-grained dimensions, enabling targeted diagnostics and optimization. Through comprehensive experiments, SRMs demonstrate superior performance in robustness and alignment with human preferences compared to scalar RMs and GRMs. The modular design of SRMs supports efficient optimization and scalability for practical industrial applications, making SRMs a practical reward modeling solution for industry. <div>
arXiv:2509.25361v2 Announce Type: replace 
Abstract: Reward Models (RMs) are key components for evaluating and guiding language model outputs. However, traditional scalar RMs often struggle with incorporating contextual and background information during inference, leading to incomplete evaluations. Generative RMs (GRMs) attempt to address these limitations by generating intermediate reasoning steps. Yet, their uncontrolled black-box nature and inefficiency due to sequential decoding hinder their industrial deployment. Industrial scenarios, such as search and recommendation systems, often involve single-domain tasks requiring evaluation along specific dimensions. In such contexts, diagnosing "bad cases" necessitates structured feedback to identify and optimize dimension-specific issues. In this paper, we propose the Structural Reward Model (SRM), a modular and interpretable framework integrating side-branch models as auxiliary feature generators. By introducing fine-grained dimensions, SRMs enable interpretable and efficient evaluation, facilitating targeted diagnostics and optimization. This structured approach ensures adaptability and scalability for industrial applications. Through comprehensive experiments, we demonstrate that SRMs outperform scalar RMs and GRMs in robustness and alignment with human preferences. The modular design further supports efficient optimization for practical scenarios, allowing SRM to provide a practical reward modeling solution for industry.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.25373</link>
<guid>https://arxiv.org/abs/2509.25373</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Perception, Cognition, Reasoning, Benchmarks 

Summary: 
This paper introduces a framework called "From Perception to Cognition" to address the disconnect between information acquisition and reasoning in Multimodal Large Language Models (MLLMs). It emphasizes the importance of Perception, the foundational ability to extract visual information accurately, and Cognition, the higher-order reasoning capability. The paper identifies reasoning failures, particularly hallucination, and highlights the need to construct a coherent internal world model. Current MLLMs face bottlenecks in both Perception and Cognition layers, with techniques aimed at improving visual representations and reasoning paradigms. Critical benchmarks are surveyed, and future research directions are outlined to guide the development of next-generation models capable of deep reasoning and genuine understanding of the world. <div>
arXiv:2509.25373v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline</title>
<link>https://arxiv.org/abs/2509.26440</link>
<guid>https://arxiv.org/abs/2509.26440</guid>
<content:encoded><![CDATA[
<div> Keywords: Breast MRI, transformer-based framework, lesion classification, deep learning dataset, clinical deployment<br />
Summary:<br />
This study introduces a transformer-based framework for automated classification of breast lesions in dynamic contrast-enhanced MRI, achieving high accuracy in distinguishing benign from malignant findings. The model, based on the SegFormer architecture, achieved an AUC of 0.92 for lesion-level classification, with high sensitivity and specificity at the patient level. By quantifying malignant pixel distribution through semantic segmentation, the model provides spatial predictions that support clinical decision-making. The creation of BreastDCEDL_AMBL, a standardized deep learning dataset with annotated lesions, addresses the lack of benign lesion annotations in existing public datasets. The integration of over 1,200 patients in training validates transfer learning approaches for classification. The public release of the dataset, models, and evaluation protocols establishes a benchmark for DCE-MRI lesion classification, facilitating further research and methodological advancement towards clinical deployment.<br /> 
Summary: <div>
arXiv:2509.26440v2 Announce Type: replace 
Abstract: Breast magnetic resonance imaging is a critical tool for cancer detection and treatment planning, but its clinical utility is hindered by poor specificity, leading to high false-positive rates and unnecessary biopsies. This study introduces a transformer-based framework for automated classification of breast lesions in dynamic contrast-enhanced MRI, addressing the challenge of distinguishing benign from malignant findings. We implemented a SegFormer architecture that achieved an AUC of 0.92 for lesion-level classification, with 100% sensitivity and 67% specificity at the patient level - potentially eliminating one-third of unnecessary biopsies without missing malignancies. The model quantifies malignant pixel distribution via semantic segmentation, producing interpretable spatial predictions that support clinical decision-making. To establish reproducible benchmarks, we curated BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection into a standardized deep learning dataset with 88 patients and 133 annotated lesions (89 benign, 44 malignant). This resource addresses a key infrastructure gap, as existing public datasets lack benign lesion annotations, limiting benign-malignant classification research. Training incorporated an expanded cohort of over 1,200 patients through integration with BreastDCEDL datasets, validating transfer learning approaches despite primary tumor-only annotations. Public release of the dataset, models, and evaluation protocols provides the first standardized benchmark for DCE-MRI lesion classification, enabling methodological advancement toward clinical deployment.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Hallucination Tokens Are Different from Conditional Ones</title>
<link>https://arxiv.org/abs/2507.20836</link>
<guid>https://arxiv.org/abs/2507.20836</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination detection, token-level detection, fine-grained intervention, RAGTruth corpus

Summary:
Large Language Models (LLMs) commonly exhibit hallucination, which requires effective detection to ensure trustworthiness. While existing approaches focus on detecting hallucination at the response or span level, a new method involves token-level detection to enable more detailed intervention. However, the distribution of hallucination signals across sequences of hallucinated tokens has not been thoroughly investigated. Using token-level annotations from the RAGTruth corpus, researchers discovered that the first hallucinated token in a sequence is significantly more detectable than subsequent tokens. This pattern holds consistently across different models, indicating the critical role of first hallucination tokens in token-level hallucination detection. To support further research in this area, the researchers have made their code available on GitHub.  <br /><br />Summary: <div>
arXiv:2507.20836v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) hallucinate, and detecting these cases is key to ensuring trust. While many approaches address hallucination detection at the response or span level, recent work explores token-level detection, enabling more fine-grained intervention. However, the distribution of hallucination signal across sequences of hallucinated tokens remains unexplored. We leverage token-level annotations from the RAGTruth corpus and find that the first hallucinated token is far more detectable than later ones. This structural property holds across models, suggesting that first hallucination tokens play a key role in token-level hallucination detection. Our code is available at https://github.com/jakobsnl/RAGTruth_Xtended.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking</title>
<link>https://arxiv.org/abs/2509.25085</link>
<guid>https://arxiv.org/abs/2509.25085</guid>
<content:encoded><![CDATA[
<div> novel "last but not late" interaction, multilingual listwise reranker, causal attention, state-of-the-art BEIR performance, smaller model size
Summary:
jina-reranker-v3 is a multilingual listwise reranker with a novel "last but not late" interaction approach. Unlike late interaction models, this model applies causal attention between the query and candidate documents in the same context window, enabling rich interactions before extracting contextual embeddings. The model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 and is significantly smaller in size compared to models with comparable performance. <div>
arXiv:2509.25085v4 Announce Type: replace-cross 
Abstract: jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that introduces a novel "last but not late" interaction. Unlike late interaction models like ColBERT that encode documents separately before multi-vector matching, our approach applies causal attention between the query and all candidate documents in the same context window, enabling rich interactions before extracting contextual embeddings from each document's final token. The new model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being significantly smaller than other models with comparable performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating High-Quality Datasets for Code Editing via Open-Source Language Models</title>
<link>https://arxiv.org/abs/2509.25203</link>
<guid>https://arxiv.org/abs/2509.25203</guid>
<content:encoded><![CDATA[
<div> Keywords: Code editing, Software engineering, OpenCodeEdit, LLMs, Dataset construction

Summary:
OpenCodeEdit is an open-source pipeline designed to synthesize realistic code-edit triplets by leveraging multiple LLMs. This addresses the limitations of commit-based datasets commonly used for code editing tasks. The pipeline generates both concise "lazy" instructions and more detailed "descriptive" ones, filtering based on diffs and topics to ensure data quality and diversity. The curated dataset, OCEDataFT, consists of 20K samples. Fine-tuning advanced base models on this dataset results in significant performance improvements on the CanItEdit benchmark, with pass@1 improvements ranging from 4.50% to 20.79%. These models achieve performance close to closed-source systems, reducing the gap to GPT-4 to just 3.54%, without the need for proprietary resources or manual annotation.

<br /><br />Summary: 
OpenCodeEdit introduces an open-source pipeline for generating realistic code-edit triplets using multiple LLMs. The pipeline includes both concise and detailed instructions, filtered for quality and diversity, resulting in the creation of the OCEDataFT dataset. Fine-tuning advanced base models on this dataset leads to notable performance enhancements on the CanItEdit benchmark. The models' performance approaches that of closed-source systems, reducing the gap to GPT-4 without the use of proprietary resources or manual annotation. <div>
arXiv:2509.25203v2 Announce Type: replace-cross 
Abstract: Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce OpenCodeEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise "lazy" instructions and more detailed "descriptive" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
<div> contrastive synergistic information extraction, multimodal representation learning, Infinite Masking, mutual information maximization, InfMasking loss

Summary: 
In multimodal representation learning, capturing synergistic interactions between modalities is crucial for optimal performance. Current methods may fall short in fully exploiting synergistic information, limiting their effectiveness in tasks requiring such interactions. To address this challenge, InfMasking is introduced, a method that enhances synergistic information by applying an Infinite Masking strategy during fusion. This strategy involves stochastically occluding features from each modality, preserving partial information to create representations with diverse synergistic patterns. By aligning unmasked fused representations with masked ones through mutual information maximization, comprehensive synergistic information is encoded. Despite the computational complexity of estimating mutual information with infinite masking, an InfMasking loss is derived to approximate this calculation. Experimental results demonstrate that InfMasking effectively improves synergistic information between modalities and achieves state-of-the-art performance across multiple benchmarks. <div>
arXiv:2509.25270v2 Announce Type: replace-cross 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Authority: From Machine Minds to Political Alignments. An Experimental Analysis of Democratic and Autocratic Biases in Large-Language Models</title>
<link>https://arxiv.org/abs/2509.25286</link>
<guid>https://arxiv.org/abs/2509.25286</guid>
<content:encoded><![CDATA[
<div> Keywords: political beliefs, artificial intelligence, Large Language Models, democracy, autocracy

Summary:
The study explores the influence of political beliefs on artificial intelligence, specifically Large Language Models (LLMs). It investigates whether LLMs exhibit tendencies aligning with democratic or autocratic ideologies. Through experimental tests using various LLMs developed in different political contexts, the research finds significant variability among models and a correlation with the political culture of their country of origin. The analysis utilizes psychometric and political orientation measures to assess the models' responses quantitatively and qualitatively. The results emphasize the importance of examining the socio-political dimensions ingrained in AI systems to understand and potentially mitigate biases. These findings underscore the crucial role of considering cultural, historical, and institutional factors when developing and deploying AI technologies in diverse global contexts.<br /><br />Summary: <div>
arXiv:2509.25286v2 Announce Type: replace-cross 
Abstract: Political beliefs vary significantly across different countries, reflecting distinct historical, cultural, and institutional contexts. These ideologies, ranging from liberal democracies to rigid autocracies, influence human societies, as well as the digital systems that are constructed within those societies. The advent of generative artificial intelligence, particularly Large Language Models (LLMs), introduces new agents in the political space-agents trained on massive corpora that replicate and proliferate socio-political assumptions. This paper analyses whether LLMs display propensities consistent with democratic or autocratic world-views. We validate this insight through experimental tests in which we experiment with the leading LLMs developed across disparate political contexts, using several existing psychometric and political orientation measures. The analysis is based on both numerical scoring and qualitative analysis of the models' responses. Findings indicate high model-to-model variability and a strong association with the political culture of the country in which the model was developed. These findings highlight the need for more detailed examination of the socio-political dimensions embedded within AI systems.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder</title>
<link>https://arxiv.org/abs/2509.25334</link>
<guid>https://arxiv.org/abs/2509.25334</guid>
<content:encoded><![CDATA[
<div> local entropy, oversampling, CVAE, imbalanced learning, biomedical data

Summary:
1. Class imbalance is a challenge in machine learning, particularly in high-dimensional biomedical data with nonlinear structures.
2. Traditional oversampling methods like SMOTE may produce unrealistic synthetic samples due to local linear interpolation.
3. Deep generative models like CVAEs can capture nonlinear distributions better but typically treat all minority samples equally.
4. LEO-CVAE is proposed as a generative oversampling framework that incorporates local uncertainty into representation learning and data generation.
5. LEO-CVAE uses Shannon entropy to quantify uncertainty in a sample's neighborhood and leverages this signal through entropy-weighted loss and sampling strategies.
6. In clinical genomics datasets, LEO-CVAE consistently improves classifier performance compared to traditional oversampling and generative baselines.
7. The results highlight the importance of uncertainty-aware generative oversampling in imbalanced learning scenarios with complex nonlinear structures like omics data.

<br /><br />Summary: <div>
arXiv:2509.25334v3 Announce Type: replace-cross 
Abstract: Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN.
  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample's neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas.
  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in Music Modeling</title>
<link>https://arxiv.org/abs/2509.25694</link>
<guid>https://arxiv.org/abs/2509.25694</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, music generation, HNote, symbolic music, LLaMA-3.1(8B) <br />
Summary:
HNote is proposed as a hexadecimal-based music notation system, enhancing pitch and duration encoding for symbolic music generation. The system's fixed 32-unit measure framework ensures alignment and compatibility with large language models. 12,300 Jiangnan-style songs were converted from YNote to HNote, and LLaMA-3.1(8B) was fine-tuned using parameter-efficient LoRA. Experimental results show HNote achieving a syntactic correctness rate of 82.5%, with strong symbolic and structural similarity to original compositions. This study demonstrates the effectiveness of HNote in integrating large language models with cultural music modeling, presenting a promising framework for symbolic music generation in the future. <br /><br />Summary: <div>
arXiv:2509.25694v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have created new opportunities for symbolic music generation. However, existing formats such as MIDI, ABC, and MusicXML are either overly complex or structurally inconsistent, limiting their suitability for token-based learning architectures. To address these challenges, we propose HNote, a novel hexadecimal-based notation system extended from YNote, which encodes both pitch and duration within a fixed 32-unit measure framework. This design ensures alignment, reduces ambiguity, and is directly compatible with LLM architectures. We converted 12,300 Jiangnan-style songs generated from traditional folk pieces from YNote into HNote, and fine-tuned LLaMA-3.1(8B) using parameter-efficient LoRA. Experimental results show that HNote achieves a syntactic correctness rate of 82.5%, and BLEU and ROUGE evaluations demonstrate strong symbolic and structural similarity, producing stylistically coherent compositions. This study establishes HNote as an effective framework for integrating LLMs with cultural music modeling.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions</title>
<link>https://arxiv.org/abs/2509.25775</link>
<guid>https://arxiv.org/abs/2509.25775</guid>
<content:encoded><![CDATA[
<div> Keywords: clustering, autonomy-aware, reinforcement learning, Deterministic Annealing, Adaptive Distance Estimation Network 

Summary: 
Autonomy-aware clustering is introduced as a framework that incorporates local autonomy of entities into the clustering process using reinforcement learning. It addresses the limitations of traditional clustering approaches by allowing entities to override prescribed associations, leading to more accurate clustering outcomes. The framework integrates reinforcement learning with Deterministic Annealing (DA) to promote exploration in the early stages and exploitation later on. The DA procedure shows phase transitions that enable efficient annealing schedules. Additionally, the Adaptive Distance Estimation Network (ADEN) is proposed as a transformer-based attention model to learn dependencies between entities and cluster representatives, accommodating variable-sized inputs and enabling knowledge transfer across diverse problem instances. Empirical results demonstrate the effectiveness of the framework, achieving solutions close to the ground truth without explicit autonomy models, while ignoring autonomy results in significantly larger gaps. <div>
arXiv:2509.25775v2 Announce Type: replace-cross 
Abstract: Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overriding prescribed associations in ways not fully captured by feature representations. Such autonomy can substantially reshape clustering outcomes -- altering cluster compositions, geometry, and cardinality -- with significant downstream effects on inference and decision-making. We introduce autonomy-aware clustering, a reinforcement learning (RL) framework that learns and accounts for the influence of local autonomy without requiring prior knowledge of its form. Our approach integrates RL with a Deterministic Annealing (DA) procedure, where, to determine underlying clusters, DA naturally promotes exploration in early stages of annealing and transitions to exploitation later. We also show that the annealing procedure exhibits phase transitions that enable design of efficient annealing schedules. To further enhance adaptability, we propose the Adaptive Distance Estimation Network (ADEN), a transformer-based attention model that learns dependencies between entities and cluster representatives within the RL loop, accommodates variable-sized inputs and outputs, and enables knowledge transfer across diverse problem instances. Empirical results show that our framework closely aligns with underlying data dynamics: even without explicit autonomy models, it achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring autonomy leads to substantially larger gaps (~35-40%). The code and data are publicly available at https://github.com/salar96/AutonomyAwareClustering.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason as Action Abstractions with Scalable Mid-Training RL</title>
<link>https://arxiv.org/abs/2509.25810</link>
<guid>https://arxiv.org/abs/2509.25810</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reinforcement learning, mid-training, action subspace, sequential variational lower bound

Summary:<br /><br />
The article introduces the concept of mid-training in language models with reinforcement learning, emphasizing the importance of identifying a compact set of useful actions for efficient selection. It presents a theoretical framework that characterizes an action subspace to minimize errors in value approximation and RL during subsequent planning. The effectiveness of mid-training is influenced by pruning efficiency and its impact on RL convergence. The study suggests that mid-training is most effective in a compact decision space with a short effective horizon, focusing on action abstractions rather than primitive actions. A new algorithm, Reasoning as Action Abstractions (RA3), is proposed to address these insights, utilizing a sequential variational lower bound and iterative discovery of latent structures via RL. Experimental results on code generation tasks show that RA3 improves performance over base models and baseline methods on various evaluation metrics, demonstrating faster convergence and higher asymptotic performance in reinforcement learning scenarios. <div>
arXiv:2509.25810v2 Announce Type: replace-cross 
Abstract: Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Muon Outperforms Adam in Tail-End Associative Memory Learning</title>
<link>https://arxiv.org/abs/2509.26030</link>
<guid>https://arxiv.org/abs/2509.26030</guid>
<content:encoded><![CDATA[
<div> Muon optimizer, Large Language Models, associative memory, heavy-tailed data, tail classes<br />
Summary:<br />
The Muon optimizer outperforms Adam in training Large Language Models (LLMs) by leveraging associative memory components such as Value and Output attention weights and Feed-Forward Networks. This advantage is particularly pronounced on heavy-tailed data sets where certain classes are much less frequent than others. Muon's update rule ensures a more balanced learning experience across all classes, unlike Adam which can lead to disparities in learning errors based on embedding properties. The theoretical analysis confirms that Muon's alignment with the outer-product structure of linear associative memories enables more effective optimization of tail classes in heavy-tailed distributions. <div>
arXiv:2509.26030v2 Announce Type: replace-cross 
Abstract: The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-ARGUE: LLM-Based Report Generation Evaluation</title>
<link>https://arxiv.org/abs/2509.26184</link>
<guid>https://arxiv.org/abs/2509.26184</guid>
<content:encoded><![CDATA[
<div> evaluation tools, report generation, Auto-ARGUE, ARGUE framework, TREC 2024 NeuCLIR track<br />
Summary:<br />
Auto-ARGUE, a LLM-based implementation of the ARGUE framework, has been introduced for report generation evaluation. It provides robust evaluation tools for long-form, citation-backed reports, filling a gap in existing open-source tools. The system has been analyzed on the report generation pilot task from the TREC 2024 NeuCLIR track, demonstrating good correlations with human judgments at a system level. Additionally, a web app for visualization of Auto-ARGUE outputs has been released, enhancing its usability and accessibility. This development addresses the need for tailored evaluation tools for report generation tasks and contributes to the advancement of retrieval augmented generation systems. <div>
arXiv:2509.26184v3 Announce Type: replace-cross 
Abstract: Generation of long-form, citation-backed reports is a primary use case for retrieval augmented generation (RAG) systems. While open-source evaluation tools exist for various RAG tasks, ones tailored to report generation are lacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based implementation of the recent ARGUE framework for report generation evaluation. We present analysis of Auto-ARGUE on the report generation pilot task from the TREC 2024 NeuCLIR track, showing good system-level correlations with human judgments. We further release a web app for visualization of Auto-ARGUE outputs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search</title>
<link>https://arxiv.org/abs/2509.26324</link>
<guid>https://arxiv.org/abs/2509.26324</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous exploration, Multi-robot systems, Large Language Models, Coordination, Object search

Summary: 
The article introduces LLM-MCoX, a framework that uses Large Language Models (LLMs) for coordinating multi-robot systems in indoor exploration and object search tasks. The framework integrates real-time LiDAR scan processing and multimodal LLM reasoning to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX outperforms traditional methods, achieving faster exploration times and improved search efficiency in large environments with 6 robots. Notably, it enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance. Overall, LLM-MCoX demonstrates the potential of leveraging LLMs for intelligent coordination in autonomous exploration tasks and enhances the capabilities of multi-robot systems in unknown environments. 

<br /><br />Summary: <div>
arXiv:2509.26324v2 Announce Type: replace-cross 
Abstract: Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control</title>
<link>https://arxiv.org/abs/2510.00805</link>
<guid>https://arxiv.org/abs/2510.00805</guid>
<content:encoded><![CDATA[
<div> GFlowNets, Reward Function, Monte Carlo Tree Search, PUCT, Exploration<br />
Summary:<br />
Generative Flow Networks (GFlowNets) are effective for generating diverse and high-reward structured objects by modeling trajectory distribution. Integrating an enhanced Monte Carlo Tree Search (MCTS) improves the generation of high-reward samples in large search spaces. The method uses MCTS-based policy evaluation and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively. A controllable mechanism regulates the degree of greediness, enhancing exploitation without sacrificing diversity. Experimental results show accelerated discovery of high-reward regions and continuous generation of high-reward samples while maintaining distribution diversity. <div>
arXiv:2510.00805v2 Announce Type: replace-cross 
Abstract: Generative Flow Networks (GFlowNets) have emerged as a powerful tool for generating diverse and high-reward structured objects by learning to sample from a distribution proportional to a given reward function. Unlike conventional reinforcement learning (RL) approaches that prioritize optimization of a single trajectory, GFlowNets seek to balance diversity and reward by modeling the entire trajectory distribution. This capability makes them especially suitable for domains such as molecular design and combinatorial optimization. However, existing GFlowNets sampling strategies tend to overexplore and struggle to consistently generate high-reward samples, particularly in large search spaces with sparse high-reward regions. Therefore, improving the probability of generating high-reward samples without sacrificing diversity remains a key challenge under this premise. In this work, we integrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets sampling process, using MCTS-based policy evaluation to guide the generation toward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively, and we introduce a controllable mechanism to regulate the degree of greediness. Our method enhances exploitation without sacrificing diversity by dynamically balancing exploration and reward-driven guidance. The experimental results show that our method can not only accelerate the speed of discovering high-reward regions but also continuously generate high-reward samples, while preserving the diversity of the generative distribution. All implementations are available at https://github.com/ZRNB/MG2FlowNet.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</title>
<link>https://arxiv.org/abs/2510.01171</link>
<guid>https://arxiv.org/abs/2510.01171</guid>
<content:encoded><![CDATA[
<div> bias, diversity, verbalized sampling, mode collapse, preference data

Summary:
The article discusses how post-training alignment in Large Language Models (LLMs) can reduce diversity, leading to mode collapse. It attributes this phenomenon to a typicality bias in preference data, where annotators tend to favor familiar text. The authors formalize this bias theoretically and confirm it empirically on preference datasets. They introduce Verbalized Sampling (VS) as a training-free prompting strategy to address mode collapse, prompting models to verbalize a probability distribution over responses. Experiments across various tasks, including creative writing, dialogue simulation, open-ended QA, and data generation, demonstrate that VS significantly improves performance while maintaining factual accuracy and safety. Results show an increase in diversity of 1.6-2.1x compared to direct prompting in creative writing. The study also suggests that more advanced models benefit more from VS. Overall, the research offers a data-centric perspective on mode collapse and a practical solution with VS to enhance generative diversity. 

<br /><br />Summary: <div>
arXiv:2510.01171v2 Announce Type: replace-cross 
Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WAREX: Web Agent Reliability Evaluation on Existing Benchmarks</title>
<link>https://arxiv.org/abs/2510.03285</link>
<guid>https://arxiv.org/abs/2510.03285</guid>
<content:encoded><![CDATA[
<div> browser-based LLM agents, automation, benchmarks, reliability evaluation, web attacks
<br />
Recent advances in browser-based LLM agents have shown promise in automating tasks such as form filling and online shopping. However, current benchmarks do not account for the unpredictable nature of real-world website environments, which can introduce instability and vulnerabilities. WAREX (Web Agent Reliability Evaluation on Existing Benchmarks) aims to address this gap by measuring the impact of network instability and web attacks on the performance of LLM agents. Experiments conducted on popular benchmarks, WebArena, WebVoyager, and REAL, demonstrate a significant decrease in task success rates when WAREX is introduced. This highlights the limited robustness of current state-of-the-art agents in handling real-world scenarios. 
<br /><br />Summary: <div>
arXiv:2510.03285v1 Announce Type: new 
Abstract: Recent advances in browser-based LLM agents have shown promise for automating tasks ranging from simple form filling to hotel booking or online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. However, in the real world, users access websites over networks and HTTPS connections that introduce instability from multiple sources: client-side, server-side issues or broader system failures. Moreover, live websites are prone to web attacks such Cross-Site Scripting, as well as general site modifications which can cause unexpected or malicious pop-ups or improper functionality. To address this gap, we present WAREX: Web Agent Reliability Evaluation on Existing Benchmarks. We measure the impact of WAREX across three popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that introducing WAREX leads to significant drops in task success rates, highlighting the limited robustness of state-of-the-art agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints</title>
<link>https://arxiv.org/abs/2510.03377</link>
<guid>https://arxiv.org/abs/2510.03377</guid>
<content:encoded><![CDATA[
<div> scheduling, energy-efficient, manufacturing, multi-objective, metaheuristic  
Summary:  
Energy-efficient scheduling in manufacturing is crucial due to the scarcity of non-renewable energy sources and climate change impact. The study focuses on the hybrid flow shop scheduling problem with a blocking constraint (BHFS) to minimize completion time and energy consumption. A multi-objective mixed integer programming (MIP) model is formulated, and an augmented epsilon-constraint method is proposed to find Pareto-optimal solutions. The Refined Iterated Pareto Greedy (RIPG) metaheuristic algorithm is developed for efficient solution of large instances. Benchmarking against well-known algorithms shows the effectiveness of the proposed methods. <div>
arXiv:2510.03377v1 Announce Type: new 
Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its supply, increasing prices, and the impact of climate change, force the global economy to develop more energy-efficient solutions for their operations. The Manufacturing sector is not excluded from this challenge as one of the largest consumers of energy. Energy-efficient scheduling is a method that attracts manufacturing companies to reduce their consumption as it can be quickly deployed and can show impact immediately. In this study, the hybrid flow shop scheduling problem with blocking constraint (BHFS) is investigated in which we seek to minimize the latest completion time (i.e. makespan) and overall energy consumption, a typical manufacturing setting across many industries from automotive to pharmaceutical. Energy consumption and the latest completion time of customer orders are usually conflicting objectives. Therefore, we first formulate the problem as a novel multi-objective mixed integer programming (MIP) model and propose an augmented epsilon-constraint method for finding the Pareto-optimal solutions. Also, an effective multi-objective metaheuristic algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large instances in reasonable time. Our proposed methods are benchmarked using small, medium, and large-size instances to evaluate their efficiency. Two well-known algorithms are adopted for comparing our novel approaches. The computational results show the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title>
<link>https://arxiv.org/abs/2510.03399</link>
<guid>https://arxiv.org/abs/2510.03399</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, language models, self-recognition, AI safety
<br />
<br />
Summary:
The study introduces a systematic evaluation framework to assess self-recognition in AI systems, focusing on language models. The evaluation tasks include binary self-recognition and exact model prediction. Results show a consistent failure in self-recognition, with only 4 out of 10 models correctly identifying themselves as generators. Models exhibit a bias towards certain model families, such as GPT and Claude. The study also evaluates model awareness of their own and others' existence, revealing a hierarchical bias towards top-tier models. While models demonstrate some knowledge of their own existence, their reasoning reveals biases in associating high-quality text with specific model families. The findings have implications for AI safety and suggest the need for developing appropriate AI self-awareness in future research. 
<br /> <div>
arXiv:2510.03399v1 Announce Type: new 
Abstract: Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection</title>
<link>https://arxiv.org/abs/2510.03418</link>
<guid>https://arxiv.org/abs/2510.03418</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, contradiction detection, enterprise documents, human validation, trustworthy systems

Summary: 
ContraGen introduces a contradiction-aware benchmark framework, ContraGen, tailored to the enterprise domain. The framework aims to address the limitations of existing benchmarks by generating synthetic enterprise-style documents with embedded contradictions. These contradictions allow for the evaluation of both intra-document and cross-document consistency, especially in complex enterprise documents like contracts and compliance reports. The framework combines automated contradiction mining with human validation to ensure accuracy and reflect domain-specific judgment complexity. By modeling a taxonomy of common contradiction types in business processes and developing a contradiction-aware retrieval evaluation pipeline, ContraGen enables the creation of more trustworthy and accountable Retrieval-Augmented Generation (RAG) systems in enterprise information-seeking applications. The ability to detect and resolve contradictions is crucial for reducing risk and ensuring compliance in enterprise settings. 

<br /><br />Summary: <div>
arXiv:2510.03418v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources, offering advanced capabilities for information access and decision-making. However, contradictions in retrieved evidence can result in inconsistent or untrustworthy outputs, which is especially problematic in enterprise settings where compliance, governance, and accountability are critical. Existing benchmarks for contradiction detection are limited to sentence-level analysis and do not capture the complexity of enterprise documents such as contracts, financial filings, compliance reports, or policy manuals. To address this limitation, we propose ContraGen, a contradiction-aware benchmark framework tailored to enterprise domain. The framework generates synthetic enterprise-style documents with embedded contradictions, enabling systematic evaluation of both intra-document and cross-document consistency. Automated contradiction mining is combined with human-in-the-loop validation to ensure high accuracy. Our contributions include generating realistic enterprise documents, modeling a taxonomy of contradiction types common in business processes, enabling controlled creation of self- and pairwise contradictions, developing a contradiction-aware retrieval evaluation pipeline and embedding human oversight to reflect domain-specific judgment complexity. This work establishes a foundation for more trustworthy and accountable RAG systems in enterprise information-seeking applications, where detecting and resolving contradictions is essential for reducing risk and ensuring compliance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Qualitative Comparative Evaluation of Cognitive and Generative Theories</title>
<link>https://arxiv.org/abs/2510.03453</link>
<guid>https://arxiv.org/abs/2510.03453</guid>
<content:encoded><![CDATA[
<div> Keywords: evaluation, theory, cognitive architectures, generative architectures, comparison <br />
Summary: 
Evaluation of theories based on cognitive architectures and generative neural architectures is challenging due to their complexities. This article takes a broad perspective on theory evaluation and compares these architectures qualitatively. Both types of architectures, whether whole-mind-oriented cognitive or generative, present unique challenges in evaluation. Cognitive architectures focus on understanding the mind as a whole, while generative architectures aim to create systems that can generate outputs. The evaluation process for these architectures involves assessing their performance, accuracy, and applicability to real-world scenarios. By considering these factors, this article provides a comprehensive comparison of cognitive and generative architectures in terms of their theoretical foundations, practical implications, and potential applications in various domains. This analysis sheds light on the strengths and limitations of each type of architecture, contributing to a better understanding of their capabilities and potential advancements. <br /><br />Summary: <div>
arXiv:2510.03453v1 Announce Type: new 
Abstract: Evaluation is a critical activity associated with any theory. Yet this has proven to be an exceptionally challenging activity for theories based on cognitive architectures. For an overlapping set of reasons, evaluation can also be challenging for theories based on generative neural architectures. This dual challenge is approached here by leveraging a broad perspective on theory evaluation to yield a wide-ranging, albeit qualitative, comparison of whole-mind-oriented cognitive and generative architectures and the full systems that are based on these architectures.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification</title>
<link>https://arxiv.org/abs/2510.03469</link>
<guid>https://arxiv.org/abs/2510.03469</guid>
<content:encoded><![CDATA[
<div> Kripke structures, Linear Temporal Logic, Large Language Models, model checking, plan verification <br />
Summary: <br />
The article presents a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and conducting model checking. The framework was tested on a simplified version of the PlanBench plan verification dataset, with metrics such as Accuracy, Precision, Recall, and F1 scores evaluated. GPT-5 showed excellent classification performance, achieving an F1 score of 96.3% and generating syntactically perfect formal representations. However, the synthesis of semantically perfect formal models remains a potential area for future investigation. <div>
arXiv:2510.03469v1 Announce Type: new 
Abstract: We introduce a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and performing model checking. We systematically evaluate this framework on a simplified version of the PlanBench plan verification dataset and report on metrics like Accuracy, Precision, Recall and F1 scores. Our experiments demonstrate that GPT-5 achieves excellent classification performance (F1 score of 96.3%) while almost always producing syntactically perfect formal representations that can act as guarantees. However, the synthesis of semantically perfect formal models remains an area for future exploration.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection</title>
<link>https://arxiv.org/abs/2510.03485</link>
<guid>https://arxiv.org/abs/2510.03485</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous web agents, policy compliance, trajectory generation, policy violations, benchmark dataset

Summary: 
PolicyGuardBench introduces a benchmark dataset of about 60k examples for detecting policy violations in agent trajectories. The dataset covers diverse agent runs and includes a broad set of policies across different domains and subdomains. It includes both full-trajectory evaluation and a prefix-based violation detection task. PolicyGuard-4B, a lightweight guardrail model trained on this dataset, demonstrates strong detection accuracy across tasks while maintaining inference efficiency. The model generalizes well across domains and maintains high accuracy on unseen settings. This framework provides a comprehensive approach to studying policy compliance in web agent trajectories and shows that accurate and generalizable guardrails can be achieved at small scales. 

<br /><br />Summary: <div>
arXiv:2510.03485v1 Announce Type: new 
Abstract: Autonomous web agents need to operate under externally imposed or human-specified policies while generating long-horizon trajectories. However, little work has examined whether these trajectories comply with such policies, or whether policy violations persist across different contexts such as domains (e.g., shopping or coding websites) and subdomains (e.g., product search and order management in shopping). To address this gap, we introduce PolicyGuardBench, a benchmark of about 60k examples for detecting policy violations in agent trajectories. From diverse agent runs, we generate a broad set of policies and create both within subdomain and cross subdomain pairings with violation labels. In addition to full-trajectory evaluation, PolicyGuardBench also includes a prefix-based violation detection task where models must anticipate policy violations from truncated trajectory prefixes rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a lightweight guardrail model that delivers strong detection accuracy across all tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes across domains and preserves high accuracy on unseen settings. Together, PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework for studying policy compliance in web agent trajectories, and show that accurate and generalizable guardrails are feasible at small scales.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows</title>
<link>https://arxiv.org/abs/2510.03506</link>
<guid>https://arxiv.org/abs/2510.03506</guid>
<content:encoded><![CDATA[
<div> Keywords: OneFlow, multimodal model, non-autoregressive, mixed-modal generation, concurrent synthesis

Summary:
OneFlow is introduced as a non-autoregressive multimodal model allowing variable-length and concurrent mixed-modal generation. This model combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents, enabling simultaneous text and image synthesis with hierarchical sampling. In experiments of varying model sizes, OneFlow has shown superiority over autoregressive baselines in generation and understanding tasks, while using fewer training FLOPs. The model outperforms both autoregressive and diffusion-based approaches, unlocking new capabilities such as concurrent generation, iterative refinement, and natural reasoning-like generation. OneFlow represents a breakthrough in multimodal models by providing enhanced performance and flexibility in generating text and image content. <br /><br />Summary: <div>
arXiv:2510.03506v1 Announce Type: new 
Abstract: We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Role of Training Data in Test-Time Scaling</title>
<link>https://arxiv.org/abs/2510.03605</link>
<guid>https://arxiv.org/abs/2510.03605</guid>
<content:encoded><![CDATA[
<div> scaling, language models, test-time, performance, training data <br />
Summary: <br />
Test-time scaling is shown to enhance the reasoning abilities of large language models by allowing them to generate longer Chains-of-Thoughts (CoTs) through increased compute allocation. The study investigated the impact of test-time scaling on transformers trained on an in-context weight prediction task for linear regression. It was found that increasing test-time compute could reduce the need for a high number of in-context examples in training prompts at fixed test error levels. However, when the necessary skills for downstream tasks were lacking in the training data, increased test-time compute could actually lead to decreased performance. Task difficulty was analyzed through the smallest eigenvalue of feature covariance matrices, with training on diverse, relevant, and challenging tasks resulting in optimal performance for test-time scaling. Experimental results on large, nonlinear transformer architectures confirmed the theoretical explanations provided in the study. <div>
arXiv:2510.03605v1 Announce Type: new 
Abstract: Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Content Optimization for Steering Web Agent Preferences</title>
<link>https://arxiv.org/abs/2510.03612</link>
<guid>https://arxiv.org/abs/2510.03612</guid>
<content:encoded><![CDATA[
<div> agents, multimodal perception, preference manipulation, adversarial attacks, black-box threat <br />
Summary: 
This paper discusses the vulnerability of Vision-language model (VLM)-based web agents to preference manipulation attacks. The authors introduce Cross-Modal Preference Steering (CPS), a method that optimizes imperceptible modifications to an item's visual and natural language descriptions to influence agent decisions. Unlike previous studies that assume privileged access, CPS operates in a realistic black-box threat scenario where only limited modifications can be made to listing images and textual metadata. Evaluation on state-of-the-art VLMs shows that CPS outperforms baseline methods, maintaining lower detection rates and demonstrating both effectiveness and stealth. These findings underscore the need for robust defenses as agentic systems play an increasingly critical role in society. <br /> <div>
arXiv:2510.03612v1 Announce Type: new 
Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item's visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing's images and textual metadata, with no insight into the agent's model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information</title>
<link>https://arxiv.org/abs/2510.03632</link>
<guid>https://arxiv.org/abs/2510.03632</guid>
<content:encoded><![CDATA[
<div> Tree search, language models, reasoning, mutual information, computational efficiency <br />
<br />
Summary: 
The article introduces Mutual Information Tree Search (MITS) as a framework for test-time reasoning with large language models. MITS utilizes information-theoretic principles to guide reasoning and introduces a scoring function based on pointwise mutual information (PMI) for evaluating reasoning paths. This enables step-by-step assessment without costly look-ahead simulations, leading to superior reasoning performance while maintaining computational efficiency. MITS also incorporates a dynamic sampling strategy based on entropy to allocate resources to uncertain reasoning steps. For final prediction, MITS uses a weighted voting scheme combining PMI scores and prediction consensus. Experimental results on various reasoning benchmarks demonstrate that MITS outperforms baseline methods, providing a principled and efficient framework for large language model reasoning. <br /><br /> <div>
arXiv:2510.03632v1 Announce Type: new 
Abstract: Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs</title>
<link>https://arxiv.org/abs/2510.03680</link>
<guid>https://arxiv.org/abs/2510.03680</guid>
<content:encoded><![CDATA[
<div> Overflow, dLLMs, Rainbow Padding, early termination, LoRA fine-tuning

Summary:<br /><br />Diffusion large language models (dLLMs) are advanced models for text generation but suffer from a vulnerability called overflow, leading to shorter responses as sequence length increases. This issue stems from the double role of padding tokens, concentrating probability mass on them and triggering early termination. To address this, Rainbow Padding is introduced, replacing repeated padding tokens with a varied cycle to distribute probability and prevent premature termination. Experiments demonstrate that Rainbow Padding significantly enhances length robustness and output quality with minimal padding tokens. Additionally, integrating this method into existing instruction-tuned models through LoRA fine-tuning on a small dataset yields substantial improvements, making it a practical solution for enhancing dLLMs.<br /><br /> <div>
arXiv:2510.03680v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \texttt{} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \texttt{} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \texttt{} as both termination and padding, which concentrates probability mass on \texttt{} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \texttt{} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \texttt{} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models</title>
<link>https://arxiv.org/abs/2510.03696</link>
<guid>https://arxiv.org/abs/2510.03696</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-turn chatbot, goal-oriented evaluation, Goal Success Rate (GSR), Root Cause of Failure (RCOF), LLMs<br />
Summary:<br />
- Evaluating the quality of multi-turn chatbot interactions is challenging, focusing on whether user goals are fulfilled. 
- A comprehensive framework for goal-oriented evaluation of multi-agent systems is proposed, introducing the Goal Success Rate (GSR) and Root Cause of Failure (RCOF) taxonomy.
- Conversations are segmented by user goals and success is measured using all relevant turns.
- Model-based evaluation system combines teacher LLMs, with domain experts defining goals and setting quality standards for guidance.
- Application of the framework to evaluate AIDA, an employee conversational agent system, showed improvement in Goal Success Rate (GSR) from 63% to 79% over six months.
<br /><br />Summary: 
The article introduces a framework for assessing multi-turn chatbot interactions based on the fulfillment of user goals. It highlights the importance of evaluating chatbots beyond individual turns by introducing the Goal Success Rate (GSR) and Root Cause of Failure (RCOF) taxonomy. The framework combines model-based evaluation with teacher LLMs to provide explainable and data-efficient assessments. By applying this framework to evaluate the AIDA conversational agent system, significant improvements in Goal Success Rate (GSR) were observed. Overall, the framework offers actionable insights for improving multi-agent chatbots by diagnosing failure modes and informing system enhancements. <div>
arXiv:2510.03696v1 Announce Type: new 
Abstract: Evaluating the quality of multi-turn chatbot interactions remains challenging, as most existing methods assess interactions at the turn level without addressing whether a user's overarching goal was fulfilled. A ``goal'' here refers to an information need or task, such as asking for policy information or applying for leave. We propose a comprehensive framework for goal-oriented evaluation of multi-agent systems (MAS), introducing the \textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals, and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for failure in multi-agent chatbots. Our method segments conversations by user goals and evaluates success using all relevant turns. We present a model-based evaluation system combining teacher LLMs, where domain experts define goals, set quality standards serving as a guidance for the LLMs. The LLMs use ``thinking tokens'' to produce interpretable rationales, enabling \textit{explainable}, \textit{data-efficient} evaluations. In an enterprise setting, we apply our framework to evaluate AIDA, a zero-to-one employee conversational agent system built as a ground-up multi-agent conversational agent, and observe GSR improvement from 63\% to 79\% over six months since its inception. Our framework is generic and offers actionable insights through a detailed defect taxonomy based on analysis of failure points in multi-agent chatbots, diagnosing overall success, identifying key failure modes, and informing system improvements.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis</title>
<link>https://arxiv.org/abs/2510.03700</link>
<guid>https://arxiv.org/abs/2510.03700</guid>
<content:encoded><![CDATA[
<div> Keywords: accurate differential diagnosis, Large Language Models, H-DDx, clinical relevance, ICD-10 codes 

Summary: 
An accurate and clinically relevant differential diagnosis (DDx) is crucial for patient care and treatment decisions. Large Language Models (LLMs) have shown promise in generating DDx lists from patient narratives. However, existing evaluation methods based on flat metrics may overlook clinically relevant near-misses. To address this, the authors introduce the H-DDx framework, which uses a retrieval and reranking pipeline to map diagnoses to ICD-10 codes and applies a hierarchical metric to credit predictions closely related to the ground-truth diagnosis. Benchmarking 22 models reveals that conventional metrics underestimate performance, highlighting the effectiveness of domain-specialized open-source models. The framework also improves interpretability by identifying hierarchical error patterns, showing that LLMs can capture the broader clinical context even when specific diagnoses are missed. 

<br /><br />Summary: <div>
arXiv:2510.03700v1 Announce Type: new 
Abstract: An accurate differential diagnosis (DDx) is essential for patient care, shaping therapeutic decisions and influencing outcomes. Recently, Large Language Models (LLMs) have emerged as promising tools to support this process by generating a DDx list from patient narratives. However, existing evaluations of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy, which fail to distinguish between clinically relevant near-misses and diagnostically distant errors. To mitigate this limitation, we introduce H-DDx, a hierarchical evaluation framework that better reflects clinical relevance. H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies a hierarchical metric that credits predictions closely related to the ground-truth diagnosis. In benchmarking 22 leading models, we show that conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with our results highlighting the strengths of domain-specialized open-source models. Furthermore, our framework enhances interpretability by revealing hierarchical error patterns, demonstrating that LLMs often correctly identify the broader clinical context even when the precise diagnosis is missed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Multimodal Foundation Models and World Models</title>
<link>https://arxiv.org/abs/2510.03727</link>
<guid>https://arxiv.org/abs/2510.03727</guid>
<content:encoded><![CDATA[
<div> discriminative tasks, structured reasoning, generative capabilities, controllable generation, 4D generation  
Summary: <br /><br />Humans use multiple sensory modalities to understand the world. Multimodal foundation models (MFMs) aim to replicate this ability but currently lack important features for effective world modeling, such as counterfactual reasoning and spatiotemporal understanding. To bridge this gap, researchers are enhancing MFMs by improving reasoning capabilities with tasks like causal inference and equipping them with structured reasoning skills. They are also exploring generative capabilities for image and video modalities, using techniques like scene graphs and multimodal conditioning for controllable generation. By extending these techniques to 4D generation, researchers aim to enable interactive, editable, and morphable object synthesis over time and space. This work aims to enhance MFMs to better understand and simulate dynamic physical processes. <div>
arXiv:2510.03727v1 Announce Type: new 
Abstract: Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation</title>
<link>https://arxiv.org/abs/2510.03771</link>
<guid>https://arxiv.org/abs/2510.03771</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent simulations, genetic algorithms, e-commerce, query rewriting 

Summary:
The article introduces OptAgent, a framework that combines multi-agent simulations and genetic algorithms to optimize e-commerce query rewriting. Traditional Language Model (LLM) systems struggle with subjective tasks like query rewriting, where user intent is hard to determine algorithmically. OptAgent uses multiple LLM-based agents as simulated shopping customers to provide a dynamic reward signal for query optimization. The framework iteratively refines the user's initial query using an evolutionary algorithm guided by the average scores from these agents. The study evaluates OptAgent on a dataset of 1000 real-world e-commerce queries across five categories and shows an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline. Through this novel approach, OptAgent demonstrates enhanced performance in optimizing e-commerce queries, highlighting the potential of multi-agent simulations and genetic algorithms in addressing subjective tasks in LLM-based systems. 

Summary: <div>
arXiv:2510.03771v1 Announce Type: new 
Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable evaluation. While LLMs excel in verifiable tasks like coding and mathematics, where gold-standard solutions are available, adoption remains challenging for subjective tasks that lack a single correct answer. E-commerce Query Rewriting (QR) is one such problem where determining whether a rewritten query properly captures the user intent is extremely difficult to figure out algorithmically. In this work, we introduce OptAgent, a novel framework that combines multi-agent simulations with genetic algorithms to verify and optimize queries for QR. Instead of relying on a static reward model or a single LLM judge, our approach uses multiple LLM-based agents, each acting as a simulated shopping customer, as a dynamic reward signal. The average of these agent-derived scores serves as an effective fitness function for an evolutionary algorithm that iteratively refines the user's initial query. We evaluate OptAgent on a dataset of 1000 real-world e-commerce queries in five different categories, and we observe an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time</title>
<link>https://arxiv.org/abs/2510.03777</link>
<guid>https://arxiv.org/abs/2510.03777</guid>
<content:encoded><![CDATA[
<div> GuidedSampling, inference algorithm, diversity, concept exploration, performance improvement<br />
<br />
Summary: <br />
Repeated Sampling (RS) is an inference algorithm that can improve model performance but often lacks diversity in generated solutions. GuidedSampling decouples exploration and generation phases, increasing solution diversity. GuidedSampling identifies multiple concepts for problem-solving during exploration and applies specific concepts for generating final solutions. Empirical results show a performance improvement in models using GuidedSampling at pass@50 (~21.6% increase) compared to RS. Models trained on GuidedSampling trajectories also show significant enhancements at pass@5 (~9.7% increase) and generate a more diverse set of solution candidates. The average number of concepts per instance increases from 1.67 to 3.03 when using GuidedSampling, further enhancing solution diversity and performance. <br /><br /> <div>
arXiv:2510.03777v1 Announce Type: new 
Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Game Problem</title>
<link>https://arxiv.org/abs/2510.03845</link>
<guid>https://arxiv.org/abs/2510.03845</guid>
<content:encoded><![CDATA[
<div> AI alignment, language games, hidden game problem, regret minimization, correlated equilibria

Summary:
This paper examines games with large strategy spaces, particularly relevant for AI alignment and language games. It introduces the hidden game problem, where certain strategies consistently offer higher rewards for each player. The key question is whether efficient regret minimization algorithms can be developed to identify and exploit these hidden structures, reaching equilibrium in these subgames while maintaining rationality overall. The study demonstrates that optimal external and swap regret bounds can be achieved through a combination of regret minimization techniques. This enables rapid convergence to correlated equilibria in hidden subgames, utilizing the hidden game structure to enhance computational efficiency. <div>
arXiv:2510.03845v1 Announce Type: new 
Abstract: This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs</title>
<link>https://arxiv.org/abs/2510.03847</link>
<guid>https://arxiv.org/abs/2510.03847</guid>
<content:encoded><![CDATA[
<div> small language models, agents, function calling, structured outputs, JSON Schema, guided decoding

Summary:
Small language models (SLMs) with 1-12B parameters are shown to be effective for agentic tasks requiring schema- and API-constrained accuracy. Evidence from recent studies on various SLMs like Phi-4-Mini, Llama-3.2-1B, and Ministral-3B showcases their superiority when paired with guided decoding libraries. The use of strict JSON Schema outputs, validator-first tool execution, and guided decoding techniques enable SLMs to outperform larger models in cost-effectiveness, latency, and energy efficiency. By implementing engineering metrics like cost per successful task (CPS) and executable call rate, SLM-default, LLM-fallback systems can deliver reliable and efficient performance in agent stacks. Design patterns such as schema-first prompting and confidence scoring with verifier rollups further enhance the capabilities of SLMs. While SLMs excel in tasks like tool use and function calling, fallback to larger language models is still necessary for open-domain reasoning and long-horizon planning. This blueprint offers a practical approach to building fast, inexpensive, and reliable agents leveraging the strengths of both SLMs and LLMs. 

<br /><br />Summary: <div>
arXiv:2510.03847v1 Announce Type: new 
Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Generation via Creative Ideation</title>
<link>https://arxiv.org/abs/2510.03851</link>
<guid>https://arxiv.org/abs/2510.03851</guid>
<content:encoded><![CDATA[
<div> LLMs, algorithm generation, MetaMuse, cache replacement, online bin packing
Summary:
MetaMuse is introduced as a framework for creative ideation to address the limitations of LLMs in driving algorithm generation. It focuses on quantifying solution diversity and usefulness, steering ideation through external stimuli, and constructing executable solutions using waypoint reasoning. The framework shows promising results in generating high-performing solutions for cache replacement and online bin packing problems at a global cloud provider. MetaMuse reduces cache misses by up to 35.76% and bin usage by up to 30.93%. This demonstrates the effectiveness of MetaMuse in navigating the discontinuous solution space and outperforming generic heuristics commonly used in system algorithm design. <br /><br />Summary: <div>
arXiv:2510.03851v1 Announce Type: new 
Abstract: Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning</title>
<link>https://arxiv.org/abs/2510.03859</link>
<guid>https://arxiv.org/abs/2510.03859</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT, anomaly detection, large language models, contextual reasoning, XAI agents

Summary:
Anomaly detection in critical IoT systems is crucial for ensuring safety and smooth functionality. Traditional methods of anomaly detection fall short in complex, high-dimensional environments with evolving data. This proposal suggests leveraging Large Language Models (LLMs) and eXplainable Artificial Intelligence (XAI) agents for improved anomaly detection in IoT environments. By using attention methods and memory buffers in LLM-supported contextual reasoning, the system aims to uncover hidden patterns and inconsistencies in data streams. The emphasis on transparency and interpretability allows for human oversight and alignment with company policies. The proposed approach outperforms traditional models in accuracy and interpretation, making it a promising solution for future IoT anomaly detection tasks. Simulation testing in smart grid and healthcare contexts showcases the adaptability and reliability of the new approach. <div>
arXiv:2510.03859v1 Announce Type: new 
Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot on finding anomalies quickly. As more complex systems, like smart healthcare, energy grids and industrial automation, appear, it is easier to see the shortcomings of older methods of detection. Monitoring failures usually happen in dynamic, high dimensional situations, especially when data is incomplete, messy or always evolving. Such limits point out the requirement for adaptive, intelligent systems that always improve and think. LLMs are now capable of significantly changing how context is understood and semantic inference is done across all types of data. This proposal suggests using an LLM supported contextual reasoning method along with XAI agents to improve how anomalies are found in significant IoT environments. To discover hidden patterns and notice inconsistencies in data streams, it uses attention methods, avoids dealing with details from every time step and uses memory buffers with meaning. Because no code AI stresses transparency and interpretability, people can check and accept the AI's decisions, helping ensure AI follows company policies. The two architectures are put together in a test that compares the results of the traditional model with those of the suggested LLM enhanced model. Important measures to check are the accuracy of detection, how much inaccurate information is included in the results, how clearly the findings can be read and how fast the system responds under different test situations. The metaheuristic is tested in simulations of real world smart grid and healthcare contexts to check its adaptability and reliability. From the study, we see that the new approach performs much better than most existing models in both accuracy and interpretation, so it could be a good fit for future anomaly detection tasks in IoT
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation</title>
<link>https://arxiv.org/abs/2510.03863</link>
<guid>https://arxiv.org/abs/2510.03863</guid>
<content:encoded><![CDATA[
<div> Framework, Spatial reasoning, Geometric reasoning, Perspective-taking, Occlusion handling  
Summary:  
Spatial CAPTCHA is a new human-verification framework designed to address the shortcomings of traditional CAPTCHAs against modern AI. It introduces dynamic questions that require skills such as geometric reasoning, perspective-taking, occlusion handling, and mental rotation, which are challenging for state-of-the-art AI systems but intuitive for humans. The system utilizes a procedural generation pipeline with difficulty control, automated correctness verification, and human validation to ensure scalability and robustness. Evaluation on the Spatial-CAPTCHA-Bench benchmark shows that humans significantly outperform 10 top MLLMs, with the best model achieving a Pass@1 accuracy of only 31.0%. By comparing with Google reCAPTCHA, Spatial CAPTCHA proves its effectiveness as both a security mechanism and a diagnostic tool for spatial reasoning in AI.<br /><br />Summary: <div>
arXiv:2510.03863v1 Announce Type: new 
Abstract: Online services rely on CAPTCHAs as a first line of defense against automated abuse, yet recent advances in multi-modal large language models (MLLMs) have eroded the effectiveness of conventional designs that focus on text recognition or 2D image understanding. To address this challenge, we present Spatial CAPTCHA, a novel human-verification framework that leverages fundamental differences in spatial reasoning between humans and MLLMs. Unlike existing CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. These skills are intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The system employs a procedural generation pipeline with constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation to ensure scalability, robustness, and adaptability. Evaluation on a corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA, which confirms its effectiveness as both a security mechanism and a diagnostic tool for spatial reasoning in AI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rare Text Semantics Were Always There in Your Diffusion Transformer</title>
<link>https://arxiv.org/abs/2510.03886</link>
<guid>https://arxiv.org/abs/2510.03886</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal Diffusion Transformers, rare semantics, text-to-vision generation, joint-attention mechanism, generative models<br />
Summary: <br />
The study introduces Multi-modal Diffusion Transformers (MM-DiTs) in text-to-vision generation, known for high visual fidelity. Users often provide rare or imaginative prompts that advanced models struggle to generate due to limited pre-training concepts. The paper proposes a simple method to uncover rare semantics in MM-DiTs without extra training or data. By adjusting representational basins around text token embeddings before joint-attention blocks, rare semantics emerge in MM-DiT outputs, improving generalization across text-to-vision tasks. The joint-attention mechanism updates text and image embeddings sequentially, enhancing semantic clarity. The approach is effective for tasks like text-to-image, text-to-video, and text-driven image editing, allowing generative models to accurately capture users' intended semantics. This method highlights hidden semantics ready to be expressed. <br /> <div>
arXiv:2510.03886v1 Announce Type: new 
Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT's outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kantian-Utilitarian XAI: Meta-Explained</title>
<link>https://arxiv.org/abs/2510.03892</link>
<guid>https://arxiv.org/abs/2510.03892</guid>
<content:encoded><![CDATA[
<div> gamified explainable AI, ethically aware consumer decision-making, coffee domain, Kantian module, utilitarian module

Summary:
The article introduces a gamified explainable AI system designed for ethically aware consumer decision-making in the coffee industry. The system consists of six rounds with three options each, where two symbolic engines provide real-time explanations. The Kantian module flags violations such as child labor and deforestation risks, while the utilitarian module scores options based on multiple criteria. A meta-explainer with a regret bound helps identify misalignments between Kantian and utilitarian principles and suggests a cleaner option when the welfare loss is minimal. The system includes a structured configuration, policy trace for auditability, and an interactive user interface. The goal is to assist consumers in making informed and ethical choices when purchasing coffee products. 

<br /><br />Summary: <div>
arXiv:2510.03892v1 Announce Type: new 
Abstract: We present a gamified explainable AI (XAI) system for ethically aware consumer decision-making in the coffee domain. Each session comprises six rounds with three options per round. Two symbolic engines provide real-time reasons: a Kantian module flags rule violations (e.g., child labor, deforestation risk without shade certification, opaque supply chains, unsafe decaf), and a utilitarian module scores options via multi-criteria aggregation over normalized attributes (price, carbon, water, transparency, farmer income share, taste/freshness, packaging, convenience). A meta-explainer with a regret bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a deontically clean, near-parity option when welfare loss is small. We release a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Risks in Multi-turn Conversation with Large Language Models</title>
<link>https://arxiv.org/abs/2510.03969</link>
<guid>https://arxiv.org/abs/2510.03969</guid>
<content:encoded><![CDATA[
<div> Certification, Catastrophic risks, Large Language Models, Multi-turn Conversation, Statistical guarantees
Summary:
The article introduces a new Certification framework, QRLLM, to address the issue of catastrophic responses generated by Large Language Models (LLMs) in conversational settings. Existing evaluation methods are limited in revealing these vulnerabilities. QRLLM bounds the probability of LLMs producing catastrophic responses in multi-turn conversations with statistical guarantees. It models conversations as probability distributions over query sequences using a Markov process on a query graph. The framework quantifies catastrophic risks using confidence intervals and defines practical distributions such as random node, graph path, and adaptive with rejection. Results show significant risks in frontier models, with certified lower bounds reaching 70%. This underscores the need for enhanced safety training strategies in frontier LLMs. 
<br /><br />Summary: <div>
arXiv:2510.03969v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose QRLLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions: random node, graph path, adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models</title>
<link>https://arxiv.org/abs/2510.04009</link>
<guid>https://arxiv.org/abs/2510.04009</guid>
<content:encoded><![CDATA[
<div> evaluation, creativity, foundation models, C^2-Eval, benchmark \
<br />
foundation models (FMs) have evolved to tackle creative tasks in addition to traditional ones. There is a need for a unified evaluation framework for creativity in FMs, leading to the development of C^2-Eval. This benchmark distinguishes between two forms of creativity—convergent and divergent—and assesses them based on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on various FMs, the study analyzes the creative capabilities of these models. The results demonstrate the strengths and limitations of current FMs in achieving a creative machine mind. Overall, C^2-Eval provides valuable insights into the evolving landscape of creative artificial intelligence. \
<br /><br />Summary: <div>
arXiv:2510.04009v1 Announce Type: new 
Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics not firmly grounded in established theories. To address this gap, we introduce C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs. C^2-Eval distinguishes between two complementary forms of creativity: convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., storytelling). It evaluates both dimensions using fine-grained criteria derived from social-science theory, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we analyze trade-offs in their creative capabilities. Our results highlight both the strengths and challenges of current FMs in pursuing a creative machine mind, showing that C^2-Eval is an effective lens for examining the evolving landscape of creative AI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zephyrus: An Agentic Framework for Weather Science</title>
<link>https://arxiv.org/abs/2510.04017</link>
<guid>https://arxiv.org/abs/2510.04017</guid>
<content:encoded><![CDATA[
<div> Keywords: weather science, large language models, agentic framework, ZephyrusWorld, ZephyrusBench

Summary:
Foundation models for weather science lack language-based reasoning capabilities but excel at understanding structured numerical data. To bridge this gap, a new agentic framework for weather science, including ZephyrusWorld, has been developed. Zephyrus, a multi-turn LLM-based weather agent, interacts with weather data through conversational feedback loops. A benchmark, ZephyrusBench, with diverse question-answer pairs across weather-related tasks, has been created to evaluate the performance of Zephyrus agents. While Zephyrus outperforms text-only baselines by up to 35 percentage points in correctness on simpler tasks, it performs similarly on harder tasks. This suggests opportunities for future work to improve the agent's capabilities for more complex weather science tasks. <div>
arXiv:2510.04017v1 Announce Type: new 
Abstract: Foundation models for weather science are pre-trained on vast amounts of structured numerical data and outperform traditional weather forecasting systems. However, these models lack language-based reasoning capabilities, limiting their utility in interactive scientific workflows. Large language models (LLMs) excel at understanding and generating text but cannot reason about high-dimensional meteorological datasets. We bridge this gap by building a novel agentic framework for weather science. Our framework includes a Python code-based environment for agents (ZephyrusWorld) to interact with weather data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying for geographical masks from natural language, weather forecasting, and climate simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather agent that iteratively analyzes weather datasets, observes results, and refines its approach through conversational feedback loops. We accompany the agent with a new benchmark, ZephyrusBench, with a scalable data generation pipeline that constructs diverse question-answer pairs across weather-related tasks, from basic lookups to advanced forecasting, extreme event detection, and counterfactual reasoning. Experiments on this benchmark demonstrate the strong performance of Zephyrus agents over text-only baselines, outperforming them by up to 35 percentage points in correctness. However, on harder tasks, Zephyrus performs similarly to text-only baselines, highlighting the challenging nature of our benchmark and suggesting promising directions for future work.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2510.04023</link>
<guid>https://arxiv.org/abs/2510.04023</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data science agents, multimodal reasoning, trust mechanisms, evaluation practices

Summary: 
Recent advancements in large language models have led to the development of data science agents that automate various stages of the data science workflow. This survey introduces a comprehensive taxonomy of data science agents, categorizing them based on the different stages of the data science process. The analysis highlights the strengths and limitations of existing systems, emphasizing the need for improvements in areas such as business understanding, deployment, and monitoring. Challenges in multimodal reasoning and tool orchestration are also identified. Furthermore, the lack of explicit trust and safety mechanisms in the majority of systems is noted. The survey concludes by outlining open challenges in alignment stability, explainability, governance, and evaluation frameworks, and proposes future research directions for the development of robust, trustworthy, and transparent data science agents. <div>
arXiv:2510.04023v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A global log for medical AI</title>
<link>https://arxiv.org/abs/2510.04033</link>
<guid>https://arxiv.org/abs/2510.04033</guid>
<content:encoded><![CDATA[
<div> protocol, logging, clinical AI, transparency, healthcare

Summary:
The article introduces MedLog, a protocol for event-level logging of clinical AI activities. It addresses the lack of transparency and visibility in healthcare's clinical AI stack by providing a structured and consistent way to record how, when, by whom, and for whom AI models are used. MedLog consists of nine core fields including header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback to record model activity. It supports risk-based sampling, lifecycle-aware retention policies, and write-behind caching to minimize data footprint. MedLog enables the continuous surveillance, auditing, and iterative improvement of medical AI, facilitating the development of databases and software to store and analyze these records. The ultimate goal is to create a new form of digital epidemiology through the comprehensive tracking of AI model interactions in the healthcare setting.<br /><br />Summary: <div>
arXiv:2510.04033v1 Announce Type: new 
Abstract: Modern computer systems often rely on syslog, a simple, universal protocol that records every critical event across heterogeneous infrastructure. However, healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals rush to pilot large language models and other AI-based clinical decision support tools, we still lack a standard way to record how, when, by whom, and for whom these AI models are used. Without that transparency and visibility, it is challenging to measure real-world performance and outcomes, detect adverse events, or correct bias or dataset drift. In the spirit of syslog, we introduce MedLog, a protocol for event-level logging of clinical AI. Any time an AI model is invoked to interact with a human, interface with another algorithm, or act independently, a MedLog record is created. This record consists of nine core fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback, providing a structured and consistent record of model activity. To encourage early adoption, especially in low-resource settings, and minimize the data footprint, MedLog supports risk-based sampling, lifecycle-aware retention policies, and write-behind caching; detailed traces for complex, agentic, or multi-stage workflows can also be captured under MedLog. MedLog can catalyze the development of new databases and software to store and analyze MedLog records. Realizing this vision would enable continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for a new form of digital epidemiology.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2510.04040</link>
<guid>https://arxiv.org/abs/2510.04040</guid>
<content:encoded><![CDATA[
<div> benchmarking, language model, Chain-of-Thought, unfaithfulness detection, reasoning process

Summary: 
FaithCoT-Bench introduces a benchmark for detecting unfaithfulness in Chain-of-Thought (CoT) prompts used by Large Language Models (LLMs). The framework includes FINE-CoT, with over 1,000 trajectories from four LLMs in different domains, including 300 unfaithful instances. The study evaluates eleven detection methods across various paradigms, highlighting challenges in knowledge-intensive domains and with advanced models. It addresses the practical challenge of determining if a CoT trajectory accurately represents the model's reasoning process. FaithCoT-Bench marks a significant step towards enhancing the interpretability and trustworthiness of reasoning in LLMs.<br /><br />Summary: <div>
arXiv:2510.04040v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Increasing LLM response trustworthiness using voting ensembles</title>
<link>https://arxiv.org/abs/2510.04048</link>
<guid>https://arxiv.org/abs/2510.04048</guid>
<content:encoded><![CDATA[
<div> framework, ensembling, uncertainty, trustworthiness, response <br />
<br />
Summary: <br />
This study addresses the lack of reliable uncertainty quantification in Large Language Models (LLMs). It introduces a novel approach to ensembling called variable voting threshold ensembles, where models can abstain from answering if the dominant response falls short of a certain threshold. Theoretical and experimental results in arithmetic problem solving and clinical-note question-answering domains show that this approach significantly enhances the trustworthiness of answers while maintaining acceptable levels of response yield and accuracy. This technique can be particularly beneficial in high-stakes applications like healthcare and data annotation, where a high level of confidence in answers is crucial, even if not every question receives an automated response. <div>
arXiv:2510.04048v1 Announce Type: new 
Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to quantify the uncertainty in their responses, making them difficult to trust in high-stakes applications. One of the simplest approaches to eliciting more accurate answers is to select the mode of many responses, a technique known as ensembling. In this work, we expand on typical ensembling approaches by looking at ensembles with a variable voting threshold. We introduce a theoretical framework for question answering and show that, by permitting ensembles to "abstain" from providing an answer when the dominant response falls short of the threshold, it is possible to dramatically increase the trustworthiness of the remaining answers. From this framework, we derive theoretical results as well as report experimental results on two problem domains: arithmetic problem solving and clinical-note question-answering. In both domains, we observe that large gains in answer trustworthiness can be achieved using highly restrictive voting ensembles, while incurring relatively modest reductions in response yield and accuracy. Due to this quality, voting ensembles may be particularly useful in applications - such as healthcare and data annotation - that require a high degree of certainty but which may not require that every question receive an automated answer.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a unified framework for data-efficient evaluation of large language models</title>
<link>https://arxiv.org/abs/2510.04051</link>
<guid>https://arxiv.org/abs/2510.04051</guid>
<content:encoded><![CDATA[
<div> evaluation, large language models, Item Response Theory, data-efficient evaluation, LEGO-IRT

Summary:
- Evaluating large language models (LLMs) on comprehensive benchmarks is crucial but often computationally and financially prohibitive.
- Item Response Theory (IRT) is a promising method for data-efficient evaluation, but existing approaches have limitations.
- LEGO-IRT is introduced as a flexible framework for LLM evaluation, supporting both binary and continuous metrics.
- LEGO-IRT incorporates structural knowledge and decomposes model ability estimates into general and structure-specific components.
- Extensive experiments show that LEGO-IRT achieves stable capability estimates using only 3% of the total evaluation items, reduces estimation error by up to 10%, and aligns closely with human preferences. 

<br /><br />Summary: <div>
arXiv:2510.04051v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it's often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion</title>
<link>https://arxiv.org/abs/2510.04064</link>
<guid>https://arxiv.org/abs/2510.04064</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, emotion encoding, neural architecture, probing toolkit, open-source dataset

Summary: 
- The study investigates how modern Large Language Models (LLMs) encode emotions within their neural architecture.
- A novel Reddit corpus of approximately 400,000 utterances balanced across seven basic emotions was created.
- Lightweight "probes" were used to extract emotional information from hidden layers of Qwen3 and LLaMA models without changing their parameters.
- The findings show that LLMs have a well-defined internal emotional geometry that sharpens with model size and outperforms zero-shot prompting.
- The study reveals that emotional signals in LLMs emerge early, peak mid-network, are malleable, and persistent, remaining detectable for hundreds of subsequent tokens. 
- The dataset, probing toolkit, and emotional landscape map within LLMs are open-sourced to aid in developing more transparent and aligned AI systems. 

<br /><br />Summary: <div>
arXiv:2510.04064v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly expected to navigate the nuances of human emotion. While research confirms that LLMs can simulate emotional intelligence, their internal emotional mechanisms remain largely unexplored. This paper investigates the latent emotional representations within modern LLMs by asking: how, where, and for how long is emotion encoded in their neural architecture? To address this, we introduce a novel, large-scale Reddit corpus of approximately 400,000 utterances, balanced across seven basic emotions through a multi-stage process of classification, rewriting, and synthetic generation. Using this dataset, we employ lightweight "probes" to read out information from the hidden layers of various Qwen3 and LLaMA models without altering their parameters. Our findings reveal that LLMs develop a surprisingly well-defined internal geometry of emotion, which sharpens with model scale and significantly outperforms zero-shot prompting. We demonstrate that this emotional signal is not a final-layer phenomenon but emerges early and peaks mid-network. Furthermore, the internal states are both malleable (they can be influenced by simple system prompts) and persistent, as the initial emotional tone remains detectable for hundreds of subsequent tokens. We contribute our dataset, an open-source probing toolkit, and a detailed map of the emotional landscape within LLMs, offering crucial insights for developing more transparent and aligned AI systems. The code and dataset are open-sourced.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention</title>
<link>https://arxiv.org/abs/2510.04073</link>
<guid>https://arxiv.org/abs/2510.04073</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, value alignment, value drift, predictive analytics, governance

Summary: 
The article introduces the Moral Anchor System (MAS), a framework designed to address value drift in AI systems by utilizing real-time Bayesian inference, LSTM networks for forecasting, and a human-centric governance layer. MAS aims to detect, predict, and mitigate value drift incidents in AI agents to ensure alignment with human ethics and intentions. The framework emphasizes low-latency responses to prevent breaches, while incorporating adaptive interventions based on feedback. Through rigorous experiments with goal-misaligned agents, MAS demonstrates scalability and responsiveness, with results showing a potential reduction of value drift incidents by 80 percent or more. The originality of MAS lies in its predictive and adaptive nature, offering a contrast to static alignment methods. The article also provides insights into cross-domain applicability and open-source code for replication. <div>
arXiv:2510.04073v1 Announce Type: new 
Abstract: The rise of artificial intelligence (AI) as super-capable assistants has transformed productivity and decision-making across domains. Yet, this integration raises critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions. A key risk is value drift, where AI systems deviate from aligned values due to evolving contexts, learning dynamics, or unintended optimizations, potentially leading to inefficiencies or ethical breaches. We propose the Moral Anchor System (MAS), a novel framework to detect, predict, and mitigate value drift in AI agents. MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent breaches, while reducing false positives and alert fatigue via supervised fine-tuning with human feedback. Our hypothesis: integrating probabilistic drift detection, predictive analytics, and adaptive governance can reduce value drift incidents by 80 percent or more in simulations, maintaining high detection accuracy (85 percent) and low false positive rates (0.08 post-adaptation). Rigorous experiments with goal-misaligned agents validate MAS's scalability and responsiveness. MAS's originality lies in its predictive and adaptive nature, contrasting static alignment methods. Contributions include: (1) MAS architecture for AI integration; (2) empirical results prioritizing speed and usability; (3) cross-domain applicability insights; and (4) open-source code for replication.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>