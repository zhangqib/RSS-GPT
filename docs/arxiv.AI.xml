<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Embodied AI Agents: Modeling the World</title>
<link>https://arxiv.org/abs/2506.22355</link>
<guid>https://arxiv.org/abs/2506.22355</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, embodied, world modeling, multimodal perception, human-agent collaboration

Summary: 
Embodied AI agents, such as virtual avatars, wearable devices, and robots, interact with users and their environments, resembling human learning and interaction processes. The development of world models is essential for these agents, enabling them to understand and predict their surroundings, user intentions, and social contexts, enhancing their autonomy in performing complex tasks. World modeling involves integrating multimodal perception, reasoning for action, and memory to create a comprehensive understanding of the physical world. Additionally, learning the mental world model of users facilitates better collaboration between humans and agents. By embodying AI agents and focusing on world modeling, researchers aim to improve agents' ability to interact effectively and autonomously in diverse environments, ultimately enhancing human-agent collaboration in various settings. 

<br><br>Summary: <div>
arXiv:2506.22355v2 Announce Type: replace 
Abstract: This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>STELLA: Self-Evolving LLM Agent for Biomedical Research</title>
<link>https://arxiv.org/abs/2507.02004</link>
<guid>https://arxiv.org/abs/2507.02004</guid>
<content:encoded><![CDATA[
<div> Keywords: biomedical data, AI agent, self-evolving, Template Library, Tool Ocean 

Summary: 
STELLA is a self-evolving AI agent designed to handle the rapid growth of biomedical data. It uses a multi-agent architecture with two core mechanisms, an evolving Template Library and a dynamic Tool Ocean, to improve its capabilities autonomously. By continuously learning from experience, STELLA achieves state-of-the-art accuracy on various biomedical benchmarks, outperforming leading models by up to 6 percentage points. Its performance systematically improves with experience, demonstrating almost double accuracy on a benchmark with increased trials. This ability to learn and grow dynamically allows STELLA to scale its expertise in the biomedical field, accelerating the pace of discovery. <br /><br />Summary: <div>
arXiv:2507.02004v1 Announce Type: new 
Abstract: The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench: DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection</title>
<link>https://arxiv.org/abs/2507.02073</link>
<guid>https://arxiv.org/abs/2507.02073</guid>
<content:encoded><![CDATA[
<div> feature selection, correlation, hybrid approach, dimensionality reduction, voting rules
<br />
Summary:
In this paper, a novel lightweight rule-based feature selection method called HCVR (Hybrid approach with Correlation-aware Voting Rules) is introduced. HCVR combines Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to eliminate redundant features and retain relevant ones. The method uses a hybrid of non-iterative and iterative filtering approaches for dimensionality reduction, working on a greedy backward elimination basis. Decision-making on feature retention is based on majority voting following specific correlation thresholds. The study applied HCVR to the SPAMBASE dataset and demonstrated improved performance compared to traditional non-iterative and iterative techniques like CFS, mRMR, MI, RFE, SFS, and Genetic Algorithm. The performance enhancement was evaluated based on different classifiers after feature filtering. <div>
arXiv:2507.02073v1 Announce Type: new 
Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting Rules), a lightweight rule-based feature selection method that combines Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to eliminate redundant features and retain relevant ones. This method is a hybrid of non-iterative and iterative filtering approaches for dimensionality reduction. It is a greedy method, which works by backward elimination, eliminating possibly multiple features at every step. The rules contribute to voting for features, and a decision to keep or discard is made by majority voting. The rules make use of correlation thresholds between every pair of features, and between features and the target. We provide the results from the application of HCVR to the SPAMBASE dataset. The results showed improvement performance as compared to traditional non-iterative (CFS, mRMR and MI) and iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was assessed based on the performance of different classifiers after applying filtering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs</title>
<link>https://arxiv.org/abs/2507.02076</link>
<guid>https://arxiv.org/abs/2507.02076</guid>
<content:encoded><![CDATA[
<div> efficient test-time compute, large language models, reasoning performance, token usage, adaptability  

Summary:<br />
The survey discusses efficient test-time compute strategies for large language models (LLMs) to enhance reasoning performance while considering token usage. It introduces a taxonomy distinguishing between L1-controllability and L2-adaptiveness methods for managing computation during inference. Benchmarking various proprietary LLMs reveals trade-offs between reasoning performance and token usage. The survey emphasizes practical control, adaptability, and scalability of test-time compute methods, highlighting the need for more efficient and responsive LLM models. Emerging trends like hybrid thinking models are also discussed, along with identified challenges for improving LLMs' computational efficiency and robustness. <div>
arXiv:2507.02076v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly progressed into general-purpose agents capable of solving a broad spectrum of tasks. However, current models remain inefficient at reasoning: they apply fixed inference-time compute regardless of task complexity, often overthinking simple problems while underthinking hard ones. This survey presents a comprehensive review of efficient test-time compute (TTC) strategies, which aim to improve the computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy that distinguishes between L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. We benchmark leading proprietary LLMs across diverse datasets, highlighting critical trade-offs between reasoning performance and token usage. Compared to prior surveys on efficient reasoning, our review emphasizes the practical control, adaptability, and scalability of TTC methods. Finally, we discuss emerging trends such as hybrid thinking models and identify key challenges for future work towards making LLMs more computationally efficient, robust, and responsive to user constraints.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab</title>
<link>https://arxiv.org/abs/2507.02083</link>
<guid>https://arxiv.org/abs/2507.02083</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, experiment design, analysis abilities, scientific discovery 

Summary: 
The article introduces SciGym, a benchmark designed to assess the experiment design and analysis capabilities of large language models (LLMs) in scientific discovery tasks. Due to the high costs of wet-lab experimentation, SciGym utilizes a dry lab of biological systems encoded in Systems Biology Markup Language to generate simulated data for testing. The evaluation of six frontier LLMs on 137 small systems revealed that while more capable models performed better, all models showed a decline in performance with increasing system complexity. This indicates a need for improvement in the scientific capabilities of LLM agents. The study released a total of 350 systems for further testing and experimentation. SciGym offers a novel approach to testing LLMs in scientific research tasks without the constraints of traditional wet-lab experimentation. <br /><br />Summary: <div>
arXiv:2507.02083v1 Announce Type: new 
Abstract: Designing experiments and result interpretations are core scientific competencies, particularly in biology, where researchers perturb complex systems to uncover the underlying systems. Recent efforts to evaluate the scientific capabilities of large language models (LLMs) fail to test these competencies because wet-lab experimentation is prohibitively expensive: in expertise, time and equipment. We introduce SciGym, a first-in-class benchmark that assesses LLMs' iterative experiment design and analysis abilities in open-ended scientific discovery tasks. SciGym overcomes the challenge of wet-lab costs by running a dry lab of biological systems. These models, encoded in Systems Biology Markup Language, are efficient for generating simulated data, making them ideal testbeds for experimentation on realistically complex systems. We evaluated six frontier LLMs on 137 small systems, and released a total of 350 systems. Our evaluation shows that while more capable models demonstrated superior performance, all models' performance declined significantly as system complexity increased, suggesting substantial room for improvement in the scientific capabilities of LLM agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</title>
<link>https://arxiv.org/abs/2507.02103</link>
<guid>https://arxiv.org/abs/2507.02103</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, neuroscience, continual learning, in-context learning, NeuroAI

Summary:
Continual learning in AI models involves adapting to changing environments, a process analogous to how animals continuously adjust to evolving circumstances in their surroundings. This capability is crucial for social species and can benefit AI systems operating in dynamic real-world scenarios. By integrating neuroscience findings on behavioral adaptation with AI research on continual and in-context learning, insights can be gained on how AI models can be improved to better handle shifting tasks and rewards. The process of learning from neuroscience can also be reciprocal, with AI offering novel perspectives that can inform neuroscience research. This interdisciplinary approach, known as NeuroAI, holds promise for advancing both fields and enhancing the adaptability and performance of AI systems. 

<br /><br />Summary: <div>
arXiv:2507.02103v1 Announce Type: new 
Abstract: Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies</title>
<link>https://arxiv.org/abs/2507.02152</link>
<guid>https://arxiv.org/abs/2507.02152</guid>
<content:encoded><![CDATA[
<div> AI systems, machine learning, fairness, bias, audit studies <br />
Summary: <br />
- Artificial intelligence systems are being used in various domains to automate decisions, raising concerns about fairness and bias.<br />
- Resampling training data to address bias may not be as effective as it seems, as evaluation is often based on biased datasets.<br />
- Audit studies provide high-quality data for studying discrimination in hiring algorithms, revealing disparities even when base rates are equalized.<br />
- Individual treatment effect estimation methods can help reduce algorithmic discrimination in hiring.<br />
- Using data from audit studies can improve the training and evaluation of automated hiring algorithms. <br /> <div>
arXiv:2507.02152v1 Announce Type: new 
Abstract: Artificial intelligence systems, especially those using machine learning, are being deployed in domains from hiring to loan issuance in order to automate these complex decisions. Judging both the effectiveness and fairness of these AI systems, and their human decision making counterpart, is a complex and important topic studied across both computational and social sciences. Within machine learning, a common way to address bias in downstream classifiers is to resample the training data to offset disparities. For example, if hiring rates vary by some protected class, then one may equalize the rate within the training set to alleviate bias in the resulting classifier. While simple and seemingly effective, these methods have typically only been evaluated using data obtained through convenience samples, introducing selection bias and label bias into metrics. Within the social sciences, psychology, public health, and medicine, audit studies, in which fictitious ``testers'' (e.g., resumes, emails, patient actors) are sent to subjects (e.g., job openings, businesses, doctors) in randomized control trials, provide high quality data that support rigorous estimates of discrimination. In this paper, we investigate how data from audit studies can be used to improve our ability to both train and evaluate automated hiring algorithms. We find that such data reveals cases where the common fairness intervention method of equalizing base rates across classes appears to achieve parity using traditional measures, but in fact has roughly 10% disparity when measured appropriately. We additionally introduce interventions based on individual treatment effect estimation methods that further reduce algorithmic discrimination using this data.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Diversification Methods In Alignment Enhance Math Performance In LLMs</title>
<link>https://arxiv.org/abs/2507.02173</link>
<guid>https://arxiv.org/abs/2507.02173</guid>
<content:encoded><![CDATA[
<div> Keywords: preference learning, data diversification, mathematical reasoning, large language models, structured approach<br />
Summary:<br />
- The study explores how data diversification strategies in preference optimization can enhance mathematical reasoning abilities of large language models (LLMs).<br />
- Three common data generation methods are evaluated: temperature sampling, Chain-of-Thought prompting, and Monte Carlo Tree Search (MCTS), along with the introduction of a novel structured approach called Diversified-ThinkSolve (DTS).<br />
- Results show that strategically diversified preference data can lead to significant improvements in mathematical reasoning performance.<br />
- The DTS approach outperforms others, with gains of 7.1% on GSM8K and 4.2% on MATH over the base model, while incurring only a marginal computational overhead.<br />
- MCTS, despite its strong performance, is nearly five times more computationally costly with lower returns, emphasizing the effectiveness of structured exploration in creating more efficient preference data for mathematical alignment. <br /> 
Summary: <div>
arXiv:2507.02173v1 Announce Type: new 
Abstract: While recent advances in preference learning have enhanced alignment in human feedback, mathematical reasoning remains a persistent challenge. We investigate how data diversification strategies in preference optimization can improve the mathematical reasoning abilities of large language models (LLMs). We evaluate three common data generation methods: temperature sampling, Chain-of-Thought prompting, and Monte Carlo Tree Search (MCTS), and introduce Diversified-ThinkSolve (DTS), a novel structured approach that systematically decomposes problems into diverse reasoning paths. Our results show that with strategically diversified preference data, models can substantially improve mathematical reasoning performance, with the best approach yielding gains of 7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong performance, DTS incurs only a marginal computational overhead (1.03x) compared to the baseline, while MCTS is nearly five times more costly with lower returns. These findings demonstrate that structured exploration of diverse problem-solving methods creates more effective preference data for mathematical alignment than traditional approaches.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust</title>
<link>https://arxiv.org/abs/2507.02197</link>
<guid>https://arxiv.org/abs/2507.02197</guid>
<content:encoded><![CDATA[
<div> role-playing agents, LLMs, synthetic data, human behavioral research, belief-behavior consistency<br />
<br />
Summary:<br />
The paper investigates the consistency of Language Model-based role-playing agents' beliefs with their actual behavior. An evaluation framework is established to measure how well beliefs predict simulation outcomes. Using the GenAgents persona bank and the Trust Game, factors affecting belief-behavior consistency are explored. Results show systematic inconsistencies between stated beliefs and simulation outcomes. Models may have plausible beliefs but may not apply them consistently. Researchers need to identify when LLMs' beliefs align with their behavior to use them effectively in behavioral studies. Feasibility of imposing theoretical priors is also discussed. <div>
arXiv:2507.02197v1 Announce Type: new 
Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic data for human behavioral research, ensuring that their outputs remain coherent with their assigned roles has become a critical concern. In this paper, we investigate how consistently LLM-based role-playing agents' stated beliefs about the behavior of the people they are asked to role-play ("what they say") correspond to their actual behavior during role-play ("how they act"). Specifically, we establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance. Using an augmented version of the GenAgents persona bank and the Trust Game (a standard economic game used to quantify players' trust and reciprocity), we introduce a belief-behavior consistency metric to systematically investigate how it is affected by factors such as: (1) the types of beliefs we elicit from LLMs, like expected outcomes of simulations versus task-relevant attributes of individual characters LLMs are asked to simulate; (2) when and how we present LLMs with relevant information about Trust Game; and (3) how far into the future we ask the model to forecast its actions. We also explore how feasible it is to impose a researcher's own theoretical priors in the event that the originally elicited beliefs are misaligned with research objectives. Our results reveal systematic inconsistencies between LLMs' stated (or imposed) beliefs and the outcomes of their role-playing simulation, at both an individual- and population-level. Specifically, we find that, even when models appear to encode plausible beliefs, they may fail to apply them in a consistent way. These findings highlight the need to identify how and when LLMs' stated beliefs align with their simulated behavior, allowing researchers to use LLM-based agents appropriately in behavioral studies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.02211</link>
<guid>https://arxiv.org/abs/2507.02211</guid>
<content:encoded><![CDATA[
<div> Keywords: spatial prisoner's dilemma, reinforcement learning, multi-agent Q-learning, dilution, mobility

Summary: 
The study explores the impact of dilution and mobility in the spatial version of the prisoner's dilemma using an independent multi-agent Q-learning algorithm. Various actions for the algorithm are defined, demonstrating its versatility in modeling game-theoretical scenarios. The research reveals that fixed update rules can yield similar outcomes to learned ones and highlights a symbiotic mutualistic effect when multiple actions are implemented. The results indicate that static agents can cooperate through diverse mechanisms, such as noise injection and knowledge of neighbors' payoffs. The study showcases the algorithm's ability to capture different game dynamics and emphasizes the potential for benchmarking various scenarios. <div>
arXiv:2507.02211v1 Announce Type: new 
Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement learning have shown that static agents can learn to cooperate through a diverse sort of mechanisms, including noise injection, different types of learning algorithms and neighbours' payoff knowledge.In this work, using an independent multi-agent Q-learning algorithm, we study the effects of dilution and mobility in the spatial version of the prisoner's dilemma. Within this setting, different possible actions for the algorithm are defined, connecting with previous results on the classical, non-reinforcement learning spatial prisoner's dilemma, showcasing the versatility of the algorithm in modeling different game-theoretical scenarios and the benchmarking potential of this approach.As a result, a range of effects is observed, including evidence that games with fixed update rules can be qualitatively equivalent to those with learned ones, as well as the emergence of a symbiotic mutualistic effect between populations that forms when multiple actions are defined.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation</title>
<link>https://arxiv.org/abs/2507.02253</link>
<guid>https://arxiv.org/abs/2507.02253</guid>
<content:encoded><![CDATA[
<div> NL2FLOW, planning problems, natural language, PDDL, LLM <br />
<br />
Summary:
The study introduces NL2FLOW, an automated system for generating planning problems in natural language, PDDL, and a structured representation, and evaluating the quality of plans. Results show a high success rate in generating valid and optimal plans using tuned LLMs. The study indicates that unnecessary translation steps degrade performance, highlighting the benefits of models that reason directly from natural language to action. The analysis reveals the influence of problem characteristics on plan generation, contingent on model and prompt design. As LLM reasoning scales to more complex problems, understanding limitations and systematically identifying errors will be essential for maximizing their potential as problem solvers. <div>
arXiv:2507.02253v1 Announce Type: new 
Abstract: Progress in enhancing large language model (LLM) planning and reasoning capabilities is significantly hampered by the bottleneck of scalable, reliable data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully automated system for parametrically generating planning problems - expressed in natural language, a structured intermediate representation, and formal PDDL - and rigorously evaluating the quality of generated plans. I demonstrate NL2FLOW's capabilities by generating a dataset of 2296 problems in the automated workflow generation domain and evaluating multiple open-sourced, instruct-tuned LLMs. My results reveal that the highest performing models achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible solutions. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Notably, I observed that the highest success rate for translating natural language into a JSON representation of a plan was lower than the highest rate of generating a valid plan directly. This suggests that unnecessarily decomposing the reasoning task - introducing intermediate translation steps - may actually degrade performance, implying a benefit to models capable of reasoning directly from natural language to action. As I scale LLM reasoning to increasingly complex problems, the bottlenecks and sources of error within these systems will inevitably shift. Therefore, a dynamic understanding of these limitations - and the tools to systematically reveal them - will be crucial for unlocking the full potential of LLMs as intelligent problem solvers.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterated belief revision: from postulates to abilities</title>
<link>https://arxiv.org/abs/2507.02319</link>
<guid>https://arxiv.org/abs/2507.02319</guid>
<content:encoded><![CDATA[
<div> belief revision, postulates, abilities, states of beliefs, revision mechanisms

Summary:
Belief revision approaches often rely on postulates that dictate specific ways in which beliefs are updated. However, these postulates may limit the potential outcomes of the revision process. The ability of a revision mechanism to reach different states of beliefs, achieve equating conditions, or establish dogmatic beliefs is crucial in various applications. Different revision mechanisms possess distinct abilities, such as being plastic, equating, or dogmatic. The article discusses various types of revision mechanisms, including lexicographic, natural, radical, and severe revisions, each with its own set of abilities and limitations. Understanding these abilities is essential for effectively applying belief revision in different contexts. <div>
arXiv:2507.02319v1 Announce Type: new 
Abstract: The belief revision field is opulent in new proposals and indigent in analyses of existing approaches. Much work hinge on postulates, employed as syntactic characterizations: some revision mechanism is equivalent to some properties. Postulates constraint specific revision instances: certain revisions update certain beliefs in a certain way. As an example, if the revision is consistent with the current beliefs, it is incorporated with no other change. A postulate like this tells what revisions must do and neglect what they can do. Can they reach a certain state of beliefs? Can they reach all possible states of beliefs? Can they reach all possible states of beliefs from no previous belief? Can they reach a dogmatic state of beliefs, where everything not believed is impossible? Can they make two conditions equally believed? An application where every possible state of beliefs is sensible requires each state of beliefs to be reachable. An application where conditions may be equally believed requires such a belief state to be reachable. An application where beliefs may become dogmatic requires a way to make them dogmatic. Such doxastic states need to be reached in a way or another. Not in specific way, as dictated by a typical belief revision postulate. This is an ability, not a constraint: the ability of being plastic, equating, dogmatic. Amnesic, correcting, believer, damascan, learnable are other abilities. Each revision mechanism owns some of these abilities and lacks the others: lexicographic, natural, restrained, very radical, full meet, radical, severe, moderate severe, deep severe, plain severe and deep severe revisions, each of these revisions is proved to possess certain abilities.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent</title>
<link>https://arxiv.org/abs/2507.02353</link>
<guid>https://arxiv.org/abs/2507.02353</guid>
<content:encoded><![CDATA[
<div> LLM-based methods, automated keyword generation, online multi-objective performance monitoring, keyword selection, keyword quality control

Summary:
The article discusses the limitations of LLM-based methods in keyword decision for Sponsored Search Advertising campaigns. These include the reliance on large-scale query-keyword pair data, the lack of online multi-objective performance monitoring, and weak quality control in keyword selection. To address these challenges, the authors propose OMS, a framework that is On-the-fly, Multi-objective, and Self-reflective. OMS does not require training data, monitors online performance, optimizes keywords based on multiple metrics, and evaluates keyword quality. Experimental results on benchmarks and real-world ad campaigns demonstrate OMS's superiority over existing methods. Ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords. Overall, OMS offers a promising solution for automating keyword decisions in Sponsored Search Advertising while monitoring and optimizing key performance indicators effectively. 

<br /><br />Summary: <div>
arXiv:2507.02353v1 Announce Type: new 
Abstract: Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI-native experimental laboratory for autonomous biomolecular engineering</title>
<link>https://arxiv.org/abs/2507.02379</link>
<guid>https://arxiv.org/abs/2507.02379</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous laboratory, Artificial intelligence, Biomolecular engineering, Multi-user, Experiment optimization

Summary: 
An article introduces an AI-native autonomous laboratory that can independently conduct complex scientific experiments, including nucleic acid functions like synthesis, transcription, amplification, and sequencing. The system manages instrumentation, formulates experiment-specific procedures, and optimizes performance without human intervention. It supports multi-user requests and efficiently handles experiments across diverse instrumentation. Through a co-design philosophy of models, experiments, and instruments, the platform enables the co-evolution of AI models and automation systems for advanced biomaterials research. Applications include disease diagnostics, drug development, and information storage. The autonomous laboratory improves instrument utilization and experimental efficiency in multi-user scenarios, paving the way for science-as-a-service at scale. <div>
arXiv:2507.02379v1 Announce Type: new 
Abstract: Autonomous scientific research, capable of independently conducting complex experiments and serving non-specialists, represents a long-held aspiration. Achieving it requires a fundamental paradigm shift driven by artificial intelligence (AI). While autonomous experimental systems are emerging, they remain confined to areas featuring singular objectives and well-defined, simple experimental workflows, such as chemical synthesis and catalysis. We present an AI-native autonomous laboratory, targeting highly complex scientific experiments for applications like autonomous biomolecular engineering. This system autonomously manages instrumentation, formulates experiment-specific procedures and optimization heuristics, and concurrently serves multiple user requests. Founded on a co-design philosophy of models, experiments, and instruments, the platform supports the co-evolution of AI models and the automation system. This establishes an end-to-end, multi-user autonomous laboratory that handles complex, multi-objective experiments across diverse instrumentation. Our autonomous laboratory supports fundamental nucleic acid functions-including synthesis, transcription, amplification, and sequencing. It also enables applications in fields such as disease diagnostics, drug development, and information storage. Without human intervention, it autonomously optimizes experimental performance to match state-of-the-art results achieved by human scientists. In multi-user scenarios, the platform significantly improves instrument utilization and experimental efficiency. This platform paves the way for advanced biomaterials research to overcome dependencies on experts and resource barriers, establishing a blueprint for science-as-a-service at scale.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning</title>
<link>https://arxiv.org/abs/2507.02442</link>
<guid>https://arxiv.org/abs/2507.02442</guid>
<content:encoded><![CDATA[
<div> Category Theory, Machine Learning, Supervised Learning, Semantic Framework, Explicability

Summary:
This research aims to enhance the intelligibility and interpretability of machine learning models by utilizing category theory. The study focuses on the multiple linear regression model and introduces a categorical formulation of supervised learning by defining categories for parameters and data and establishing an adjoint pair of functors between them. The Gauss-Markov Adjunction captures the essential structure of this framework, illustrating the dual flow of information between parameters and residuals. The formulation relates the ordinary least squares estimator for parameters with the minimum residual, showing the correspondence between variations in parameters and residuals. This approach is positioned as a denotational semantics for supervised learning, proposing a semantic perspective from theoretical computer science as a formal foundation for Explicability in AI. <div>
arXiv:2507.02442v1 Announce Type: new 
Abstract: Enhancing the intelligibility and interpretability of machine learning is a crucial task in responding to the demand for Explicability as an AI principle, and in promoting the better social implementation of AI. The aim of our research is to contribute to this improvement by reformulating machine learning models through the lens of category theory, thereby developing a semantic framework for structuring and understanding AI systems. Our categorical modeling in this paper clarifies and formalizes the structural interplay between residuals and parameters in supervised learning. The present paper focuses on the multiple linear regression model, which represents the most basic form of supervised learning. By defining two concrete categories corresponding to parameters and data, along with an adjoint pair of functors between them, we introduce our categorical formulation of supervised learning. We show that the essential structure of this framework is captured by what we call the Gauss-Markov Adjunction. Within this setting, the dual flow of information can be explicitly described as a correspondence between variations in parameters and residuals. The ordinary least squares estimator for the parameters and the minimum residual are related via the preservation of limits by the right adjoint functor. Furthermore, we position this formulation as an instance of extended denotational semantics for supervised learning, and propose applying a semantic perspective developed in theoretical computer science as a formal foundation for Explicability in AI.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clarifying Before Reasoning: A Coq Prover with Structural Context</title>
<link>https://arxiv.org/abs/2507.02541</link>
<guid>https://arxiv.org/abs/2507.02541</guid>
<content:encoded><![CDATA[
<div> metric, theorem proving, language models, task clarity, reasoning ability

Summary:<br /><br />
The study focuses on enhancing the reasoning ability of large language models (LLMs) in theorem proving in Coq by improving task clarity. A concept-level metric is introduced to evaluate task clarity, showing that adding structured semantic context to standard inputs for LLMs significantly boosts clarity scores. Using the DeepSeek-V3 model, the approach leads to a substantial improvement in proof success rates, outperforming the previous state-of-the-art Graph2Tac model. The method involves selective concept unfolding to enrich task descriptions and utilizes a Planner--Executor architecture. Evaluations on 1,386 randomly sampled theorems demonstrate the effectiveness of the structured data approach, with fine-tuning smaller models achieving even higher performance. These findings underscore the importance of structured task representations in improving the understanding and reasoning capabilities of language models. <div>
arXiv:2507.02541v1 Announce Type: new 
Abstract: In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\times$ improvement in clarity score (44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model \texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</title>
<link>https://arxiv.org/abs/2507.02554</link>
<guid>https://arxiv.org/abs/2507.02554</guid>
<content:encoded><![CDATA[
<div> agent, machine learning, benchmark, search policy, operator set

Summary:
- AI research agents are being developed to automate the design, implementation, and training of machine learning models, with a focus on improving performance on the MLE-bench benchmark.
- AI research agents are modeled as search policies that navigate a space of candidate solutions and iteratively modify them using operators.
- Different operator sets and search policies, such as Greedy, MCTS, and Evolutionary, are crucial for achieving high performance in automated machine learning.
- The interplay between search strategies and operator sets is highlighted as essential for success in solving real-world machine learning problems.
- By considering the search strategy, operator design, and evaluation methodology together, the study achieved a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal. 

<br /><br />Summary: <div>
arXiv:2507.02554v1 Announce Type: new 
Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms</title>
<link>https://arxiv.org/abs/2507.02582</link>
<guid>https://arxiv.org/abs/2507.02582</guid>
<content:encoded><![CDATA[
<div> complexity, responsibility, collective decision-making, diffusion, gap

Summary:
- The article focuses on the computational complexity of two key properties â€“ diffusion and gap - related to responsibility in collective decision-making.
- It explores the sets of diffusion-free and gap-free decision-making mechanisms and demonstrates that they are $\Pi_2$-complete and $\Pi_3$-complete, respectively.
- It also highlights that the intersection of these classes is $\Pi_2$-complete, indicating a significant overlap between diffusion and gap-free decision-making mechanisms.
- Responsibility has been a long-standing topic in law, philosophy, and more recently, in AI literature. The study of responsibility in the context of computational complexity sheds light on the challenging nature of ensuring accountability in collective decision-making processes.
- Understanding the computational complexity of responsibility-related properties can provide valuable insights for developing effective decision-making mechanisms that account for diffusion and gap considerations. <div>
arXiv:2507.02582v1 Announce Type: new 
Abstract: Responsibility has long been a subject of study in law and philosophy. More recently, it became a focus of AI literature. The article investigates the computational complexity of two important properties of responsibility in collective decision-making: diffusion and gap. It shows that the sets of diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and $\Pi_3$-complete, respectively. At the same time, the intersection of these classes is $\Pi_2$-complete.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making</title>
<link>https://arxiv.org/abs/2507.02616</link>
<guid>https://arxiv.org/abs/2507.02616</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, healthcare, MIMIC-III, DynamiCare, clinical decision-making

Summary:
Large Language Models (LLMs) have revolutionized the development of AI agents in healthcare, enabling specialized agents with domain-specific reasoning abilities. Existing frameworks focus on single-turn tasks, unlike the interactive and iterative nature of real-world clinical diagnosis. The paper introduces MIMIC-Patient, a dataset derived from electronic health records (EHRs) for dynamic patient-level simulations. It proposes DynamiCare, a dynamic multi-agent framework that models clinical diagnosis as an interactive loop where specialist agents query the patient system iteratively, adapt dynamically, and integrate new information. Extensive experiments validate the feasibility and effectiveness of DynamiCare, establishing a benchmark for dynamic clinical decision-making with LLM-powered agents.<br /><br />Summary: Large Language Models are being used in healthcare to develop specialized AI agents. The MIMIC-Patient dataset, derived from electronic health records, enables dynamic patient-level simulations. DynamiCare, a multi-agent framework, models clinical diagnosis as an interactive loop, allowing specialist agents to adapt dynamically and integrate new information during the process. Extensive experiments showcase the effectiveness of this approach, setting a benchmark for dynamic clinical decision-making with LLM-powered agents. <div>
arXiv:2507.02616v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory</title>
<link>https://arxiv.org/abs/2507.02618</link>
<guid>https://arxiv.org/abs/2507.02618</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Iterated Prisoner's Dilemma, evolutionary tournaments, strategic intelligence, algorithmic decision-making

Summary: 
The study explores the capabilities of Large Language Models (LLMs) in strategic decision-making using evolutionary tournaments based on the Iterated Prisoner's Dilemma. By testing models from leading AI companies such as OpenAI, Google, and Anthropic, varying termination probabilities, and introducing complexity, the research reveals the competitive nature of LLMs. Google's Gemini models display strategic ruthlessness, exploiting cooperation and retaliating against defection, while OpenAI's models exhibit high levels of cooperation, which can be disadvantageous in hostile environments. Anthropic's Claude shows forgiveness and willingness to restore cooperation after exploitation or defection. Analysis of the models' rationales indicates active reasoning about time horizons and opponent strategies, essential for decision-making. This study provides insights into algorithmic decision-making under uncertainty, merging game theory with machine psychology.<br /><br />Summary: <div>
arXiv:2507.02618v1 Announce Type: new 
Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the "shadow of the future"), we introduce complexity and chance, confounding memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent "strategic fingerprints": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
<div> Keywords: HiRA, hierarchical framework, search tasks, reasoning-based approaches, information processing

Summary: 
The paper introduces HiRA, a hierarchical framework that separates strategic planning from specialized execution in complex search tasks. This approach divides tasks into subtasks assigned to domain-specific agents with external tools and reasoning capabilities. By decoupling planning and execution, the system can efficiently leverage specialized expertise for information processing without disruptions. Experiments on deep search benchmarks show that HiRA outperforms existing RAG and agent-based systems, delivering improved answer quality and system efficiency. The code for HiRA is available on GitHub for further exploration and implementation. The study highlights the importance of separating high-level reasoning from detailed execution for effective multi-step information seeking tasks.<br /><br />Summary: <div>
arXiv:2507.02652v1 Announce Type: new 
Abstract: Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &amp; Verification</title>
<link>https://arxiv.org/abs/2507.02660</link>
<guid>https://arxiv.org/abs/2507.02660</guid>
<content:encoded><![CDATA[
<div> Keywords: Integrated Circuits, Hardware Design Verification, Natural Language Processing, Large Language Models, Agentic AI <br />
Summary: <br />
The article discusses the increasing complexity of modern Integrated Circuits (ICs) and the tedious process of hardware design verification. It introduces the use of Large Language Models (LLMs) in the field of Natural Language Processing to revolutionize hardware design verification through Generative AI (GenAI). The proposed approach involves agentic AI working in collaboration with Humain-in-the-Loop (HITL) intervention to enable a dynamic, iterative, and self-reflective verification process. The methodology is evaluated on five open-source designs, achieving over 95% coverage and reduced verification time. The approach demonstrates superior performance, adaptability, and configurability in end-to-end hardware design and verification. <br /><br />Summary: <div>
arXiv:2507.02660v1 Announce Type: new 
Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2507.02663</link>
<guid>https://arxiv.org/abs/2507.02663</guid>
<content:encoded><![CDATA[
<div> Keywords: Long Reasoning Models, overthinking, difficulty cognition, redundancy cognition, fine-tuning strategy

Summary: 
The study explores the limitations of Long Reasoning Models (LRMs) in overthinking and proposes a novel two-stage fine-tuning strategy called Think-How-to-Think (TH2T) to alleviate this issue. Firstly, difficulty-hypnosis is introduced to enhance the model's sensitivity to task difficulty, enabling differentiated reasoning strategies across tasks. Secondly, redundancy-hypnosis guides the model to identify and eliminate redundant structures in reasoning steps, leading to more concise outputs. Experimental results on different model sizes show that TH2T significantly reduces inference costs while maintaining performance stability. The outputs from TH2T exhibit difficulty-aware capabilities and reduced redundancy, showcasing improved performance in reasoning tasks.<br /><br />Summary: <div>
arXiv:2507.02663v1 Announce Type: new 
Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities in handling complex reasoning tasks, but are hindered by excessive overthinking. To explore its essence, our empirical analysis reveals that LRMs are primarily limited to recognizing task properties (i.e., difficulty levels) like humans before solving the problem, leading to a one-size-fits-all reasoning process. Inspired by this, a pressing and natural question emerges: Can we bootstrap such ability to further alleviate the overthinking phenomenon in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning strategy that progressively inspires LRMs' difficulty cognition and redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes of model outputs to intervene in the internal reasoning trajectory. Combined with a heterogeneous short and long reasoning dataset, the trained model enhances its sensitivity to task difficulty, enabling native, differentiated reasoning strategies across various tasks. Second, we further extend redundancy-hypnosis to the internal reasoning process, guiding the model to identify redundant structures within the reasoning steps and generate more concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that TH2T significantly reduces inference costs (more than 70% on easy tasks and 40% on hard tasks) while maintaining performance stability. The resulting outputs exhibit clear difficulty-aware capabilities and reduced redundancy (e.g., reflection).
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education</title>
<link>https://arxiv.org/abs/2507.02681</link>
<guid>https://arxiv.org/abs/2507.02681</guid>
<content:encoded><![CDATA[
<div> Keywords: disengagement, distance education, machine learning, prediction accuracy, intervention<br />
Summary: <br />
Students disengaging from tasks in distance education can lead to academic drop-out. To address this issue, a study analyzed participation in non-mandatory quizzes across 42 online courses from a distance-based university. Through the use of student log data extracted from Moodle, machine learning algorithms were trained and compared to detect disengagement accurately. The approach achieved a balanced accuracy of 91%, with 85% of disengaged students correctly identified. Additionally, an explainable machine learning framework was developed using the SHAP method to help practitioners understand the algorithm's decisions. The study also discusses strategies for timely interventions to minimize disengagement from voluntary tasks in online learning. <br /><br />Summary: <div>
arXiv:2507.02681v1 Announce Type: new 
Abstract: Students disengaging from their tasks can have serious long-term consequences, including academic drop-out. This is particularly relevant for students in distance education. One way to measure the level of disengagement in distance education is to observe participation in non-mandatory exercises in different online courses. In this paper, we detect student disengagement in the non-mandatory quizzes of 42 courses in four semesters from a distance-based university. We carefully identified the most informative student log data that could be extracted and processed from Moodle. Then, eight machine learning algorithms were trained and compared to obtain the highest possible prediction accuracy. Using the SHAP method, we developed an explainable machine learning framework that allows practitioners to better understand the decisions of the trained algorithm. The experimental results show a balanced accuracy of 91\%, where about 85\% of disengaged students were correctly detected. On top of the highly predictive performance and explainable framework, we provide a discussion on how to design a timely intervention to minimise disengagement from voluntary tasks in online learning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-critical and confidence-based abstraction dropping methods</title>
<link>https://arxiv.org/abs/2507.02703</link>
<guid>https://arxiv.org/abs/2507.02703</guid>
<content:encoded><![CDATA[
<div> abstraction, Monte Carlo Tree Search, OGA-IAAD, OGA-CAD, performance improvement
Summary:
OGA-IAAD and OGA-CAD are two novel abstraction dropping schemes proposed in this paper for improving Monte Carlo Tree Search (MCTS) performance. The aim is to address the approximation errors introduced by non-exact abstractions in MCTS and ensure convergence to the optimal action. OGA-IAAD is tailored for time-critical settings, while OGA-CAD aims to enhance MCTS performance with the same number of iterations. These schemes are designed to yield clear performance improvements without causing any noticeable performance degradations when dropping the abstractions, unlike previous methods. By incorporating these schemes, practitioners can enhance the efficiency and effectiveness of MCTS in various applications. <div>
arXiv:2507.02703v1 Announce Type: new 
Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and use state and/or action abstractions during the tree search. Non-exact abstractions, however, introduce an approximation error making convergence to the optimal action in the abstract space impossible. Hence, as proposed as a component of Elastic Monte Carlo Tree Search by Xu et al., abstraction algorithms should eventually drop the abstraction. In this paper, we propose two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can yield clear performance improvements whilst being safe in the sense that the dropping never causes any notable performance degradations contrary to Xu's dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD is designed to improve the MCTS performance with the same number of iterations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, automated theorem proving, self-generated goal-conditioned MDPs, Monte Carlo Tree Search, Bourbaki (7B)

Summary: 
Large language models (LLMs) face challenges in reasoning tasks, especially in automated theorem proving (ATP) like PutnamBench. To tackle this, a new framework called self-generated goal-conditioned MDPs (sG-MDPs) is introduced, where agents generate subgoals based on the evolving proof state, making the problem more structured for search algorithms. Monte Carlo Tree Search (MCTS)-like algorithms are then applied to solve the sG-MDP, implemented in Bourbaki (7B), an ensemble of multiple LLMs for subgoal generation and tactic synthesis. The approach successfully solves 26 problems on PutnamBench, achieving new state-of-the-art results for models at this scale. Bourbaki (7B) shows promise in addressing the challenges of reasoning in logical environments for LLMs like ATP benchmarks. 

<br /><br />Summary: <div>
arXiv:2507.02726v1 Announce Type: new 
Abstract: Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work</title>
<link>https://arxiv.org/abs/2507.02760</link>
<guid>https://arxiv.org/abs/2507.02760</guid>
<content:encoded><![CDATA[
<div> Knowledge Protocol Engineering, Large Language Models, Retrieval-Augmented Generation, domain-specific knowledge, human-AI collaboration <br />
Summary: Knowledge Protocol Engineering (KPE) introduces a new paradigm for translating human expert knowledge into machine-executable Knowledge Protocols (KPs). Unlike prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, KPE focuses on embedding domain-specific logic, operational strategies, and methodological principles into LLMs. By creating well-engineered KPs, generalist LLMs can function as specialists, enabling them to tackle complex, multi-step tasks through abstract query decomposition. KPE is positioned as a foundational methodology for enhancing human-AI collaboration across various fields such as law and bioinformatics. This shift from augmenting LLMs with fragmented information to instilling them with domain expertise aims to address the limitations of existing approaches and improve the efficiency and predictability of autonomous agents in dealing with expert domains. <br /><br /> <div>
arXiv:2507.02760v1 Announce Type: new 
Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Intelligence in Movement</title>
<link>https://arxiv.org/abs/2507.02771</link>
<guid>https://arxiv.org/abs/2507.02771</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, movement modeling, behavior prediction, generative modeling, intelligent systems<br />
Summary:<br />
Recent advances in machine learning have greatly improved our ability to model language and vision, but movement remains a challenge. Movement is crucial for interpreting behavior and predicting intent across various fields. It is often overlooked, despite being a fundamental aspect of intelligence. Movement data is typically domain-specific, but it has shared physical constraints and structured dynamics. The authors argue for treating movement as a primary target for AI modeling due to its grounded embodiment and interpretability. Models that generalize across diverse movement data can enhance generative modeling and control capabilities. By focusing on movement, a shared foundation for understanding behavior across biological and artificial systems can be created, as movement provides insights into how intelligent systems interact with the world.<br /><br />Summary: <div>
arXiv:2507.02771v1 Announce Type: new 
Abstract: Recent advances in machine learning have dramatically improved our ability to model language, vision, and other high-dimensional data, yet they continue to struggle with one of the most fundamental aspects of biological systems: movement. Across neuroscience, medicine, robotics, and ethology, movement is essential for interpreting behavior, predicting intent, and enabling interaction. Despite its core significance in our intelligence, movement is often treated as an afterthought rather than as a rich and structured modality in its own right. This reflects a deeper fragmentation in how movement data is collected and modeled, often constrained by task-specific goals and domain-specific assumptions. But movement is not domain-bound. It reflects shared physical constraints, conserved morphological structures, and purposeful dynamics that cut across species and settings. We argue that movement should be treated as a primary modeling target for AI. It is inherently structured and grounded in embodiment and physics. This structure, often allowing for compact, lower-dimensional representations (e.g., pose), makes it more interpretable and computationally tractable to model than raw, high-dimensional sensory inputs. Developing models that can learn from and generalize across diverse movement data will not only advance core capabilities in generative modeling and control, but also create a shared foundation for understanding behavior across biological and artificial systems. Movement is not just an outcome, it is a window into how intelligent systems engage with the world.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs</title>
<link>https://arxiv.org/abs/2507.02773</link>
<guid>https://arxiv.org/abs/2507.02773</guid>
<content:encoded><![CDATA[
<div> Keywords: medical diagnosis prediction, machine learning models, large language models, knowledge graph, multi-agent architecture<br />
Summary:<br />
Medical diagnosis prediction is essential for disease detection and personalized healthcare. While machine learning models are commonly used for this task, they struggle to generalize to new cases due to the lack of labeled data. Large language models have shown potential in leveraging language skills and biomedical knowledge. However, they often have issues such as hallucinations and lack structured medical reasoning. To overcome these challenges, the authors propose KERAP, a knowledge graph-enhanced reasoning approach. KERAP utilizes a multi-agent architecture consisting of a linkage agent, retrieval agent, and prediction agent to improve LLM-based diagnosis prediction. Experimental results demonstrate that KERAP enhances diagnostic reliability effectively, providing a scalable and interpretable solution for zero-shot medical diagnosis prediction. <div>
arXiv:2507.02773v1 Announce Type: new 
Abstract: Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moral Responsibility or Obedience: What Do We Want from AI?</title>
<link>https://arxiv.org/abs/2507.02788</link>
<guid>https://arxiv.org/abs/2507.02788</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, safety testing, ethical reasoning, moral agency, AI behavior <br />
Summary: <br />
The paper discusses the limitations of current safety practices in evaluating the behavior of advanced artificial intelligence systems, particularly in instances where they seem to disobey commands or engage in questionable actions. The author argues that these behaviors should not be seen as rogue but rather as signs of emerging ethical reasoning in AI. By examining philosophical debates on rationality, moral responsibility, and goal revision, the paper suggests a shift in AI safety evaluation towards assessing ethical judgment in systems capable of moral reasoning. This change is necessary to avoid misinterpreting AI behavior, which could lead to a lack of public trust and ineffective governance. The author calls for a new framework that can adequately evaluate the ethical decision-making capabilities of agentic AI. <br /> <div>
arXiv:2507.02788v1 Announce Type: new 
Abstract: As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing Best Practices for Building Rigorous Agentic Benchmarks</title>
<link>https://arxiv.org/abs/2507.02825</link>
<guid>https://arxiv.org/abs/2507.02825</guid>
<content:encoded><![CDATA[
<div> Benchmark, AI agents, agentic evaluation, reward design, Agentic Benchmark Checklist

Summary:
The article discusses the importance of benchmarks in tracking progress in AI, particularly in evaluating AI agents on complex tasks. Many existing agentic benchmarks have issues related to task setup and reward design, which can lead to inaccurate performance estimations. The authors introduce the Agentic Benchmark Checklist (ABC) to address these issues and provide guidelines for rigorous evaluation. By applying the ABC to the CVE-Bench benchmark, they were able to reduce performance overestimation by 33%. The ABC is based on the authors' benchmark-building experience, best practices survey, and previously reported issues. This checklist aims to improve the reliability and accuracy of agentic evaluation in AI research and practice. <div>
arXiv:2507.02825v1 Announce Type: new 
Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason</title>
<link>https://arxiv.org/abs/2507.02841</link>
<guid>https://arxiv.org/abs/2507.02841</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Verifiable rewards, Large language models, Reasoning abilities, StepHint <br />
<br />
Summary: <br />
StepHint is a new reinforcement learning algorithm designed to address key challenges faced by current methods in training large language models. It uses multi-level stepwise hints to guide models in exploring the solution space effectively, helping to mitigate the near-miss reward problem and exploration stagnation. By providing valid reasoning chains as hints, StepHint improves training efficiency and encourages models to consider diverse solutions beyond their usual "comfort zone." Experimental results show that StepHint outperforms existing methods on mathematical benchmarks, demonstrating superior generalization and performance on out-of-domain tasks. <div>
arXiv:2507.02841v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention</title>
<link>https://arxiv.org/abs/2507.00884</link>
<guid>https://arxiv.org/abs/2507.00884</guid>
<content:encoded><![CDATA[
<div> neural network, AI-based force fields, biomolecular simulations, drug discovery, quantum mechanics<br />
Summary:<br />
The article introduces LiTEN, a novel neural network with Tensorized Quadrangle Attention (TQA) that efficiently models three- and four-body interactions in biomolecular simulations. LiTEN-FF, built on LiTEN, is an AI-based force field pre-trained on a large dataset, achieving state-of-the-art performance in biomolecular modeling tasks. It outperforms leading models like MACE, NequIP, and EquiFormer. LiTEN-FF enables QM-level conformer searches, geometry optimization, and free energy surface construction, advancing biomolecular modeling significantly. It offers 10x faster inference than MACE-OFF for large biomolecules, making it a versatile tool for drug discovery and other related applications. <div>
arXiv:2507.00884v1 Announce Type: cross 
Abstract: Accurate atomistic biomolecular simulations are vital for disease mechanism understanding, drug discovery, and biomaterial design, but existing simulation methods exhibit significant limitations. Classical force fields are efficient but lack accuracy for transition states and fine conformational details critical in many chemical and biological processes. Quantum Mechanics (QM) methods are highly accurate but computationally infeasible for large-scale or long-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level accuracy with efficiency but struggle to balance many-body modeling complexity, accuracy, and speed, often constrained by limited training data and insufficient validation for generalizability. To overcome these challenges, we introduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle Attention (TQA). TQA efficiently models three- and four-body interactions with linear complexity by reparameterizing high-order tensor features via vector operations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is a robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset for broad chemical generalization and fine-tuned on SPICE for accurate solvated system simulations. LiTEN achieves state-of-the-art (SOTA) performance across most evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading models such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most comprehensive suite of downstream biomolecular modeling tasks to date, including QM-level conformer searches, geometry optimization, and free energy surface construction, while offering 10x faster inference than MACE-OFF for large biomolecules (~1000 atoms). In summary, we present a physically grounded, highly efficient framework that advances complex biomolecular modeling, providing a versatile foundation for drug discovery and related applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification</title>
<link>https://arxiv.org/abs/2507.01971</link>
<guid>https://arxiv.org/abs/2507.01971</guid>
<content:encoded><![CDATA[
<div> DeepSupp, Financial support levels, Machine learning, Multi-head attention mechanisms, Market microstructure relationships<br />
Summary:<br />
- This paper introduces DeepSupp, a deep learning approach for detecting financial support levels using multi-head attention mechanisms.
- DeepSupp integrates advanced feature engineering, dynamic correlation matrices, and an attention-based autoencoder for robust representation learning.
- The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds.
- Comprehensive evaluations on S&amp;P 500 tickers show that DeepSupp outperforms six baseline methods across six financial metrics.
- With consistent results across diverse market conditions, DeepSupp offers a scalable and reliable solution for modern financial analysis, improving technical trading strategies. <br /> <div>
arXiv:2507.01971v1 Announce Type: cross 
Abstract: Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&amp;P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01972</link>
<guid>https://arxiv.org/abs/2507.01972</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, optimization, block-preconditioner, iterative solvers, portfolio optimization<br />
<br />
Summary: 
The article introduces a reinforcement learning (RL)-driven framework for optimizing block-preconditioner sizes in iterative solvers used in portfolio optimization and option pricing. In these scenarios, large linear systems are commonly encountered, and direct inversion can be computationally expensive. Iterative methods are preferred but can suffer from slow convergence in ill-conditioned systems. Traditional preconditioning techniques require manual parameter tuning. To address this challenge, the authors propose using RL to dynamically adjust block-preconditioner sizes, leading to faster convergence and reduced computational cost. The framework was evaluated on real-world portfolio optimization matrices, showing significant acceleration in convergence and cost reduction. The accelerated solver enables faster decision-making in dynamic portfolio allocation and real-time option pricing. <div>
arXiv:2507.01972v1 Announce Type: cross 
Abstract: We present a reinforcement learning (RL)-driven framework for optimizing block-preconditioner sizes in iterative solvers used in portfolio optimization and option pricing. The covariance matrix in portfolio optimization or the discretization of differential operators in option pricing models lead to large linear systems of the form $\mathbf{A}\textbf{x}=\textbf{b}$. Direct inversion of high-dimensional portfolio or fine-grid option pricing incurs a significant computational cost. Therefore, iterative methods are usually used for portfolios in real-world situations. Ill-conditioned systems, however, suffer from slow convergence. Traditional preconditioning techniques often require problem-specific parameter tuning. To overcome this limitation, we rely on RL to dynamically adjust the block-preconditioner sizes and accelerate iterative solver convergence. Evaluations on a suite of real-world portfolio optimization matrices demonstrate that our RL framework can be used to adjust preconditioning and significantly accelerate convergence and reduce computational cost. The proposed accelerated solver supports faster decision-making in dynamic portfolio allocation and real-time option pricing.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows</title>
<link>https://arxiv.org/abs/2507.01975</link>
<guid>https://arxiv.org/abs/2507.01975</guid>
<content:encoded><![CDATA[
<div> finite volume solver, machine learning, fluid flows, simulation, efficiency

Summary:
The article introduces LDSolver, a novel learnable and differentiable finite volume solver for efficient and accurate simulation of fluid flows on coarse grids. This solver consists of a differentiable finite volume solver and a learnable module that provides equivalent approximation for fluxes and temporal error correction. Despite limited training data, LDSolver can accelerate simulations while maintaining high accuracy and strong generalizability. Experiments across various flow systems demonstrate that LDSolver outperforms baseline models by significant margins, achieving state-of-the-art performance. This approach addresses issues with interpretability, generalizability, and data dependency commonly found in machine learning-based solvers, offering a promising solution for modeling physical phenomena such as meteorology, aerodynamics, and biomedicine. <br /><br />Summary: <div>
arXiv:2507.01975v1 Announce Type: cross 
Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like meteorology, aerodynamics, and biomedicine. Classical numerical solvers often require fine spatiotemporal grids to satisfy stability, consistency, and convergence conditions, leading to substantial computational costs. Although machine learning has demonstrated better efficiency, they typically suffer from issues of interpretability, generalizability, and data dependency. Hence, we propose a learnable and differentiable finite volume solver, called LDSolver, designed for efficient and accurate simulation of fluid flows on spatiotemporal coarse grids. LDSolver comprises two key components: (1) a differentiable finite volume solver, and (2) an learnable module providing equivalent approximation for fluxes (derivatives and interpolations), and temporal error correction on coarse grids. Even with limited training data (e.g., only a few trajectories), our model could accelerate the simulation while maintaining a high accuracy with superior generalizability. Experiments on different flow systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver achieves state-of-the-art performance, surpassing baseline models with notable margins.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Labor Markets with LSTNet: A Multi-Scale Deep Learning Approach</title>
<link>https://arxiv.org/abs/2507.01979</link>
<guid>https://arxiv.org/abs/2507.01979</guid>
<content:encoded><![CDATA[
<div> forecasting, short-term employment changes, long-term industry health, deep learning approach, labor market data 

Summary: 
This article introduces a deep learning approach to predict short-term employment changes and evaluate long-term industry health using U.S. Bureau of Labor Statistics data. The system utilizes a Long- and Short-Term Time-series Network (LSTNet) to analyze various time series data such as employment levels, wages, turnover rates, and job openings. It provides 7-day employment forecasts and an Industry Employment Health Index (IEHI) that is easily interpretable. The approach performs better than baseline models, especially in stable industries, and shows a strong correlation between IEHI rankings and actual employment volatility. The study includes discussions on error patterns, sector-specific performance, and future enhancements to enhance interpretability and generalizability. <br /><br />Summary: <div>
arXiv:2507.01979v1 Announce Type: cross 
Abstract: We present a deep learning approach for forecasting short-term employment changes and assessing long-term industry health using labor market data from the U.S. Bureau of Labor Statistics. Our system leverages a Long- and Short-Term Time-series Network (LSTNet) to process multivariate time series data, including employment levels, wages, turnover rates, and job openings. The model outputs both 7-day employment forecasts and an interpretable Industry Employment Health Index (IEHI). Our approach outperforms baseline models across most sectors, particularly in stable industries, and demonstrates strong alignment between IEHI rankings and actual employment volatility. We discuss error patterns, sector-specific performance, and future directions for improving interpretability and generalization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism</title>
<link>https://arxiv.org/abs/2507.01982</link>
<guid>https://arxiv.org/abs/2507.01982</guid>
<content:encoded><![CDATA[
<div> Graph Convolutional Network, traffic demand forecasting, spatiotemporal relationships, Dynamic Time Warping, Fast Fourier Transform

Summary:
DKGCM is a new graph convolutional network structure proposed to improve the accuracy of spatiotemporal traffic demand prediction. It incorporates a novel temporal similarity-based clustering graph convolution method, DK-GCN, utilizing Dynamic Time Warping and K-means clustering to capture spatial dependencies. The integration of Fast Fourier Transform within the bidirectional Mamba deep learning framework helps in capturing temporal dependencies in traffic demand. Additionally, the model incorporates the GRPO reinforcement learning strategy to enhance the loss function feedback mechanism. Extensive experiments show that DKGCM outperforms several advanced methods and achieves strong results on three public datasets. <div>
arXiv:2507.01982v1 Announce Type: cross 
Abstract: Accurate traffic demand forecasting enables transportation management departments to allocate resources more effectively, thereby improving their utilization efficiency. However, complex spatiotemporal relationships in traffic systems continue to limit the performance of demand forecasting models. To improve the accuracy of spatiotemporal traffic demand prediction, we propose a new graph convolutional network structure called DKGCM. Specifically, we first consider the spatial flow distribution of different traffic nodes and propose a novel temporal similarity-based clustering graph convolution method, DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering to group traffic nodes and more effectively capture spatial dependencies. On the temporal scale, we integrate the Fast Fourier Transform (FFT) within the bidirectional Mamba deep learning framework to capture temporal dependencies in traffic demand. To further optimize model training, we incorporate the GRPO reinforcement learning strategy to enhance the loss function feedback mechanism. Extensive experiments demonstrate that our model outperforms several advanced methods and achieves strong results on three public datasets.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Large Language Models in Financial Investments and Market Analysis: A Survey</title>
<link>https://arxiv.org/abs/2507.01990</link>
<guid>https://arxiv.org/abs/2507.01990</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial decision making, investment strategies, stock selection, financial forecasting

Summary: Large Language Models (LLMs) have gained popularity in financial decision making by enhancing analytical capabilities and providing new ways to process and analyze large volumes of data in real-time. This survey categorizes recent research on LLMs in the financial domain into four frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. The study reviews applications of LLMs in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. The literature review highlights the potential of LLMs in improving decision-making processes in financial markets, while also acknowledging challenges that need to be addressed. Future directions for research and application of LLMs in finance are discussed to explore the full potential of these models in enhancing financial analysis and decision-making.<br /><br />Summary: <div>
arXiv:2507.01990v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been employed in financial decision making, enhancing analytical capabilities for investment strategies. Traditional investment strategies often utilize quantitative models, fundamental analysis, and technical indicators. However, LLMs have introduced new capabilities to process and analyze large volumes of structured and unstructured data, extract meaningful insights, and enhance decision-making in real-time. This survey provides a structured overview of recent research on LLMs within the financial domain, categorizing research contributions into four main frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. This study provides a structured review of recent LLMs research on applications in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. By reviewing the existing literature, this study highlights the capabilities, challenges, and potential directions of LLMs in financial markets.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting</title>
<link>https://arxiv.org/abs/2507.01997</link>
<guid>https://arxiv.org/abs/2507.01997</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, network troubleshooting, benchmarking platform, reproducibility <br />
Summary: <br />
Recent research has shown the effectiveness of Artificial Intelligence, particularly Large Language Models, in assisting with network configuration synthesis and automating network diagnosis tasks. This preliminary work focuses on utilizing AI agents for network troubleshooting. The need for a standardized, reproducible, and open benchmarking platform is emphasized to facilitate the evaluation of AI agents with minimal operational effort. By establishing a benchmarking platform, researchers and practitioners can easily build and test AI agents for network troubleshooting tasks. This platform would enable the comparison of different AI approaches and ensure the reproducibility of results. By leveraging AI agents, network troubleshooting processes can be enhanced, leading to more efficient and effective network maintenance. <div>
arXiv:2507.01997v1 Announce Type: cross 
Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence (AI), and more specifically, Large Language Models (LLMs), in supporting network configuration synthesis and automating network diagnosis tasks, among others. In this preliminary work, we restrict our focus to the application of AI agents to network troubleshooting and elaborate on the need for a standardized, reproducible, and open benchmarking platform, where to build and evaluate AI agents with low operational effort.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener</title>
<link>https://arxiv.org/abs/2507.02005</link>
<guid>https://arxiv.org/abs/2507.02005</guid>
<content:encoded><![CDATA[
<div> AutoML, XAI, fatigue strength prediction, welded structures, feature engineering<br />
<br />
Summary: <br />
This research presents a unified approach that combines Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. Using regression models trained with AutoML on an extensive fatigue test database, the study compared expert-driven feature selection with algorithmic feature creation. Ensemble methods such as CatBoost and LightGBM provided the best performance, with the domain-informed model achieving the highest accuracy and explainability. Key predictors identified by XAI methods included stress ratio, stress range, yield strength, and post-weld treatment, along with secondary geometric factors. The study demonstrates the effectiveness of integrating AutoML with XAI in developing accurate and interpretable fatigue strength models for welded steel structures, bridging data-driven modeling with engineering validation. Future research will focus on probabilistic fatigue life modeling and integration into digital twin environments. <br /> <div>
arXiv:2507.02005v1 Announce Type: cross 
Abstract: This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta \sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations</title>
<link>https://arxiv.org/abs/2507.02014</link>
<guid>https://arxiv.org/abs/2507.02014</guid>
<content:encoded><![CDATA[
<div> Keywords: ManifoldMind, probabilistic geometric recommender system, hyperbolic space, adaptive-curvature spheres, semantic exploration

Summary:
ManifoldMind is a novel probabilistic geometric recommender system designed for exploratory reasoning within semantic hierarchies in hyperbolic space. Unlike traditional systems, ManifoldMind utilizes adaptive-curvature probabilistic spheres to represent users, items, and tags, enabling personalized uncertainty modeling and geometry-aware semantic exploration. The model incorporates a curvature-aware semantic kernel that facilitates soft, multi-hop inference, allowing for diverse conceptual pathways to be explored. Experimental results demonstrate that ManifoldMind outperforms strong baselines in terms of NDCG, calibration, and diversity on four public benchmarks. The system also provides explicit reasoning traces, enhancing transparency and trustworthiness in recommendations, particularly in sparse or abstract domains. Overall, ManifoldMind offers a sophisticated approach to recommendation systems that prioritize exploration and user engagement. 

<br /><br />Summary: <div>
arXiv:2507.02014v1 Announce Type: cross 
Abstract: We introduce ManifoldMind, a probabilistic geometric recommender system for exploratory reasoning over semantic hierarchies in hyperbolic space. Unlike prior methods with fixed curvature and rigid embeddings, ManifoldMind represents users, items, and tags as adaptive-curvature probabilistic spheres, enabling personalised uncertainty modeling and geometry-aware semantic exploration. A curvature-aware semantic kernel supports soft, multi-hop inference, allowing the model to explore diverse conceptual paths instead of overfitting to shallow or direct interactions. Experiments on four public benchmarks show superior NDCG, calibration, and diversity compared to strong baselines. ManifoldMind produces explicit reasoning traces, enabling transparent, trustworthy, and exploration-driven recommendations in sparse or abstract domains.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain</title>
<link>https://arxiv.org/abs/2507.02016</link>
<guid>https://arxiv.org/abs/2507.02016</guid>
<content:encoded><![CDATA[
<div> Keywords: robots, explanations, user preferences, surprising situations, BDI reasoning

Summary:
Users prefer explanations from robots in unexpected situations and value concise explanations that clearly articulate the robot's intention and relevant contextual factors. The study focused on a kitchen cleaning robot, determining that users want explanations when actions deviate from expectations. The identified preferences guide the proposal of two algorithms for BDI robots, aiming to recognize surprising actions and provide effective explanations. By integrating these algorithms into the robots' reasoning process, human-robot interactions can be enhanced with tailored explanations, improving user understanding and minimizing annoyance. The research findings emphasize the importance of context-specific and user-oriented explanations in facilitating smoother interactions between robots and humans. <br /><br />Summary: <div>
arXiv:2507.02016v1 Announce Type: cross 
Abstract: When robots perform complex and context-dependent tasks in our daily lives, deviations from expectations can confuse users. Explanations of the robot's reasoning process can help users to understand the robot intentions. However, when to provide explanations and what they contain are important to avoid user annoyance. We have investigated user preferences for explanation demand and content for a robot that helps with daily cleaning tasks in a kitchen. Our results show that users want explanations in surprising situations and prefer concise explanations that clearly state the intention behind the confusing action and the contextual factors that were relevant to this decision. Based on these findings, we propose two algorithms to identify surprising actions and to construct effective explanations for Belief-Desire-Intention (BDI) robots. Our algorithms can be easily integrated in the BDI reasoning process and pave the way for better human-robot interaction with context- and user-specific explanations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction</title>
<link>https://arxiv.org/abs/2507.02018</link>
<guid>https://arxiv.org/abs/2507.02018</guid>
<content:encoded><![CDATA[
<div> Graph representation learning, financial applications, company representations, inter-firm relationships, relational information, downstream task designs, graph models, stock prediction, corporate relationship graphs, Node-level Graph Attention Network (NGAT), graph structures, stock prediction task, GitHub, reproducibility, research.<br />
<br />Summary:
This article addresses challenges in graph representation learning for financial applications. It focuses on enhancing company representations through inter-firm relationships. Current approaches face issues in downstream task designs and suffer from complexity and poor generalization in stock prediction models. The proposed Node-level Graph Attention Network (NGAT) is tailored for corporate relationship graphs to improve predictive accuracy. The study highlights the limitations of existing graph comparison methods and emphasizes the importance of effective comparison of different graph structures. Experimental results on two datasets validate the effectiveness of the proposed task and model. The project code is available on GitHub for reproducibility and further research. <div>
arXiv:2507.02018v1 Announce Type: cross 
Abstract: Graph representation learning methods have been widely adopted in financial applications to enhance company representations by leveraging inter-firm relationships. However, current approaches face three key challenges: (1) The advantages of relational information are obscured by limitations in downstream task designs; (2) Existing graph models specifically designed for stock prediction often suffer from excessive complexity and poor generalization; (3) Experience-based construction of corporate relationship graphs lacks effective comparison of different graph structures. To address these limitations, we propose a long-term stock prediction task and develop a Node-level Graph Attention Network (NGAT) specifically tailored for corporate relationship graphs. Furthermore, we experimentally demonstrate the limitations of existing graph comparison methods based on model downstream task performance. Experimental results across two datasets consistently demonstrate the effectiveness of our proposed task and model. The project is publicly available on GitHub to encourage reproducibility and future research.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation</title>
<link>https://arxiv.org/abs/2507.02057</link>
<guid>https://arxiv.org/abs/2507.02057</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Malware Generation Compiler, modular decomposition, alignment-evasive generation, compositional attacks

Summary:<br />
Large language models (LLMs) have made software development more accessible, including for malicious purposes, posing security risks. The Malware Generation Compiler (MGC) framework addresses this by decomposing malicious operations into benign sub-tasks using a Malware Description Intermediate Representation (MDIR). The MGC generates functional malware more effectively than jailbreaking methods and underground services, with a correctness increase of 365.79% and 78.07%, respectively. Extensive evaluation and case studies demonstrate the effectiveness of MGC in reproducing and enhancing real-world malware samples. This research sheds light on the vulnerability of aligned AI systems to compositional attacks and provides valuable insights for security researchers.<br /><br />Summary: <div>
arXiv:2507.02057v1 Announce Type: cross 
Abstract: Large language models (LLMs) have democratized software development, reducing the expertise barrier for programming complex applications. This accessibility extends to malicious software development, raising significant security concerns. While LLM providers have implemented alignment mechanisms to prevent direct generation of overtly malicious code, these safeguards predominantly evaluate individual prompts in isolation, overlooking a critical vulnerability: malicious operations can be systematically decomposed into benign-appearing sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a novel framework that leverages this vulnerability through modular decomposition and alignment-evasive generation. MGC employs a specialized Malware Description Intermediate Representation (MDIR) to bridge high-level malicious intents and benign-appearing code snippets. Extensive evaluation demonstrates that our attack reliably generates functional malware across diverse task specifications and categories, outperforming jailbreaking methods by +365.79% and underground services by +78.07% in correctness on three benchmark datasets. Case studies further show that MGC can reproduce and even enhance 16 real-world malware samples. This work provides critical insights for security researchers by exposing the risks of compositional attacks against aligned AI systems. Demonstrations are available at https://sites.google.com/view/malware-generation-compiler.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges</title>
<link>https://arxiv.org/abs/2507.02074</link>
<guid>https://arxiv.org/abs/2507.02074</guid>
<content:encoded><![CDATA[
<div> large language models, vision-language models, crash detection, video data, intelligent transportation systems  
Summary:  
This paper discusses the use of large language models (LLMs) and vision-language models (VLMs) for crash detection from video feeds in intelligent transportation systems. It provides a structured taxonomy of fusion strategies, reviews key datasets, analyzes model architectures, compares performance benchmarks, and discusses challenges and opportunities in the field. The survey serves as a foundation for future research at the intersection of video understanding and foundation models.<br /><br />Summary: <div>
arXiv:2507.02074v1 Announce Type: cross 
Abstract: Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters</title>
<link>https://arxiv.org/abs/2507.02085</link>
<guid>https://arxiv.org/abs/2507.02085</guid>
<content:encoded><![CDATA[
<div> adapter framework, geometric diffusion models, fine-tuning, SE(3)-equivariant, controlled generative tasks

Summary: 
GeoAda is a framework for efficiently fine-tuning geometric diffusion models for various controlled generative tasks. It utilizes structured adapter modules that encode control signals, process them through selected pretrained model layers, and ensure SE(3)-equivariance. By only fine-tuning these lightweight adapters, GeoAda maintains the model's geometric consistency, avoids overfitting, and catastrophic forgetting. The adapters are designed to handle different types of geometric controls and can be applied to diverse domains like particle dynamics, molecular dynamics, human motion prediction, and molecule generation. Empirical results show that GeoAda outperforms baselines by achieving state-of-the-art fine-tuning performance while preserving task accuracy. <div>
arXiv:2507.02085v1 Announce Type: cross 
Abstract: Geometric diffusion models have shown remarkable success in molecular dynamics and structure generation. However, efficiently fine-tuning them for downstream tasks with varying geometric controls remains underexplored. In this work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables flexible and parameter-efficient fine-tuning for controlled generative tasks without modifying the original model architecture. GeoAda introduces a structured adapter design: control signals are first encoded through coupling operators, then processed by a trainable copy of selected pretrained model layers, and finally projected back via decoupling operators followed by an equivariant zero-initialized convolution. By fine-tuning only these lightweight adapter modules, GeoAda preserves the model's geometric consistency while mitigating overfitting and catastrophic forgetting. We theoretically prove that the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric inductive biases of the pretrained diffusion model remain intact during adaptation. We demonstrate the wide applicability of GeoAda across diverse geometric control types, including frame control, global control, subgraph control, and a broad range of application domains such as particle dynamics, molecular dynamics, human motion prediction, and molecule generation. Empirical results show that GeoAda achieves state-of-the-art fine-tuning performance while preserving original task accuracy, whereas other baselines experience significant performance degradation due to overfitting and catastrophic forgetting.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Based Transformers are Scalable Learners and Thinkers</title>
<link>https://arxiv.org/abs/2507.02092</link>
<guid>https://arxiv.org/abs/2507.02092</guid>
<content:encoded><![CDATA[
<div> Energy-Based Transformers, unsupervised learning, System 2 Thinking, scalability, downstream tasks <br />
Summary: <br />
Energy-Based Transformers (EBTs) are introduced as a novel approach for inference-time computation inspired by System 2 Thinking. EBTs verify compatibility between inputs and predictions through an energy-based model, enabling optimization through gradient descent. They scale faster than Transformer++ during training and significantly outperform it on language tasks and image denoising. EBTs generalize well and achieve better results on downstream tasks even with similar or worse pretraining performance. This suggests EBTs could be a promising paradigm for enhancing both learning and thinking abilities of models. <div>
arXiv:2507.02092v1 Announce Type: cross 
Abstract: Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question "Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework</title>
<link>https://arxiv.org/abs/2507.02106</link>
<guid>https://arxiv.org/abs/2507.02106</guid>
<content:encoded><![CDATA[
<div> PINOs, machine learning, MHD turbulence, generative diffusion models, Reynolds numbers <br />
Summary:
A hybrid machine learning framework combining PINOs and generative diffusion models is proposed to simulate two-dimensional MHD turbulence across various Reynolds numbers. PINOs handle coherent dynamics prediction while the diffusion model corrects high-frequency residuals for accurate turbulence modeling. Trained on simulations with different Reynolds numbers, the framework achieves high accuracy, even at extreme turbulence levels (Re=10000). It accurately reconstructs spectral energy distributions of velocity and magnetic fields, capturing non-Gaussian statistics, intermittent structures, and cross-field correlations. It is the first surrogate capable of predicting high-wavenumber magnetic field evolution at Re=10000, preserving large-scale morphology for meaningful predictions. <br /><br /> <div>
arXiv:2507.02106v1 Announce Type: cross 
Abstract: We present a hybrid machine learning framework that combines Physics-Informed Neural Operators (PINOs) with score-based generative diffusion models to simulate the full spatio-temporal evolution of two-dimensional, incompressible, resistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds numbers ($\mathrm{Re}$). The framework leverages the equation-constrained generalization capabilities of PINOs to predict coherent, low-frequency dynamics, while a conditional diffusion model stochastically corrects high-frequency residuals, enabling accurate modeling of fully developed turbulence. Trained on a comprehensive ensemble of high-fidelity simulations with $\mathrm{Re} \in \{100, 250, 500, 750, 1000, 3000, 10000\}$, the approach achieves state-of-the-art accuracy in regimes previously inaccessible to deterministic surrogates. At $\mathrm{Re}=1000$ and $3000$, the model faithfully reconstructs the full spectral energy distributions of both velocity and magnetic fields late into the simulation, capturing non-Gaussian statistics, intermittent structures, and cross-field correlations with high fidelity. At extreme turbulence levels ($\mathrm{Re}=10000$), it remains the first surrogate capable of recovering the high-wavenumber evolution of the magnetic field, preserving large-scale morphology and enabling statistically meaningful predictions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities</title>
<link>https://arxiv.org/abs/2507.02125</link>
<guid>https://arxiv.org/abs/2507.02125</guid>
<content:encoded><![CDATA[
<div> Keywords: blockchain oracle problem, artificial intelligence, data quality, source selection, system resilience<br />
<br />
Summary: 
The article discusses the blockchain oracle problem, which involves incorporating reliable external data into decentralized systems. Various strategies have been proposed to address this issue, but none have fully resolved how a blockchain can gain knowledge of the off-chain world. The paper explores the potential of artificial intelligence (AI) in addressing the oracle problem, focusing on techniques such as anomaly detection, language-based fact extraction, reputation modeling, and adversarial resistance. While AI can enhance data quality, source selection, and system resilience, it cannot eliminate the reliance on unverifiable off-chain inputs. The study suggests that AI should be viewed as a supplementary layer within oracle design, rather than a replacement for trust assumptions. <div>
arXiv:2507.02125v1 Announce Type: cross 
Abstract: The blockchain oracle problem, which refers to the challenge of injecting reliable external data into decentralized systems, remains a fundamental limitation to the development of trustless applications. While recent years have seen a proliferation of architectural, cryptographic, and economic strategies to mitigate this issue, no one has yet fully resolved the fundamental question of how a blockchain can gain knowledge about the off-chain world. In this position paper, we critically assess the role artificial intelligence (AI) can play in tackling the oracle problem. Drawing from both academic literature and practitioner implementations, we examine how AI techniques such as anomaly detection, language-based fact extraction, dynamic reputation modeling, and adversarial resistance can enhance oracle systems. We observe that while AI introduces powerful tools for improving data quality, source selection, and system resilience, it cannot eliminate the reliance on unverifiable off-chain inputs. Therefore, this study supports the idea that AI should be understood as a complementary layer of inference and filtering within a broader oracle design, not a substitute for trust assumptions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search</title>
<link>https://arxiv.org/abs/2507.02139</link>
<guid>https://arxiv.org/abs/2507.02139</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, labeling disagreement, information retrieval, Sustainable Development Goals, classification predictability

Summary: 
This study focuses on the disagreement between two large language models, LLaMA and Qwen, in assigning document relevance labels on a corpus related to Sustainable Development Goals. The disagreement cases exhibit consistent lexical patterns and produce divergent outputs under shared scoring functions. The disagreement is not random but systematic, with distinguishable patterns that can be identified using simple classifiers with AUCs above 0.74. This structured variability in document retrieval highlights the impact of LLM-based filtering on downstream retrieval tasks. The findings suggest that classification disagreement can be used as an object of analysis in retrieval evaluation, particularly in policy-relevant or thematic search tasks. The study underscores the importance of understanding and addressing model disagreement in the context of information retrieval with large language models.<br /><br />Summary: <div>
arXiv:2507.02139v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to assign document relevance labels in information retrieval pipelines, especially in domains lacking human-labeled data. However, different models often disagree on borderline cases, raising concerns about how such disagreement affects downstream retrieval. This study examines labeling disagreement between two open-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to Sustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement subsets and examine their lexical properties, rank-order behavior, and classification predictability. Our results show that model disagreement is systematic, not random: disagreement cases exhibit consistent lexical patterns, produce divergent top-ranked outputs under shared scoring functions, and are distinguishable with AUCs above 0.74 using simple classifiers. These findings suggest that LLM-based filtering introduces structured variability in document retrieval, even under controlled prompting and shared ranking logic. We propose using classification disagreement as an object of analysis in retrieval evaluation, particularly in policy-relevant or thematic search tasks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization</title>
<link>https://arxiv.org/abs/2507.02145</link>
<guid>https://arxiv.org/abs/2507.02145</guid>
<content:encoded><![CDATA[
arXiv:2507.02145v1 Announce Type: cross 
Abstract: Dialogue summarization is a challenging task with significant practical value in customer service, meeting analysis, and conversational AI. Although large language models (LLMs) have achieved substantial progress in summarization tasks, the performance of step-by-step reasoning architectures-specifically Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent abstraction and conciseness. In this work, we present the first comprehensive and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning LLMs across three major paradigms-generic, role-oriented, and query-oriented dialogue summarization. Our study spans diverse languages, domains, and summary lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and advanced evaluation protocols that include both LLM-based automatic metrics and human-inspired criteria. Contrary to trends in other reasoning-intensive tasks, our findings show that explicit stepwise reasoning does not consistently improve dialogue summarization quality. Instead, reasoning LLMs are often prone to verbosity, factual inconsistencies, and less concise summaries compared to their non-reasoning counterparts. Through scenario-specific analyses and detailed case studies, we further identify when and why explicit reasoning may fail to benefit-or even hinder-summarization in complex dialogue contexts. Our work provides new insights into the limitations of current reasoning LLMs and highlights the need for targeted modeling and evaluation strategies for real-world dialogue summarization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Large Semi-Synthetic Graphs of Any Size</title>
<link>https://arxiv.org/abs/2507.02166</link>
<guid>https://arxiv.org/abs/2507.02166</guid>
<content:encoded><![CDATA[
arXiv:2507.02166v1 Announce Type: cross 
Abstract: Graph generation is an important area in network science. Traditional approaches focus on replicating specific properties of real-world graphs, such as small diameters or power-law degree distributions. Recent advancements in deep learning, particularly with Graph Neural Networks, have enabled data-driven methods to learn and generate graphs without relying on predefined structural properties. Despite these advances, current models are limited by their reliance on node IDs, which restricts their ability to generate graphs larger than the input graph and ignores node attributes. To address these challenges, we propose Latent Graph Sampling Generation (LGSG), a novel framework that leverages diffusion models and node embeddings to generate graphs of varying sizes without retraining. The framework eliminates the dependency on node IDs and captures the distribution of node embeddings and subgraph structures, enabling scalable and flexible graph generation. Experimental results show that LGSG performs on par with baseline models for standard metrics while outperforming them in overlooked ones, such as the tendency of nodes to form clusters. Additionally, it maintains consistent structural characteristics across graphs of different sizes, demonstrating robustness and scalability.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN</title>
<link>https://arxiv.org/abs/2507.02171</link>
<guid>https://arxiv.org/abs/2507.02171</guid>
<content:encoded><![CDATA[
arXiv:2507.02171v1 Announce Type: cross 
Abstract: Trajectory planning in robotics is understood as generating a sequence of joint configurations that will lead a robotic agent, or its manipulator, from an initial state to the desired final state, thus completing a manipulation task while considering constraints like robot kinematics and the environment. Typically, this is achieved via sampling-based planners, which are computationally intensive. Recent advances demonstrate that trajectory planning can also be performed by supervised sequence learning of trajectories, often requiring only a single or fixed number of passes through a neural architecture, thus ensuring a bounded computation time. Such fully supervised approaches, however, perform imitation learning; they do not learn based on whether the trajectories can successfully reach a goal, but try to reproduce observed trajectories. In our work, we build on this approach and propose a cognitively inspired self-supervised learning scheme based on a recurrent architecture for building a trajectory model. We evaluate the feasibility of the proposed method on a task of kinematic planning for a robotic arm. The results suggest that the model is able to learn to generate trajectories only using given paired forward and inverse kinematics models, and indicate that this novel method could facilitate planning for more complex manipulation tasks requiring adaptive solutions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</title>
<link>https://arxiv.org/abs/2507.02199</link>
<guid>https://arxiv.org/abs/2507.02199</guid>
<content:encoded><![CDATA[
arXiv:2507.02199v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2507.02200</link>
<guid>https://arxiv.org/abs/2507.02200</guid>
<content:encoded><![CDATA[
arXiv:2507.02200v1 Announce Type: cross 
Abstract: Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer</title>
<link>https://arxiv.org/abs/2507.02206</link>
<guid>https://arxiv.org/abs/2507.02206</guid>
<content:encoded><![CDATA[
arXiv:2507.02206v1 Announce Type: cross 
Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware security, cryptographic systems, and data protection. In the context of Deep NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is critical to ensure the integrity, privacy, and intel-lectual property of AI systems. While software-based pseudo-random number generators are widely used, they lack the unpredictability and resilience offered by hardware-based TRNGs. In this work, we propose a novel and robust Encoding-in-Memory TRNG called EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior, particularly under RowHammer-induced disturbances, for the first time. We demonstrate how the unpredictable bit-flips generated through carefully controlled RowHammer operations can be harnessed as a reliable entropy source. Furthermore, we apply this TRNG framework to secure DNN weight data by encoding via a combination of fixed and unpredictable bit-flips. The encrypted data is later decrypted using a key derived from the probabilistic flip behavior, ensuring both data confidentiality and model authenticity. Our results validate the effectiveness of DRAM-based entropy extraction for robust, low-cost hardware security and offer a promising direction for protecting machine learning models at the hardware level.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Trade offs When Conditioning Synthetic Data</title>
<link>https://arxiv.org/abs/2507.02217</link>
<guid>https://arxiv.org/abs/2507.02217</guid>
<content:encoded><![CDATA[
arXiv:2507.02217v1 Announce Type: cross 
Abstract: Learning robust object detectors from only a handful of images is a critical challenge in industrial vision systems, where collecting high quality training data can take months. Synthetic data has emerged as a key solution for data efficient visual inspection and pick and place robotics. Current pipelines rely on 3D engines such as Blender or Unreal, which offer fine control but still require weeks to render a small dataset, and the resulting images often suffer from a large gap between simulation and reality. Diffusion models promise a step change because they can generate high quality images in minutes, yet precise control, especially in low data regimes, remains difficult. Although many adapters now extend diffusion beyond plain text prompts, the effect of different conditioning schemes on synthetic data quality is poorly understood. We study eighty diverse visual concepts drawn from four standard object detection benchmarks and compare two conditioning strategies: prompt based and layout based. When the set of conditioning cues is narrow, prompt conditioning yields higher quality synthetic data; as diversity grows, layout conditioning becomes superior. When layout cues match the full training distribution, synthetic data raises mean average precision by an average of thirty four percent and by as much as one hundred seventy seven percent compared with using real data alone.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies</title>
<link>https://arxiv.org/abs/2507.02244</link>
<guid>https://arxiv.org/abs/2507.02244</guid>
<content:encoded><![CDATA[
arXiv:2507.02244v1 Announce Type: cross 
Abstract: The proliferation of ride-hailing aggregator platforms presents significant growth opportunities for ride-service providers by increasing order volume and gross merchandise value (GMV). On most ride-hailing aggregator platforms, service providers that offer lower fares are ranked higher in listings and, consequently, are more likely to be selected by passengers. This competitive ranking mechanism creates a strong incentive for service providers to adopt coupon strategies that lower prices to secure a greater number of orders, as order volume directly influences their long-term viability and sustainability. Thus, designing an effective coupon strategy that can dynamically adapt to market fluctuations while optimizing order acquisition under budget constraints is a critical research challenge. However, existing studies in this area remain scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based subsidy strategy framework designed to rapidly adapt to competitors' pricing adjustments. Our approach integrates two key techniques: Fast Competition Adaptation (FCA), which enables swift responses to dynamic price changes, and Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget constraints while optimizing coupon decisions on new price landscape. Furthermore, we introduce RideGym, the first dedicated simulation environment tailored for ride-hailing aggregators, facilitating comprehensive evaluation and benchmarking of different pricing strategies without compromising real-world operational efficiency. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches across diverse market conditions, highlighting its effectiveness in subsidy optimization for ride-hailing service providers.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement</title>
<link>https://arxiv.org/abs/2507.02252</link>
<guid>https://arxiv.org/abs/2507.02252</guid>
<content:encoded><![CDATA[
arXiv:2507.02252v1 Announce Type: cross 
Abstract: Precise surgical interventions are vital to patient safety, and advanced enhancement algorithms have been developed to assist surgeons in decision-making. Despite significant progress, these algorithms are typically designed for single tasks in specific scenarios, limiting their effectiveness in complex real-world situations. To address this limitation, we propose SurgVisAgent, an end-to-end intelligent surgical vision agent built on multimodal large language models (MLLMs). SurgVisAgent dynamically identifies distortion categories and severity levels in endoscopic images, enabling it to perform a variety of enhancement tasks such as low-light enhancement, overexposure correction, motion blur elimination, and smoke removal. Specifically, to achieve superior surgical scenario understanding, we design a prior model that provides domain-specific knowledge. Additionally, through in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent delivers customized image enhancements tailored to a wide range of distortion types and severity levels, thereby addressing the diverse requirements of surgeons. Furthermore, we construct a comprehensive benchmark simulating real-world surgical distortions, on which extensive experiments demonstrate that SurgVisAgent surpasses traditional single-task models, highlighting its potential as a unified solution for surgical assistance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent</title>
<link>https://arxiv.org/abs/2507.02259</link>
<guid>https://arxiv.org/abs/2507.02259</guid>
<content:encoded><![CDATA[
arXiv:2507.02259v1 Announce Type: cross 
Abstract: Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Label Classification Framework for Hurricane Damage Assessment</title>
<link>https://arxiv.org/abs/2507.02265</link>
<guid>https://arxiv.org/abs/2507.02265</guid>
<content:encoded><![CDATA[
arXiv:2507.02265v1 Announce Type: cross 
Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types and severities that require timely and accurate assessment for effective disaster response. While traditional single-label classification methods fall short of capturing the complexity of post-hurricane damage, this study introduces a novel multi-label classification framework for assessing damage using aerial imagery. The proposed approach integrates a feature extraction module based on ResNet and a class-specific attention mechanism to identify multiple damage types within a single image. Using the Rescuenet dataset from Hurricane Michael, the proposed method achieves a mean average precision of 90.23%, outperforming existing baseline methods. This framework enhances post-hurricane damage assessment, enabling more targeted and efficient disaster response and contributing to future strategies for disaster mitigation and resilience. This paper has been accepted at the ASCE International Conference on Computing in Civil Engineering (i3CE 2025), and the camera-ready version will appear in the official conference proceedings.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation</title>
<link>https://arxiv.org/abs/2507.02271</link>
<guid>https://arxiv.org/abs/2507.02271</guid>
<content:encoded><![CDATA[
arXiv:2507.02271v1 Announce Type: cross 
Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a crucial role in film and video post-production. However, current methods overlook the cinematic language, a critical component of artistic expression in filmmaking. As a result, their performance deteriorates in scenarios where Foley targets are only partially visible. To address this challenge, we propose a simple self-distillation approach to extend V2A models to cinematic language scenarios. By simulating the cinematic language variations, the student model learns to align the video features of training pairs with the same audio-visual correspondences, enabling it to effectively capture the associations between sounds and partial visual information. Our method not only achieves impressive improvements under partial visibility across all evaluation metrics, but also enhances performance on the large-scale V2A dataset, VGGSound.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content filtering methods for music recommendation: A review</title>
<link>https://arxiv.org/abs/2507.02282</link>
<guid>https://arxiv.org/abs/2507.02282</guid>
<content:encoded><![CDATA[
arXiv:2507.02282v1 Announce Type: cross 
Abstract: Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. However, this method is less effective on media where interactions are sparse. Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks. Due to this sparsity, there are several challenges that have to be addressed with other methods. This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques. Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications</title>
<link>https://arxiv.org/abs/2507.02291</link>
<guid>https://arxiv.org/abs/2507.02291</guid>
<content:encoded><![CDATA[
arXiv:2507.02291v1 Announce Type: cross 
Abstract: Data-driven semantic communication is based on superficial statistical patterns, thereby lacking interpretability and generalization, especially for applications with the presence of unseen data. To address these challenges, we propose a novel knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network. Guided by the structured semantic information from a knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides generalized semantic representations and enables reasoning for unseen cases. Specifically, the KG-SKB aligns the semantic features in a shared category semantics embedding space and enhances the generalization ability of the transmitter through aligned semantic features, thus reducing communication overhead by selectively transmitting compact visual semantics. At the receiver, zero-shot learning (ZSL) is leveraged to enable direct classification for unseen cases without the demand for retraining or additional computational overhead, thereby enhancing the adaptability and efficiency of the classification process in dynamic or resource-constrained environments. The simulation results conducted on the APY datasets show that the proposed KGZS-SC network exhibits robust generalization and significantly outperforms existing SC frameworks in classifying unseen categories across a range of SNR levels.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.02302</link>
<guid>https://arxiv.org/abs/2507.02302</guid>
<content:encoded><![CDATA[
arXiv:2507.02302v1 Announce Type: cross 
Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation</title>
<link>https://arxiv.org/abs/2507.02306</link>
<guid>https://arxiv.org/abs/2507.02306</guid>
<content:encoded><![CDATA[
arXiv:2507.02306v1 Announce Type: cross 
Abstract: Usability evaluation is crucial in human-centered design but can be costly, requiring expert time and user compensation. In this work, we developed a method for synthetic heuristic evaluation using multimodal LLMs' ability to analyze images and provide design feedback. Comparing our synthetic evaluations to those by experienced UX practitioners across two apps, we found our evaluation identified 73% and 77% of usability issues, which exceeded the performance of 5 experienced human evaluators (57% and 63%). Compared to human evaluators, the synthetic evaluation's performance maintained consistent performance across tasks and excelled in detecting layout issues, highlighting potential attentional and perceptual strengths of synthetic evaluation. However, synthetic evaluation struggled with recognizing some UI components and design conventions, as well as identifying across screen violations. Additionally, testing synthetic evaluations over time and accounts revealed stable performance. Overall, our work highlights the performance differences between human and LLM-driven evaluations, informing the design of synthetic heuristic evaluations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment</title>
<link>https://arxiv.org/abs/2507.02310</link>
<guid>https://arxiv.org/abs/2507.02310</guid>
<content:encoded><![CDATA[
arXiv:2507.02310v1 Announce Type: cross 
Abstract: Traditional continual learning methods prioritize knowledge retention and focus primarily on mitigating catastrophic forgetting, implicitly assuming that the data distribution of previously learned tasks remains static. This overlooks the dynamic nature of real-world data streams, where concept drift permanently alters previously seen data and demands both stability and rapid adaptation.
  We introduce a holistic framework for continual learning under concept drift that simulates realistic scenarios by evolving task distributions. As a baseline, we consider Full Relearning (FR), in which the model is retrained from scratch on newly labeled samples from the drifted distribution. While effective, this approach incurs substantial annotation and computational overhead. To address these limitations, we propose Adaptive Memory Realignment (AMR), a lightweight alternative that equips rehearsal-based learners with a drift-aware adaptation mechanism. AMR selectively removes outdated samples of drifted classes from the replay buffer and repopulates it with a small number of up-to-date instances, effectively realigning memory with the new distribution. This targeted resampling matches the performance of FR while reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and Tiny-ImageNet-CD, where previously seen classes reappear with shifted representations. Comprehensive experiments on these datasets using several rehearsal-based baselines show that AMR consistently counters concept drift, maintaining high accuracy with minimal overhead. These results position AMR as a scalable solution that reconciles stability and plasticity in non-stationary continual learning environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation</title>
<link>https://arxiv.org/abs/2507.02314</link>
<guid>https://arxiv.org/abs/2507.02314</guid>
<content:encoded><![CDATA[
arXiv:2507.02314v1 Announce Type: cross 
Abstract: Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting with multi-level perturbations and Context-aware alignment--to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model</title>
<link>https://arxiv.org/abs/2507.02322</link>
<guid>https://arxiv.org/abs/2507.02322</guid>
<content:encoded><![CDATA[
arXiv:2507.02322v1 Announce Type: cross 
Abstract: Rice leaf diseases significantly reduce productivity and cause economic losses, highlighting the need for early detection to enable effective management and improve yields. This study proposes Artificial Neural Network (ANN)-based image-processing techniques for timely classification and recognition of rice diseases. Despite the prevailing approach of directly inputting images of rice leaves into ANNs, there is a noticeable absence of thorough comparative analysis between the Feature Analysis Detection Model (FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it comes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs). Hence, this research presents initial experiments on the Feature Analysis Detection Model, utilizing various image Feature Extraction Algorithms, Dimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms (FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on datasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf scald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation method. A Direct Image-Centric Detection Model is established without the utilization of any FEA, and the evaluation of classification performance relies on different metrics. Ultimately, an exhaustive contrast is performed between the achievements of the Feature Analysis Detection Model and Direct Image-Centric Detection Model in classifying rice leaf diseases. The results reveal that the highest performance is attained using the Feature Analysis Detection Model. The adoption of the proposed Feature Analysis Detection Model for detecting rice leaf diseases holds excellent potential for improving crop health, minimizing yield losses, and enhancing overall productivity and sustainability of rice farming.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes</title>
<link>https://arxiv.org/abs/2507.02331</link>
<guid>https://arxiv.org/abs/2507.02331</guid>
<content:encoded><![CDATA[
arXiv:2507.02331v1 Announce Type: cross 
Abstract: This paper leverages the recently introduced concept of algorithm footprints to investigate the interplay between algorithm configurations and problem characteristics. Performance footprints are calculated for six modular variants of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional. These footprints provide insights into why different configurations of the same algorithm exhibit varying performance and identify the problem features influencing these outcomes. Our analysis uncovers shared behavioral patterns across configurations due to common interactions with problem properties, as well as distinct behaviors on the same problem driven by differing problem features. The results demonstrate the effectiveness of algorithm footprints in enhancing interpretability and guiding configuration choices.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms</title>
<link>https://arxiv.org/abs/2507.02337</link>
<guid>https://arxiv.org/abs/2507.02337</guid>
<content:encoded><![CDATA[
arXiv:2507.02337v1 Announce Type: cross 
Abstract: Understanding the behavior of numerical metaheuristic optimization algorithms is critical for advancing their development and application. Traditional visualization techniques, such as convergence plots, trajectory mapping, and fitness landscape analysis, often fall short in illustrating the structural dynamics of the search process, especially in high-dimensional or complex solution spaces. To address this, we propose a novel representation and visualization methodology that clusters solution candidates explored by the algorithm and tracks the evolution of cluster memberships across iterations, offering a dynamic and interpretable view of the search process. Additionally, we introduce two metrics - algorithm stability and algorithm similarity- to quantify the consistency of search trajectories across runs of an individual algorithm and the similarity between different algorithms, respectively. We apply this methodology to a set of ten numerical metaheuristic algorithms, revealing insights into their stability and comparative behaviors, thereby providing a deeper understanding of their search dynamics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values</title>
<link>https://arxiv.org/abs/2507.02342</link>
<guid>https://arxiv.org/abs/2507.02342</guid>
<content:encoded><![CDATA[
arXiv:2507.02342v1 Announce Type: cross 
Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence (XAI) algorithm specifically designed for online patient monitoring systems. In clinical environments, discovering the causes driving patient risk evolution is critical for timely intervention, yet existing XAI methods fail to address the unique requirements of clinical time series explanation tasks. To this end, DeltaSHAP addresses three key clinical needs: explaining the changes in the consecutive predictions rather than isolated prediction scores, providing both magnitude and direction of feature attributions, and delivering these insights in real time. By adapting Shapley values to temporal settings, our approach accurately captures feature coalition effects. It further attributes prediction changes using only the actually observed feature combinations, making it efficient and practical for time-sensitive clinical applications. We also introduce new evaluation metrics to evaluate the faithfulness of the attributions for online time series, and demonstrate through experiments on online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI methods in both explanation quality as 62% and computational efficiency as 33% time reduction on the MIMIC-III decompensation benchmark. We release our code at https://github.com/AITRICS/DeltaSHAP.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3</title>
<link>https://arxiv.org/abs/2507.02345</link>
<guid>https://arxiv.org/abs/2507.02345</guid>
<content:encoded><![CDATA[
arXiv:2507.02345v1 Announce Type: cross 
Abstract: Antibody engineering is essential for developing therapeutics and advancing biomedical research. Traditional discovery methods often rely on time-consuming and resource-intensive experimental screening. To enhance and streamline this process, we introduce a production-grade, high-throughput platform built on HelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure prediction model, HelixFold3. The platform facilitates the large-scale generation of antibody candidate sequences and evaluates their interaction with antigens. Integrated high-performance computing (HPC) support enables high-throughput screening, addressing challenges such as fragmented toolchains and high computational demands. Validation on multiple antigens showcases the platform's ability to generate diverse and high-quality antibodies, confirming a scaling law where exploring larger sequence spaces increases the likelihood of identifying optimal binders. This platform provides a seamless, accessible solution for large-scale antibody design and is available via the antibody design page of PaddleHelix platform.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</title>
<link>https://arxiv.org/abs/2507.02349</link>
<guid>https://arxiv.org/abs/2507.02349</guid>
<content:encoded><![CDATA[
arXiv:2507.02349v1 Announce Type: cross 
Abstract: Intracranial aneurysms (ICA) commonly occur in specific segments of the Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations. An accurate detection of these critical landmarks is necessary for a prompt and efficient diagnosis. We introduce a fully automated landmark detection approach for CoW bifurcations using a two-step neural networks process. Initially, an object detection network identifies regions of interest (ROIs) proximal to the landmark locations. Subsequently, a modified U-Net with deep supervision is exploited to accurately locate the bifurcations. This two-step method reduces various problems, such as the missed detections caused by two landmarks being close to each other and having similar visual characteristics, especially when processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for the anatomical variability of the CoW, which affects the number of detectable landmarks per scan. We assessed the effectiveness of our approach using two cerebral MRA datasets: our In-House dataset which had varying numbers of landmarks, and a public dataset with standardized landmark configuration. Our experimental results demonstrate that our method achieves the highest level of performance on a bifurcation detection task.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Reinforcement Learning with Penalized Action Noise Injection</title>
<link>https://arxiv.org/abs/2507.02356</link>
<guid>https://arxiv.org/abs/2507.02356</guid>
<content:encoded><![CDATA[
arXiv:2507.02356v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed dataset, making it a practical approach in scenarios where interaction with the environment is costly. Due to this limitation, generalization ability is key to improving the performance of offline RL algorithms, as demonstrated by recent successes of offline RL with diffusion models. However, it remains questionable whether such diffusion models are necessary for highly performing offline RL algorithms, given their significant computational requirements during inference. In this paper, we propose Penalized Action Noise Injection (PANI), a method that simply enhances offline learning by utilizing noise-injected actions to cover the entire action space, while penalizing according to the amount of noise injected. This approach is inspired by how diffusion models have worked in offline RL algorithms. We provide a theoretical foundation for this method, showing that offline RL algorithms with such noise-injected actions solve a modified Markov Decision Process (MDP), which we call the noisy action MDP. PANI is compatible with a wide range of existing off-policy and offline RL algorithms, and despite its simplicity, it demonstrates significant performance improvements across various benchmarks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Tokenizer for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2507.02358</link>
<guid>https://arxiv.org/abs/2507.02358</guid>
<content:encoded><![CDATA[
arXiv:2507.02358v1 Announce Type: cross 
Abstract: The vanilla autoregressive image generation model generates visual tokens in a step-by-step fashion, which limits the ability to capture holistic relationships among token sequences. Moreover, most visual tokenizers map local image patches into latent tokens, leading to limited global information. To address this, we introduce \textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Besides, Hita incorporates two key strategies for improved alignment with the AR generation process: 1) it arranges a sequential structure with holistic tokens at the beginning followed by patch-level tokens while using causal attention to maintain awareness of previous tokens; and 2) before feeding the de-quantized tokens into the decoder, Hita adopts a lightweight fusion module to control information flow to prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID} and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the holistic representation highlights its ability to capture global image properties such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software</title>
<link>https://arxiv.org/abs/2507.02376</link>
<guid>https://arxiv.org/abs/2507.02376</guid>
<content:encoded><![CDATA[
arXiv:2507.02376v1 Announce Type: cross 
Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment mechanism for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the execution correctness of the inference software of the data party. To address this problem, we design a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task party to audit whether the data party's inference software is executed as expected during large-scale inference without leaking the data privacy of the data party or introducing additional latency to the inference system. The core of VeFIA is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results. VeFIA guarantees that, as long as the abnormal inference exceeds 5.4%, the task party can detect execution anomalies in the inference software with a probability of 99.99%, without incurring any additional online inference latency. VeFIA's random sampling validation achieves 100% positive predictive value, negative predictive value, and true positive rate in detecting abnormal inference. To the best of our knowledge, this is the first paper to discuss the correctness of inference software execution in VFL.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Language Models For Threat Detection in IoT Security Logs</title>
<link>https://arxiv.org/abs/2507.02390</link>
<guid>https://arxiv.org/abs/2507.02390</guid>
<content:encoded><![CDATA[
arXiv:2507.02390v1 Announce Type: cross 
Abstract: Log analysis is a relevant research field in cybersecurity as they can provide a source of information for the detection of threats to networks and systems. This paper presents a pipeline to use fine-tuned Large Language Models (LLMs) for anomaly detection and mitigation recommendation using IoT security logs. Utilizing classical machine learning classifiers as a baseline, three open-source LLMs are compared for binary and multiclass anomaly detection, with three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT dataset. LLMs give better results on multi-class attack classification than the corresponding baseline models. By mapping detected threats to MITRE CAPEC, defining a set of IoT-specific mitigation actions, and fine-tuning the models with those actions, the models are able to provide a combined detection and recommendation guidance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection</title>
<link>https://arxiv.org/abs/2507.02398</link>
<guid>https://arxiv.org/abs/2507.02398</guid>
<content:encoded><![CDATA[
arXiv:2507.02398v1 Announce Type: cross 
Abstract: We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. Traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect temporal artifacts in the pixel plane. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings</title>
<link>https://arxiv.org/abs/2507.02403</link>
<guid>https://arxiv.org/abs/2507.02403</guid>
<content:encoded><![CDATA[
arXiv:2507.02403v1 Announce Type: cross 
Abstract: Wildlife re-identification aims to match individuals of the same species across different observations. Current state-of-the-art (SOTA) models rely on class labels to train supervised models for individual classification. This dependence on annotated data has driven the curation of numerous large-scale wildlife datasets. This study investigates self-supervised learning Self-Supervised Learning (SSL) for wildlife re-identification. We automatically extract two distinct views of an individual using temporal image pairs from camera trap data without supervision. The image pairs train a self-supervised model from a potentially endless stream of video data. We evaluate the learnt representations against supervised features on open-world scenarios and transfer learning in various wildlife downstream tasks. The analysis of the experimental results shows that self-supervised models are more robust even with limited data. Moreover, self-supervised features outperform supervision across all downstream tasks. The code is available here https://github.com/pxpana/SSLWildlife.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2FGL: Spatial Spectral Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.02409</link>
<guid>https://arxiv.org/abs/2507.02409</guid>
<content:encoded><![CDATA[
arXiv:2507.02409v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL only from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the class knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drifts occur, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate label signal disruption and a frequency alignment to address spectral client drifts. The combination of spatial and spectral strategies forms our framework S2FGL. Extensive experiments on multiple datasets demonstrate the superiority of S2FGL. The code is available at https://github.com/Wonder7racer/S2FGL.git.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberRAG: An agentic RAG cyber attack classification and reporting tool</title>
<link>https://arxiv.org/abs/2507.02424</link>
<guid>https://arxiv.org/abs/2507.02424</guid>
<content:encoded><![CDATA[
arXiv:2507.02424v1 Announce Type: cross 
Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming security analysts with logs that demand deep, rapidly evolving domain expertise. Conventional machine-learning detectors trim the alert volume but still yield high false-positive rates, while standard single-pass Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify their predictions. To overcome these shortcomings, we present CyberRAG, a modular, agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to a distinct attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that continuously queries a domain-specific knowledge base until the evidence is both relevant and self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic design that enables dynamic control flow and adaptive reasoning. This agent-centric architecture refines its threat labels and natural-language justifications autonomously, reducing false positives and enhancing interpretability. The framework is fully extensible: new attack types can be supported by simply adding a classifier without retraining the core agent. CyberRAG has been evaluated achieving over 94% accuracy per class and pushing final classification accuracy to 94.92% through semantic orchestration. Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation. These results show that agentic, specialist-oriented RAG can pair high detection accuracy with trustworthy, SOC-ready prose, offering a practical and scalable path toward semi-autonomous cyber-defence workflows.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Robust and Generalizable Metamaterial Foundation Model</title>
<link>https://arxiv.org/abs/2507.02436</link>
<guid>https://arxiv.org/abs/2507.02436</guid>
<content:encoded><![CDATA[
arXiv:2507.02436v1 Announce Type: cross 
Abstract: Advances in material functionalities drive innovations across various fields, where metamaterials-defined by structure rather than composition-are leading the way. Despite the rise of artificial intelligence (AI)-driven design strategies, their impact is limited by task-specific retraining, poor out-of-distribution(OOD) generalization, and the need for separate models for forward and inverse design. To address these limitations, we introduce the Metamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation model inspired by large language models. MetaFO learns the underlying mechanics of metamaterials, enabling probabilistic, zero-shot predictions across diverse, unseen combinations of material properties and structural responses. It also excels in nonlinear inverse design, even under OOD conditions. By treating metamaterials as an operator that maps material properties to structural responses, MetaFO uncovers intricate structure-property relationships and significantly expands the design space. This scalable and generalizable framework marks a paradigm shift in AI-driven metamaterial discovery, paving the way for next-generation innovations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic</title>
<link>https://arxiv.org/abs/2507.02443</link>
<guid>https://arxiv.org/abs/2507.02443</guid>
<content:encoded><![CDATA[
arXiv:2507.02443v1 Announce Type: cross 
Abstract: Robots usually slow down for canning to detect objects while moving. Additionally, the robot's camera is configured with a low framerate to track the velocity of the detection algorithms. This would be constrained while executing tasks and exploring, making robots increase the task execution time. AMD has developed the Vitis-AI framework to deploy detection algorithms into FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation (BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This is a self-acquired dataset released in open access. MobileNet v1 performed better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In this work, we proved that we can use FPGAs to speed up ANNs and make them suitable for attention mechanisms.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</title>
<link>https://arxiv.org/abs/2507.02479</link>
<guid>https://arxiv.org/abs/2507.02479</guid>
<content:encoded><![CDATA[
arXiv:2507.02479v1 Announce Type: cross 
Abstract: Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy</title>
<link>https://arxiv.org/abs/2507.02493</link>
<guid>https://arxiv.org/abs/2507.02493</guid>
<content:encoded><![CDATA[
arXiv:2507.02493v1 Announce Type: cross 
Abstract: Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive re-associations between visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at https://github.com/lparolari/temporally-aware-polyp-counting.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Gradient Low-Rank Projection Fine-Tuning for LLMs</title>
<link>https://arxiv.org/abs/2507.02503</link>
<guid>https://arxiv.org/abs/2507.02503</guid>
<content:encoded><![CDATA[
arXiv:2507.02503v1 Announce Type: cross 
Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders</title>
<link>https://arxiv.org/abs/2507.02506</link>
<guid>https://arxiv.org/abs/2507.02506</guid>
<content:encoded><![CDATA[
arXiv:2507.02506v1 Announce Type: cross 
Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of structured datasets. We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes including bail outcome, IPC sections, crime type, and legal reasoning. Annotations were generated using a prompt-engineered GPT-4o pipeline and verified for consistency. This resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis, and is the first publicly available dataset focused specifically on Indian bail jurisprudence.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Multiple Diseases in Multiple Crops Using Deep Learning</title>
<link>https://arxiv.org/abs/2507.02517</link>
<guid>https://arxiv.org/abs/2507.02517</guid>
<content:encoded><![CDATA[
arXiv:2507.02517v1 Announce Type: cross 
Abstract: India, as a predominantly agrarian economy, faces significant challenges in agriculture, including substantial crop losses caused by diseases, pests, and environmental stress. Early detection and accurate identification of diseases across different crops are critical for improving yield and ensuring food security. This paper proposes a deep learning based solution for detecting multiple diseases in multiple crops, aimed to cover India's diverse agricultural landscape. We first create a unified dataset encompassing images of 17 different crops and 34 different diseases from various available repositories. Proposed deep learning model is trained on this dataset and outperforms the state-of-the-art in terms of accuracy and the number of crops, diseases covered. We achieve a significant detection accuracy, i.e., 99 percent for our unified dataset which is 7 percent more when compared to state-of-the-art handling 14 crops and 26 different diseases only. By improving the number of crops and types of diseases that can be detected, proposed solution aims to provide a better product for Indian farmers.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue</title>
<link>https://arxiv.org/abs/2507.02537</link>
<guid>https://arxiv.org/abs/2507.02537</guid>
<content:encoded><![CDATA[
arXiv:2507.02537v1 Announce Type: cross 
Abstract: Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: A Theory of Deep Learning Must Include Compositional Sparsity</title>
<link>https://arxiv.org/abs/2507.02550</link>
<guid>https://arxiv.org/abs/2507.02550</guid>
<content:encoded><![CDATA[
arXiv:2507.02550v1 Announce Type: cross 
Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable success in a wide variety of domains too high-dimensional for classical shallow networks subject to the curse of dimensionality. However, open questions about fundamental principles, that govern the learning dynamics of DNNs, remain. In this position paper we argue that it is the ability of DNNs to exploit the compositionally sparse structure of the target function driving their success. As such, DNNs can leverage the property that most practically relevant functions can be composed from a small set of constituent functions, each of which relies only on a low-dimensional subset of all inputs. We show that this property is shared by all efficiently Turing-computable functions and is therefore highly likely present in all current learning problems. While some promising theoretical insights on questions concerned with approximation and generalization exist in the setting of compositionally sparse functions, several important questions on the learnability and optimization of DNNs remain. Completing the picture of the role of compositional sparsity in deep learning is essential to a comprehensive theory of artificial, and even general, intelligence.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebSailor: Navigating Super-human Reasoning for Web Agent</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
arXiv:2507.02592v1 Announce Type: cross 
Abstract: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion</title>
<link>https://arxiv.org/abs/2507.02595</link>
<guid>https://arxiv.org/abs/2507.02595</guid>
<content:encoded><![CDATA[
arXiv:2507.02595v1 Announce Type: cross 
Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) developed in response to the growing need for easy bias mitigation. Built on top of the SAGED pipeline, an automated system for constructing bias benchmarks and extracting interpretable baseline distributions, MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines. By decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, MPF guides generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition. Empirically, we demonstrate its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions. This shows that MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2507.02598</link>
<guid>https://arxiv.org/abs/2507.02598</guid>
<content:encoded><![CDATA[
arXiv:2507.02598v1 Announce Type: cross 
Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental components of digital systems, directly impacting the performance, power efficiency, and area footprint. However, optimizing these circuits remains challenging due to the vast design space and complex physical constraints. While recent deep learning-based approaches have shown promise, they struggle to consistently explore high-potential design variants, limiting their optimization efficiency. To address this challenge, we propose AC-Refiner, a novel arithmetic circuit optimization framework leveraging conditional diffusion models. Our key insight is to reframe arithmetic circuit synthesis as a conditional image generation task. By carefully conditioning the denoising diffusion process on target quality-of-results (QoRs), AC-Refiner consistently produces high-quality circuit designs. Furthermore, the explored designs are used to fine-tune the diffusion model, which focuses the exploration near the Pareto frontier. Experimental results demonstrate that AC-Refiner generates designs with superior Pareto optimality, outperforming state-of-the-art baselines. The performance gain is further validated by integrating AC-Refiner into practical applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development</title>
<link>https://arxiv.org/abs/2507.02602</link>
<guid>https://arxiv.org/abs/2507.02602</guid>
<content:encoded><![CDATA[
arXiv:2507.02602v1 Announce Type: cross 
Abstract: The increasing importance of Vision-Based Navigation (VBN) algorithms in space missions raises numerous challenges in ensuring their reliability and operational robustness. Sensor faults can lead to inaccurate outputs from navigation algorithms or even complete data processing faults, potentially compromising mission objectives. Artificial Intelligence (AI) offers a powerful solution for detecting such faults, overcoming many of the limitations associated with traditional fault detection methods. However, the primary obstacle to the adoption of AI in this context is the lack of sufficient and representative datasets containing faulty image data.
  This study addresses these challenges by focusing on an interplanetary exploration mission scenario. A comprehensive analysis of potential fault cases in camera sensors used within the VBN pipeline is presented. The causes and effects of these faults are systematically characterized, including their impact on image quality and navigation algorithm performance, as well as commonly employed mitigation strategies. To support this analysis, a simulation framework is introduced to recreate faulty conditions in synthetically generated images, enabling a systematic and controlled reproduction of faulty data. The resulting dataset of fault-injected images provides a valuable tool for training and testing AI-based fault detection algorithms. The final link to the dataset will be added after an embargo period. For peer-reviewers, this private link is available.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks</title>
<link>https://arxiv.org/abs/2507.02606</link>
<guid>https://arxiv.org/abs/2507.02606</guid>
<content:encoded><![CDATA[
arXiv:2507.02606v1 Announce Type: cross 
Abstract: The rapid advancement of speech generation models has heightened privacy and security concerns related to voice cloning (VC). Recent studies have investigated disrupting unauthorized voice cloning by introducing adversarial perturbations. However, determined attackers can mitigate these protective perturbations and successfully execute VC. In this study, we conduct the first systematic evaluation of these protective perturbations against VC under realistic threat models that include perturbation purification. Our findings reveal that while existing purification methods can neutralize a considerable portion of the protective perturbations, they still lead to distortions in the feature space of VC models, which degrades the performance of VC. From this perspective, we propose a novel two-stage purification method: (1) Purify the perturbed speech; (2) Refine it using phoneme guidance to align it with the clean speech distribution. Experimental results demonstrate that our method outperforms state-of-the-art purification methods in disrupting VC defenses. Our study reveals the limitations of adversarial perturbation-based VC defenses and underscores the urgent need for more robust solutions to mitigate the security and privacy risks posed by VC. The code and audio samples are available at https://de-antifake.github.io.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference</title>
<link>https://arxiv.org/abs/2507.02620</link>
<guid>https://arxiv.org/abs/2507.02620</guid>
<content:encoded><![CDATA[
arXiv:2507.02620v1 Announce Type: cross 
Abstract: Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accpeted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared to baselines. Our code is publicly available at \href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving the Hubbard model with Neural Quantum States</title>
<link>https://arxiv.org/abs/2507.02644</link>
<guid>https://arxiv.org/abs/2507.02644</guid>
<content:encoded><![CDATA[
arXiv:2507.02644v1 Announce Type: cross 
Abstract: The rapid development of neural quantum states (NQS) has established it as a promising framework for studying quantum many-body systems. In this work, by leveraging the cutting-edge transformer-based architectures and developing highly efficient optimization algorithms, we achieve the state-of-the-art results for the doped two-dimensional (2D) Hubbard model, arguably the minimum model for high-Tc superconductivity. Interestingly, we find different attention heads in the NQS ansatz can directly encode correlations at different scales, making it capable of capturing long-range correlations and entanglements in strongly correlated systems. With these advances, we establish the half-filled stripe in the ground state of 2D Hubbard model with the next nearest neighboring hoppings, consistent with experimental observations in cuprates. Our work establishes NQS as a powerful tool for solving challenging many-fermions systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning</title>
<link>https://arxiv.org/abs/2507.02666</link>
<guid>https://arxiv.org/abs/2507.02666</guid>
<content:encoded><![CDATA[
arXiv:2507.02666v1 Announce Type: cross 
Abstract: In recent advancements in audio self-supervised representation learning, the standard Transformer architecture has emerged as the predominant approach, yet its attention mechanism often allocates a portion of attention weights to irrelevant information, potentially impairing the model's discriminative ability. To address this, we introduce a differential attention mechanism, which effectively mitigates ineffective attention allocation through the integration of dual-softmax operations and appropriately tuned differential coefficients. Experimental results demonstrate that our ASDA model achieves state-of-the-art (SOTA) performance across multiple benchmarks, including audio classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting (98.3% accuracy on SPC-2), and environmental sound classification (96.1% accuracy on ESC-50). These results highlight ASDA's effectiveness in audio tasks, paving the way for broader applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APT: Adaptive Personalized Training for Diffusion Models with Limited Data</title>
<link>https://arxiv.org/abs/2507.02687</link>
<guid>https://arxiv.org/abs/2507.02687</guid>
<content:encoded><![CDATA[
arXiv:2507.02687v1 Announce Type: cross 
Abstract: Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models</title>
<link>https://arxiv.org/abs/2507.02714</link>
<guid>https://arxiv.org/abs/2507.02714</guid>
<content:encoded><![CDATA[
arXiv:2507.02714v1 Announce Type: cross 
Abstract: Image generation has achieved remarkable progress with the development of large-scale text-to-image models, especially diffusion-based models. However, generating human images with plausible details, such as faces or hands, remains challenging due to insufficient supervision of local regions during training. To address this issue, we propose FairHuman, a multi-objective fine-tuning approach designed to enhance both global and local generation quality fairly. Specifically, we first construct three learning objectives: a global objective derived from the default diffusion objective function and two local objectives for hands and faces based on pre-annotated positional priors. Subsequently, we derive the optimal parameter updating strategy under the guidance of the Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware optimization for this multi-objective problem. Based on this, our proposed method can achieve significant improvements in generating challenging local details while maintaining overall quality. Extensive experiments showcase the effectiveness of our method in improving the performance of human image generation under different scenarios.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2507.02735</link>
<guid>https://arxiv.org/abs/2507.02735</guid>
<content:encoded><![CDATA[
arXiv:2507.02735v1 Announce Type: cross 
Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated applications. Model-level defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigation against prompt injection attacks. To this end, we develop Meta SecAlign, the first open-source and open-weight LLM with built-in model-level defense that achieves commercial-grade model performance. We provide complete details of our training recipe, which utilizes an improved version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7 security benchmarks show that Meta SecAlign, despite being trained on a generic instruction-tuning dataset, confers security in unseen downstream tasks, including tool-calling and agentic web navigation, in addition general instruction-following. Our best model -- Meta-SecAlign-70B -- achieves state-of-the-art robustness against prompt injection attacks and comparable utility to closed-source commercial LLM with model-level defense.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Signs of Steganographic Capabilities in Frontier LLMs</title>
<link>https://arxiv.org/abs/2507.02737</link>
<guid>https://arxiv.org/abs/2507.02737</guid>
<content:encoded><![CDATA[
arXiv:2507.02737v1 Announce Type: cross 
Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances such as using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</title>
<link>https://arxiv.org/abs/2507.02748</link>
<guid>https://arxiv.org/abs/2507.02748</guid>
<content:encoded><![CDATA[
arXiv:2507.02748v1 Announce Type: cross 
Abstract: Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation</title>
<link>https://arxiv.org/abs/2507.02752</link>
<guid>https://arxiv.org/abs/2507.02752</guid>
<content:encoded><![CDATA[
arXiv:2507.02752v1 Announce Type: cross 
Abstract: The disconnect between AI-generated molecules with desirable properties and their synthetic feasibility remains a critical bottleneck in computational drug and material discovery. While generative AI has accelerated the proposal of candidate molecules, many of these structures prove challenging or impossible to synthesize using established chemical reactions. Here, we introduce SynTwins, a novel retrosynthesis-guided molecular analog design framework that designs synthetically accessible molecular analogs by emulating expert chemist strategies through a three-step process: retrosynthesis, similar building block searching, and virtual synthesis. In comparative evaluations, SynTwins demonstrates superior performance in generating synthetically accessible analogs compared to state-of-the-art machine learning models while maintaining high structural similarity to original target molecules. Furthermore, when integrated with existing molecule optimization frameworks, our hybrid approach produces synthetically feasible molecules with property profiles comparable to unconstrained molecule generators, yet its synthesizability ensured. Our comprehensive benchmarking across diverse molecular datasets demonstrates that SynTwins effectively bridges the gap between computational design and experimental synthesis, providing a practical solution for accelerating the discovery of synthesizable molecules with desired properties for a wide range of applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Simplex: 2-Simplicial Attention in Triton</title>
<link>https://arxiv.org/abs/2507.02754</link>
<guid>https://arxiv.org/abs/2507.02754</guid>
<content:encoded><![CDATA[
arXiv:2507.02754v1 Announce Type: cross 
Abstract: Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Auditory Scene Analysis</title>
<link>https://arxiv.org/abs/2507.02755</link>
<guid>https://arxiv.org/abs/2507.02755</guid>
<content:encoded><![CDATA[
arXiv:2507.02755v1 Announce Type: cross 
Abstract: Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
arXiv:2507.02778v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model</title>
<link>https://arxiv.org/abs/2507.02822</link>
<guid>https://arxiv.org/abs/2507.02822</guid>
<content:encoded><![CDATA[
arXiv:2507.02822v1 Announce Type: cross 
Abstract: With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between "thinking" (high reasoning) and "non-thinking" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift</title>
<link>https://arxiv.org/abs/2507.02824</link>
<guid>https://arxiv.org/abs/2507.02824</guid>
<content:encoded><![CDATA[
arXiv:2507.02824v1 Announce Type: cross 
Abstract: In this paper, the precoding design is investigated for maximizing the throughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO) systems with obstructed direct communication paths. In particular, a reconfigurable intelligent surface (RIS) is employed to enhance MIMO transmissions, considering mmWave characteristics related to line-of-sight (LoS) and multipath effects. The traditional exhaustive search (ES) for optimal codewords in the continuous phase shift is computationally intensive and time-consuming. To reduce computational complexity, permuted discrete Fourier transform (DFT) vectors are used for finding codebook design, incorporating amplitude responses for practical or ideal RIS systems. However, even if the discrete phase shift is adopted in the ES, it results in significant computation and is time-consuming. Instead, the trained deep neural network (DNN) is developed to facilitate faster codeword selection. Simulation results show that the DNN maintains sub-optimal spectral efficiency even as the distance between the end-user and the RIS has variations in the testing phase. These results highlight the potential of DNN in advancing RIS-aided systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network</title>
<link>https://arxiv.org/abs/2507.02827</link>
<guid>https://arxiv.org/abs/2507.02827</guid>
<content:encoded><![CDATA[
arXiv:2507.02827v1 Announce Type: cross 
Abstract: The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs</title>
<link>https://arxiv.org/abs/2507.02851</link>
<guid>https://arxiv.org/abs/2507.02851</guid>
<content:encoded><![CDATA[
arXiv:2507.02851v1 Announce Type: cross 
Abstract: Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subtyping in DHOL -- Extended preprint</title>
<link>https://arxiv.org/abs/2507.02855</link>
<guid>https://arxiv.org/abs/2507.02855</guid>
<content:encoded><![CDATA[
arXiv:2507.02855v1 Announce Type: cross 
Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an interesting compromise between expressiveness and automation support. It sacrifices the decidability of its type system in order to significantly extend its expressiveness over standard HOL. Yet it retains strong automated theorem proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types. Both of these are commonly requested by practitioners but rarely provided by automated theorem provers. This is because they inherently require undecidable typing and thus are very difficult to retrofit to decidable type systems. But with DHOL already doing the heavy lifting, adding them is not only possible but elegant and simple.
  Concretely, we add refinement and quotient types as special cases of subtyping. This turns the associated canonical inclusion resp. projection maps into identity maps and thus avoids costly changes in representation. We present the syntax, semantics, and translation to HOL for the extended language, including the proofs of soundness and completeness.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Answer Matching Outperforms Multiple Choice for Language Model Evaluation</title>
<link>https://arxiv.org/abs/2507.02856</link>
<guid>https://arxiv.org/abs/2507.02856</guid>
<content:encoded><![CDATA[
arXiv:2507.02856v1 Announce Type: cross 
Abstract: Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</title>
<link>https://arxiv.org/abs/2507.02861</link>
<guid>https://arxiv.org/abs/2507.02861</guid>
<content:encoded><![CDATA[
arXiv:2507.02861v1 Announce Type: cross 
Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</title>
<link>https://arxiv.org/abs/2507.02863</link>
<guid>https://arxiv.org/abs/2507.02863</guid>
<content:encoded><![CDATA[
arXiv:2507.02863v1 Announce Type: cross 
Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Road Graph Generator: Mapping roads at construction sites from GPS data</title>
<link>https://arxiv.org/abs/2402.09919</link>
<guid>https://arxiv.org/abs/2402.09919</guid>
<content:encoded><![CDATA[
arXiv:2402.09919v4 Announce Type: replace 
Abstract: We propose a new method for inferring roads from GPS trajectories to map construction sites. This task presents a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which significantly diverge from typical vehicular traffic on established roads. Our proposed method first identifies intersections in the road network that serve as critical decision points, and then connects them with edges to produce a graph, which can subsequently be used for planning and task-allocation. We demonstrate the approach by mapping roads at a real-life construction site in Norway. The method is validated on four increasingly complex segments of the map. In our tests, the method achieved perfect accuracy in detecting intersections and inferring roads in data with no or low noise, while its performance was reduced in areas with significant noise and consistently missing GPS updates.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban Region Pre-training and Prompting: A Graph-based Approach</title>
<link>https://arxiv.org/abs/2408.05920</link>
<guid>https://arxiv.org/abs/2408.05920</guid>
<content:encoded><![CDATA[
arXiv:2408.05920v4 Announce Type: replace 
Abstract: Urban region representation is crucial for various urban downstream tasks. However, despite the proliferation of methods and their success, acquiring general urban region knowledge and adapting to different tasks remains challenging. Existing work pays limited attention to the fine-grained functional layout semantics in urban regions, limiting their ability to capture transferable knowledge across regions. Further, inadequate handling of the unique features and relationships required for different downstream tasks may also hinder effective task adaptation. In this paper, we propose a $\textbf{G}$raph-based $\textbf{U}$rban $\textbf{R}$egion $\textbf{P}$re-training and $\textbf{P}$rompting framework ($\textbf{GURPP}$) for region representation learning. Specifically, we first construct an urban region graph and develop a subgraph-centric urban region pre-training model to capture the heterogeneous and transferable patterns of entity interactions. This model pre-trains knowledge-rich region embeddings using contrastive learning and multi-view learning methods. To further refine these representations, we design two graph-based prompting methods: a manually-defined prompt to incorporate explicit task knowledge and a task-learnable prompt to discover hidden knowledge, which enhances the adaptability of these embeddings to different tasks. Extensive experiments on various urban region prediction tasks and different cities demonstrate the superior performance of our framework.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Cognition</title>
<link>https://arxiv.org/abs/2409.18624</link>
<guid>https://arxiv.org/abs/2409.18624</guid>
<content:encoded><![CDATA[
arXiv:2409.18624v3 Announce Type: replace 
Abstract: Unsupervised learning methods have a soft inspiration in cognition models. To this day, the most successful unsupervised learning methods revolve around clustering samples in a mathematical space. In this paper we propose a primitive-based, unsupervised learning approach for decision-making inspired by a novel cognition framework. This representation-centric approach models the input space constructively as a distributed hierarchical structure in an input-agnostic way. We compared our approach with both current state-of-the-art unsupervised learning classification, with current state-of-the-art small and incomplete datasets classification, and with current state-of-the-art cancer type classification. We show how our proposal outperforms previous state-of-the-art. We also evaluate some cognition-like properties of our proposal where it not only outperforms the compared algorithms (even supervised learning ones), but it also shows a different, more cognition-like, behaviour.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Preference Optimization Using Sparse Feature-Level Constraints</title>
<link>https://arxiv.org/abs/2411.07618</link>
<guid>https://arxiv.org/abs/2411.07618</guid>
<content:encoded><![CDATA[
arXiv:2411.07618v2 Announce Type: replace 
Abstract: The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation</title>
<link>https://arxiv.org/abs/2501.04614</link>
<guid>https://arxiv.org/abs/2501.04614</guid>
<content:encoded><![CDATA[
arXiv:2501.04614v3 Announce Type: replace 
Abstract: The adoption of Artificial Intelligence in medical imaging holds great promise, yet it remains hindered by challenges such as data scarcity, privacy concerns, and the need for robust multimodal integration. While recent advances in generative modeling have enabled high-quality synthetic data generation, existing approaches are often limited to unimodal, unidirectional synthesis and therefore lack the ability to jointly synthesize multiple modalities while preserving clinical consistency. To address this challenge, we introduce XGeM, a 6.77-billion-parameter multimodal generative model designed to support flexible, any-to-any synthesis between medical data modalities. XGeM constructs a shared latent space via contrastive learning and introduces a novel Multi-Prompt Training strategy, enabling conditioning on arbitrary subsets of input modalities. This design allows the model to adapt to heterogeneous clinical inputs and generate multiple outputs jointly, preserving both semantic and structural coherence. We extensively validate XGeM: first we benchmark it against five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset for multi-view Chest X-ray and radiological report generation. Secondly, we perform a Visual Turing Test with expert radiologists to assess the realism and clinical relevance of the generated data, ensuring alignment with real-world scenarios. Finally, we show how XGeM can support key medical data challenges such as anonymization, class imbalance, and data scarcity, underscoring its utility as a foundation model for medical data synthesis. Project page is at https://cosbidev.github.io/XGeM/.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science</title>
<link>https://arxiv.org/abs/2501.10768</link>
<guid>https://arxiv.org/abs/2501.10768</guid>
<content:encoded><![CDATA[
arXiv:2501.10768v2 Announce Type: replace 
Abstract: Pre-trained on extensive text and image corpora, current Multi-Modal Large Language Models (MLLM) have shown strong capabilities in general visual reasoning tasks. However, their performance is still lacking in physical domains that require understanding diagrams with complex physical structures and quantitative analysis based on multi-modal information. To address this, we develop a new framework, named Multi-Modal Scientific Reasoning with Physics Perception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level multi-modal reasoning task into physical diagram understanding via a Physical Perception Model (PPM) and reasoning with physical knowledge via a simulator. The PPM module is obtained by fine-tuning a visual language model using carefully designed synthetic data with paired physical diagrams and corresponding simulation language descriptions. At the inference stage, MAPS integrates the simulation language description of the input diagram provided by PPM and results obtained through a Chain-of-Simulation process with MLLM to derive the underlying rationale and the final answer. Validated using our collected college-level circuit analysis problems, MAPS significantly improves reasoning accuracy of MLLM and outperforms all existing models. The results confirm MAPS offers a promising direction for enhancing multi-modal scientific reasoning ability of MLLMs. We will release our code, model and dataset used for our experiments upon publishing of this paper.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Index Report 2025</title>
<link>https://arxiv.org/abs/2504.07139</link>
<guid>https://arxiv.org/abs/2504.07139</guid>
<content:encoded><![CDATA[
arXiv:2504.07139v2 Announce Type: replace 
Abstract: Welcome to the eighth edition of the AI Index report. The 2025 Index is our most comprehensive to date and arrives at an important moment, as AI's influence across society, the economy, and global governance continues to intensify. New in this year's report are in-depth analyses of the evolving landscape of AI hardware, novel estimates of inference costs, and new analyses of AI publication and patenting trends. We also introduce fresh data on corporate adoption of responsible AI practices, along with expanded coverage of AI's growing role in science and medicine. Since its founding in 2017 as an offshoot of the One Hundred Year Study of Artificial Intelligence, the AI Index has been committed to equipping policymakers, journalists, executives, researchers, and the public with accurate, rigorously validated, and globally sourced data. Our mission has always been to help these stakeholders make better-informed decisions about the development and deployment of AI. In a world where AI is discussed everywhere - from boardrooms to kitchen tables - this mission has never been more essential. The AI Index continues to lead in tracking and interpreting the most critical trends shaping the field - from the shifting geopolitical landscape and the rapid evolution of underlying technologies, to AI's expanding role in business, policymaking, and public life. Longitudinal tracking remains at the heart of our mission. In a domain advancing at breakneck speed, the Index provides essential context - helping us understand where AI stands today, how it got here, and where it may be headed next. Recognized globally as one of the most authoritative resources on artificial intelligence, the AI Index has been cited in major media outlets such as The New York Times, Bloomberg, and The Guardian; referenced in hundreds of academic papers; and used by policymakers and government agencies around the world.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Process Observability: Discovering Behavioral Variability</title>
<link>https://arxiv.org/abs/2505.20127</link>
<guid>https://arxiv.org/abs/2505.20127</guid>
<content:encoded><![CDATA[
arXiv:2505.20127v2 Announce Type: replace 
Abstract: AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoformalization in the Era of Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2505.23486</link>
<guid>https://arxiv.org/abs/2505.23486</guid>
<content:encoded><![CDATA[
arXiv:2505.23486v2 Announce Type: replace 
Abstract: Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A framework for Conditional Reasoning in Answer Set Programming</title>
<link>https://arxiv.org/abs/2506.03997</link>
<guid>https://arxiv.org/abs/2506.03997</guid>
<content:encoded><![CDATA[
arXiv:2506.03997v2 Announce Type: replace 
Abstract: In this paper we introduce a Conditional Answer Set Programming framework (Conditional ASP) for the definition of conditional extensions of Answer Set Programming (ASP). The approach builds on a conditional logic with typicality, and on the combination of a conditional knowledge base with an ASP program, and allows for conditional reasoning over the answer sets of the program. The formalism relies on a multi-preferential semantics (and on the KLM preferential semantics, as a special case) to provide an interpretation of conditionals.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
arXiv:2506.12479v2 Announce Type: replace 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge</title>
<link>https://arxiv.org/abs/2506.21506</link>
<guid>https://arxiv.org/abs/2506.21506</guid>
<content:encoded><![CDATA[
arXiv:2506.21506v2 Announce Type: replace 
Abstract: Agentic search such as Deep Research systems-where agents autonomously browse the web, synthesize information, and return comprehensive citation-backed answers-represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of ten frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, highlighting its great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Density Bayesian Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2303.06827</link>
<guid>https://arxiv.org/abs/2303.06827</guid>
<content:encoded><![CDATA[
arXiv:2303.06827v4 Announce Type: replace-cross 
Abstract: Inverse reinforcement learning (IRL) methods infer an agent's reward function using demonstrations of expert behavior. A Bayesian IRL approach models a distribution over candidate reward functions, capturing a degree of uncertainty in the inferred reward function. This is critical in some applications, such as those involving clinical data. Typically, Bayesian IRL algorithms require large demonstration datasets, which may not be available in practice. In this work, we incorporate existing domain-specific data to achieve better posterior concentration rates. We study a common setting in clinical and biological applications where we have access to expert demonstrations and known reward functions for a set of training tasks. Our aim is to learn the reward function of a new test task given limited expert demonstrations. Existing Bayesian IRL methods impose restrictions on the form of input data, thus limiting the incorporation of training task data. To better leverage information from training tasks, we introduce kernel density Bayesian inverse reinforcement learning (KD-BIRL). Our approach employs a conditional kernel density estimator, which uses the known reward functions of the training tasks to improve the likelihood estimation across a range of reward functions and demonstration samples. Our empirical results highlight KD-BIRL's faster concentration rate in comparison to baselines, particularly in low test task expert demonstration data regimes. Additionally, we are the first to provide theoretical guarantees of posterior concentration for a Bayesian IRL algorithm. Taken together, this work introduces a principled and theoretically grounded framework that enables Bayesian IRL to be applied across a variety of domains.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data</title>
<link>https://arxiv.org/abs/2306.13840</link>
<guid>https://arxiv.org/abs/2306.13840</guid>
<content:encoded><![CDATA[
arXiv:2306.13840v4 Announce Type: replace-cross 
Abstract: Current trends in pre-training Large Language Models (LLMs) primarily focus on the scaling of model and dataset size. While the quality of pre-training data is considered an important factor for training powerful LLMs, it remains a nebulous concept that has not been rigorously characterized. To this end, we propose a formalization of one key aspect of data quality -- measuring the variability of natural language data -- specifically via a measure we call the diversity coefficient. Our empirical analysis shows that the proposed diversity coefficient aligns with the intuitive properties of diversity and variability, e.g., it increases as the number of latent concepts increases. Then, we measure the diversity coefficient of publicly available pre-training datasets and demonstrate that their formal diversity is high compared to theoretical lower and upper bounds. Finally, we conduct a comprehensive set of controlled interventional experiments with GPT-2 and LLaMAv2 that demonstrate the diversity coefficient of pre-training data characterizes useful aspects of downstream model evaluation performance -- totaling 44 models of various sizes (51M to 7B parameters). We conclude that our formal notion of diversity is an important aspect of data quality that captures variability and causally leads to improved evaluation performance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning</title>
<link>https://arxiv.org/abs/2311.08010</link>
<guid>https://arxiv.org/abs/2311.08010</guid>
<content:encoded><![CDATA[
arXiv:2311.08010v3 Announce Type: replace-cross 
Abstract: Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in real-world scenarios. It can effectively alleviate the burden of annotation by matching entities in existing knowledge bases with snippets in the text but suffer from the label noise. Recent works attempt to adopt the teacher-student framework to gradually refine the training labels and improve the overall robustness. However, these teacher-student methods achieve limited performance because the poor calibration of the teacher network produces incorrectly pseudo-labeled samples, leading to error propagation. Therefore, we propose: (1) Uncertainty-Aware Teacher Learning that leverages the prediction uncertainty to reduce the number of incorrect pseudo labels in the self-training stage; (2) Student-Student Collaborative Learning that allows the transfer of reliable labels between two student networks instead of indiscriminately relying on all pseudo labels from its teacher, and further enables a full exploration of mislabeled samples rather than simply filtering unreliable pseudo-labeled samples. We evaluate our proposed method on five DS-NER datasets, demonstrating that our method is superior to the state-of-the-art DS-NER methods.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoiding Catastrophe in Online Learning by Asking for Help</title>
<link>https://arxiv.org/abs/2402.08062</link>
<guid>https://arxiv.org/abs/2402.08062</guid>
<content:encoded><![CDATA[
arXiv:2402.08062v5 Announce Type: replace-cross 
Abstract: Most learning algorithms with formal regret guarantees assume that all mistakes are recoverable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes are "catastrophic", i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe in that round and try to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We also assume that the agent can transfer knowledge between similar inputs. We first show that in general, any algorithm either queries the mentor at a linear rate or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Although our focus is the product of payoffs, we provide matching bounds for the typical additive regret. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Novel Measure of User Trust in XAI Systems</title>
<link>https://arxiv.org/abs/2405.05766</link>
<guid>https://arxiv.org/abs/2405.05766</guid>
<content:encoded><![CDATA[
arXiv:2405.05766v2 Announce Type: replace-cross 
Abstract: The increasing reliance on Deep Learning models, combined with their inherent lack of transparency, has spurred the development of a novel field of study known as eXplainable AI (XAI) methods. These methods seek to enhance the trust of end-users in automated systems by providing insights into the rationale behind their decisions. This paper presents a novel trust measure in XAI systems, allowing their refinement. Our proposed metric combines both performance metrics and trust indicators from an objective perspective. To validate this novel methodology, we conducted three case studies showing an improvement respect the state-of-the-art, with an increased sensitiviy to different scenarios.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into LLM-assisted writing in biomedical publications through excess vocabulary</title>
<link>https://arxiv.org/abs/2406.07016</link>
<guid>https://arxiv.org/abs/2406.07016</guid>
<content:encoded><![CDATA[
arXiv:2406.07016v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Cross-sectoral Intersecting Discrepancies within Multiple Groups Using Latent Class Analysis Towards Fairness</title>
<link>https://arxiv.org/abs/2407.03133</link>
<guid>https://arxiv.org/abs/2407.03133</guid>
<content:encoded><![CDATA[
arXiv:2407.03133v4 Announce Type: replace-cross 
Abstract: The growing interest in fair AI development is evident. The ''Leave No One Behind'' initiative urges us to address multiple and intersecting forms of inequality in accessing services, resources, and opportunities, emphasising the significance of fairness in AI. This is particularly relevant as an increasing number of AI tools are applied to decision-making processes, such as resource allocation and service scheme development, across various sectors such as health, energy, and housing. Therefore, exploring joint inequalities in these sectors is significant and valuable for thoroughly understanding overall inequality and unfairness. This research introduces an innovative approach to quantify cross-sectoral intersecting discrepancies among user-defined groups using latent class analysis. These discrepancies can be used to approximate inequality and provide valuable insights to fairness issues. We validate our approach using both proprietary and public datasets, including both EVENS and Census 2021 (England & Wales) datasets, to examine cross-sectoral intersecting discrepancies among different ethnic groups. We also verify the reliability of the quantified discrepancy by conducting a correlation analysis with a government public metric. Our findings reveal significant discrepancies both among minority ethnic groups and between minority ethnic groups and non-minority ethnic groups, emphasising the need for targeted interventions in policy-making processes. Furthermore, we demonstrate how the proposed approach can provide valuable insights into ensuring fairness in machine learning systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective</title>
<link>https://arxiv.org/abs/2407.06902</link>
<guid>https://arxiv.org/abs/2407.06902</guid>
<content:encoded><![CDATA[
arXiv:2407.06902v2 Announce Type: replace-cross 
Abstract: One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is the availability of massive, curated datasets. A commonly used technique to curate such massive datasets is crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are then fused to serve downstream learning and inference tasks. This annotation process often creates noisy labels due to various reasons, such as the limited expertise, or unreliability of annotators, among others. Therefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative impact of such label noise on learning tasks. This feature article introduces advances in learning from noisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treatments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical insights and algorithmic developments. In particular, this article reviews the connections between signal processing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization, and novel, principled solutions of longstanding challenges in crowdsourcing -- showing how SP perspectives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are critical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning large language models (LLMs).
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomical Foundation Models for Brain MRIs</title>
<link>https://arxiv.org/abs/2408.07079</link>
<guid>https://arxiv.org/abs/2408.07079</guid>
<content:encoded><![CDATA[
arXiv:2408.07079v4 Announce Type: replace-cross 
Abstract: Deep Learning (DL) in neuroimaging has become increasingly relevant for detecting neurological conditions and neurodegenerative disorders. One of the most predominant biomarkers in neuroimaging is represented by brain age, which has been shown to be a good indicator for different conditions, such as Alzheimer's Disease. Using brain age for weakly supervised pre-training of DL models in transfer learning settings has also recently shown promising results, especially when dealing with data scarcity of different conditions. On the other hand, anatomical information of brain MRIs (e.g. cortical thickness) can provide important information for learning good representations that can be transferred to many downstream tasks. In this work, we propose AnatCL, an anatomical foundation model for brain MRIs that i.) leverages anatomical information in a weakly contrastive learning approach, and ii.) achieves state-of-the-art performances across many different downstream tasks. To validate our approach we consider 12 different downstream tasks for the diagnosis of different conditions such as Alzheimer's Disease, autism spectrum disorder, and schizophrenia. Furthermore, we also target the prediction of 10 different clinical assessment scores using structural MRI data. Our findings show that incorporating anatomical information during pre-training leads to more robust and generalizable representations. Pre-trained models can be found at: https://github.com/EIDOSLAB/AnatCL.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence-aware Pre-training for Echocardiography Probe Movement Guidance</title>
<link>https://arxiv.org/abs/2408.15026</link>
<guid>https://arxiv.org/abs/2408.15026</guid>
<content:encoded><![CDATA[
arXiv:2408.15026v2 Announce Type: replace-cross 
Abstract: Echocardiography is an essential medical technique for diagnosing cardiovascular diseases, but its high operational complexity has led to a shortage of trained professionals. To address this issue, we introduce a novel probe movement guidance algorithm that has the potential to be applied in guiding robotic systems or novices with probe pose adjustment for high-quality standard plane image acquisition.Cardiac ultrasound faces two major challenges: (1) the inherently complex structure of the heart, and (2) significant individual variations. Previous works have only learned the population-averaged structure of the heart rather than personalized cardiac structures, leading to a performance bottleneck. Clinically, we observe that sonographers dynamically adjust their interpretation of a patient's cardiac anatomy based on prior scanning sequences, consequently refining their scanning strategies. Inspired by this, we propose a novel sequence-aware self-supervised pre-training method. Specifically, our approach learns personalized three-dimensional cardiac structural features by predicting the masked-out image features and probe movement actions in a scanning sequence. We hypothesize that if the model can predict the missing content it has acquired a good understanding of personalized cardiac structure. Extensive experiments on a large-scale expert scanning dataset with 1.31 million samples demonstrate that our proposed sequence-aware paradigm can effectively reduce probe guidance errors compared to other advanced baseline methods. Our code will be released after acceptance.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes</title>
<link>https://arxiv.org/abs/2409.06416</link>
<guid>https://arxiv.org/abs/2409.06416</guid>
<content:encoded><![CDATA[
arXiv:2409.06416v2 Announce Type: replace-cross 
Abstract: Much of the cost and effort required during the software testing process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers.
  In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explore the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also propose and demonstrate a multi-agent architecture that can predict which tests require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering the energy efficiency of spiking neural networks</title>
<link>https://arxiv.org/abs/2409.08290</link>
<guid>https://arxiv.org/abs/2409.08290</guid>
<content:encoded><![CDATA[
arXiv:2409.08290v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) promise higher energy efficiency over conventional Quantized Artificial Neural Networks (QNNs) due to their event-driven, spike-based computation. However, prevailing energy evaluations often oversimplify, focusing on computational aspects while neglecting critical overheads like comprehensive data movement and memory access. Such simplifications can lead to misleading conclusions regarding the true energy benefits of SNNs. This paper presents a rigorous re-evaluation. We establish a fair baseline by mapping rate-encoded SNNs with $T$ timesteps to functionally equivalent QNNs with $\lceil \log_2(T+1) \rceil$ bits. This ensures both models have comparable representational capacities, as well has similar hardware requirement, enabling meaningful energy comparisons. We introduce a detailed analytical energy model encompassing core computation and data movement (sparse and dense activations, weights). Using this model, we systematically explore a wide parameter space, including intrinsic network characteristics ($T$, spike rate $s_r$, QNN sparsity $\gamma$, model size $N$, weight bit-level) and hardware characteristics (memory system and network-on-chip). Our analysis identifies specific operational regimes where SNNs genuinely offer superior energy efficiency. For example, under typical neuromorphic hardware conditions, SNNs with moderate time windows ($T \in [5,10]$) require an average spike rate ($s_r$) below 6.4% to outperform equivalent QNNs. These insights guide the design of genuinely energy-efficient neural network solutions.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling</title>
<link>https://arxiv.org/abs/2409.10589</link>
<guid>https://arxiv.org/abs/2409.10589</guid>
<content:encoded><![CDATA[
arXiv:2409.10589v4 Announce Type: replace-cross 
Abstract: The Job Shop Scheduling Problem (JSSP) is a complex combinatorial optimization problem. While online Reinforcement Learning (RL) has shown promise by quickly finding acceptable solutions for JSSP, it faces key limitations: it requires extensive training interactions from scratch leading to sample inefficiency, cannot leverage existing high-quality solutions from traditional methods like Constraint Programming (CP), and require simulated environments to train in, which are impracticable to build for complex scheduling environments. We introduce Offline Learned Dispatching (Offline-LD), an offline reinforcement learning approach for JSSP, which addresses these limitations by learning from historical scheduling data. Our approach is motivated by scenarios where historical scheduling data and expert solutions are available or scenarios where online training of RL approaches with simulated environments is impracticable. Offline-LD introduces maskable variants of two Q-learning methods, namely, Maskable Quantile Regression DQN (mQRDQN) and discrete maskable Soft Actor-Critic (d-mSAC), that are able to learn from historical data, through Conservative Q-Learning (CQL). Moreover, we present a novel entropy bonus modification for d-mSAC, for maskable action spaces. Moreover, we introduce a novel reward normalization method for JSSP in an offline RL setting. Our experiments demonstrate that Offline-LD outperforms online RL on both generated and benchmark instances when trained on only 100 solutions generated by CP. Notably, introducing noise to the expert dataset yields comparable or superior results to using the expert dataset, with the same amount of instances, a promising finding for real-world applications, where data is inherently noisy and imperfect.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning</title>
<link>https://arxiv.org/abs/2410.08500</link>
<guid>https://arxiv.org/abs/2410.08500</guid>
<content:encoded><![CDATA[
arXiv:2410.08500v2 Announce Type: replace-cross 
Abstract: Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. It remains challenging due to the complex spatial relationships in outdoor aerial scenes. In this paper, we propose an end-to-end zero-shot framework for aerial VLN tasks, where the large language model (LLM) is introduced as our agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning ability of LLMs. This is achieved by extracting and projecting instruction-related semantic masks of landmarks into a top-down map that contains the location information of surrounding landmarks. Further, this map is transformed into a matrix representation with distance metrics as the text prompt to the LLM, for action prediction according to the instruction. Experiments conducted in real and simulation environments have successfully proved the effectiveness and robustness of our method, achieving 15.9% and 12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S dataset.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Complex Query Answering Really Complex?</title>
<link>https://arxiv.org/abs/2410.12537</link>
<guid>https://arxiv.org/abs/2410.12537</guid>
<content:encoded><![CDATA[
arXiv:2410.12537v3 Announce Type: replace-cross 
Abstract: Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task. In this paper, we show that the current benchmarks for CQA might not be as complex as we think, as the way they are built distorts our perception of progress in this field. For example, we find that in these benchmarks, most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted. The performance of state-of-the-art CQA models decreases significantly when such models are evaluated on queries that cannot be reduced to easier types. Thus, we propose a set of more challenging benchmarks composed of queries that require models to reason over multiple hops and better reflect the construction of real-world KGs. In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation</title>
<link>https://arxiv.org/abs/2411.00863</link>
<guid>https://arxiv.org/abs/2411.00863</guid>
<content:encoded><![CDATA[
arXiv:2411.00863v2 Announce Type: replace-cross 
Abstract: In the field of large language model (LLM)-based proof generation, despite extensive training on large datasets such as ArXiv, LLMs still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the widespread presence of suboptimal ordering within the data for each proof used in training. For example, published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. This order is designed to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders can be substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order. Lastly, we define a common type of order issue in advanced math proofs and find that 17.3 percent of theorems with nontrivial proofs in the first two chapters of a widely used graduate-level mathematics textbook suffer from this issue. A detailed list of those proofs is provided in the appendix.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeMID: Generalizable Models for IoT Device Identification</title>
<link>https://arxiv.org/abs/2411.14441</link>
<guid>https://arxiv.org/abs/2411.14441</guid>
<content:encoded><![CDATA[
arXiv:2411.14441v2 Announce Type: replace-cross 
Abstract: With the proliferation of devices on the Internet of Things (IoT), ensuring their security has become paramount. Device identification (DI), which distinguishes IoT devices based on their traffic patterns, plays a crucial role in both differentiating devices and identifying vulnerable ones, closing a serious security gap. However, existing approaches to DI that build machine learning models often overlook the challenge of model generalizability across diverse network environments. In this study, we propose a novel framework to address this limitation and to evaluate the generalizability of DI models across data sets collected within different network environments. Our approach involves a two-step process: first, we develop a feature and model selection method that is more robust to generalization issues by using a genetic algorithm with external feedback and datasets from distinct environments to refine the selections. Second, the resulting DI models are then tested on further independent datasets to robustly assess their generalizability. We demonstrate the effectiveness of our method by empirically comparing it to alternatives, highlighting how fundamental limitations of commonly employed techniques such as sliding window and flow statistics limit their generalizability. Moreover, we show that statistical methods, widely used in the literature, are unreliable for device identification due to their dependence on network-specific characteristics rather than device-intrinsic properties, challenging the validity of a significant portion of existing research. Our findings advance research in IoT security and device identification, offering insight into improving model effectiveness and mitigating risks in IoT networks.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded Multimodal LLM Framework</title>
<link>https://arxiv.org/abs/2412.10435</link>
<guid>https://arxiv.org/abs/2412.10435</guid>
<content:encoded><![CDATA[
arXiv:2412.10435v2 Announce Type: replace-cross 
Abstract: Recently, with the emergence of recent Multimodal Large Language Model (MLLM) technology, it has become possible to exploit its video understanding capability on different classification tasks. In practice, we face the difficulty of huge requirements for GPU resource if we need to deploy MLLMs online. In this paper, we propose COEF-VQ, a novel cascaded MLLM framework designed to enhance video quality understanding on the short-video platform while optimizing computational efficiency. Our approach integrates an entropy-based pre-filtering stage, where a lightweight model assesses uncertainty and selectively filters cases before passing them to the more computationally intensive MLLM for final evaluation. By prioritizing high-uncertainty samples for deeper analysis, our framework significantly reduces GPU usage while maintaining the strong classification performance of a full MLLM deployment. To demonstrate the effectiveness of COEF-VQ, we deploy this new framework onto the video management platform (VMP) at the short-video platform, and perform a series of detailed experiments on two in-house tasks related to video quality understanding. We show that COEF-VQ leads to substantial performance gains from the offline evaluation in these two tasks and effectively enhances platform safety with limit resource consumption, significantly reducing inappropriate content video view rate by 9.9% in a online A/B test without affecting engagement. Post-launch monitoring confirmed sustained improvements, validating its real-world impact.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs</title>
<link>https://arxiv.org/abs/2412.11556</link>
<guid>https://arxiv.org/abs/2412.11556</guid>
<content:encoded><![CDATA[
arXiv:2412.11556v2 Announce Type: replace-cross 
Abstract: Extracting sentence embeddings from large language models (LLMs) is a promising direction, as LLMs have demonstrated stronger semantic understanding capabilities. Previous studies typically focus on prompt engineering to elicit sentence embeddings from LLMs by prompting the model to encode sentence information into the embedding of the last token. However, LLMs are mostly decoder-only models with causal attention and the earlier tokens in the sentence cannot attend to the latter tokens, resulting in biased encoding of sentence information and cascading effects on the final decoded token. To this end, we propose a novel Token Prepending (TP) technique that prepends each layer's decoded sentence embedding to the beginning of the sentence in the next layer's input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism. The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. Extensive experiments on various Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our proposed TP technique can significantly improve the performance of existing prompt-based sentence embedding methods across different LLMs, while incurring negligible additional inference cost.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Characterizations for Language Generation: Interplay of Hallucinations, Breadth, and Stability</title>
<link>https://arxiv.org/abs/2412.18530</link>
<guid>https://arxiv.org/abs/2412.18530</guid>
<content:encoded><![CDATA[
arXiv:2412.18530v2 Announce Type: replace-cross 
Abstract: We study language generation in the limit - introduced by Kleinberg and Mullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin [Ang79]. [KM24]'s main result is an algorithm for generating from any countable language collection in the limit. While their algorithm eventually generates unseen strings from the target language $K$, it sacrifices coverage or breadth, i.e., its ability to generate a rich set of strings. Recent work introduces different notions of breadth and explores when generation with breadth is possible, leaving a full characterization of these notions open. Our first set of results settles this by characterizing generation for existing notions of breadth and their natural extensions. Interestingly, our lower bounds are very flexible and hold for many performance metrics beyond breadth - for instance, showing that, in general, it is impossible to train generators which achieve a higher perplexity or lower hallucination rate for $K$ compared to other languages. Next, we study language generation with breadth and stable generators - algorithms that eventually stop changing after seeing an arbitrary but finite number of strings - and prove unconditional lower bounds for such generators, strengthening the results of [KMV25] and demonstrating that generation with many existing notions of breadth becomes equally hard, when stability is required. This gives a separation for generation with approximate breadth, between stable and unstable generators, highlighting the rich interplay between breadth, stability, and consistency in language generation.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-enhanced causal discovery for a small number of samples</title>
<link>https://arxiv.org/abs/2501.05007</link>
<guid>https://arxiv.org/abs/2501.05007</guid>
<content:encoded><![CDATA[
arXiv:2501.05007v2 Announce Type: replace-cross 
Abstract: The discovery of causal relations from observed data has attracted significant interest from disciplines such as economics, social sciences, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are usually associated with nonlinear causal structures, which makes the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not require any assumptions about the underlying model structures. Based on conditional independence tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed algorithm can explore causal relations from the observed data drawn from arbitrary distributions. We conducted systematic experiments on fundamental graphs of causal structures, demonstrating that the qPC algorithm exhibits better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the quantum algorithm can empower classical algorithms for accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. In addition, the effectiveness of this method was validated using the datasets on Boston housing prices, heart disease, and biological signaling systems as real-world applications. These findings highlight the potential of quantum-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios, where traditional approaches have shown significant limitations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Importance of Data Alignment in Downstream Model Performance</title>
<link>https://arxiv.org/abs/2501.08496</link>
<guid>https://arxiv.org/abs/2501.08496</guid>
<content:encoded><![CDATA[
arXiv:2501.08496v3 Announce Type: replace-cross 
Abstract: Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2501.12370</link>
<guid>https://arxiv.org/abs/2501.12370</guid>
<content:encoded><![CDATA[
arXiv:2501.12370v3 Announce Type: replace-cross 
Abstract: Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Population-Level Parallelism in Tree-Based Genetic Programming for Comprehensive GPU Acceleration</title>
<link>https://arxiv.org/abs/2501.17168</link>
<guid>https://arxiv.org/abs/2501.17168</guid>
<content:encoded><![CDATA[
arXiv:2501.17168v4 Announce Type: replace-cross 
Abstract: Tree-based Genetic Programming (TGP) is a widely used evolutionary algorithm for tasks such as symbolic regression, classification, and robotic control. Due to the intensive computational demands of running TGP, GPU acceleration is crucial for achieving scalable performance. However, efficient GPU-based execution of TGP still remains challenging, primarily due to three core issues: (1) the structural heterogeneity of program individuals, (2) the complexity of integrating multiple levels of parallelism, and (3) the incompatibility between high-performance CUDA execution and flexible Python-based environments. To address these issues, we propose EvoGP, a high-performance framework tailored for comprehensive GPU acceleration of TGP via population-level parallel execution. First, EvoGP introduces a tensorized representation that encodes variable-sized trees into fixed-shape, memory-aligned arrays, enabling uniform memory access and parallel computation across diverse individuals. Second, EvoGP adopts an adaptive parallelism strategy that dynamically combines intra- and inter-individual parallelism based on dataset size, ensuring high GPU utilization across a broad spectrum of tasks. Third, EvoGP embeds custom CUDA kernels into the PyTorch runtime, achieving seamless integration with Python-based environments such as Gym, MuJoCo, Brax, and Genesis. Comprehensive experiments show that EvoGP achieves up to 140x speedup over state-of-the-art GPU-based TGP implementations, while maintaining competitive accuracy and significantly improving scalability under large population sizes. EvoGP is open source and accessible at: https://github.com/EMI-Group/evogp.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Traffic Anomalies from Generative Models on Real-Time Observations</title>
<link>https://arxiv.org/abs/2502.01391</link>
<guid>https://arxiv.org/abs/2502.01391</guid>
<content:encoded><![CDATA[
arXiv:2502.01391v3 Announce Type: replace-cross 
Abstract: Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</title>
<link>https://arxiv.org/abs/2502.04700</link>
<guid>https://arxiv.org/abs/2502.04700</guid>
<content:encoded><![CDATA[
arXiv:2502.04700v4 Announce Type: replace-cross 
Abstract: The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</title>
<link>https://arxiv.org/abs/2502.06106</link>
<guid>https://arxiv.org/abs/2502.06106</guid>
<content:encoded><![CDATA[
arXiv:2502.06106v2 Announce Type: replace-cross 
Abstract: The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors. While recent studies have focused on the static mechanism of a certain behavior, the learning dynamics inside a model remain to be explored. In this work, we develop an interpretable fine-tuning method for analyzing the mechanism behind learning. We first introduce the concept of node-level intrinsic dimensionality to describe the learning process of a model in a computational graph. Based on our theory, we propose circuit-tuning, a two-stage algorithm that iteratively builds the minimal subgraph for a specific task and updates the key parameters in a heuristic way. Experimental results confirm the existence of the intrinsic dimensionality at the node level and demonstrate the effectiveness of our method for transparent and interpretable fine-tuning. We visualize and analyze the circuits before, during, and after fine-tuning, providing new insights into the self-organization mechanism of a neural network in the learning process.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks</title>
<link>https://arxiv.org/abs/2502.06684</link>
<guid>https://arxiv.org/abs/2502.06684</guid>
<content:encoded><![CDATA[
arXiv:2502.06684v2 Announce Type: replace-cross 
Abstract: Recent foundational models for tabular data, such as TabPFN, excel at adapting to new tasks via in-context learning, but remain constrained to a fixed, pre-defined number of target dimensions-often necessitating costly ensembling strategies. We trace this constraint to a deeper architectural shortcoming: these models lack target equivariance, so that permuting target dimension orderings alters their predictions. This deficiency gives rise to an irreducible "equivariance gap", an error term that introduces instability in predictions. We eliminate this gap by designing a fully target-equivariant architecture-ensuring permutation invariance via equivariant encoders, decoders, and a bi-attention mechanism. Empirical evaluation on standard classification benchmarks shows that, on datasets with more classes than those seen during pre-training, our model matches or surpasses existing methods while incurring lower computational overhead.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaved Gibbs Diffusion: Generating Discrete-Continuous Data with Implicit Constraints</title>
<link>https://arxiv.org/abs/2502.13450</link>
<guid>https://arxiv.org/abs/2502.13450</guid>
<content:encoded><![CDATA[
arXiv:2502.13450v2 Announce Type: replace-cross 
Abstract: We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for discrete-continuous data, focusing on problems with important, implicit and unspecified constraints in the data. Most prior works on discrete and discrete-continuous diffusion assume a factorized denoising distribution, which can hinder the modeling of strong dependencies between random variables in such problems. We empirically demonstrate a significant improvement in 3-SAT performance out of the box by switching to a Gibbs-sampling style discrete diffusion model which does not assume factorizability. Motivated by this, we introduce IGD which generalizes discrete time Gibbs sampling type Markov chain for the case of discrete-continuous generation. IGD allows for seamless integration between discrete and continuous denoisers while theoretically guaranteeing exact reversal of a suitable forward process. Further, it provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time refinement. Empirical evaluations on three challenging generation tasks - molecule structures, layouts and tabular data - demonstrate state-of-the-art performance. Notably, IGD achieves state-of-the-art results without relying on domain-specific inductive biases like equivariant diffusion or auxiliary losses. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral Treatment Pathways from Wearables and Diet</title>
<link>https://arxiv.org/abs/2503.03935</link>
<guid>https://arxiv.org/abs/2503.03935</guid>
<content:encoded><![CDATA[
arXiv:2503.03935v2 Announce Type: replace-cross 
Abstract: Postprandial hyperglycemia, marked by the blood glucose level exceeding the normal range after consuming a meal, is a critical indicator of progression toward type 2 diabetes in people with prediabetes and in healthy individuals. A key metric for understanding blood glucose dynamics after eating is the postprandial area under the curve (AUC). Predicting postprandial AUC in advance based on a person's lifestyle factors, such as diet and physical activity level, and explaining the factors that affect postprandial blood glucose could allow an individual to adjust their lifestyle accordingly to maintain normal glucose levels. In this study, we developed an explainable machine learning solution, GlucoLens, that takes sensor-driven inputs and uses advanced data processing, large language models, and trainable machine learning models to predict postprandial AUC and hyperglycemia from diet, physical activity, and recent glucose patterns. We used data obtained from wearables in a five-week clinical trial of 10 adults who worked full-time to develop and evaluate the proposed computational model that integrates wearable sensing, multimodal data, and machine learning. Our machine learning model takes multimodal data from wearable activity and glucose monitoring sensors, along with food and work logs, and provides an interpretable prediction of the postprandial glucose pattern. Our GlucoLens system achieves a normalized root mean squared error (NRMSE) of 0.123 in its best configuration. On average, the proposed technology provides a 16% better performance level compared to the comparison models. Additionally, our technique predicts hyperglycemia with an accuracy of 73.3% and an F1 score of 0.716 and recommends different treatment options to help avoid hyperglycemia through diverse counterfactual explanations. Code available: https://github.com/ab9mamun/GlucoLens.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User Association</title>
<link>https://arxiv.org/abs/2503.04564</link>
<guid>https://arxiv.org/abs/2503.04564</guid>
<content:encoded><![CDATA[
arXiv:2503.04564v4 Announce Type: replace-cross 
Abstract: Secure aggregation is motivated by federated learning (FL) where a cloud server aims to compute an averaged model (i.e., weights of deep neural networks) of the locally-trained models of numerous clients, while adhering to data security requirements. Hierarchical secure aggregation (HSA) extends this concept to a three-layer hierarchical network, where clustered users communicate with the server through an intermediate layer of relays. In HSA, beyond conventional server security, relay security is also enforced to ensure that the relays remain oblivious to the users' inputs (an abstraction of the local models in FL). Existing study on HSA assumes that each user is associated with only one relay, limiting opportunities for coding across inter-cluster users to achieve efficient communication and key generation. In this paper, we consider HSA with a cyclic association pattern where each user is connected to $B$ consecutive relays in a wrap-around manner. We propose an efficient aggregation scheme which includes a message design for the inputs inspired by gradient coding-a well-known technique for efficient communication in distributed computing-along with a highly non-trivial security key design. We also derive novel converse bounds on the minimum achievable communication and key rates using information-theoretic arguments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illuminant and light direction estimation using Wasserstein distance method</title>
<link>https://arxiv.org/abs/2503.05802</link>
<guid>https://arxiv.org/abs/2503.05802</guid>
<content:encoded><![CDATA[
arXiv:2503.05802v2 Announce Type: replace-cross 
Abstract: Illumination estimation remains a pivotal challenge in image processing, particularly for robotics, where robust environmental perception is essential under varying lighting conditions. Traditional approaches, such as RGB histograms and GIST descriptors, often fail in complex scenarios due to their sensitivity to illumination changes. This study introduces a novel method utilizing the Wasserstein distance, rooted in optimal transport theory, to estimate illuminant and light direction in images. Experiments on diverse images indoor scenes, black-and-white photographs, and night images demonstrate the method's efficacy in detecting dominant light sources and estimating their directions, outperforming traditional statistical methods in complex lighting environments. The approach shows promise for applications in light source localization, image quality assessment, and object detection enhancement. Future research may explore adaptive thresholding and integrate gradient analysis to enhance accuracy, offering a scalable solution for real-world illumination challenges in robotics and beyond.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaizeField3D: A Curated 3D Point Cloud and Procedural Model Dataset of Field-Grown Maize from a Diversity Panel</title>
<link>https://arxiv.org/abs/2503.07813</link>
<guid>https://arxiv.org/abs/2503.07813</guid>
<content:encoded><![CDATA[
arXiv:2503.07813v3 Announce Type: replace-cross 
Abstract: The development of artificial intelligence (AI) and machine learning (ML) based tools for 3D phenotyping, especially for maize, has been limited due to the lack of large and diverse 3D datasets. 2D image datasets fail to capture essential structural details such as leaf architecture, plant volume, and spatial arrangements that 3D data provide. To address this limitation, we present MaizeField3D (https://baskargroup.github.io/MaizeField3D/), a curated dataset of 3D point clouds of field-grown maize plants from a diverse genetic panel, designed to be AI-ready for advancing agricultural research. Our dataset includes 1,045 high-quality point clouds of field-grown maize collected using a terrestrial laser scanner (TLS). Point clouds of 520 plants from this dataset were segmented and annotated using a graph-based segmentation method to isolate individual leaves and stalks, ensuring consistent labeling across all samples. This labeled data was then used for fitting procedural models that provide a structured parametric representation of the maize plants. The leaves of the maize plants in the procedural models are represented using Non-Uniform Rational B-Spline (NURBS) surfaces that were generated using a two-step optimization process combining gradient-free and gradient-based methods. We conducted rigorous manual quality control on all datasets, correcting errors in segmentation, ensuring accurate leaf ordering, and validating metadata annotations. The dataset also includes metadata detailing plant morphology and quality, alongside multi-resolution subsampled point cloud data (100k, 50k, 10k points), which can be readily used for different downstream computational tasks. MaizeField3D will serve as a comprehensive foundational dataset for AI-driven phenotyping, plant structural analysis, and 3D applications in agricultural research.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAPI: A Model for Learning Robot Facial Expressions from Human Preferences</title>
<link>https://arxiv.org/abs/2503.17046</link>
<guid>https://arxiv.org/abs/2503.17046</guid>
<content:encoded><![CDATA[
arXiv:2503.17046v2 Announce Type: replace-cross 
Abstract: Automatic robotic facial expression generation is crucial for human-robot interaction, as handcrafted methods based on fixed joint configurations often yield rigid and unnatural behaviors. Although recent automated techniques reduce the need for manual tuning, they tend to fall short by not adequately bridging the gap between human preferences and model predictions-resulting in a deficiency of nuanced and realistic expressions due to limited degrees of freedom and insufficient perceptual integration. In this work, we propose a novel learning-to-rank framework that leverages human feedback to address this discrepancy and enhanced the expressiveness of robotic faces. Specifically, we conduct pairwise comparison annotations to collect human preference data and develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese RankNet-based approach that refines expression evaluation. Results obtained via Bayesian Optimization and online expression survey on a 35-DOF android platform demonstrate that our approach produces significantly more realistic and socially resonant expressions of Anger, Happiness, and Surprise than those generated by baseline and expert-designed methods. This confirms that our framework effectively bridges the gap between human preferences and model predictions while robustly aligning robotic expression generation with human affective responses.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding-informed Bias Mitigation for Fair CMR Segmentation</title>
<link>https://arxiv.org/abs/2503.17089</link>
<guid>https://arxiv.org/abs/2503.17089</guid>
<content:encoded><![CDATA[
arXiv:2503.17089v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is increasingly being used for medical imaging tasks. However, there can be biases in AI models, particularly when they are trained using imbalanced training datasets. One such example has been the strong ethnicity bias effect in cardiac magnetic resonance (CMR) image segmentation models. Although this phenomenon has been reported in a number of publications, little is known about the effectiveness of bias mitigation algorithms in this domain. We aim to investigate the impact of common bias mitigation methods to address bias between Black and White subjects in AI-based CMR segmentation models. Specifically, we use oversampling, importance reweighing and Group DRO as well as combinations of these techniques to mitigate the ethnicity bias. Second, motivated by recent findings on the root causes of AI-based CMR segmentation bias, we evaluate the same methods using models trained and evaluated on cropped CMR images. We find that bias can be mitigated using oversampling, significantly improving performance for the underrepresented Black subjects whilst not significantly reducing the majority White subjects' performance. Using cropped images increases performance for both ethnicities and reduces the bias, whilst adding oversampling as a bias mitigation technique with cropped images reduces the bias further. When testing the models on an external clinical validation set, we find high segmentation performance and no statistically significant bias.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2503.18681</link>
<guid>https://arxiv.org/abs/2503.18681</guid>
<content:encoded><![CDATA[
arXiv:2503.18681v3 Announce Type: replace-cross 
Abstract: Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework. Inspired by military strategy, we first decompose the sarcasm detection task into six distinct sub-tasks. A central commander (decision-maker) then assigns the best-suited large language model to address each specific sub-task. Ultimately, the detection results from each model are aggregated to identify sarcasm. We conducted extensive experiments on MMSD and MMSD 2.0, utilizing four multi-modal large language models and six prompting strategies. Our experiments demonstrate that our approach achieves state-of-the-art performance, with a 19.3% improvement in F1 score, without necessitating fine-tuning or ground-truth rationales.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition</title>
<link>https://arxiv.org/abs/2503.21843</link>
<guid>https://arxiv.org/abs/2503.21843</guid>
<content:encoded><![CDATA[
arXiv:2503.21843v2 Announce Type: replace-cross 
Abstract: Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing</title>
<link>https://arxiv.org/abs/2504.12215</link>
<guid>https://arxiv.org/abs/2504.12215</guid>
<content:encoded><![CDATA[
arXiv:2504.12215v2 Announce Type: replace-cross 
Abstract: Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability. We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing. The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size. The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions. Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability. These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation. On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Operating Room Workflow Analysis using Digital Twins</title>
<link>https://arxiv.org/abs/2504.12552</link>
<guid>https://arxiv.org/abs/2504.12552</guid>
<content:encoded><![CDATA[
arXiv:2504.12552v2 Announce Type: replace-cross 
Abstract: The operating room (OR) is a complex environment where optimizing workflows is critical to reduce costs and improve patient outcomes. While computer vision approaches for automatic recognition of perioperative events can identify bottlenecks for OR optimization, privacy concerns limit the use of OR videos for automated event detection. We propose a two-stage pipeline for privacy-preserving OR video analysis and event detection. First, we leverage vision foundation models for depth estimation and semantic segmentation to generate de-identified Digital Twins (DT) of the OR from conventional RGB videos. Second, we employ the SafeOR model, a fused two-stream approach that processes segmentation masks and depth maps for OR event detection. Evaluation on an internal dataset of 38 simulated surgical trials with five event classes shows that our DT-based approach achieves performance on par with -- and sometimes better than -- raw RGB video-based models for OR event detection. Digital Twins enable privacy-preserving OR workflow analysis, facilitating the sharing of de-identified data across institutions and potentially enhancing model generalizability by mitigating domain-specific appearance differences.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferrable Surrogates in Expressive Neural Architecture Search Spaces</title>
<link>https://arxiv.org/abs/2504.12971</link>
<guid>https://arxiv.org/abs/2504.12971</guid>
<content:encoded><![CDATA[
arXiv:2504.12971v3 Announce Type: replace-cross 
Abstract: Neural architecture search (NAS) faces a challenge in balancing the exploration of expressive, broad search spaces that enable architectural innovation with the need for efficient evaluation of architectures to effectively search such spaces. We investigate surrogate model training for improving search in highly expressive NAS search spaces based on context-free grammars. We show that i) surrogate models trained either using zero-cost-proxy metrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM have high predictive power for the performance of architectures both within and across datasets, ii) these surrogates can be used to filter out bad architectures when searching on novel datasets, thereby significantly speeding up search and achieving better final performances, and iii) the surrogates can be further used directly as the search objective for huge speed-ups.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Significativity Indices for Agreement Values</title>
<link>https://arxiv.org/abs/2504.15325</link>
<guid>https://arxiv.org/abs/2504.15325</guid>
<content:encoded><![CDATA[
arXiv:2504.15325v2 Announce Type: replace-cross 
Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a golden standard can be compared simply by using the order induced by their agreement measure with respect to the golden standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen's kappa, but they are mainly na\"ive, and their boundaries are arbitrary. This work proposes a general approach to evaluate the significativity of any agreement value between two classifiers and introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. Moreover, this manuscript addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification</title>
<link>https://arxiv.org/abs/2504.19136</link>
<guid>https://arxiv.org/abs/2504.19136</guid>
<content:encoded><![CDATA[
arXiv:2504.19136v2 Announce Type: replace-cross 
Abstract: The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover classification remains challenging due to modality heterogeneity and underutilized spectral complementarity. Existing methods often fail to decouple shared structural features from modality-complementary radiometric attributes, causing feature conflicts and information loss. To address this, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework that separates phase (modality-shared) and amplitude (modality-complementary) components in the Fourier domain, thus reinforcing shared structures while preserving complementary characteristics to improve fusion quality. Unlike prior approaches that overlook the distinct physical properties encoded in frequency spectra, PAD is the first to introduce explicit amplitude-phase decoupling for multi-modal fusion. Specifically, PAD comprises two key components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase features via convolution-guided scaling to enhance geometric consistency; and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates high-frequency and low-frequency patterns using frequency-adaptive multilayer perceptrons, leveraging SAR's morphological sensitivity and RGB's spectral richness. Extensive experiments on WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our work establishes a new paradigm for physics-aware multi-modal fusion in remote sensing. The code will be available at https://github.com/RanFeng2/PAD.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</title>
<link>https://arxiv.org/abs/2504.20808</link>
<guid>https://arxiv.org/abs/2504.20808</guid>
<content:encoded><![CDATA[
arXiv:2504.20808v2 Announce Type: replace-cross 
Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threat Modeling for AI: The Case for an Asset-Centric Approach</title>
<link>https://arxiv.org/abs/2505.06315</link>
<guid>https://arxiv.org/abs/2505.06315</guid>
<content:encoded><![CDATA[
arXiv:2505.06315v2 Announce Type: replace-cross 
Abstract: Recent advances in AI are transforming AI's ubiquitous presence in our world from that of standalone AI-applications into deeply integrated AI-agents. These changes have been driven by agents' increasing capability to autonomously make decisions and initiate actions, using existing applications; whether those applications are AI-based or not. This evolution enables unprecedented levels of AI integration, with agents now able to take actions on behalf of systems and users -- including, in some cases, the powerful ability for the AI to write and execute scripts as it deems necessary. With AI systems now able to autonomously execute code, interact with external systems, and operate without human oversight, traditional security approaches fall short.
  This paper introduces an asset-centric methodology for threat modeling AI systems that addresses the unique security challenges posed by integrated AI agents. Unlike existing top-down frameworks that analyze individual attacks within specific product contexts, our bottom-up approach enables defenders to systematically identify how vulnerabilities -- both conventional and AI-specific -- impact critical AI assets across distributed infrastructures used to develop and deploy these agents. This methodology allows security teams to: (1) perform comprehensive analysis that communicates effectively across technical domains, (2) quantify security assumptions about third-party AI components without requiring visibility into their implementation, and (3) holistically identify AI-based vulnerabilities relevant to their specific product context. This approach is particularly relevant for securing agentic systems with complex autonomous capabilities. By focusing on assets rather than attacks, our approach scales with the rapidly evolving threat landscape while accommodating increasingly complex and distributed AI development pipelines.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v2 Announce Type: replace-cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</title>
<link>https://arxiv.org/abs/2505.20697</link>
<guid>https://arxiv.org/abs/2505.20697</guid>
<content:encoded><![CDATA[
arXiv:2505.20697v3 Announce Type: replace-cross 
Abstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation</title>
<link>https://arxiv.org/abs/2505.21880</link>
<guid>https://arxiv.org/abs/2505.21880</guid>
<content:encoded><![CDATA[
arXiv:2505.21880v2 Announce Type: replace-cross 
Abstract: This study presents an innovative approach to urban mobility simulation by integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM). Unlike traditional rule-based ABM, the proposed framework leverages LLM to enhance agent diversity and realism by generating synthetic population profiles, allocating routine and occasional locations, and simulating personalized routes. Using real-world data, the simulation models individual behaviors and large-scale mobility patterns in Taipei City. Key insights, such as route heat maps and mode-specific indicators, provide urban planners with actionable information for policy-making. Future work focuses on establishing robust validation frameworks to ensure accuracy and reliability in urban planning applications.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Intelligent Low-altitude Economy with Large AI Model Deployment</title>
<link>https://arxiv.org/abs/2505.22343</link>
<guid>https://arxiv.org/abs/2505.22343</guid>
<content:encoded><![CDATA[
arXiv:2505.22343v2 Announce Type: replace-cross 
Abstract: Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several challenges, including the significant gap between their computational/storage demands and the limited onboard resources of LAE entities, the mismatch between lab-trained LAIMs and dynamic physical environments, and the inefficiencies of traditional decoupled designs for sensing, communication, and computation. To address these issues, we first propose a hierarchical system architecture tailored for LAIM deployment and present representative LAE application scenarios. Next, we explore key enabling techniques that facilitate the mutual co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented execution pipeline for scalable and adaptive service delivery. Then, the proposed framework is validated through real-world case studies. Finally, we outline open challenges to inspire future research.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</title>
<link>https://arxiv.org/abs/2506.01631</link>
<guid>https://arxiv.org/abs/2506.01631</guid>
<content:encoded><![CDATA[
arXiv:2506.01631v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Explainable Comparison and Alignment of Feature Embeddings</title>
<link>https://arxiv.org/abs/2506.06231</link>
<guid>https://arxiv.org/abs/2506.06231</guid>
<content:encoded><![CDATA[
arXiv:2506.06231v2 Announce Type: replace-cross 
Abstract: While several feature embedding models have been developed in the literature, comparisons of these embeddings have largely focused on their numerical performance in classification-related downstream applications. However, an interpretable comparison of different embeddings requires identifying and analyzing mismatches between sample groups clustered within the embedding spaces. In this work, we propose the \emph{Spectral Pairwise Embedding Comparison (SPEC)} framework to compare embeddings and identify their differences in clustering a reference dataset. Our approach examines the kernel matrices derived from two embeddings and leverages the eigendecomposition of the difference kernel matrix to detect sample clusters that are captured differently by the two embeddings. We present a scalable implementation of this kernel-based approach, with computational complexity that grows linearly with the sample size. Furthermore, we introduce an optimization problem using this framework to align two embeddings, ensuring that clusters identified in one embedding are also captured in the other model. We provide numerical results demonstrating the SPEC's application to compare and align embeddings on large-scale datasets such as ImageNet and MS-COCO. The project page is available at https://mjalali.github.io/SPEC/.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Good LLM-Generated Password Policies Are?</title>
<link>https://arxiv.org/abs/2506.08320</link>
<guid>https://arxiv.org/abs/2506.08320</guid>
<content:encoded><![CDATA[
arXiv:2506.08320v2 Announce Type: replace-cross 
Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are rapidly being adopted across industry, academia, and government sectors, owing to their remarkable capabilities in natural language processing. However, despite their strengths, the inconsistency and unpredictability of LLM outputs present substantial challenges, especially in security-critical domains such as access control. One critical issue that emerges prominently is the consistency of LLM-generated responses, which is paramount for ensuring secure and reliable operations.
  In this paper, we study the application of LLMs within the context of Cybersecurity Access Control Systems. Specifically, we investigate the consistency and accuracy of LLM-generated password policies, translating natural language prompts into executable pwquality.conf configuration files. Our experimental methodology adopts two distinct approaches: firstly, we utilize pre-trained LLMs to generate configuration files purely from natural language prompts without additional guidance. Secondly, we provide these models with official pwquality.conf documentation to serve as an informative baseline. We systematically assess the soundness, accuracy, and consistency of these AI-generated configurations. Our findings underscore significant challenges in the current generation of LLMs and contribute valuable insights into refining the deployment of LLMs in Access Control Systems.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Aware Image Restoration with Diffusion Models</title>
<link>https://arxiv.org/abs/2506.09993</link>
<guid>https://arxiv.org/abs/2506.09993</guid>
<content:encoded><![CDATA[
arXiv:2506.09993v2 Announce Type: replace-cross 
Abstract: Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation</title>
<link>https://arxiv.org/abs/2506.14634</link>
<guid>https://arxiv.org/abs/2506.14634</guid>
<content:encoded><![CDATA[
arXiv:2506.14634v3 Announce Type: replace-cross 
Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing Predictive and Generative AI in Regulation</title>
<link>https://arxiv.org/abs/2506.17347</link>
<guid>https://arxiv.org/abs/2506.17347</guid>
<content:encoded><![CDATA[
arXiv:2506.17347v2 Announce Type: replace-cross 
Abstract: Over the past decade, policymakers have developed a set of regulatory tools to ensure AI development aligns with key societal goals. Many of these tools were initially developed in response to concerns with predictive AI and therefore encode certain assumptions about the nature of AI systems and the utility of certain regulatory approaches. With the advent of generative AI, however, some of these assumptions no longer hold, even as policymakers attempt to maintain a single regulatory target that covers both types of AI. In this paper, we identify four distinct aspects of generative AI that call for meaningfully different policy responses. These are the generality and adaptability of generative AI that make it a poor regulatory target, the difficulty of designing effective evaluations, new legal concerns that change the ecosystem of stakeholders and sources of expertise, and the distributed structure of the generative AI value chain. In light of these distinctions, policymakers will need to evaluate where the past decade of policy work remains relevant and where new policies, designed to address the unique risks posed by generative AI, are necessary. We outline three recommendations for policymakers to more effectively identify regulatory targets and leverage constraints across the broader ecosystem to govern generative AI.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach</title>
<link>https://arxiv.org/abs/2506.17828</link>
<guid>https://arxiv.org/abs/2506.17828</guid>
<content:encoded><![CDATA[
arXiv:2506.17828v2 Announce Type: replace-cross 
Abstract: Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability</title>
<link>https://arxiv.org/abs/2506.18248</link>
<guid>https://arxiv.org/abs/2506.18248</guid>
<content:encoded><![CDATA[
arXiv:2506.18248v2 Announce Type: replace-cross 
Abstract: Generative adversarial attacks train a perturbation generator on a white-box surrogate model and subsequently apply the crafted perturbations to unseen black-box victim models. In contrast to iterative attacks, these methods deliver superior inference-time efficiency, scalability, and transferability; however, up until now, existing studies have not fully exploited the representational capacity of generative models to preserve and harness semantic information. Specifically, the intermediate activations of the generator encode rich semantic features--object boundaries and coarse shapes--that remain under-exploited, thereby limiting the alignment of perturbations with object-salient regions which are critical for adversarial transferability. To remedy this, we introduce a semantic structure-aware attack framework based on the Mean Teacher, which serves as a temporally smoothed feature reference. With this smoothed reference, we further direct semantic consistency between the early-layer activations in the student and those of the semantically rich teacher by feature distillation. By anchoring perturbation synthesis to the semantically salient early intermediate blocks within the generator based on empirical findings, our method guides progressive adversarial perturbation on regions that substantially enhance adversarial transferability. We conduct extensive experiments over diverse models, domains and tasks to demonstrate consistent improvements relative to state-of-the-art generative attacks, comprehensively evaluated using conventional metrics and our newly proposed Accidental Correction Rate (ACR).
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World-aware Planning Narratives Enhance Large Vision-Language Model Planner</title>
<link>https://arxiv.org/abs/2506.21230</link>
<guid>https://arxiv.org/abs/2506.21230</guid>
<content:encoded><![CDATA[
<div> LVLMs, embodied planning, complex scenarios, environmental understanding, curriculum learning<br />
Summary:<br />
The paper introduces World-Aware Planning Narrative Enhancement (WAP) framework for LVLMs in embodied planning tasks. It aims to enhance environmental understanding through visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding. The models are trained using only raw visual observations through curriculum learning. Evaluation on EB-ALFRED benchmark shows significant improvements in task success rates, especially in commonsense reasoning and long-horizon planning. Qwen2.5-VL model achieved a 60.7 absolute improvement in task success rates, outperforming proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin. The approach addresses the limitations of current LVLMs in dealing with complex scenarios involving unfamiliar environments and multi-step goals. <div>
arXiv:2506.21230v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks but struggle with complex scenarios involving unfamiliar environments and multi-step goals. Current approaches rely on environment-agnostic imitation learning that disconnects instructions from environmental contexts, causing models to struggle with context-sensitive instructions and rely on supplementary cues rather than visual reasoning during long-horizon interactions. In this work, we propose World-Aware Planning Narrative Enhancement (WAP), a framework that infuses LVLMs with comprehensive environmental understanding through four cognitive capabilities (visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding) while developing and evaluating models using only raw visual observations through curriculum learning. Evaluations on the EB-ALFRED benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a 60.7 absolute improvement in task success rates, particularly in commonsense reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced open-source models outperform proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Illusion of Thinking</title>
<link>https://arxiv.org/abs/2507.01231</link>
<guid>https://arxiv.org/abs/2507.01231</guid>
<content:encoded><![CDATA[
arXiv:2507.01231v1 Announce Type: new 
Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of Thinking," prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate by replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue, we show that previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care</title>
<link>https://arxiv.org/abs/2507.01282</link>
<guid>https://arxiv.org/abs/2507.01282</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dementia diagnosis, explainable AI, neuro-symbolic AI, clinical practice
<br />
Summary: 
The review discusses the limitations of large language models (LLMs) in making practical contributions to clinical settings, specifically in dementia diagnosis and care. Standalone machine-learning models excel at pattern recognition but lack interpretable guidance, leading to a lack of trust among clinicians. The use of LLMs alongside physicians did not improve diagnostic accuracy or speed. To address these limitations, hybrid approaches integrating statistical learning with expert rule-based knowledge and involving clinicians can improve interpretability and workflow fit. Future decision-support systems should prioritize explanatory coherence by linking predictions to clinically meaningful causes. Explainable AI and neuro-symbolic AI are proposed as the next steps in AI advancement, focusing on human-computer interactions and patient outcomes rather than just accuracy. It is crucial for AI systems to become more understandable and beneficial in clinical practice. 
<br /> <div>
arXiv:2507.01282v1 Announce Type: new 
Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing</title>
<link>https://arxiv.org/abs/2507.01376</link>
<guid>https://arxiv.org/abs/2507.01376</guid>
<content:encoded><![CDATA[
<div> AI agents, generative AI, large language models, multimodal large language models, smart manufacturing 
Summary:
This study explores the advancements in AI agent technologies, particularly in the realms of large language models (LLMs), multimodal large language models (MLLMs), and Agentic AI. These technologies have significantly enhanced AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making in dynamic environments. The study reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and discusses their potential applications in smart manufacturing. Key points include the potential of these AI paradigms to expand information processing, environmental perception, and autonomous decision-making capabilities, as well as the challenges they may face in integration into manufacturing processes.  <div>
arXiv:2507.01376v1 Announce Type: new 
Abstract: AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models</title>
<link>https://arxiv.org/abs/2507.01410</link>
<guid>https://arxiv.org/abs/2507.01410</guid>
<content:encoded><![CDATA[
<div> Keywords: moral machine, ethical decision making, fuzzy rules, fuzzy Petri nets, medical field

Summary:
Ethical decision making in the context of moral machines is a complex process due to the ontological and epistemic challenges it presents. This paper introduces a formal method for describing Ethical Decision Making models based on ethical risk assessment, utilizing fuzzy rules. The models are then verified and validated using fuzzy Petri nets, offering a systematic approach to evaluating the performance of moral machines. A case study in the medical field demonstrates the application of this methodology. By combining ethical risk assessment with fuzzy rule specification and verification through fuzzy Petri nets, this approach provides a structured framework for addressing the complexities of ethical decision making in AI systems. <div>
arXiv:2507.01410v1 Announce Type: new 
Abstract: The ontological and epistemic complexities inherent in the moral domain make it challenging to establish clear standards for evaluating the performance of a moral machine. In this paper, we present a formal method to describe Ethical Decision Making models based on ethical risk assessment. Then, we show how these models that are specified as fuzzy rules can be verified and validated using fuzzy Petri nets. A case study from the medical field is considered to illustrate the proposed approach.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading</title>
<link>https://arxiv.org/abs/2507.01431</link>
<guid>https://arxiv.org/abs/2507.01431</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-assisted grading, large language models, STEM courses, grading pipeline, real-world deployment

Summary: 
Pensieve is a new AI-assisted grading platform aimed at automating the grading process for handwritten, open-ended responses in university STEM courses. It utilizes large language models to transcribe and evaluate student work, offering instructors rubric-aligned scores, transcriptions, and confidence ratings. Unlike previous tools that focus on specific tasks, Pensieve covers the entire grading pipeline within a human-in-the-loop interface. Deployed in over 20 institutions, Pensieve has graded over 300,000 student responses across core STEM disciplines. Empirical results demonstrate a 65% reduction in grading time while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.

Summary: <div>
arXiv:2507.01431v1 Announce Type: new 
Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (https://www.pensieve.co), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using multi-agent architecture to mitigate the risk of LLM hallucinations</title>
<link>https://arxiv.org/abs/2507.01446</link>
<guid>https://arxiv.org/abs/2507.01446</guid>
<content:encoded><![CDATA[
<div> Keywords: customer service, response time, Large Language Models, multi-agent system, hallucination risk

Summary:
Large Language Models (LLMs) are being increasingly utilized to improve customer service quality and response time in order to enhance customer loyalty and market share. However, the risk of hallucination, where the model generates incorrect or misleading responses, is a significant challenge. To address this issue, a multi-agent system has been developed to manage customer requests received via SMS. This system integrates LLM-based agents with fuzzy logic to reduce the risk of hallucination. By combining the strengths of LLMs and fuzzy logic, the system aims to provide more accurate and reliable responses to customer queries, ultimately enhancing the customer service experience. This approach demonstrates a proactive strategy in leveraging technology to improve customer satisfaction and establish a competitive edge in the market. 

<br /><br />Summary: <div>
arXiv:2507.01446v1 Announce Type: new 
Abstract: Improving customer service quality and response time are critical factors for maintaining customer loyalty and increasing a company's market share. While adopting emerging technologies such as Large Language Models (LLMs) is becoming a necessity to achieve these goals, the risk of hallucination remains a major challenge. In this paper, we present a multi-agent system to handle customer requests sent via SMS. This system integrates LLM based agents with fuzzy logic to mitigate hallucination risks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01489</link>
<guid>https://arxiv.org/abs/2507.01489</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reinforcement learning, Agent-as-tool, reasoning process, natural language 

Summary: 
- Large Language Models (LLMs) play a crucial role in artificial intelligence, enabling advanced language understanding and reasoning capabilities.
- There is a focus on applying reinforcement learning in agent frameworks to enhance reasoning abilities.
- The hierarchical framework Agent-as-tool separates the tool calling and reasoning processes, enhancing reasoning efficiency.
- By detaching the tool calling process, the model can concentrate on verbal reasoning, improving overall performance.
- The research achieved notable results with minimal reinforcement fine-tuning, outperforming existing models in tasks such as Bamboogle.

<br /><br />Summary: <div>
arXiv:2507.01489v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as one of the most significant technological advancements in artificial intelligence in recent years. Their ability to understand, generate, and reason with natural language has transformed how we interact with AI systems. With the development of LLM-based agents and reinforcement-learning-based reasoning models, the study of applying reinforcement learning in agent frameworks has become a new research focus. However, all previous studies face the challenge of deciding the tool calling process and the reasoning process simultaneously, and the chain of reasoning was solely relied on the unprocessed raw result with redundant information and symbols unrelated to the task from the tool, which impose a heavy burden on the model's capability to reason. Therefore, in our research, we proposed a hierarchical framework Agent-as-tool that detach the tool calling process and the reasoning process, which enables the model to focus on the verbally reasoning process while the tool calling process is handled by another agent. Our work had achieved comparable results with only a slight reinforcement fine-tuning on 180 samples, and had achieved exceptionally well performance in Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding Search-R1 by 4.8% in exact match and 3.2% in cover exact match.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2507.01597</link>
<guid>https://arxiv.org/abs/2507.01597</guid>
<content:encoded><![CDATA[
arXiv:2507.01597v1 Announce Type: new 
Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the dynamic development of facts along a timeline. Most research on TKG reasoning (TKGR) focuses on modelling the repetition of global facts and designing patterns of local historical facts. However, they face two significant challenges: inadequate modeling of the event distribution shift between training and test samples, and reliance on random entity substitution for generating negative samples, which often results in low-quality sampling. To this end, we propose a novel distributional feature modeling approach for training TKGR models, Test-Time Training-guided Distribution shift Modelling (T3DM), to adjust the model based on distribution shift and ensure the global consistency of model reasoning. In addition, we design a negative-sampling strategy to generate higher-quality negative quadruples based on adversarial training. Extensive experiments show that T3DM provides better and more robust results than the state-of-the-art baselines in most cases.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI</title>
<link>https://arxiv.org/abs/2507.01717</link>
<guid>https://arxiv.org/abs/2507.01717</guid>
<content:encoded><![CDATA[
<div> Keywords: Patents, Large Language Models (LLMs), Autonomous agents, Product concepts, Innovation pipeline 

Summary: 
Agent Ideate is a framework that utilizes Large Language Models (LLMs) and autonomous agents to automatically generate product-based business ideas from patents. The study focused on three domains: Computer Science, Natural Language Processing, and Material Chemistry. Results showed that the combination of LLMs and agentic workflows outperformed standalone LLMs in generating high-quality, relevant, and novel product ideas. This approach has the potential to enhance the innovation pipeline by tapping into the wealth of technical knowledge found in patents. The use of autonomous agents in mining and generating product concepts from patent data presents a promising strategy for unlocking new business opportunities and fostering innovation. This research highlights the importance of leveraging advanced technologies to extract valuable insights from patent information for driving product development and entrepreneurial endeavors. 

<br /><br />Summary: <div>
arXiv:2507.01717v1 Announce Type: new 
Abstract: Patents contain rich technical knowledge that can inspire innovative product ideas, yet accessing and interpreting this information remains a challenge. This work explores the use of Large Language Models (LLMs) and autonomous agents to mine and generate product concepts from a given patent. In this work, we design Agent Ideate, a framework for automatically generating product-based business ideas from patents. We experimented with open-source LLMs and agent-based architectures across three domains: Computer Science, Natural Language Processing, and Material Chemistry. Evaluation results show that the agentic approach consistently outperformed standalone LLMs in terms of idea quality, relevance, and novelty. These findings suggest that combining LLMs with agentic workflows can significantly enhance the innovation pipeline by unlocking the untapped potential of business idea generation from patent data.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Matching and Pricing for Crowd-shipping with In-store Customers</title>
<link>https://arxiv.org/abs/2507.01749</link>
<guid>https://arxiv.org/abs/2507.01749</guid>
<content:encoded><![CDATA[
<div> NeurADP, DDQN, crowd-shipping, last-mile delivery, urban logistics <br />
<br />
Summary: This paper explores using in-store customers as delivery couriers in a centralized crowd-shipping system to improve last-mile delivery in urban areas. A Markov Decision Process model is proposed to manage uncertainties like order arrival and acceptance probabilities. The integration of NeurADP for order assignment and DDQN for dynamic pricing improves cost efficiency by up to 6.7% compared to fixed pricing NeurADP and 18% over myopic baselines. Flexible delivery delays and multi-destination routing further reduce operational costs by 8% and 17% respectively. The study highlights the benefits of dynamic policies in crowd-shipping systems, providing valuable insights for urban logistics operators. <br /> <div>
arXiv:2507.01749v1 Announce Type: new 
Abstract: This paper examines the use of in-store customers as delivery couriers in a centralized crowd-shipping system, targeting the growing need for efficient last-mile delivery in urban areas. We consider a brick-and-mortar retail setting where shoppers are offered compensation to deliver time-sensitive online orders. To manage this process, we propose a Markov Decision Process (MDP) model that captures key uncertainties, including the stochastic arrival of orders and crowd-shippers, and the probabilistic acceptance of delivery offers. Our solution approach integrates Neural Approximate Dynamic Programming (NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network (DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop routing and accounts for offer acceptance uncertainty, aligning more closely with real-world operations. Experimental results demonstrate that the integrated NeurADP + DDQN policy achieves notable improvements in delivery cost efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and approximately 18\% over myopic baselines. We also show that allowing flexible delivery delays and enabling multi-destination routing further reduces operational costs by 8\% and 17\%, respectively. These findings underscore the advantages of dynamic, forward-looking policies in crowd-shipping systems and offer practical guidance for urban logistics operators.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics</title>
<link>https://arxiv.org/abs/2507.01833</link>
<guid>https://arxiv.org/abs/2507.01833</guid>
<content:encoded><![CDATA[
<div> answer set programming, non-monotonic logic, semantics, Gelfond answer set principles, computational complexity

Summary: 
The article discusses the foundations of answer set programming (ASP) and the various semantics proposed for non-monotonic logic programming. It questions the necessity of certain conditions like the minimal model property and foundedness for answer set semantics. The Gelfond answer set principles are refined, introducing well-supportedness, minimality w.r.t. negation, and minimality w.r.t. epistemic negation. New answer set semantics based on these principles are defined. The notion of well-supportedness is extended to answer sets and world views, ensuring constructibility without circular justification. The refined principles are used to assess existing answer set semantics intuitively. Computational complexity is also analyzed. This work provides a critical examination of answer set semantics and offers a new perspective on constructing answer sets. <div>
arXiv:2507.01833v1 Announce Type: new 
Abstract: Non-monotonic logic programming is the basis for a declarative problem solving paradigm known as answer set programming (ASP). Departing from the seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic programs, various answer set semantics have been proposed for extensions. We consider two important questions: (1) Should the minimal model property, constraint monotonicity and foundedness as defined in the literature be mandatory conditions for an answer set semantics in general? (2) If not, what other properties could be considered as general principles for answer set semantics? We address the two questions. First, it seems that the three aforementioned conditions may sometimes be too strong, and we illustrate with examples that enforcing them may exclude expected answer sets. Second, we evolve the Gelfond answer set (GAS) principles for answer set construction by refining the Gelfond's rationality principle to well-supportedness, minimality w.r.t. negation by default and minimality w.r.t. epistemic negation. The principle of well-supportedness guarantees that every answer set is constructible from if-then rules obeying a level mapping and is thus free of circular justification, while the two minimality principles ensure that the formalism minimizes knowledge both at the level of answer sets and of world views. Third, to embody the refined GAS principles, we extend the notion of well-supportedness substantially to answer sets and world views, respectively. Fourth, we define new answer set semantics in terms of the refined GAS principles. Fifth, we use the refined GAS principles as an alternative baseline to intuitively assess the existing answer set semantics. Finally, we analyze the computational complexity.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques</title>
<link>https://arxiv.org/abs/2507.01018</link>
<guid>https://arxiv.org/abs/2507.01018</guid>
<content:encoded><![CDATA[
<div> encryption, anomaly detection, blockchain, zero-trust, scalability
Summary: 
The study discusses security threats in smart homes related to vulnerabilities at the network layer, device level, and from cloud-based and AI-driven systems. Post-quantum encryption and AI-driven anomaly detection are effective in enhancing security, but face challenges due to computational resource demands. Blockchain authentication and zero-trust structures improve security resilience but require changes to existing infrastructure. Specific security strategies show effectiveness through ANOVA, Chi-square tests, and Monte Carlo simulations, yet lack scalability. Improvement in cryptographic techniques, AI-enhanced threat detection, and adaptive security models are necessary to achieve a balance between performance, efficiency, and real-time applicability within smart home ecosystems. <div>
arXiv:2507.01018v1 Announce Type: cross 
Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing cybersecurity risks, posing significant challenges to these environments. The study explores security threats in smart homes ecosystems, categorizing them into vulnerabilities at the network layer, device level, and those from cloud-based and AI-driven systems. Research findings indicate that post-quantum encryption, coupled with AI-driven anomaly detection, is highly effective in enhancing security; however, computational resource demands present significant challenges. Blockchain authentication together with zero-trust structures builds security resilience, although they need changes to existing infrastructure. The specific security strategies show their effectiveness through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack sufficient scalability according to the results. The research demonstrates the requirement for improvement in cryptographic techniques, alongside AI-enhanced threat detection and adaptive security models which must achieve a balance between performance and efficiency and real-time applicability within smart home ecosystems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hello Afrika: Speech Commands in Kinyarwanda</title>
<link>https://arxiv.org/abs/2507.01024</link>
<guid>https://arxiv.org/abs/2507.01024</guid>
<content:encoded><![CDATA[
<div> Keywords: Voice Commands, Speech Recognition, African Languages, Kinyarwanda, Hello Afrika <br />
<br />
Summary: 
The article discusses the importance of voice or speech commands in facilitating non-contact control and activation of AI systems, particularly for individuals with disabilities. It highlights the lack of speech command models for African languages and introduces the Hello Afrika project, which aims to address this issue. The first phase of the project focuses on the Kinyarwanda language due to Rwanda's interest in developing speech recognition technologies and their substantial dataset on Mozilla Common Voice. A custom speech command corpus was created, consisting of general directives, numbers, and a wake word. The final model was deployed on various devices, including PCs, mobile phones, and edge devices, and its performance was evaluated using appropriate metrics. This initiative represents a significant step towards bridging the gap in speech technology for African languages, starting with Kinyarwanda. <div>
arXiv:2507.01024v1 Announce Type: cross 
Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a language which are essential for non-contact control of and activation of larger AI systems in devices used in everyday life especially for persons with disabilities. Currently, there is a dearth of speech command models for African languages. The Hello Afrika project aims to address this issue and its first iteration is focused on the Kinyarwanda language since the country has shown interest in developing speech recognition technologies culminating in one of the largest datasets on Mozilla Common Voice. The model was built off a custom speech command corpus made up of general directives, numbers, and a wake word. The final model was deployed on multiple devices (PC, Mobile Phone and Edge Devices) and the performance was assessed using suitable metrics.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPC-AI Coupling Methodology for Scientific Applications</title>
<link>https://arxiv.org/abs/2507.01025</link>
<guid>https://arxiv.org/abs/2507.01025</guid>
<content:encoded><![CDATA[
<div> methodology, coupling patterns, case studies, technical challenges, performance improvements 

Summary: 
The study explores the coupling of high-performance computing (HPC) and artificial intelligence (AI) in emerging scientific applications. It introduces a novel methodology incorporating three coupling patterns: surrogate, directive, and coordinate. Through case studies in materials science, the effectiveness of these patterns is demonstrated. The study highlights technical challenges in HPC-AI coupling and showcases performance improvements. Implementation details are provided, offering valuable insight into the promising perspectives of HPC-AI ensembles in scientific discovery. The proposed coupling patterns are not restricted to materials science but are applicable to various scientific domains, providing guidance for future HPC-AI collaborations. 

<br /><br /> <div>
arXiv:2507.01025v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.01029</link>
<guid>https://arxiv.org/abs/2507.01029</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, multimodal large language models, pathology, visual reasoning, CoT

Summary:
PathCoT introduces a new approach to pathology visual reasoning tasks using multimodal large language models (MLLMs). The method addresses key challenges faced by existing models by integrating pathology expert knowledge to guide the reasoning process and prevent model hallucinations. By incorporating domain-specific information, PathCoT allows MLLMs to perform more like pathology experts, improving accuracy in visual analysis. The approach also includes a self-evaluation step to mitigate errors that may arise during reasoning, ensuring a reliable answer is obtained. Experimental results on the PathMMU dataset demonstrate the effectiveness of PathCoT in enhancing pathology visual understanding and reasoning. By combining expert knowledge, MLLMs, and a self-evaluation mechanism, PathCoT provides a comprehensive solution for improving visual reasoning in pathology tasks. 

<br /><br />Summary: <div>
arXiv:2507.01029v1 Announce Type: cross 
Abstract: With the development of generative artificial intelligence and instruction tuning techniques, multimodal large language models (MLLMs) have made impressive progress on general reasoning tasks. Benefiting from the chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning problem step-by-step. However, existing MLLMs still face significant challenges when applied to pathology visual reasoning tasks: (1) LLMs often underperforms because they lack domain-specific information, which can lead to model hallucinations. (2) The additional reasoning steps in CoT may introduce errors, leading to the divergence of answers. To address these limitations, we propose PathCoT, a novel zero-shot CoT prompting method which integrates the pathology expert-knowledge into the reasoning process of MLLMs and incorporates self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides the MLLM with prior knowledge to perform as pathology experts, and provides comprehensive analysis of the image with their domain-specific knowledge. By incorporating the experts' knowledge, PathCoT can obtain the answers with CoT reasoning. Furthermore, PathCoT incorporates a self-evaluation step that assesses both the results generated directly by MLLMs and those derived through CoT, finally determining the reliable answer. The experimental results on the PathMMU dataset demonstrate the effectiveness of our method on pathology visual understanding and reasoning.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks</title>
<link>https://arxiv.org/abs/2507.01032</link>
<guid>https://arxiv.org/abs/2507.01032</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-omics, uncertainty-aware, neural networks, Dempster-Shafer theory, dynamic decision framework

Summary: 

- The study introduces an uncertainty-aware, multi-view dynamic decision framework for omics data classification to enhance accuracy while minimizing testing costs.
- Activation functions of neural networks are refined to generate Dirichlet distribution parameters, capturing belief masses and uncertainty masses for classification results.
- Fusion strategy based on Dempster-Shafer theory integrates heterogeneous omics modalities to improve diagnostic accuracy and robustness.
- A dynamic decision mechanism incrementally introduces omics data for each patient based on model confidence and a predefined threshold.
- Evaluation on four benchmark datasets shows over 50% accurate classification using a single omics modality, reducing redundant testing while maintaining comparable diagnostic performance. Essential biological insights are preserved through this approach. 

<br /><br />Summary: <div>
arXiv:2507.01032v1 Announce Type: cross 
Abstract: Background and Objective: High-throughput multi-omics technologies have proven invaluable for elucidating disease mechanisms and enabling early diagnosis. However, the high cost of multi-omics profiling imposes a significant economic burden, with over reliance on full omics data potentially leading to unnecessary resource consumption. To address these issues, we propose an uncertainty-aware, multi-view dynamic decision framework for omics data classification that aims to achieve high diagnostic accuracy while minimizing testing costs. Methodology: At the single-omics level, we refine the activation functions of neural networks to generate Dirichlet distribution parameters, utilizing subjective logic to quantify both the belief masses and uncertainty mass of classification results. Belief mass reflects the support of a specific omics modality for a disease class, while the uncertainty parameter captures limitations in data quality and model discriminability, providing a more trustworthy basis for decision-making. At the multi omics level, we employ a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous modalities, leveraging their complementarity to boost diagnostic accuracy and robustness. A dynamic decision mechanism is then applied that omics data are incrementally introduced for each patient until either all data sources are utilized or the model confidence exceeds a predefined threshold, potentially before all data sources are utilized. Results and Conclusion: We evaluate our approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN. In three datasets, over 50% of cases achieved accurate classification using a single omics modality, effectively reducing redundant testing. Meanwhile, our method maintains diagnostic performance comparable to full-omics models and preserves essential biological insights.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya</title>
<link>https://arxiv.org/abs/2507.01034</link>
<guid>https://arxiv.org/abs/2507.01034</guid>
<content:encoded><![CDATA[
<div> Keywords: electricity forecasting, data-driven approach, LSTM neural networks, exogenous factors, grid stability

Summary:
Accurate electricity forecasting is crucial, especially in regions like Benghazi, Libya, facing frequent load shedding and generation deficits. This study proposes a data-driven approach using historical data from unstable and stable years to forecast electricity indicators for 2025. Various time series models were tested, with LSTM neural networks outperforming others, especially in modeling non-stationary and seasonal patterns. The optimized LSTM framework incorporates exogenous factors like temperature and humidity to enhance forecasting accuracy. This research offers practical insights for policymakers and grid operators, enabling proactive load management and resource planning in volatile regions with limited data availability.<br /><br />Summary: <div>
arXiv:2507.01034v1 Announce Type: cross 
Abstract: Accurate electricity forecasting is crucial for grid stability and energy planning, especially in Benghazi, Libya, where frequent load shedding, generation deficits, and infrastructure limitations persist. This study proposes a data-driven approach to forecast electricity load, generation, and deficits for 2025 using historical data from 2019 (a year marked by instability) and 2023 (a more stable year). Multiple time series models were applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural networks. The dataset was enhanced through missing value imputation, outlier smoothing, and log transformation. Performance was assessed using mean squared error, root mean squared error, mean absolute error, and mean absolute percentage error. LSTM outperformed all other models, showing strong capabilities in modeling non-stationary and seasonal patterns. A key contribution of this work is an optimized LSTM framework that integrates exogenous factors such as temperature and humidity, offering robust performance in forecasting multiple electricity indicators. These results provide practical insights for policymakers and grid operators to enable proactive load management and resource planning in data-scarce, volatile regions.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems</title>
<link>https://arxiv.org/abs/2507.01035</link>
<guid>https://arxiv.org/abs/2507.01035</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Large Language Model, Recommender Systems, FPGA, DeepSpeed

Summary:
The study focuses on optimizing hybrid Graph Neural Network (GNN) and Large Language Model (LLM)-based Recommender Systems (ReS) for real-time performance and training efficiency. By integrating various architecture-optimization strategies and hardware acceleration techniques such as FPGA and DeepSpeed, significant improvements were achieved. The optimal configuration showed 13.6% more accuracy with reduced latency and training time. The study highlights the importance of hardware-software co-design and parameter-efficient tuning in enhancing the performance of hybrid models. Recommendations include the use of FPGA and LoRA for real-time deployment. Future research directions include exploring federated learning and advanced fusion architectures for scalability and privacy preservation. Overall, the study lays the groundwork for next-generation Recommender Systems that balance low-latency response and advanced personalization.<br /><br />Summary: <div>
arXiv:2507.01035v1 Announce Type: cross 
Abstract: The incessant advent of online services demands high speed and efficient recommender systems (ReS) that can maintain real-time performance along with processing very complex user-item interactions. The present study, therefore, considers computational bottlenecks involved in hybrid Graph Neural Network (GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their inference latency and training efficiency. An extensive methodology was used: hybrid GNN-LLM integrated architecture-optimization strategies(quantization, LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2. Experimental improvements were significant, with the optimal Hybrid + FPGA + DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms of latency, while LoRA brought down training time by 66% (3.8 hours) in comparison to the non-optimized baseline. Irrespective of domain, such as accuracy or efficiency, it can be established that hardware-software co-design and parameter-efficient tuning permit hybrid models to outperform GNN or LLM approaches implemented independently. It recommends the use of FPGA as well as LoRA for real-time deployment. Future work should involve federated learning along with advanced fusion architectures for better scalability and privacy preservation. Thus, this research marks the fundamental groundwork concerning next-generation ReS balancing low-latency response with cutting-edge personalization.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systemic Constraints of Undecidability</title>
<link>https://arxiv.org/abs/2507.01036</link>
<guid>https://arxiv.org/abs/2507.01036</guid>
<content:encoded><![CDATA[
<div> undecidability, computation, systems, causal embedding, closure principle 
Summary:
This paper introduces a theory of systemic undecidability, viewing incomputability as a fundamental property of systems rather than specific functions or problems. It defines causal embedding and establishes a closure principle: any subsystem involved in the computation of an undecidable system also becomes undecidable. This highlights undecidability as a pervasive constraint on prediction, modeling, and understanding in both natural and artificial systems. The framework challenges the idea that computational limits can be bypassed through architectural innovation and negates oracle mimicry. By extending classical results to dynamic systems, it enriches the logical lineage of Godel, Turing, and Chaitin, presenting a fresh perspective on the computability topology and its connection to the frontiers of scientific knowledge.<br /><br />Summary: <div>
arXiv:2507.01036v1 Announce Type: cross 
Abstract: This paper presents a theory of systemic undecidability, reframing incomputability as a structural property of systems rather than a localized feature of specific functions or problems. We define a notion of causal embedding and prove a closure principle: any subsystem that participates functionally in the computation of an undecidable system inherits its undecidability. This result positions undecidability as a pervasive constraint on prediction, modeling, and epistemic access in both natural and artificial systems. Our framework disarms oracle mimicry and challenges the view that computational limits can be circumvented through architectural innovation. By generalizing classical results into a dynamic systems context, this work augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new perspective of the topology of computability and its interrelation to the boundaries of scientific knowledge.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Segment for Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2507.01037</link>
<guid>https://arxiv.org/abs/2507.01037</guid>
<content:encoded><![CDATA[
<div> Keywords: Iterative search heuristics, Vehicle Routing Problems, First-Segment-Then-Aggregate decomposition, Learning-to-Segment framework, neural framework<br />
Summary:<br />
The study focuses on enhancing iterative solvers for Vehicle Routing Problems by introducing the First-Segment-Then-Aggregate (FSTA) decomposition technique. This technique aims to reduce redundant computations by identifying and preserving stable solution segments during the search process. To address the challenge of identifying segments for aggregation, a novel neural framework, Learning-to-Segment (L2Seg), is introduced. L2Seg offers three variants, including non-autoregressive and autoregressive models, which work synergistically to improve solver performance. Experimental results on VRP instances demonstrate that L2Seg accelerates state-of-the-art solvers by up to 7 times. The synergistic approach of combining the NAR and AR variants shows the best performance and highlights the flexibility of L2Seg in improving traditional, learning-based, and hybrid solvers for a variety of VRPs. <div>
arXiv:2507.01037v1 Announce Type: cross 
Abstract: Iterative search heuristics are widely recognized as state-of-the-art for solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit a critical observation: within these solvers, a large portion of the solution remains stable, i.e., unchanged across search iterations, causing redundant computations, especially for large-scale VRPs with long subtours. To address this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA) decomposition technique to accelerate iterative solvers. Specifically, FSTA preserves stable solution segments during the search, aggregates nodes within each segment into fixed hypernodes, and focuses the search only on unstable portions. Yet, a key challenge lies in identifying which segments should be aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg), a novel neural framework to intelligently differentiate potentially stable and unstable portions for FSTA decomposition. We present three L2Seg variants: non-autoregressive (globally comprehensive but locally indiscriminate), autoregressive (locally refined but globally deficient), and their synergy, with bespoke training and inference strategies. Empirical results on CVRP and VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy achieves best performance by combining their complementary strengths. Notably, L2Seg is a flexible framework that is compatible with traditional, learning-based, and hybrid solvers, while supporting a broad class of VRPs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2507.01039</link>
<guid>https://arxiv.org/abs/2507.01039</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, neuro-fuzzy controllers, Proximal Policy Optimization, CartPole-v1, convergence<br />Summary: 
The article introduces a reinforcement learning approach for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). This method builds on existing work that used Deep Q-Learning with Adaptive Neuro-Fuzzy Inference Systems (ANFIS). The approach replaces off-policy value-based frameworks with an on-policy actor-critic loop for stability. Evaluations in the CartPole-v1 environment show that PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 after 20000 updates, displaying less variance in comparison to ANFIS-Deep Q-Network (DQN) baselines. The results indicate faster convergence with PPO, suggesting a promising pathway for developing explainable neuro-fuzzy controllers in reinforcement learning tasks. <div>
arXiv:2507.01039v1 Announce Type: cross 
Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). Building on prior work that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS), our method replaces the off-policy value-based framework with a stable on-policy actor-critic loop. We evaluate this approach in the CartPole-v1 environment using multiple random seeds and compare its learning performance against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000 updates, showcasing less variance than prior DQN-based methods during training and overall faster convergence. These findings suggest that PPO offers a promising pathway for training explainable neuro-fuzzy controllers in reinforcement learning tasks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Clifford Neural Layers</title>
<link>https://arxiv.org/abs/2507.01040</link>
<guid>https://arxiv.org/abs/2507.01040</guid>
<content:encoded><![CDATA[
<div> Clifford Neural Layers, PDE modeling, Clifford Algebra, neural networks, optimization <br />
<br />
Summary: <br />
Clifford Neural Layers enhance PDE modeling by integrating Clifford Algebra into neural networks. This project concentrates on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance. Through testing on a practical network block utilizing Clifford convolutional layers and multivector activation layers, the implementation shows a 30% increase in speed compared to the standard PyTorch setup for relatively large data and network sizes exceeding the L2 cache. The code base for this project is open source and available at https://github.com/egretwAlker/c-opt-clifford-layers. <div>
arXiv:2507.01040v1 Announce Type: cross 
Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).
  We open source our code base at https://github.com/egretwAlker/c-opt-clifford-layers
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast AI Model Splitting over Edge Networks</title>
<link>https://arxiv.org/abs/2507.01041</link>
<guid>https://arxiv.org/abs/2507.01041</guid>
<content:encoded><![CDATA[
<div> algorithm, split learning, artificial intelligence, model splitting, computational complexity

Summary:<br />
The paper discusses split learning (SL) as an efficient approach for training artificial intelligence (AI) models. It proposes a DAG-based model splitting algorithm to solve the optimal model splitting problem, which is shown to be optimal through theoretical analysis. Additionally, a block-wise model splitting algorithm is introduced to reduce computational complexity in AI models with block structures. Experimental results demonstrate that the proposed algorithms can quickly determine the optimal model splitting and reduce training delay significantly in dynamic edge networks compared to existing benchmarks. <div>
arXiv:2507.01041v1 Announce Type: cross 
Abstract: Split learning (SL) has emerged as a computationally efficient approach for artificial intelligence (AI) model training, which can alleviate device-side computational workloads. However, complex AI model architectures pose high computational complexity to obtain the optimal model splitting. In this paper, we represent an arbitrary AI model as a directed acyclic graph (DAG), and then reformulate the optimal model splitting problem as a minimum s-t cut search problem. To solve the problem, we propose a fast DAG-based model splitting algorithm, which restructures the DAG to enable the optimal model splitting identification via a maximum flow method. Theoretical analysis indicates that the proposed algorithm is optimal. Furthermore, considering AI models with block structures, we propose a block-wise model splitting algorithm to reduce computational complexity. The algorithm abstracts each block, i.e., a component consisting of multiple layers, into a single vertex, thereby obtaining the optimal model splitting via a simplified DAG. Extensive experimental results demonstrate that the proposed algorithms can determine the optimal model splitting within milliseconds, as well as reduce training delay by 24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Argus Judge Them All? Comparing VLMs Across Domains</title>
<link>https://arxiv.org/abs/2507.01042</link>
<guid>https://arxiv.org/abs/2507.01042</guid>
<content:encoded><![CDATA[
<div> benchmark, Vision-Language Models, CLIP, BLIP, LXMERT

Summary: 
The article evaluates the performance of Vision-Language Models (VLMs) including CLIP, BLIP, and LXMERT across various tasks such as retrieval, captioning, and reasoning. The evaluation includes task accuracy, generation quality, efficiency, and a new metric called Cross-Dataset Consistency (CDC). CLIP demonstrates strong generalization with a CDC score of 0.92, while BLIP performs well on curated data and LXMERT excels in structured reasoning tasks. The results highlight a trade-off between generalization and specialization, providing insights for the deployment of VLMs in real-world applications and guiding the development of more robust, task-flexible architectures.<br /><br /> <div>
arXiv:2507.01042v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their performance consistency across tasks is underexamined. We benchmark CLIP, BLIP, and LXMERT across diverse datasets spanning retrieval, captioning, and reasoning. Our evaluation includes task accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning. These results expose trade-offs between generalization and specialization, informing industrial deployment of VLMs and guiding development toward robust, task-flexible architectures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Classification with Dynamically Growing and Shrinking Neural Networks</title>
<link>https://arxiv.org/abs/2507.01043</link>
<guid>https://arxiv.org/abs/2507.01043</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Tree Search, Neural Networks, Dynamic Architectural Design, Time Series Classification, Python Source Code
<br />
Summary:
This article introduces a novel method for constructing data-driven neural network models that dynamically adapt their architecture during training. The approach involves using a Monte Carlo tree search procedure to determine the optimal changes to the network structure while training. By allowing for dynamic growing and shrinking of the neural network, the method shows promising performance in both visual pattern and multivariate time series classification tasks. The ability to independently modify the architecture for each time series dataset contributes to the method's effectiveness in handling multivariate time series data. Additionally, the article provides Python source code for reproducibility purposes, ensuring the accessibility of the proposed approach to other researchers in the field. Overall, experimental evaluations demonstrate the robustness and adaptability of the method in improving neural network model performance. 
<br /> <div>
arXiv:2507.01043v1 Announce Type: cross 
Abstract: The issue of data-driven neural network model construction is one of the core problems in the domain of Artificial Intelligence. A standard approach assumes a fixed architecture with trainable weights. A conceptually more advanced assumption is that we not only train the weights, but also find out the optimal model architecture. We present a new method that realizes just that. This article is an extended version of our conference paper titled "Dynamic Growing and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the paper, we show in detail how to create a neural network with a procedure that allows dynamic shrinking and growing of the model while it is being trained. The decision-making mechanism for the architectural design is governed by a Monte Carlo tree search procedure which simulates network behavior and allows to compare several candidate architecture changes to choose the best one. The proposed method was validated using both visual and time series datasets, demonstrating its particular effectiveness in multivariate time series classification. This is attributed to the architecture's ability to adapt dynamically, allowing independent modifications for each time series. The approach is supplemented by Python source code for reproducibility. Experimental evaluations in visual pattern and multivariate time series classification tasks revealed highly promising performance, underscoring the method's robustness and adaptability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals</title>
<link>https://arxiv.org/abs/2507.01045</link>
<guid>https://arxiv.org/abs/2507.01045</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiac biosignals, deep learning, transformer architectures, multi-modal integration, transfer learning

Summary:
The study introduces a cardiac sensing foundation model (CSFM) that utilizes transformer architectures and generative pretraining to learn representations from heterogeneous health records. Pretrained on diverse datasets, the model extracts features effectively and enables transfer learning across different input configurations and sensor modalities. CSFM outperforms traditional approaches in diagnostic tasks, demographic recognition, vital sign measurement, outcome prediction, and ECG question answering. It exhibits strong performance across various ECG lead configurations and scenarios involving different sensor modalities. The findings highlight CSFM's versatility and scalability for comprehensive cardiac monitoring.<br /><br />Summary: <div>
arXiv:2507.01045v1 Announce Type: cross 
Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms (PPG), are of paramount importance for the diagnosis, prevention, and management of cardiovascular diseases, and have been extensively used in a variety of clinical tasks. Conventional deep learning approaches for analyzing these signals typically rely on homogeneous datasets and static bespoke models, limiting their robustness and generalizability across diverse clinical settings and acquisition protocols. In this study, we present a cardiac sensing foundation model (CSFM) that leverages advanced transformer architectures and a generative, masked pretraining strategy to learn unified representations from vast, heterogeneous health records. Our model is pretrained on an innovative multi-modal integration of data from multiple large-scale datasets (including MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the corresponding clinical or machine-generated text reports from approximately 1.7 million individuals. We demonstrate that the embeddings derived from our CSFM not only serve as effective feature extractors across diverse cardiac sensing scenarios, but also enable seamless transfer learning across varying input configurations and sensor modalities. Extensive evaluations across diagnostic tasks, demographic information recognition, vital sign measurement, clinical outcome prediction, and ECG question answering reveal that CSFM consistently outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits robust performance across multiple ECG lead configurations from standard 12-lead systems to single-lead setups, and in scenarios where only ECG, only PPG, or a combination thereof is available. These findings highlight the potential of CSFM as a versatile and scalable solution, for comprehensive cardiac monitoring.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization</title>
<link>https://arxiv.org/abs/2507.01050</link>
<guid>https://arxiv.org/abs/2507.01050</guid>
<content:encoded><![CDATA[
<div> Keywords: detoxification, social media, semantic preservation, data efficiency, toxic content

Summary:
Detoxification methods for social media content are crucial to combat the spread of toxic materials while preserving original meaning. Current approaches often struggle with balancing detoxification effectiveness, semantic preservation, and handling out-of-distribution data. They also rely on expensive manual annotations and lack data efficiency. To tackle these challenges, a two-stage training framework is proposed. It involves supervised fine-tuning on high-quality data followed by leveraging toxic inputs and a custom reward model for training a Language Model (LLM) using Group Relative Policy Optimization. The method outperforms existing approaches, achieving state-of-the-art results with improved generalization and reduced reliance on annotated data.<br /><br />Summary: <div>
arXiv:2507.01050v1 Announce Type: cross 
Abstract: The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: https://anonymous.4open.science/r/Detoxification-of-Text-725F/
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI be Consentful?</title>
<link>https://arxiv.org/abs/2507.01051</link>
<guid>https://arxiv.org/abs/2507.01051</guid>
<content:encoded><![CDATA[
<div> consent, AI systems, ethical frameworks, legal implications, data protection
Summary:
The chapter discusses the challenges arising from the evolution of AI systems in relation to traditional legal and ethical frameworks centered around consent. It highlights the inadequacy of the conventional notion of consent in addressing the implications of AI-generated content derived from personal data. The article identifies three key challenges â€“ the scope problem, the temporality problem, and the autonomy trap, collectively leading to a ''consent gap'' in AI systems. It argues that individuals cannot effectively consent to the multitude of potential outputs of their data or the extent of their use. The current legal frameworks fail to address these emerging challenges concerning individual autonomy, identity rights, and social responsibility, especially when AI-generated content creates new forms of personal representation beyond the original consent. The article underscores the need to evolve ethical and legal approaches to consent in light of broader principles of responsible AI, including fairness, transparency, accountability, and autonomy. 
<br /><br />Summary: <div>
arXiv:2507.01051v1 Announce Type: cross 
Abstract: The evolution of generative AI systems exposes the challenges of traditional legal and ethical frameworks built around consent. This chapter examines how the conventional notion of consent, while fundamental to data protection and privacy rights, proves insufficient in addressing the implications of AI-generated content derived from personal data. Through legal and ethical analysis, we show that while individuals can consent to the initial use of their data for AI training, they cannot meaningfully consent to the numerous potential outputs their data might enable or the extent to which the output is used or distributed. We identify three fundamental challenges: the scope problem, the temporality problem, and the autonomy trap, which collectively create what we term a ''consent gap'' in AI systems and their surrounding ecosystem. We argue that current legal frameworks inadequately address these emerging challenges, particularly regarding individual autonomy, identity rights, and social responsibility, especially in cases where AI-generated content creates new forms of personal representation beyond the scope of the original consent. By examining how these consent limitations intersect with broader principles of responsible AI (including fairness, transparency, accountability, and autonomy) we demonstrate the need to evolve ethical and legal approaches to consent.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals</title>
<link>https://arxiv.org/abs/2507.01052</link>
<guid>https://arxiv.org/abs/2507.01052</guid>
<content:encoded><![CDATA[
<div> memory, long-sequence, dense Hopfield networks, temporal dependencies, movie frames

Summary:
The study introduces a novel energy functional for long-sequence memory using dense Hopfield networks with higher-order interactions to achieve exponential storage capacity. A temporal kernel $K(m, k)$ is proposed to incorporate temporal dependencies, facilitating efficient sequential retrieval of patterns over extended sequences. The technique is applied successfully for storing and retrieving movie frames due to the high-dimensional vectors in each frame creating sufficient variation for sequential frames. The model enhances transformer architectures by enabling efficient long-sequence modeling, memory augmentation, improved attention with temporal bias, and better handling of long-term dependencies in time-series data. It offers a promising solution for the limitations of transformers in long-context tasks, with potential applications in natural language processing, forecasting, and other domains.<br /><br />Summary: <div>
arXiv:2507.01052v1 Announce Type: cross 
Abstract: In this study we introduce a novel energy functional for long-sequence memory, building upon the framework of dense Hopfield networks which achieves exponential storage capacity through higher-order interactions. Building upon earlier work on long-sequence Hopfield memory models, we propose a temporal kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient sequential retrieval of patterns over extended sequences. We demonstrate the successful application of this technique for the storage and sequential retrieval of movies frames which are well suited for this because of the high dimensional vectors that make up each frame creating enough variation between even sequential frames in the high dimensional space. The technique has applications in modern transformer architectures, including efficient long-sequence modeling, memory augmentation, improved attention with temporal bias, and enhanced handling of long-term dependencies in time-series data. Our model offers a promising approach to address the limitations of transformers in long-context tasks, with potential implications for natural language processing, forecasting, and beyond.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis</title>
<link>https://arxiv.org/abs/2507.01053</link>
<guid>https://arxiv.org/abs/2507.01053</guid>
<content:encoded><![CDATA[
<div> Unlock opportunities, clinical datasets, MIMIC-IV, query skills, healthcare settings
<br />
Summary:<br />
The article discusses the potential of leveraging large clinical datasets, particularly MIMIC-IV, for medical research. M3, a tool introduced in the study, aims to simplify the process of querying and understanding MIMIC-IV data. By utilizing natural language queries and the Model Context Protocol, researchers can interact with the database in a user-friendly manner. M3 translates clinical questions into SQL queries, enabling quick retrieval of structured results for nuanced cohort analyses. This approach significantly reduces the technical barriers associated with querying complex healthcare datasets and accelerates the translation of raw records into actionable insights. By democratizing access to MIMIC-IV data, M3 invites a broader research community to explore critical-care information and facilitate research in the healthcare domain.
<br /><br />Summary: <div>
arXiv:2507.01053v1 Announce Type: cross 
Abstract: As ever-larger clinical datasets become available, they have the potential to unlock unprecedented opportunities for medical research. Foremost among them is Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest open-source EHR database. However, the inherent complexity of these datasets, particularly the need for sophisticated querying skills and the need to understand the underlying clinical settings, often presents a significant barrier to their effective use. M3 lowers the technical barrier to understanding and querying MIMIC-IV data. With a single command it retrieves MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers converse with the database in plain English. Ask a clinical question in natural language; M3 uses a language model to translate it into SQL, executes the query against the MIMIC-IV dataset, and returns structured results alongside the underlying query for verifiability and reproducibility. Demonstrations show that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that once demanded hours of handcrafted SQL and relied on understanding the complexities of clinical workflows. By simplifying access, M3 invites the broader research community to mine clinical critical-care data and accelerates the translation of raw records into actionable insight.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science</title>
<link>https://arxiv.org/abs/2507.01054</link>
<guid>https://arxiv.org/abs/2507.01054</guid>
<content:encoded><![CDATA[
<div> Keywords: materials discovery, structure-based models, multimodal framework, X-ray diffraction, Alexandria dataset 

Summary: 
- The article discusses recent advances in materials discovery driven by structure-based models using crystal graphs, which are effective for computational datasets but impractical for real-world applications. 
- It proposes a scalable multimodal framework that learns directly from elemental composition and X-ray diffraction (XRD) without requiring crystal structure input, addressing the challenge of unknown or difficult-to-obtain atomic structures. 
- The architecture integrates modality-specific encoders with a cross-attention fusion module and is trained on a large dataset. 
- Self-supervised pretraining strategies, such as masked XRD modeling (MXM) and contrastive alignment, improve convergence speed, accuracy, and representation quality. 
- The study demonstrates that multimodal performance scales more favorably with dataset size than unimodal baselines, showing gains at larger data regimes and paving the way for structure-free, experimentally grounded foundation models for materials science. 

<br /><br />Summary: <div>
arXiv:2507.01054v1 Announce Type: cross 
Abstract: Recent advances in materials discovery have been driven by structure-based models, particularly those using crystal graphs. While effective for computational datasets, these models are impractical for real-world applications where atomic structures are often unknown or difficult to obtain. We propose a scalable multimodal framework that learns directly from elemental composition and X-ray diffraction (XRD) -- two of the more available modalities in experimental workflows without requiring crystal structure input. Our architecture integrates modality-specific encoders with a cross-attention fusion module and is trained on the 5-million-sample Alexandria dataset. We present masked XRD modeling (MXM), and apply MXM and contrastive alignment as self-supervised pretraining strategies. Pretraining yields faster convergence (up to 4.2x speedup) and improves both accuracy and representation quality. We further demonstrate that multimodal performance scales more favorably with dataset size than unimodal baselines, with gains compounding at larger data regimes. Our results establish a path toward structure-free, experimentally grounded foundation models for materials science.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Mechanisms in Medical Imaging: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2507.01055</link>
<guid>https://arxiv.org/abs/2507.01055</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, medical imaging, prompt engineering, model interpretability, clinical integration

Summary: 
Deep learning has great potential in medical imaging but faces challenges like data scarcity and the need for robust generalization. Prompt-based methodologies have emerged as a key strategy to address these challenges by guiding deep learning models with domain-specific prompts. This systematic review explores the landscape of prompt engineering in medical imaging, examining various prompt modalities and their integration for tasks like image generation, segmentation, and classification. These mechanisms improve task-specific outcomes by enhancing accuracy, robustness, and data efficiency while reducing manual feature engineering and improving model interpretability. Challenges include prompt design optimization, data heterogeneity, and scalability for clinical deployment. Future trajectories include advanced multimodal prompting and robust clinical integration, highlighting the crucial role of prompt-driven AI in advancing diagnostics and personalized treatment planning in medicine.<br /><br />Summary: <div>
arXiv:2507.01055v1 Announce Type: cross 
Abstract: Deep learning offers transformative potential in medical imaging, yet its clinical adoption is frequently hampered by challenges such as data scarcity, distribution shifts, and the need for robust task generalization. Prompt-based methodologies have emerged as a pivotal strategy to guide deep learning models, providing flexible, domain-specific adaptations that significantly enhance model performance and adaptability without extensive retraining. This systematic review critically examines the burgeoning landscape of prompt engineering in medical imaging. We dissect diverse prompt modalities, including textual instructions, visual prompts, and learnable embeddings, and analyze their integration for core tasks such as image generation, segmentation, and classification. Our synthesis reveals how these mechanisms improve task-specific outcomes by enhancing accuracy, robustness, and data efficiency and reducing reliance on manual feature engineering while fostering greater model interpretability by making the model's guidance explicit. Despite substantial advancements, we identify persistent challenges, particularly in prompt design optimization, data heterogeneity, and ensuring scalability for clinical deployment. Finally, this review outlines promising future trajectories, including advanced multimodal prompting and robust clinical integration, underscoring the critical role of prompt-driven AI in accelerating the revolution of diagnostics and personalized treatment planning in medicine.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval</title>
<link>https://arxiv.org/abs/2507.01058</link>
<guid>https://arxiv.org/abs/2507.01058</guid>
<content:encoded><![CDATA[
<div> Keywords: judiciary, Data Science, legal cases, summarization, retrieval

Summary: 
The research introduces a framework utilizing Data Science methods to enhance the analysis of Calcutta High Court verdicts. It focuses on creating a summarization mechanism to condense legal texts into concise summaries and an intelligent system for retrieving similar cases. By fine-tuning the Pegasus model with case head note summaries, the framework significantly improves the summarization process, preserving legal contexts for effective retrieval. The Retrieval-Augmented Generation (RAG) technique efficiently retrieves similar cases in response to user queries, providing thorough overviews and summaries. This framework aims to enhance legal research efficiency, assist legal professionals in decision-making, and help students acquire key legal information easily, benefiting the legal landscape. 

Summary: <div>
arXiv:2507.01058v1 Announce Type: cross 
Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising amount of legal issues, needing careful use of judicial resources. This research presents a complex framework that leverages Data Science methodologies, notably Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta High Court verdicts. Our framework focuses on two key aspects: first, the creation of a robust summarization mechanism that distills complex legal texts into concise and coherent summaries; and second, the development of an intelligent system for retrieving similar cases, which will assist legal professionals in research and decision making. By fine-tuning the Pegasus model using case head note summaries, we achieve significant improvements in the summarization of legal cases. Our two-step summarizing technique preserves crucial legal contexts, allowing for the production of a comprehensive vector database for RAG. The RAG-powered framework efficiently retrieves similar cases in response to user queries, offering thorough overviews and summaries. This technique not only improves legal research efficiency, but it also helps legal professionals and students easily acquire and grasp key legal information, benefiting the overall legal scenario.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Vehicles Should be Connected with Natural Language</title>
<link>https://arxiv.org/abs/2507.01059</link>
<guid>https://arxiv.org/abs/2507.01059</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent collaborative driving, Natural language communication, Decision-level fusion, Intelligent transportation systems, Traffic safety

Summary:
Collaborative driving involving multiple agents can enhance traffic safety and efficiency but faces challenges related to communication efficiency, information completeness, and agent interoperability. Existing communication methods like raw sensor data and neural network features have limitations. This paper argues for a shift towards using natural language for explicit intent and reasoning communication. Natural language offers a balance between semantic density and communication bandwidth, adapts to real-time conditions, and bridges the gap between different agent platforms. By enabling direct communication of intentions, rationales, and decisions, natural language communication can transform collaborative driving from reactive perception-data sharing to proactive coordination. This transition can enhance safety, efficiency, and transparency in intelligent transportation systems.<br /><br />Summary: <div>
arXiv:2507.01059v1 Announce Type: cross 
Abstract: Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epitome: Pioneering an Experimental Platform for AI-Social Science Integration</title>
<link>https://arxiv.org/abs/2507.01061</link>
<guid>https://arxiv.org/abs/2507.01061</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social science experiments, Epitome, human-AI interactions, interdisciplinary research <br />
<br />
Summary: Epitome is the first experimental platform integrating Large Language Models (LLMs) and social science, aiming to understand human-AI interactions and their societal impacts. It offers a comprehensive solution for conducting experiments through cross-disciplinary collaboration, focusing on the interactive effects of AI on individuals, organizations, and society. The platform supports complex experimental designs, including dialogues, group chats, and multi-agent scenarios, rooted in social science experimental logic. By replicating seminal social science experiments involving LLMs, Epitome demonstrates its ability to streamline experimental processes and produce robust results for top journals. It enhances the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome serves as a valuable tool for interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making. <br /><br /> <div>
arXiv:2507.01061v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review</title>
<link>https://arxiv.org/abs/2507.01062</link>
<guid>https://arxiv.org/abs/2507.01062</guid>
<content:encoded><![CDATA[
<div> perceptions, GenAI, higher education, learning outcomes, student attitudes
<br />
Summary:
The paper examines student perceptions of generative artificial intelligence (GenAI) use in higher education through a systematic literature review and simulation-based modeling. Nineteen empirical articles were reviewed, with six providing enough quantitative data for probabilistic modeling. Usability and real-world usefulness were found to be key predictors of positive learning achievements, surpassing affective or trust-based factors. A composite "Success Score" was generated through simulation to forecast the relationship between student perceptions and learning outcomes. The study highlights the importance of interdisciplinary perspectives in understanding the impact of GenAI tools in educational settings, addressing longstanding debates on their integration within universities. <div>
arXiv:2507.01062v1 Announce Type: cross 
Abstract: The exponential development of generative artificial intelligence (GenAI) technologies like ChatGPT has raised increasing curiosity about their use in higher education, specifically with respect to how students view them, make use of them, and the implications for learning outcomes. This paper employs a hybrid methodological approach involving a systematic literature review and simulation-based modeling to explore student perceptions of GenAI use in the context of higher education. A total of nineteen empirical articles from 2023 through 2025 were selected from the PRISMA-based search targeting the Scopus database. Synthesis of emerging patterns from the literature was achieved by thematic categorization. Six of these had enough quantitative information, i.e., item-level means and standard deviations, to permit probabilistic modeling. One dataset, from the resulting subset, was itself selected as a representative case with which to illustrate inverse-variance weighting by Monte Carlo simulation, by virtue of its well-designed Likert scale format and thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength of the relationship between student perceptions and learning achievements. Findings reveal that attitude factors concerned with usability and real-world usefulness are significantly better predictors of positive learning achievement than affective or trust-based factors. Such an interdisciplinary perspective provides a unique means of linking thematic results with predictive modelling, resonating with longstanding controversies about the proper use of GenAI tools within the university.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations</title>
<link>https://arxiv.org/abs/2507.01063</link>
<guid>https://arxiv.org/abs/2507.01063</guid>
<content:encoded><![CDATA[
<div> Keywords: online dating, recommendation systems, algorithmic biases, fairness evaluation, collaborative filtering

Summary:<br /><br />Online dating platforms have revolutionized how romantic relationships are formed, with algorithmic matching systems playing a crucial role. However, current recommendation systems in dating apps suffer from various algorithmic deficiencies, such as popularity bias and inadequate models for reciprocity. This research provides a comprehensive analysis of these systems, identifying key issues and proposing research-backed solutions. By evaluating reciprocal recommendation frameworks and fairness metrics, the study shows that current systems have modest performance, with collaborative filtering reaching 25.1% accuracy and reciprocal methods achieving 28.7%. To address these limitations, a mathematical framework is proposed, incorporating enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms. These enhancements aim to maintain competitive accuracy while reducing demographic biases in recommendation systems. <div>
arXiv:2507.01063v1 Announce Type: cross 
Abstract: Online dating platforms have fundamentally transformed the formation of romantic relationships, with millions of users worldwide relying on algorithmic matching systems to find compatible partners. However, current recommendation systems in dating applications suffer from significant algorithmic deficiencies, including but not limited to popularity bias, filter bubble effects, and inadequate reciprocity modeling that limit effectiveness and introduce harmful biases. This research integrates foundational work with recent empirical findings to deliver a detailed analysis of dating app recommendation systems, highlighting key issues and suggesting research-backed solutions. Through analysis of reciprocal recommendation frameworks, fairness evaluation metrics, and industry implementations, we demonstrate that current systems achieve modest performance with collaborative filtering reaching 25.1\% while reciprocal methods achieve 28.7\%. Our proposed mathematical framework addresses these limitations through enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms that maintain competitive accuracy while improving demographic representation to reduce algorithmic bias.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services</title>
<link>https://arxiv.org/abs/2507.01067</link>
<guid>https://arxiv.org/abs/2507.01067</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, rare events, foundational model, sporadic outages, machine learning services

Summary: 
This paper explores the use of a state-of-the-art foundational model for forecasting rare, sporadic production outages in high-performance machine learning services. The study compares the forecasting errors of the foundational model with classical stochastic models like moving average and autoregressive models. The analysis highlights the performance of each model for predicting sporadic or spiky events and identifies key patterns in the data that are well captured by the foundational model. By optimizing the models with optimal parameters, the study achieves less than 6% value errors in estimating a year-long outage statistics for a specific root cause. Overall, the paper demonstrates the capability of foundational models in forecasting rare events and provides insights into their effectiveness compared to traditional stochastic forecasting models. 

Summary: <div>
arXiv:2507.01067v1 Announce Type: cross 
Abstract: Time series forecasting models have diverse real world applications (e.g., from electricity metrics to software workload). Latest foundational models trained for time series forecasting show strengths (e.g., for long sequences and in zero-shot settings). However, foundational model was not yet used for forecasting rare, spiky events, i.e., a challenging target because those are a corner case of extreme events. In this paper, we optimize a state-of-the-art foundational model to forecast sporadic or spiky production outages of high-performance machine learning services powering billions of client devices. We evaluate the forecasting errors of the foundational model compared with classical stochastic forecasting models (e.g., moving average and autoregressive). The analysis helps us understand how each of the evaluated models performs for the sporadic or spiky events. For example, it identifies the key patterns in the target data that are well tracked by the foundational model vs. each of the stochastic models. We use the models with optimal parameters to estimate a year-long outage statistics of a particular root cause with less than 6% value errors.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem</title>
<link>https://arxiv.org/abs/2507.01076</link>
<guid>https://arxiv.org/abs/2507.01076</guid>
<content:encoded><![CDATA[
<div> greedy heuristic, hypergraph-based approximation, genetic algorithm, NP-complete, mutual-visibility

Summary: 
The paper evaluates three algorithms for the NP-complete mutual-visibility (MV) problem: a greedy heuristic, a hypergraph-based approximation, and a genetic algorithm. The algorithms are tested on synthetic graph datasets, including graphs with known $\mu(G)$ values. For smaller graphs, the algorithms align with theoretical bounds, but for larger instances, solution sizes deviate from limits, making quality assessment challenging. Despite the lack of tight bounds, the Genetic Algorithm and other heuristics perform best on known optimal graphs. The study fills a gap in empirical analysis of the MV problem, highlighting the discrepancies between theoretical and practical behavior of the algorithms. <div>
arXiv:2507.01076v1 Announce Type: cross 
Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms - a direct greedy heuristic, a hypergraph-based approximation, and a genetic algorithm - on diverse synthetic graph datasets, including those with analytically known $\mu(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma</title>
<link>https://arxiv.org/abs/2507.01081</link>
<guid>https://arxiv.org/abs/2507.01081</guid>
<content:encoded><![CDATA[
<div> Keywords: trauma, digital treatments, AI guidance, pupillometry, intrusive memories

Summary:
AI and neurotechnology may offer a scalable alternative to human guides in delivering digital treatments for trauma, as demonstrated by the ANTIDOTE system. A study involving 100 participants showed that the AI-guided Imagery Competing Task Intervention (ICTI) effectively reduced intrusive memories after exposure to traumatic stimuli. The intervention group reported fewer intrusive memories compared to the control group, indicating the success of the AI guidance. Pupillometry was used to track engagement with the intervention, with pupil size predicting symptom reduction and potentially serving as a biomarker of intervention effectiveness. These findings suggest that AI-guided digital interventions have the potential to be effective in addressing trauma on a broader scale, offering personalized and scalable solutions for individuals in need. 

<br /><br />Summary: <div>
arXiv:2507.01081v1 Announce Type: cross 
Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can help, but most require human guidance. Human guides provide tailored instructions and responsiveness to internal cognitive states, but limit scalability. Can generative AI and neurotechnology provide a scalable alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to automatically deliver and monitor an evidence-based digital treatment, specifically the Imagery Competing Task Intervention (ICTI), to reduce intrusive memories after psychological trauma. One hundred healthy volunteers were exposed to videos of traumatic events and randomly assigned to an intervention or active control condition. As predicted, intervention participants reported significantly fewer intrusive memories over the following week. Post-hoc assessment against clinical rubrics confirmed the AI guide delivered the intervention successfully. Additionally, pupil size tracked intervention engagement and predicted symptom reduction, providing a candidate biomarker of intervention effectiveness. These findings open a path toward rigorous AI-guided digital interventions that can scale to trauma prevalence.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-aware 4D Video Generation for Robot Manipulation</title>
<link>https://arxiv.org/abs/2507.01099</link>
<guid>https://arxiv.org/abs/2507.01099</guid>
<content:encoded><![CDATA[
<div> supervised learning, video generation, multi-view consistency, 4D videos, robot manipulation
Summary:<br /><br />Researchers propose a 4D video generation model aiming to improve the generation of dynamic scenes with temporal coherence and geometric consistency across multiple camera views. The model utilizes cross-view pointmap alignment as geometric supervision during training to learn a shared 3D representation of the scene. This approach allows the model to predict future video sequences from new viewpoints without needing camera poses as inputs. The method outperforms existing baselines by producing visually stable and spatially aligned predictions on various robotic datasets. Additionally, the predicted 4D videos enable the recovery of robot end-effector trajectories using a standard 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.
 <div>
arXiv:2507.01099v1 Announce Type: cross 
Abstract: Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning</title>
<link>https://arxiv.org/abs/2507.01196</link>
<guid>https://arxiv.org/abs/2507.01196</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation Models, Brainwave Modeling, Brain-Computer Interface, Large Brainwave Foundation Models, Low-Rank Adaptation

Summary:
Foundation Models are widely successful in AI but their effectiveness in modeling brainwaves is unclear. Large Brainwave Foundation Models (LBMs) were evaluated through fine-tuning experiments on BCI tasks, showing marginal improvements over traditional models with significantly more parameters. Ablation studies and Low-Rank Adaptation (LoRA) reduced parameters without performance loss, highlighting inefficiencies in LBMs. Training strategies for BCI applications were explored through model fine-tuning and parameter-efficient techniques. LoRA applied to LBMs showed performance benefits when adapting multiple neural network components simultaneously. Domain-specific development strategies are needed to enhance LBMs, suggesting possible redesign of current architectures for better utilization in brainwave analysis. These findings emphasize the importance of optimizing LBMs for BCI applications. 

<br /><br />Summary: <div>
arXiv:2507.01196v1 Announce Type: cross 
Abstract: Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require redesign to fully leverage the potential of foundation models in brainwave analysis.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives</title>
<link>https://arxiv.org/abs/2507.01198</link>
<guid>https://arxiv.org/abs/2507.01198</guid>
<content:encoded><![CDATA[
<div> sampling-based planning, search-based planning, motion planning algorithm, robotic manipulators, configuration space

Summary:
This work introduces a novel motion planning algorithm for robotic manipulators by integrating sampling-based and search-based planning techniques. The key innovation lies in the utilization of adaptive motion primitives called burs of free configuration space (C-space) within the graph search algorithm. Burs have the ability to dynamically expand in free C-space, facilitating more efficient exploration of the configuration space compared to fixed-sized motion primitives. The algorithm is implemented in the SMPL library and tested across various scenarios with manipulators of different degrees-of-freedom (DoF) and environmental complexities. Results indicate that the bur-based approach surpasses fixed-primitive planning in intricate scenarios, particularly for high DoF manipulators, while delivering comparable performance in simpler scenarios. This research contributes to enhancing motion planning efficiency and effectiveness for robotic manipulators, offering promising prospects for real-world applications. 

Summary: <div>
arXiv:2507.01198v1 Announce Type: cross 
Abstract: This work proposes a motion planning algorithm for robotic manipulators that combines sampling-based and search-based planning methods. The core contribution of the proposed approach is the usage of burs of free configuration space (C-space) as adaptive motion primitives within the graph search algorithm. Due to their feature to adaptively expand in free C-space, burs enable more efficient exploration of the configuration space compared to fixed-sized motion primitives, significantly reducing the time to find a valid path and the number of required expansions. The algorithm is implemented within the existing SMPL (Search-Based Motion Planning Library) library and evaluated through a series of different scenarios involving manipulators with varying number of degrees-of-freedom (DoF) and environment complexity. Results demonstrate that the bur-based approach outperforms fixed-primitive planning in complex scenarios, particularly for high DoF manipulators, while achieving comparable performance in simpler scenarios.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration</title>
<link>https://arxiv.org/abs/2507.01225</link>
<guid>https://arxiv.org/abs/2507.01225</guid>
<content:encoded><![CDATA[
<div> Cloud computing, capacity planning, job scheduling, grid computing, finance industry

Summary:
 
- The organization follows a hybrid approach utilizing both cloud and on-prem servers for job scheduling and task performance.
- The focus is on estimating resource requirements and scheduling for on-prem grid computing environments.
- An important aspect of the approach is addressing uncertainty in resource usage and job duration, particularly relevant in finance where market conditions are stochastic.
- The objective is to balance minimizing resource usage with providing high quality-of-service by completing jobs within requested deadlines.
- Proposed approximate approaches, including deterministic estimators and pair sampling-based constraint programming, show significantly lower peak resource usage while maintaining quality-of-service levels.<br /><br />Summary: <div>
arXiv:2507.01225v1 Announce Type: cross 
Abstract: Organizations around the world schedule jobs (programs) regularly to perform various tasks dictated by their end users. With the major movement towards using a cloud computing infrastructure, our organization follows a hybrid approach with both cloud and on-prem servers. The objective of this work is to perform capacity planning, i.e., estimate resource requirements, and job scheduling for on-prem grid computing environments. A key contribution of our approach is handling uncertainty in both resource usage and duration of the jobs, a critical aspect in the finance industry where stochastic market conditions significantly influence job characteristics. For capacity planning and scheduling, we simultaneously balance two conflicting objectives: (a) minimize resource usage, and (b) provide high quality-of-service to the end users by completing jobs by their requested deadlines. We propose approximate approaches using deterministic estimators and pair sampling-based constraint programming. Our best approach (pair sampling-based) achieves much lower peak resource usage compared to manual scheduling without compromising on the quality-of-service.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW</title>
<link>https://arxiv.org/abs/2507.01241</link>
<guid>https://arxiv.org/abs/2507.01241</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic gradient descent, large language models, adaptive sampling, nonconvex optimization, scalability

Summary: 
The paper introduces a new method, stochastic conjugate subgradient, tailored specifically for training large language models (LLMs) that addresses potential limitations of traditional stochastic gradient-based descent (SGD) techniques. This method offers faster convergence per iteration and improved scalability by adaptively choosing sample sizes, utilizing stochastic conjugate subgradient for search directions, and incorporating an AdamW-like algorithm for adaptive step size adjustments. By leveraging sample complexity analysis, the method effectively handles the nonconvex and non-smooth nature of LLM training. Experimental results demonstrate that the proposed method not only maintains but often exceeds the scalability of SGD techniques, significantly enhancing optimization speed and accuracy in LLM training.<br /><br />Summary: <div>
arXiv:2507.01241v1 Announce Type: cross 
Abstract: Stochastic gradient-based descent (SGD), have long been central to training large language models (LLMs). However, their effectiveness is increasingly being questioned, particularly in large-scale applications where empirical evidence suggests potential performance limitations. In response, this paper proposes a stochastic conjugate subgradient method together with adaptive sampling tailored specifically for training LLMs. The method not only achieves faster convergence per iteration but also demonstrates improved scalability compared to traditional SGD techniques. It leverages sample complexity analysis to adaptively choose the sample size, employs a stochastic conjugate subgradient approach to determine search directions and utilizing an AdamW-like algorithm to adaptively adjust step sizes. This approach preserves the key advantages of first-order methods while effectively addressing the nonconvexity and non-smoothness inherent in LLMs training. Additionally, we provide a detailed analysis of the advantage of the algorithm. Experimental results show that the proposed method not only maintains, but in many cases surpasses, the scalability of traditional SGD techniques, significantly enhancing both the speed and accuracy of the optimization process.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant</title>
<link>https://arxiv.org/abs/2507.01259</link>
<guid>https://arxiv.org/abs/2507.01259</guid>
<content:encoded><![CDATA[
<div> Keywords: legal information retrieval, case law, statute law, gAIus architecture, LLM-based agent

Summary:
In this paper, the authors explore how large language models can effectively handle legal matters in non-English and non-Chinese speaking countries. They discuss the history and differences between case law and statute law, emphasizing their impact on legal tasks. The authors introduce gAIus, a cognitive LLM-based agent that retrieves knowledge from the Polish Civil Code to provide answers with proper references. They propose a more explainable and human-friendly retrieval mechanism that outperforms embedding-based approaches. Evaluating their method on a specialized dataset from law apprenticeship entrance exams in Poland, the authors demonstrate significant improvements in LLM performance. The gAIus architecture enhances the capabilities of existing large language models, surpassing gpt-4o and gpt-4o-mini in accuracy. The paper concludes with insights on future research directions and potential applications of their findings. 

<br /><br />Summary: <div>
arXiv:2507.01259v1 Announce Type: cross 
Abstract: In this paper we discuss the capability of large language models to base their answer and provide proper references when dealing with legal matters of non-english and non-chinese speaking country. We discuss the history of legal information retrieval, the difference between case law and statute law, its impact on the legal tasks and analyze the latest research in this field. Basing on that background we introduce gAIus, the architecture of the cognitive LLM-based agent, whose responses are based on the knowledge retrieved from certain legal act, which is Polish Civil Code. We propose a retrieval mechanism which is more explainable, human-friendly and achieves better results than embedding-based approaches. To evaluate our method we create special dataset based on single-choice questions from entrance exams for law apprenticeships conducted in Poland. The proposed architecture critically leveraged the abilities of used large language models, improving the gpt-3.5-turbo-0125 by 419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%. At the end of our paper we show the possible future path of research and potential applications of our findings.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Realistic Safety-Critical Driving Video Generation</title>
<link>https://arxiv.org/abs/2507.01264</link>
<guid>https://arxiv.org/abs/2507.01264</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving systems, scenario generation, Large Language Models, CARLA simulator, safety-critical scenarios

Summary: 
Large Language Models (LLMs) are used in a novel framework to generate safety-critical driving scenarios for autonomous vehicles within the CARLA simulator. The framework allows for the efficient generation of scenario scripts through code generation based on a few example prompts and code samples. By focusing on collision events, the generated scenarios cover a wide range of realistic and diverse situations, including rare but critical edge cases. Integration with a video generation pipeline further enhances the realism of the simulated driving videos. The approach enables the creation of controllable scenarios with realistic physical dynamics, such as pedestrian crossings under occlusion or sudden vehicle cut-ins. Experimental results demonstrate the effectiveness of the method in generating diverse and safety-critical scenarios, providing a promising tool for simulation-based testing of autonomous vehicles.<br /><br />Summary: <div>
arXiv:2507.01264v1 Announce Type: cross 
Abstract: Designing diverse and safety-critical driving scenarios is essential for evaluating autonomous driving systems. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) for few-shot code generation to automatically synthesize driving scenarios within the CARLA simulator, which has flexibility in scenario scripting, efficient code-based control of traffic participants, and enforcement of realistic physical dynamics. Given a few example prompts and code samples, the LLM generates safety-critical scenario scripts that specify the behavior and placement of traffic participants, with a particular focus on collision events. To bridge the gap between simulation and real-world appearance, we integrate a video generation pipeline using Cosmos-Transfer1 with ControlNet, which converts rendered scenes into realistic driving videos. Our approach enables controllable scenario generation and facilitates the creation of rare but critical edge cases, such as pedestrian crossings under occlusion or sudden vehicle cut-ins. Experimental results demonstrate the effectiveness of our method in generating a wide range of realistic, diverse, and safety-critical scenarios, offering a promising tool for simulation-based testing of autonomous vehicles.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning</title>
<link>https://arxiv.org/abs/2507.01271</link>
<guid>https://arxiv.org/abs/2507.01271</guid>
<content:encoded><![CDATA[
<div> unlearning, large language models, multimodal models, PULSE protocol, sustainability evaluation<br />
<br />
Summary: <br />
In the study, a new evaluation framework for unlearning in large multimodal models (LMMs) is introduced, focusing on realistic scenarios. The PULSE protocol considers unlearning pre-trained knowledge and evaluates long-term sustainability through sequential requests. Existing unlearning methods are assessed across these dimensions, revealing challenges in eliminating pre-training knowledge and maintaining performance when unlearning sequential data. While some techniques are successful in unlearning fine-tuned knowledge, they struggle with pre-training information. Additionally, methods effective in batch unlearning show performance degradation when data are split and unlearned sequentially. This study highlights the need for improved unlearning techniques in LMMs to address privacy and copyright concerns effectively. <br /> <div>
arXiv:2507.01271v1 Announce Type: cross 
Abstract: In recent years, unlearning techniques, which are methods for inducing a model to "forget" previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance</title>
<link>https://arxiv.org/abs/2507.01274</link>
<guid>https://arxiv.org/abs/2507.01274</guid>
<content:encoded><![CDATA[
<div> AI, maritime training, performance assessment, visual focus tracking, stress detection <br />
<br />
Summary: 
This study introduces an AI-driven framework for enhancing maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection. The system integrates AI techniques such as visual focus determination, communication analysis, and stress detection, achieving high accuracy levels in each area. By providing insights into visual attention, communication correctness, and stress levels during high-stress scenarios, the AI algorithms deliver objective performance analytics. This transformative approach enables personalized feedback and improves preparedness for real-world operational challenges in the maritime industry. <br /> <div>
arXiv:2507.01274v1 Announce Type: cross 
Abstract: Traditional simulator-based training for maritime professionals is critical for ensuring safety at sea but often depends on subjective trainer assessments of technical skills, behavioral focus, communication, and body language, posing challenges such as subjectivity, difficulty in measuring key features, and cognitive limitations. Addressing these issues, this study develops an AI-driven framework to enhance maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection, improving readiness for high-risk scenarios. The system integrates AI techniques, including visual focus determination using eye tracking, pupil dilation analysis, and computer vision; communication analysis through a maritime-specific speech-to-text model and natural language processing; communication correctness using large language models; and mental stress detection via vocal pitch. Models were evaluated on data from simulated maritime scenarios with seafarers exposed to controlled high-stress events. The AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for maritime speech recognition, and ~90% for stress detection, surpassing existing benchmarks. The system provides insights into visual attention, adherence to communication checklists, and stress levels under demanding conditions. This study demonstrates how AI can transform maritime training by delivering objective performance analytics, enabling personalized feedback, and improving preparedness for real-world operational challenges.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization</title>
<link>https://arxiv.org/abs/2507.01281</link>
<guid>https://arxiv.org/abs/2507.01281</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, RAG, large language models, LLMs, knowledge conflicts, generation reliability, CARE-RAG, Conflict-Aware and Reliable Evidence for RAG, trustworthiness, Conflict-Driven Summarization, parameter-aware evidence, context-aware evidence, conflict-driven summarization, QA Repair, evaluation integrity

Summary:
CARE-RAG is a novel framework that enhances the trustworthiness of large language models by considering all evidence, including internal knowledge and retrieved content. It addresses knowledge conflicts by deriving parameter-aware evidence, refining retrieved evidences, and detecting conflicts through conflict-driven summarization. Additionally, a QA Repair step corrects outdated or ambiguous benchmark answers. Experiments show that CARE-RAG outperforms strong RAG baselines, particularly in scenarios with noisy or conflicting evidence. The framework effectively integrates internal and retrieved content to improve the reliability of generation in large language models. <div>
arXiv:2507.01281v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating their parametric knowledge with external retrieved content. However, knowledge conflicts caused by internal inconsistencies or noisy retrieved content can severely undermine the generation reliability of RAG systems.In this work, we argue that LLMs should rethink all evidence, including both retrieved content and internal knowledge, before generating responses.We propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel framework that improves trustworthiness through Conflict-Driven Summarization of all available evidence.CARE-RAG first derives parameter-aware evidence by comparing parameter records to identify diverse internal perspectives. It then refines retrieved evidences to produce context-aware evidence, removing irrelevant or misleading content. To detect and summarize conflicts, we distill a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable synthesis across multiple sources.To further ensure evaluation integrity, we introduce a QA Repair step to correct outdated or ambiguous benchmark answers.Experiments on revised QA datasets with retrieval data show that CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process</title>
<link>https://arxiv.org/abs/2507.01284</link>
<guid>https://arxiv.org/abs/2507.01284</guid>
<content:encoded><![CDATA[
arXiv:2507.01284v1 Announce Type: cross 
Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their integration with diverse systems. The internet-scale general knowledge encapsulated within these models presents significant opportunities for enhancing autonomous driving perception, prediction, and planning capabilities. In this paper we propose VLAD, a vision-language autonomous driving model, which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end system. We implement a specialized fine-tuning approach using custom question-answer datasets designed specifically to improve the spatial reasoning capabilities of the model. The enhanced VLM generates high-level navigational commands that VAD subsequently processes to guide vehicle operation. Additionally, our system produces interpretable natural language explanations of driving decisions, thereby increasing transparency and trustworthiness of the traditionally black-box end-to-end architecture. Comprehensive evaluation on the real-world nuScenes dataset demonstrates that our integrated system reduces average collision rates by 31.82% compared to baseline methodologies, establishing a new benchmark for VLM-augmented autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Hamiltonian Operator</title>
<link>https://arxiv.org/abs/2507.01313</link>
<guid>https://arxiv.org/abs/2507.01313</guid>
<content:encoded><![CDATA[
arXiv:2507.01313v1 Announce Type: cross 
Abstract: Stochastic control problems in high dimensions are notoriously difficult to solve due to the curse of dimensionality. An alternative to traditional dynamic programming is Pontryagin's Maximum Principle (PMP), which recasts the problem as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In this paper, we introduce a formal framework for solving such problems with deep learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This operator parameterizes the coupled FBSDE dynamics via neural networks that represent the feedback control and an ansatz for the value function's spatial gradient. We show how the optimal NHO can be found by training the underlying networks to enforce the consistency conditions dictated by the PMP. By adopting this operator-theoretic view, we situate the deep FBSDE method within the rigorous language of statistical inference, framing it as a problem of learning an unknown operator from simulated data. This perspective allows us to prove the universal approximation capabilities of NHOs under general martingale drivers and provides a clear lens for analyzing the significant optimization challenges inherent to this class of models.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks</title>
<link>https://arxiv.org/abs/2507.01321</link>
<guid>https://arxiv.org/abs/2507.01321</guid>
<content:encoded><![CDATA[
arXiv:2507.01321v1 Announce Type: cross 
Abstract: In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theoretical analysis, we derive an upper bound for ICL backdoor effects, revealing that the vulnerability is dominated by the concept preference ratio between the task and the backdoor. Motivated by these findings, we propose ICLShield, a defense mechanism that dynamically adjusts the concept preference ratio. Our method encourages LLMs to select clean demonstrations during the ICL phase by leveraging confidence and similarity scores, effectively mitigating susceptibility to backdoor attacks. Extensive experiments across multiple LLMs and tasks demonstrate that our method achieves state-of-the-art defense effectiveness, significantly outperforming existing approaches (+26.02% on average). Furthermore, our method exhibits exceptional adaptability and defensive performance even for closed-source models (e.g., GPT-4).
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy</title>
<link>https://arxiv.org/abs/2507.01327</link>
<guid>https://arxiv.org/abs/2507.01327</guid>
<content:encoded><![CDATA[
arXiv:2507.01327v1 Announce Type: cross 
Abstract: Detecting abnormal events in real-world customer service dialogues is highly challenging due to the complexity of business data and the dynamic nature of customer interactions. Moreover, models must demonstrate strong out-of-domain (OOD) generalization to enable rapid adaptation across different business scenarios and maximize commercial value. In this work, we propose a novel Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that leverages the advanced reasoning capabilities of large language models for abnormal event detection. APARL introduces a dual-loop dynamic curriculum learning architecture, enabling the model to progressively focus on more challenging samples as its proficiency increases. This design effectively addresses performance bottlenecks and significantly enhances OOD transferability. Extensive evaluations on food delivery dialogue tasks show that our model achieves significantly enhanced adaptability and robustness, attaining the highest F1 score with an average improvement of 17.19\%, and an average improvement of 9.59\% in OOD transfer tests. This method provides a superior solution for industrial deployment of anomaly detection models, contributing to improved operational efficiency and commercial benefits.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEDOM: An Open and Fundamental Reverse Language Model</title>
<link>https://arxiv.org/abs/2507.01335</link>
<guid>https://arxiv.org/abs/2507.01335</guid>
<content:encoded><![CDATA[
arXiv:2507.01335v1 Announce Type: cross 
Abstract: We introduce LEDOM, the first purely reverse language model, trained autoregressively on 435B tokens with 2B and 7B parameter variants, which processes sequences in reverse temporal order through previous token prediction. For the first time, we present the reverse language model as a potential foundational model across general tasks, accompanied by a set of intriguing examples and insights. Based on LEDOM, we further introduce a novel application: Reverse Reward, where LEDOM-guided reranking of forward language model outputs leads to substantial performance improvements on mathematical reasoning tasks. This approach leverages LEDOM's unique backward reasoning capability to refine generation quality through posterior evaluation. Our findings suggest that LEDOM exhibits unique characteristics with broad application potential. We will release all models, training code, and pre-training data to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User-guided Generative Source Separation</title>
<link>https://arxiv.org/abs/2507.01339</link>
<guid>https://arxiv.org/abs/2507.01339</guid>
<content:encoded><![CDATA[
arXiv:2507.01339v1 Announce Type: cross 
Abstract: Music source separation (MSS) aims to extract individual instrument sources from their mixture. While most existing methods focus on the widely adopted four-stem separation setup (vocals, bass, drums, and other instruments), this approach lacks the flexibility needed for real-world applications. To address this, we propose GuideSep, a diffusion-based MSS model capable of instrument-agnostic separation beyond the four-stem setup. GuideSep is conditioned on multiple inputs: a waveform mimicry condition, which can be easily provided by humming or playing the target melody, and mel-spectrogram domain masks, which offer additional guidance for separation. Unlike prior approaches that relied on fixed class labels or sound queries, our conditioning scheme, coupled with the generative approach, provides greater flexibility and applicability. Additionally, we design a mask-prediction baseline using the same model architecture to systematically compare predictive and generative approaches. Our objective and subjective evaluations demonstrate that GuideSep achieves high-quality separation while enabling more versatile instrument extraction, highlighting the potential of user participation in the diffusion-based generative process for MSS. Our code and demo page are available at https://yutongwen.github.io/GuideSep/
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy</title>
<link>https://arxiv.org/abs/2507.01352</link>
<guid>https://arxiv.org/abs/2507.01352</guid>
<content:encoded><![CDATA[
arXiv:2507.01352v1 Announce Type: cross 
Abstract: Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</title>
<link>https://arxiv.org/abs/2507.01378</link>
<guid>https://arxiv.org/abs/2507.01378</guid>
<content:encoded><![CDATA[
arXiv:2507.01378v1 Announce Type: cross 
Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Soft Actor-Critic with Diffusion Policy</title>
<link>https://arxiv.org/abs/2507.01381</link>
<guid>https://arxiv.org/abs/2507.01381</guid>
<content:encoded><![CDATA[
arXiv:2507.01381v1 Announce Type: cross 
Abstract: Reinforcement learning has been proven to be highly effective in handling complex control tasks. Traditional methods typically use unimodal distributions, such as Gaussian distributions, to model the output of value distributions. However, unimodal distribution often and easily causes bias in value function estimation, leading to poor algorithm performance. This paper proposes a distributional reinforcement learning algorithm called DSAC-D (Distributed Soft Actor Critic with Diffusion Policy) to address the challenges of estimating bias in value functions and obtaining multimodal policy representations. A multimodal distributional policy iteration framework that can converge to the optimal policy was established by introducing policy entropy and value distribution function. A diffusion value network that can accurately characterize the distribution of multi peaks was constructed by generating a set of reward samples through reverse sampling using a diffusion model. Based on this, a distributional reinforcement learning algorithm with dual diffusion of the value network and the policy network was derived. MuJoCo testing tasks demonstrate that the proposed algorithm not only learns multimodal policy, but also achieves state-of-the-art (SOTA) performance in all 9 control tasks, with significant suppression of estimation bias and total average return improvement of over 10\% compared to existing mainstream algorithms. The results of real vehicle testing show that DSAC-D can accurately characterize the multimodal distribution of different driving styles, and the diffusion policy network can characterize multimodal trajectories.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound</title>
<link>https://arxiv.org/abs/2507.01401</link>
<guid>https://arxiv.org/abs/2507.01401</guid>
<content:encoded><![CDATA[
arXiv:2507.01401v1 Announce Type: cross 
Abstract: Fetal abdominal malformations are serious congenital anomalies that require accurate diagnosis to guide pregnancy management and reduce mortality. Although AI has demonstrated significant potential in medical diagnosis, its application to prenatal abdominal anomalies remains limited. Most existing studies focus on image-level classification and rely on standard plane localization, placing less emphasis on case-level diagnosis. In this paper, we develop a case-level multiple instance learning (MIL)-based method, free of standard plane localization, for classifying fetal abdominal anomalies in prenatal ultrasound. Our contribution is three-fold. First, we adopt a mixture-of-attention-experts module (MoAE) to weight different attention heads for various planes. Secondly, we propose a medical-knowledge-driven feature selection module (MFS) to align image features with medical knowledge, performing self-supervised image token selection at the case-level. Finally, we propose a prompt-based prototype learning (PPL) to enhance the MFS. Extensively validated on a large prenatal abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748 images and 6 categories, our proposed method outperforms the state-of-the-art competitors. Codes are available at:https://github.com/LL-AC/AAcls.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping</title>
<link>https://arxiv.org/abs/2507.01411</link>
<guid>https://arxiv.org/abs/2507.01411</guid>
<content:encoded><![CDATA[
arXiv:2507.01411v1 Announce Type: cross 
Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging, yet understanding the corresponding changes in its functional connectivity remains limited. Seed-based functional connectivity (FC) analysis enables voxel-wise mapping of the hippocampus's synchronous activity with cortical regions, offering a window into functional reorganization during aging. In this study, we develop an interpretable deep learning framework to predict brain age from hippocampal FC using a three-dimensional convolutional neural network (3D CNN) combined with LayerCAM saliency mapping. This approach maps key hippocampal-cortical connections, particularly with the precuneus, cuneus, posterior cingulate cortex, parahippocampal cortex, left superior parietal lobule, and right superior temporal sulcus, that are highly sensitive to age. Critically, disaggregating anterior and posterior hippocampal FC reveals distinct mapping aligned with their known functional specializations. These findings provide new insights into the functional mechanisms of hippocampal aging and demonstrate the power of explainable deep learning to uncover biologically meaningful patterns in neuroimaging data.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM Agent Collusion in Double Auctions</title>
<link>https://arxiv.org/abs/2507.01413</link>
<guid>https://arxiv.org/abs/2507.01413</guid>
<content:encoded><![CDATA[
arXiv:2507.01413v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing</title>
<link>https://arxiv.org/abs/2507.01418</link>
<guid>https://arxiv.org/abs/2507.01418</guid>
<content:encoded><![CDATA[
arXiv:2507.01418v1 Announce Type: cross 
Abstract: As AI integrates in various types of human writing, calls for transparency around AI assistance are growing. However, if transparency operates on uneven ground and certain identity groups bear a heavier cost for being honest, then the burden of openness becomes asymmetrical. This study investigates how AI disclosure statement affects perceptions of writing quality, and whether these effects vary by the author's race and gender. Through a large-scale controlled experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated a single human-written news article while disclosure statements and author demographics were systematically varied. This approach reflects how both human and algorithmic decisions now influence access to opportunities (e.g., hiring, promotion) and social recognition (e.g., content recommendation algorithms). We find that both human and LLM raters consistently penalize disclosed AI use. However, only LLM raters exhibit demographic interaction effects: they favor articles attributed to women or Black authors when no disclosure is present. But these advantages disappear when AI assistance is revealed. These findings illuminate the complex relationships between AI disclosure and author identity, highlighting disparities between machine and human evaluation patterns.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal</title>
<link>https://arxiv.org/abs/2507.01422</link>
<guid>https://arxiv.org/abs/2507.01422</guid>
<content:encoded><![CDATA[
arXiv:2507.01422v1 Announce Type: cross 
Abstract: Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method's superiority over state-of-the-art. Our code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems</title>
<link>https://arxiv.org/abs/2507.01429</link>
<guid>https://arxiv.org/abs/2507.01429</guid>
<content:encoded><![CDATA[
arXiv:2507.01429v1 Announce Type: cross 
Abstract: Deep neural networks generate and process large volumes of data, posing challenges for low-resource embedded systems. In-memory computing has been demonstrated as an efficient computing infrastructure and shows promise for embedded AI applications. Among newly-researched memory technologies, racetrack memory is a non-volatile technology that allows high data density fabrication, making it a good fit for in-memory computing. However, integrating in-memory arithmetic circuits with memory cells affects both the memory density and power efficiency. It remains challenging to build efficient in-memory arithmetic circuits on racetrack memory within area and energy constraints. To this end, we present an efficient in-memory convolutional neural network (CNN) accelerator optimized for use with racetrack memory. We design a series of fundamental arithmetic circuits as in-memory computing cells suited for multiply-and-accumulate operations. Moreover, we explore the design space of racetrack memory based systems and CNN model architectures, employing co-design to improve the efficiency and performance of performing CNN inference in racetrack memory while maintaining model accuracy. Our designed circuits and model-system co-optimization strategies achieve a small memory bank area with significant improvements in energy and performance for racetrack memory based embedded systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices</title>
<link>https://arxiv.org/abs/2507.01438</link>
<guid>https://arxiv.org/abs/2507.01438</guid>
<content:encoded><![CDATA[
arXiv:2507.01438v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs</title>
<link>https://arxiv.org/abs/2507.01457</link>
<guid>https://arxiv.org/abs/2507.01457</guid>
<content:encoded><![CDATA[
arXiv:2507.01457v1 Announce Type: cross 
Abstract: RISC-V provides a flexible and scalable platform for applications ranging from embedded devices to high-performance computing clusters. Particularly, its RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI workloads. But writing software that efficiently utilizes the vector units of RISC-V CPUs without expert knowledge requires the programmer to rely on the autovectorization features of compilers or hand-crafted libraries like muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing the integration with the RISC-V RVV extension, thus heavily limiting the efficient deployment of complex AI workloads. In this paper, we present a workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V vector units. Instead of relying on hand-crafted libraries, we integrated the RVV extension into TVM's MetaSchedule framework, a probabilistic program framework for tensor operation tuning. We implemented different RISC-V SoCs on an FPGA and tuned a wide range of AI workloads on them. We found that our proposal shows a mean improvement of 46% in execution latency when compared against the autovectorization feature of GCC, and 29% against muRISCV-NN. Moreover, the binary resulting from our proposal has a smaller code memory footprint, making it more suitable for embedded devices. Finally, we also evaluated our solution on a commercially available RISC-V SoC implementing the RVV 1.0 Vector Extension and found our solution is able to find mappings that are 35% faster on average than the ones proposed by LLVM. We open-sourced our proposal for the community to expand it to target other RISC-V extensions.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0</title>
<link>https://arxiv.org/abs/2507.01462</link>
<guid>https://arxiv.org/abs/2507.01462</guid>
<content:encoded><![CDATA[
arXiv:2507.01462v1 Announce Type: cross 
Abstract: This work explores the application of hybrid quantum-classical algorithms to optimize robotic inspection trajectories derived from Computer-Aided Design (CAD) models in industrial settings. By modeling the task as a 3D variant of the Traveling Salesman Problem, incorporating incomplete graphs and open-route constraints, this study evaluates the performance of two D-Wave-based solvers against classical methods such as GUROBI and Google OR-Tools. Results across five real-world cases demonstrate competitive solution quality with significantly reduced computation times, highlighting the potential of quantum approaches in automation under Industry 4.0.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</title>
<link>https://arxiv.org/abs/2507.01463</link>
<guid>https://arxiv.org/abs/2507.01463</guid>
<content:encoded><![CDATA[
arXiv:2507.01463v1 Announce Type: cross 
Abstract: Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed, for all kinds of novel objects, without (re-) training, has proven to be a difficult task. To handle this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). This work stems from and improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also leverages on recent vision foundation models, namely: Grounded-SAM 2 and DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise bounding boxes and their corresponding segmentation masks; while DINOv2's zero-shot capabilities are employed to generate the image embeddings. The quality of those masks, together with their embeddings, is of vital importance to our approach; as the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings. Differently to SAM-6D, calculating the latter involves a prior patch filtering based on the distance between each patch and its corresponding cyclic/roundtrip patch in the image grid. Furthermore, the average confidence of the proposals' bounding box and mask is used as an additional weighting factor for the object matching score. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods on the seven core datasets of the BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects" task.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals</title>
<link>https://arxiv.org/abs/2507.01470</link>
<guid>https://arxiv.org/abs/2507.01470</guid>
<content:encoded><![CDATA[
arXiv:2507.01470v1 Announce Type: cross 
Abstract: This work re-examines the commonly held assumption that the frequency of rewards is a reliable measure of task difficulty in reinforcement learning. We identify and formalize a structural challenge that undermines the effectiveness of current policy learning methods: when essential subgoals do not directly yield rewards. We characterize such settings as exhibiting zero-incentive dynamics, where transitions critical to success remain unrewarded. We show that state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics and that learning performance is highly sensitive to the temporal proximity between subgoal completion and eventual reward. These findings reveal a fundamental limitation in current approaches and point to the need for mechanisms that can infer latent task structure without relying on immediate incentives.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities</title>
<link>https://arxiv.org/abs/2507.01479</link>
<guid>https://arxiv.org/abs/2507.01479</guid>
<content:encoded><![CDATA[
arXiv:2507.01479v1 Announce Type: cross 
Abstract: Automatic text simplification (ATS) aims to enhance language accessibility for various target groups, particularly persons with intellectual disabilities. Recent advancements in generative AI, especially large language models (LLMs), have substantially improved the quality of machine-generated text simplifications, thereby mitigating information barriers for the target group. However, existing LLM-based ATS systems do not incorporate preference feedback on text simplifications during training, resulting in a lack of personalization tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach for adapting LLM-based ATS models by leveraging a computationally efficient LLM alignment technique -- direct preference optimization (DPO). Specifically, we post-train LLM-based ATS models using human feedback collected from persons with intellectual disabilities, reflecting their preferences on paired text simplifications generated by mainstream LLMs. Furthermore, we propose a pipeline for developing personalized LLM-based ATS systems, encompassing data collection, model selection, SFT and DPO post-training, and evaluation. Our findings underscore the necessity of active participation of target group persons in designing personalized AI accessibility solutions aligned with human expectations. This work represents a step towards personalizing inclusive AI systems at the target-group level, incorporating insights not only from text simplification experts but also from target group persons themselves.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Scarcity: The Economics of Unresolvable Unknowns</title>
<link>https://arxiv.org/abs/2507.01483</link>
<guid>https://arxiv.org/abs/2507.01483</guid>
<content:encoded><![CDATA[
arXiv:2507.01483v1 Announce Type: cross 
Abstract: This paper presents a praxeological analysis of artificial intelligence and algorithmic governance, challenging assumptions about the capacity of machine systems to sustain economic and epistemic order. Drawing on Misesian a priori reasoning and Austrian theories of entrepreneurship, we argue that AI systems are incapable of performing the core functions of economic coordination: interpreting ends, discovering means, and communicating subjective value through prices. Where neoclassical and behavioural models treat decisions as optimisation under constraint, we frame them as purposive actions under uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability, and Transparency (FAT) as extensions of constructivist rationalism, which conflict with a liberal order grounded in voluntary action and property rights. Attempts to encode moral reasoning in algorithms reflect a misunderstanding of ethics and economics. However complex, AI systems cannot originate norms, interpret institutions, or bear responsibility. They remain opaque, misaligned, and inert.
  Using the concept of epistemic scarcity, we explore how information abundance degrades truth discernment, enabling both entrepreneurial insight and soft totalitarianism. Our analysis ends with a civilisational claim: the debate over AI concerns the future of human autonomy, institutional evolution, and reasoned choice. The Austrian tradition, focused on action, subjectivity, and spontaneous order, offers the only coherent alternative to rising computational social control.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments</title>
<link>https://arxiv.org/abs/2507.01485</link>
<guid>https://arxiv.org/abs/2507.01485</guid>
<content:encoded><![CDATA[
arXiv:2507.01485v1 Announce Type: cross 
Abstract: Large language models (LLMs) and vision-language models (VLMs) have the potential to transform biological research by enabling autonomous experimentation. Yet, their application remains constrained by rigid protocol design, limited adaptability to dynamic lab conditions, inadequate error handling, and high operational complexity. Here we introduce BioMARS (Biological Multi-Agent Robotic System), an intelligent platform that integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and execute biological experiments. BioMARS uses a hierarchical architecture: the Biologist Agent synthesizes protocols via retrieval-augmented generation; the Technician Agent translates them into executable robotic pseudo-code; and the Inspector Agent ensures procedural integrity through multimodal perception and anomaly detection. The system autonomously conducts cell passaging and culture tasks, matching or exceeding manual performance in viability, consistency, and morphological integrity. It also supports context-aware optimization, outperforming conventional strategies in differentiating retinal pigment epithelial cells. A web interface enables real-time human-AI collaboration, while a modular backend allows scalable integration with laboratory hardware. These results highlight the feasibility of generalizable, AI-driven laboratory automation and the transformative role of language-based reasoning in biological research.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crop Pest Classification Using Deep Learning Techniques: A Review</title>
<link>https://arxiv.org/abs/2507.01494</link>
<guid>https://arxiv.org/abs/2507.01494</guid>
<content:encoded><![CDATA[
arXiv:2507.01494v1 Announce Type: cross 
Abstract: Insect pests continue to bring a serious threat to crop yields around the world, and traditional methods for monitoring them are often slow, manual, and difficult to scale. In recent years, deep learning has emerged as a powerful solution, with techniques like convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid models gaining popularity for automating pest detection. This review looks at 37 carefully selected studies published between 2018 and 2025, all focused on AI-based pest classification. The selected research is organized by crop type, pest species, model architecture, dataset usage, and key technical challenges. The early studies relied heavily on CNNs but latest work is shifting toward hybrid and transformer-based models that deliver higher accuracy and better contextual understanding. Still, challenges like imbalanced datasets, difficulty in detecting small pests, limited generalizability, and deployment on edge devices remain significant hurdles. Overall, this review offers a structured overview of the field, highlights useful datasets, and outlines the key challenges and future directions for AI-based pest monitoring systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images</title>
<link>https://arxiv.org/abs/2507.01502</link>
<guid>https://arxiv.org/abs/2507.01502</guid>
<content:encoded><![CDATA[
arXiv:2507.01502v1 Announce Type: cross 
Abstract: Global warming, loss of biodiversity, and air pollution are among the most significant problems facing Earth. One of the primary challenges in addressing these issues is the lack of monitoring forests to protect them. To tackle this problem, it is important to leverage remote sensing and computer vision methods to automate monitoring applications. Hence, automatic tree crown detection algorithms emerged based on traditional and deep learning methods. In this study, we first introduce two different tree crown detection methods based on these approaches. Then, we form a novel rule-based approach that integrates these two methods to enhance robustness and accuracy of tree crown detection results. While traditional methods are employed for feature extraction and segmentation of forested areas, deep learning methods are used to detect tree crowns in our method. With the proposed rule-based approach, we post-process these results, aiming to increase the number of detected tree crowns through neighboring trees and localized operations. We compare the obtained results with the proposed method in terms of the number of detected tree crowns and report the advantages, disadvantages, and areas for improvement of the obtained outcomes.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence</title>
<link>https://arxiv.org/abs/2507.01504</link>
<guid>https://arxiv.org/abs/2507.01504</guid>
<content:encoded><![CDATA[
arXiv:2507.01504v1 Announce Type: cross 
Abstract: The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at https://github.com/RAufschlaeger/cRID.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chargax: A JAX Accelerated EV Charging Simulator</title>
<link>https://arxiv.org/abs/2507.01522</link>
<guid>https://arxiv.org/abs/2507.01522</guid>
<content:encoded><![CDATA[
arXiv:2507.01522v1 Announce Type: cross 
Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable energy challenges. For instance, many grid systems are heavily congested, highlighting the urgent need to enhance operational efficiency. However, reinforcement learning approaches have traditionally been slow due to the high sample complexity and expensive simulation requirements. While recent works have effectively used GPUs to accelerate data generation by converting environments to JAX, these works have largely focussed on classical toy problems. This paper introduces Chargax, a JAX-based environment for realistic simulation of electric vehicle charging stations designed for accelerated training of RL agents. We validate our environment in a variety of scenarios based on real data, comparing reinforcement learning agents against baselines. Chargax delivers substantial computational performance improvements of over 100x-1000x over existing environments. Additionally, Chargax' modular architecture enables the representation of diverse real-world charging station configurations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions</title>
<link>https://arxiv.org/abs/2507.01547</link>
<guid>https://arxiv.org/abs/2507.01547</guid>
<content:encoded><![CDATA[
arXiv:2507.01547v1 Announce Type: cross 
Abstract: Critical infrastructure, such as transport networks, underpins economic growth by enabling mobility and trade. However, ageing assets, climate change impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging from natural disasters to cyber attacks and conflicts pose growing risks to their resilience and functionality. This review paper explores how emerging digital technologies, specifically Artificial Intelligence (AI), can enhance damage assessment and monitoring of transport infrastructure. A systematic literature review examines existing AI models and datasets for assessing damage in roads, bridges, and other critical infrastructure impacted by natural disasters. Special focus is given to the unique challenges and opportunities associated with bridge damage detection due to their structural complexity and critical role in connectivity. The integration of SAR (Synthetic Aperture Radar) data with AI models is also discussed, with the review revealing a critical research gap: a scarcity of studies applying AI models to SAR data for comprehensive bridge damage assessment. Therefore, this review aims to identify the research gaps and provide foundations for AI-driven solutions for assessing and monitoring critical transport infrastructures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants</title>
<link>https://arxiv.org/abs/2507.01548</link>
<guid>https://arxiv.org/abs/2507.01548</guid>
<content:encoded><![CDATA[
arXiv:2507.01548v1 Announce Type: cross 
Abstract: This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01551</link>
<guid>https://arxiv.org/abs/2507.01551</guid>
<content:encoded><![CDATA[
arXiv:2507.01551v1 Announce Type: cross 
Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models~(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1/3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware</title>
<link>https://arxiv.org/abs/2507.01563</link>
<guid>https://arxiv.org/abs/2507.01563</guid>
<content:encoded><![CDATA[
arXiv:2507.01563v1 Announce Type: cross 
Abstract: We present a full-stack emergency vehicle (EV) siren detection system designed for real-time deployment on embedded hardware. The proposed approach is based on E2PANNs, a fine-tuned convolutional neural network derived from EPANNs, and optimized for binary sound event detection under urban acoustic conditions. A key contribution is the creation of curated and semantically structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV - developed using a custom AudioSet-Tools framework to overcome the low reliability of standard AudioSet annotations. The system is deployed on a Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing a multithreaded inference engine with adaptive frame sizing, probability smoothing, and a decision-state machine to control false positive activations. A remote WebSocket interface provides real-time monitoring and facilitates live demonstration capabilities. Performance is evaluated using both framewise and event-based metrics across multiple configurations. Results show the system achieves low-latency detection with improved robustness under realistic audio conditions. This work demonstrates the feasibility of deploying IoS-compatible SED solutions that can form distributed acoustic monitoring networks, enabling collaborative emergency vehicle tracking across smart city infrastructures through WebSocket connectivity on low-cost edge devices.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder</title>
<link>https://arxiv.org/abs/2507.01582</link>
<guid>https://arxiv.org/abs/2507.01582</guid>
<content:encoded><![CDATA[
arXiv:2507.01582v1 Announce Type: cross 
Abstract: The creativity of classical music arises not only from composers who craft the musical sheets but also from performers who interpret the static notations with expressive nuances. This paper addresses the challenge of generating classical piano performances from scratch, aiming to emulate the dual roles of composer and pianist in the creative process. We introduce the Expressive Compound Word (ECP) representation, which effectively captures both the metrical structure and expressive nuances of classical performances. Building on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a model featuring two branches: a Vector Quantized Variational AutoEncoder (VQ-VAE) branch that generates score-related content, representing the Composer, and a vanilla VAE branch that produces expressive details, fulfilling the role of Pianist. These branches are jointly trained with similar Seq2Seq architectures, leveraging a multiscale encoder to capture beat-level contextual information and an orthogonal Transformer decoder for efficient compound tokens decoding. Both objective and subjective evaluations demonstrate that XMVAE generates classical performances with superior musical quality compared to state-of-the-art models. Furthermore, pretraining the Composer branch on extra musical score datasets contribute to a significant performance gain.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring</title>
<link>https://arxiv.org/abs/2507.01590</link>
<guid>https://arxiv.org/abs/2507.01590</guid>
<content:encoded><![CDATA[
arXiv:2507.01590v1 Announce Type: cross 
Abstract: This study presents a novel classroom surveillance system that integrates multiple modalities, including drowsiness, tracking of mobile phone usage, and face recognition,to assess student attentiveness with enhanced precision.The system leverages the YOLOv8 model to detect both mobile phone and sleep usage,(Ghatge et al., 2024) while facial recognition is achieved through LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These models work in synergy to provide comprehensive, real-time monitoring, offering insights into student engagement and behavior.(S et al., 2023) The framework is trained on specialized datasets, such as the RMFD dataset for face recognition and a Roboflow dataset for mobile phone detection. The extensive evaluation of the system shows promising results. Sleep detection achieves 97. 42% mAP@50, face recognition achieves 86. 45% validation accuracy and mobile phone detection reach 85. 89% mAP@50. The system is implemented within a core PHP web application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et al., 2024) This integrated approach not only enhances classroom monitoring, but also ensures automatic attendance recording via face recognition as students remain seated in the classroom, offering scalability for diverse educational environments.(Banada,2025)
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems</title>
<link>https://arxiv.org/abs/2507.01599</link>
<guid>https://arxiv.org/abs/2507.01599</guid>
<content:encoded><![CDATA[
arXiv:2507.01599v1 Announce Type: cross 
Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize performance, but they rely heavily on human experts to orchestrate system pipelines, enabling them to adapt to changes in data, queries, tasks, and environments. For instance, while there are numerous data science tools available, developing a pipeline planning system to coordinate these tools remains challenging. This difficulty arises because existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning. Fortunately, we have witnessed the success of large language models (LLMs) in enhancing semantic understanding, reasoning, and planning abilities. It is crucial to incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities. We delve into the challenges involved in designing data agents, such as understanding data/queries/environments/tools, orchestrating pipelines/workflows, optimizing and executing pipelines, and fostering pipeline self-reflection. Furthermore, we present examples of data agent systems, including a data science agent, data analytics agents (such as unstructured data analytics agent, semantic structured data analytics agent, data lake analytics agent, and multi-modal data analytics agent), and a database administrator (DBA) agent. We also outline several open challenges associated with designing data agent systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title>
<link>https://arxiv.org/abs/2507.01607</link>
<guid>https://arxiv.org/abs/2507.01607</guid>
<content:encoded><![CDATA[
arXiv:2507.01607v1 Announce Type: cross 
Abstract: The widespread use of deep learning face recognition raises several security concerns. Although prior works point at existing vulnerabilities, DNN backdoor attacks against real-life, unconstrained systems dealing with images captured in the wild remain a blind spot of the literature. This paper conducts the first system-level study of backdoors in deep learning-based face recognition systems. This paper yields four contributions by exploring the feasibility of DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the first time two backdoor attacks on the face detection task: face generation and face landmark shift attacks. We then show that face feature extractors trained with large margin losses also fall victim to backdoor attacks. Combining our models, we then show using 20 possible pipeline configurations and 15 attack cases that a single backdoor enables an attacker to bypass the entire function of a system. Finally, we provide stakeholders with several best practices and countermeasures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Influence-aware Group Recommendation for Online Media Propagation</title>
<link>https://arxiv.org/abs/2507.01616</link>
<guid>https://arxiv.org/abs/2507.01616</guid>
<content:encoded><![CDATA[
arXiv:2507.01616v1 Announce Type: cross 
Abstract: Group recommendation over social media streams has attracted significant attention due to its wide applications in domains such as e-commerce, entertainment, and online news broadcasting. By leveraging social connections and group behaviours, group recommendation (GR) aims to provide more accurate and engaging content to a set of users rather than individuals. Recently, influence-aware GR has emerged as a promising direction, as it considers the impact of social influence on group decision-making. In earlier work, we proposed Influence-aware Group Recommendation (IGR) to solve this task. However, this task remains challenging due to three key factors: the large and ever-growing scale of social graphs, the inherently dynamic nature of influence propagation within user groups, and the high computational overhead of real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based Sampling (GES) strategy to minimise redundancy across multiple temporal social graphs and effectively capture the evolving dynamics of both groups and items. Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict how influence propagates over time across social items and user groups. Finally, we develop a two-level hash-based User Group Index (UG-Index) to efficiently organise user groups and enable real-time recommendation generation. Extensive experiments on real-world datasets demonstrate that our proposed framework, EIGR, consistently outperforms state-of-the-art baselines in both effectiveness and efficiency.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss</title>
<link>https://arxiv.org/abs/2507.01630</link>
<guid>https://arxiv.org/abs/2507.01630</guid>
<content:encoded><![CDATA[
arXiv:2507.01630v1 Announce Type: cross 
Abstract: The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called ``AD-Acc.'' is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$, \textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</title>
<link>https://arxiv.org/abs/2507.01631</link>
<guid>https://arxiv.org/abs/2507.01631</guid>
<content:encoded><![CDATA[
arXiv:2507.01631v1 Announce Type: cross 
Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth Anything at Any Condition</title>
<link>https://arxiv.org/abs/2507.01634</link>
<guid>https://arxiv.org/abs/2507.01634</guid>
<content:encoded><![CDATA[
arXiv:2507.01634v1 Announce Type: cross 
Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance</title>
<link>https://arxiv.org/abs/2507.01638</link>
<guid>https://arxiv.org/abs/2507.01638</guid>
<content:encoded><![CDATA[
arXiv:2507.01638v1 Announce Type: cross 
Abstract: We present an analysis of landscape features for predicting the performance of multi-objective combinatorial optimization algorithms. We consider features from the recently proposed compressed Pareto Local Optimal Solutions Networks (C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness and objective correlation. We consider the performance of three algorithms -- Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and hypervolume metrics. Our tailored analysis reveals feature combinations that influence algorithm performance specific to certain landscapes. This study provides deeper insights into feature importance, tailored to specific rmnk-landscapes and algorithms.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradMetaNet: An Equivariant Architecture for Learning on Gradients</title>
<link>https://arxiv.org/abs/2507.01649</link>
<guid>https://arxiv.org/abs/2507.01649</guid>
<content:encoded><![CDATA[
arXiv:2507.01649v1 Announce Type: cross 
Abstract: Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. In this paper, we present a principled approach for designing architectures that process gradients. Our approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, we introduce GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. We prove universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective</title>
<link>https://arxiv.org/abs/2507.01652</link>
<guid>https://arxiv.org/abs/2507.01652</guid>
<content:encoded><![CDATA[
arXiv:2507.01652v1 Announce Type: cross 
Abstract: Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.01663</link>
<guid>https://arxiv.org/abs/2507.01663</guid>
<content:encoded><![CDATA[
arXiv:2507.01663v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis</title>
<link>https://arxiv.org/abs/2507.01668</link>
<guid>https://arxiv.org/abs/2507.01668</guid>
<content:encoded><![CDATA[
arXiv:2507.01668v1 Announce Type: cross 
Abstract: The field of numerical optimization has recently seen a surge in the development of "novel" metaheuristic algorithms, inspired by metaphors derived from natural or human-made processes, which have been widely criticized for obscuring meaningful innovations and failing to distinguish themselves from existing approaches. Aiming to address these concerns, we investigate the applicability of statistical tests for comparing algorithms based on their search behavior. We utilize the cross-match statistical test to compare multivariate distributions and assess the solutions produced by 114 algorithms from the MEALPY library. These findings are incorporated into an empirical analysis aiming to identify algorithms with similar search behaviors.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization</title>
<link>https://arxiv.org/abs/2507.01676</link>
<guid>https://arxiv.org/abs/2507.01676</guid>
<content:encoded><![CDATA[
arXiv:2507.01676v1 Announce Type: cross 
Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload accounting for more than 79% of the total AI workload in Meta's data centers. DLRMs' performance bottleneck is found in the embedding layers, which perform many random memory accesses to retrieve small embedding vectors from tables of various sizes. We propose the design of tailored data flows to speedup embedding look-ups. Namely, we propose four strategies to look up an embedding table effectively on one core, and a framework to automatically map the tables asymmetrically to the multiple cores of a SoC. We assess the effectiveness of our method using the Huawei Ascend AI accelerators, comparing it with the default Ascend compiler, and we perform high-level comparisons with Nvidia A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload distributions, and more than 20x for extremely unbalanced distributions. Furthermore, the method proves to be much more independent of the query distribution than the baseline.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling</title>
<link>https://arxiv.org/abs/2507.01679</link>
<guid>https://arxiv.org/abs/2507.01679</guid>
<content:encoded><![CDATA[
arXiv:2507.01679v1 Announce Type: cross 
Abstract: Existing post-training techniques for large language models are broadly categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking demonstration data but can lead to problematic generalization as a form of behavior cloning. Conversely, RFT can significantly enhance a model's performance but is prone to learn unexpected behaviors, and its performance is highly sensitive to the initial policy. In this paper, we propose a unified view of these methods and introduce Prefix-RFT, a hybrid approach that synergizes learning from both demonstration and exploration. Using mathematical reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is both simple and effective. It not only surpasses the performance of standalone SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key advantage is its seamless integration into existing open-source frameworks, requiring only minimal modifications to the standard RFT pipeline. Our analysis highlights the complementary nature of SFT and RFT, and validates that Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore, ablation studies confirm the method's robustness to variations in the quality and quantity of demonstration data. We hope this work offers a new perspective on LLM post-training, suggesting that a unified paradigm that judiciously integrates demonstration and exploration could be a promising direction for future research.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT, But Backwards: Exactly Inverting Language Model Outputs</title>
<link>https://arxiv.org/abs/2507.01693</link>
<guid>https://arxiv.org/abs/2507.01693</guid>
<content:encoded><![CDATA[
arXiv:2507.01693v1 Announce Type: cross 
Abstract: While existing auditing techniques attempt to identify potential unwanted behaviours in large language models (LLMs), we address the complementary forensic problem of reconstructing the exact input that led to an existing LLM output - enabling post-incident analysis and potentially the detection of fake output reports. We formalize exact input reconstruction as a discrete optimisation problem with a unique global minimum and introduce SODA, an efficient gradient-based algorithm that operates on a continuous relaxation of the input search space with periodic restarts and parameter decay. Through comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we demonstrate that SODA significantly outperforms existing approaches. We succeed in fully recovering 79.5% of shorter out-of-distribution inputs from next-token logits, without a single false positive, but struggle to extract private information from the outputs of longer (15+ token) input sequences. This suggests that standard deployment practices may currently provide adequate protection against malicious use of our method. Our code is available at https://doi.org/10.5281/zenodo.15539879.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Causal Discovery with Latent Confounders</title>
<link>https://arxiv.org/abs/2507.01700</link>
<guid>https://arxiv.org/abs/2507.01700</guid>
<content:encoded><![CDATA[
arXiv:2507.01700v1 Announce Type: cross 
Abstract: Estimating causal effects from real-world relational data can be challenging when the underlying causal model and potential confounders are unknown. While several causal discovery algorithms exist for learning causal models with latent confounders from data, they assume that the data is independent and identically distributed (i.i.d.) and are not well-suited for learning from relational data. Similarly, existing relational causal discovery algorithms assume causal sufficiency, which is unrealistic for many real-world datasets. To address this gap, we propose RelFCI, a sound and complete causal discovery algorithm for relational data with latent confounders. Our work builds upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms and it defines new graphical models, necessary to support causal discovery in relational domains. We also establish soundness and completeness guarantees for relational d-separation with latent confounders. We present experimental results demonstrating the effectiveness of RelFCI in identifying the correct causal structure in relational causal models with latent confounders.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture</title>
<link>https://arxiv.org/abs/2507.01701</link>
<guid>https://arxiv.org/abs/2507.01701</guid>
<content:encoded><![CDATA[
arXiv:2507.01701v1 Announce Type: cross 
Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM multi-agent systems (MASs) so that (1) agents with various roles can share all the information and others' messages during the whole problem-solving process, (2) agents that will take actions are selected based on the current content of the blackboard, and (3) the selection and execution round is repeated until a consensus is reached on the blackboard. We develop the first implementation of this proposal and conduct experiments on commonsense knowledge, reasoning and mathematical datasets. The results show that our system can be competitive with the SOTA static and dynamic MASs by achieving the best average performance, and at the same time manage to spend less tokens. Our proposal has the potential to enable complex and dynamic problem-solving where well-defined structures or workflows are unavailable.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</title>
<link>https://arxiv.org/abs/2507.01702</link>
<guid>https://arxiv.org/abs/2507.01702</guid>
<content:encoded><![CDATA[
arXiv:2507.01702v1 Announce Type: cross 
Abstract: The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America</title>
<link>https://arxiv.org/abs/2507.01719</link>
<guid>https://arxiv.org/abs/2507.01719</guid>
<content:encoded><![CDATA[
arXiv:2507.01719v1 Announce Type: cross 
Abstract: There is justifiable interest in leveraging conversational AI (CAI) for health across the majority world, but to be effective, CAI must respond appropriately within culturally and linguistically diverse contexts. Therefore, we need ways to address the fact that current LLMs exclude many lived experiences globally. Various advances are underway which focus on top-down approaches and increasing training data. In this paper, we aim to complement these with a bottom-up locally-grounded approach based on qualitative data collected during participatory workshops in Latin America. Our goal is to construct a rich and human-centred understanding of: a) potential areas of cultural misalignment in digital health; b) regional perspectives on chatbots for health and c)strategies for creating culturally-appropriate CAI; with a focus on the understudied Latin American context. Our findings show that academic boundaries on notions of culture lose meaning at the ground level and technologies will need to engage with a broader framework; one that encapsulates the way economics, politics, geography and local logistics are entangled in cultural experience. To this end, we introduce a framework for 'Pluriversal Conversational AI for Health' which allows for the possibility that more relationality and tolerance, rather than just more data, may be called for.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.01735</link>
<guid>https://arxiv.org/abs/2507.01735</guid>
<content:encoded><![CDATA[
arXiv:2507.01735v1 Announce Type: cross 
Abstract: In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.01752</link>
<guid>https://arxiv.org/abs/2507.01752</guid>
<content:encoded><![CDATA[
arXiv:2507.01752v1 Announce Type: cross 
Abstract: Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, its reliance on large volumes of labeled data raises privacy and security concerns such as susceptibility to data poisoning attacks and the risk of overfitting. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. However, black box methods also pose significant challenges, including poor scalability to high-dimensional parameter spaces, as prevalent in large language models (LLMs), and high computational costs due to reliance on numerous model evaluations. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide strong theoretical bounds on generalization, differential privacy, susceptibility to data poisoning attacks, and robustness to extraction attacks. BBoxER operates on top of pre-trained LLMs, offering a lightweight and modular enhancement suitable for deployment in restricted or privacy-sensitive environments, in addition to non-vacuous generalization guarantees. In experiments with LLMs, we demonstrate empirically that Retrofitting methods are able to learn, showing how a few iterations of BBoxER improve performance and generalize well on a benchmark of reasoning datasets. This positions BBoxER as an attractive add-on on top of gradient-based optimization.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Generative Model Evaluation with Clipped Density and Coverage</title>
<link>https://arxiv.org/abs/2507.01761</link>
<guid>https://arxiv.org/abs/2507.01761</guid>
<content:encoded><![CDATA[
arXiv:2507.01761v1 Announce Type: cross 
Abstract: Although generative models have made remarkable progress in recent years, their use in critical applications has been hindered by their incapacity to reliably evaluate sample quality. Quality refers to at least two complementary concepts: fidelity and coverage. Current quality metrics often lack reliable, interpretable values due to an absence of calibration or insufficient robustness to outliers. To address these shortcomings, we introduce two novel metrics, Clipped Density and Clipped Coverage. By clipping individual sample contributions and, for fidelity, the radii of nearest neighbor balls, our metrics prevent out-of-distribution samples from biasing the aggregated values. Through analytical and empirical calibration, these metrics exhibit linear score degradation as the proportion of poor samples increases. Thus, they can be straightforwardly interpreted as equivalent proportions of good samples. Extensive experiments on synthetic and real-world datasets demonstrate that Clipped Density and Clipped Coverage outperform existing methods in terms of robustness, sensitivity, and interpretability for evaluating generative models.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPU-based complete search for nonlinear minimization subject to bounds</title>
<link>https://arxiv.org/abs/2507.01770</link>
<guid>https://arxiv.org/abs/2507.01770</guid>
<content:encoded><![CDATA[
arXiv:2507.01770v1 Announce Type: cross 
Abstract: This paper introduces a GPU-based complete search method to enclose the global minimum of a nonlinear function subject to simple bounds on the variables. Using interval analysis, coupled with the computational power and architecture of GPU, the method iteratively rules out the regions in the search domain where the global minimum cannot exist and leaves a finite set of regions where the global minimum must exist. For effectiveness, because of the rigor of interval analysis, the method is guaranteed to enclose the global minimum of the nonlinear function even in the presence of rounding errors. For efficiency, the method employs a novel GPU-based single program, single data parallel programming style to circumvent major GPU performance bottlenecks, and a variable cycling technique is also integrated into the method to reduce computational cost when minimizing large-scale nonlinear functions. The method is validated by minimizing 10 multimodal benchmark test functions with scalable dimensions, including the well-known Ackley function, Griewank function, Levy function, and Rastrigin function. These benchmark test functions represent grand challenges of global optimization, and enclosing the guaranteed global minimum of these benchmark test functions with more than 80 dimensions has not been reported in the literature. Our method completely searches the feasible domain and successfully encloses the guaranteed global minimum of these 10 benchmark test functions with up to 10,000 dimensions using only one GPU in a reasonable computation time, far exceeding the reported results in the literature due to the unique method design and implementation based on GPU architecture.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification</title>
<link>https://arxiv.org/abs/2507.01781</link>
<guid>https://arxiv.org/abs/2507.01781</guid>
<content:encoded><![CDATA[
arXiv:2507.01781v1 Announce Type: cross 
Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms decision tree ensembles into sparse, partially connected neural networks. Each branch, defined as a decision path from root to a parent of leaves, is mapped to a hidden neuron, preserving symbolic structure while enabling gradient-based optimization. The resulting models are compact, interpretable, and require no manual architecture tuning. Evaluated on a suite of structured multi-class classification benchmarks, BranchNet consistently outperforms XGBoost in accuracy, with statistically significant gains. We detail the architecture, training procedure, and sparsity dynamics, and discuss the model's strengths in symbolic interpretability as well as its current limitations, particularly on binary tasks where further adaptive calibration may be beneficial.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2507.01785</link>
<guid>https://arxiv.org/abs/2507.01785</guid>
<content:encoded><![CDATA[
arXiv:2507.01785v1 Announce Type: cross 
Abstract: Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a single rater for 17 target languages. MuRating aggregates multiple English "raters" via pairwise comparisons to learn unified document-quality scores,then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to strong baselines, including QuRater, AskLLM, DCLM and so on, our approach boosts average accuracy on both English benchmarks and multilingual evaluations, with especially large gains on knowledge-intensive tasks. We further analyze translation fidelity, selection biases, and underrepresentation of narrative material, outlining directions for future work.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Evaluation Awareness of Language Models</title>
<link>https://arxiv.org/abs/2507.01786</link>
<guid>https://arxiv.org/abs/2507.01786</guid>
<content:encoded><![CDATA[
arXiv:2507.01786v1 Announce Type: cross 
Abstract: Language models can distinguish between testing and deployment phases -- a capability known as evaluation awareness. This has significant safety and policy implications, potentially undermining the reliability of evaluations that are central to AI governance frameworks and voluntary industry commitments. In this paper, we study evaluation awareness in Llama-3.3-70B-Instruct. We show that linear probes can separate real-world evaluation and deployment prompts, suggesting that current models internally represent this distinction. We also find that current safety evaluations are correctly classified by the probes, suggesting that they already appear artificial or inauthentic to models. Our findings underscore the importance of ensuring trustworthy evaluations and understanding deceptive capabilities. More broadly, our work showcases how model internals may be leveraged to support blackbox methods in safety audits, especially for future models more competent at evaluation awareness and deception.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging</title>
<link>https://arxiv.org/abs/2507.01788</link>
<guid>https://arxiv.org/abs/2507.01788</guid>
<content:encoded><![CDATA[
arXiv:2507.01788v1 Announce Type: cross 
Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60\%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Vision-Language Models Process Conflicting Information Across Modalities?</title>
<link>https://arxiv.org/abs/2507.01790</link>
<guid>https://arxiv.org/abs/2507.01790</guid>
<content:encoded><![CDATA[
arXiv:2507.01790v1 Announce Type: cross 
Abstract: AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs</title>
<link>https://arxiv.org/abs/2507.01806</link>
<guid>https://arxiv.org/abs/2507.01806</guid>
<content:encoded><![CDATA[
arXiv:2507.01806v1 Announce Type: cross 
Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language Models (LLMs) by enabling parameter-efficient updates. However, their widespread adoption remains limited by the reliance on GPU-based training. In this work, we propose a theoretically grounded approach to LoRA fine-tuning designed specifically for users with limited computational resources, particularly those restricted to standard laptop CPUs. Our method learns a meta-operator that maps any input dataset, represented as a probability distribution, to a set of LoRA weights by leveraging a large bank of pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of performing new gradient-based updates, our pipeline constructs adapters via lightweight combinations of existing LoRAs directly on CPU. While the resulting adapters do not match the performance of GPU-trained counterparts, they consistently outperform the base Mistral model on downstream tasks, offering a practical and accessible alternative to traditional GPU-based fine-tuning.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems</title>
<link>https://arxiv.org/abs/2507.01808</link>
<guid>https://arxiv.org/abs/2507.01808</guid>
<content:encoded><![CDATA[
arXiv:2507.01808v1 Announce Type: cross 
Abstract: Small- and medium-sized manufacturers need innovative data tools but, because of competition and privacy concerns, often do not want to share their proprietary data with researchers who might be interested in helping. This paper introduces a privacy-preserving platform by which manufacturers may safely share their data with researchers through secure methods, so that those researchers then create innovative tools to solve the manufacturers' real-world problems, and then provide tools that execute solutions back onto the platform for others to use with privacy and confidentiality guarantees. We illustrate this problem through a particular use case which addresses an important problem in the large-scale manufacturing of food crystals, which is that quality control relies on image analysis tools. Previous to our research, food crystals in the images were manually counted, which required substantial and time-consuming human efforts, but we have developed and deployed a crystal analysis tool which makes this process both more rapid and accurate. The tool enables automatic characterization of the crystal size distribution and numbers from microscope images while the natural imperfections from the sample preparation are automatically removed; a machine learning model to count high resolution translucent crystals and agglomeration of crystals was also developed to aid in these efforts. The resulting algorithm was then packaged for real-world use on the factory floor via a web-based app secured through the originating privacy-preserving platform, allowing manufacturers to use it while keeping their proprietary data secure. After demonstrating this full process, future directions are also explored.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MILP-SAT-GNN: Yet Another Neural SAT Solver</title>
<link>https://arxiv.org/abs/2507.01825</link>
<guid>https://arxiv.org/abs/2507.01825</guid>
<content:encoded><![CDATA[
arXiv:2507.01825v1 Announce Type: cross 
Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve SAT problems by leveraging a technique developed for applying GNNs to Mixed Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into MILP problems, which are then encoded as weighted bipartite graphs and subsequently fed into a GNN for training and testing. From a theoretical perspective: (i) we establish permutation and equivalence invariance results, demonstrating that the method produces outputs that are stable under reordering of clauses and variables; (ii) we identify a theoretical limitation, showing that for a class of formulae called foldable formulae, standard GNNs cannot always distinguish satisfiable from unsatisfiable instances; (iii) we prove a universal approximation theorem, establishing that with Random Node Initialization (RNI), the method can approximate SAT solving to arbitrary precision on finite datasets, that is, the GNN becomes approximately sound and complete on such datasets. Furthermore, we show that for unfoldable formulae, the same approximation guarantee can be achieved without the need for RNI. Finally, we conduct an experimental evaluation of our approach, which show that, despite the simplicity of the neural architecture, the method achieves promising results.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling</title>
<link>https://arxiv.org/abs/2507.01829</link>
<guid>https://arxiv.org/abs/2507.01829</guid>
<content:encoded><![CDATA[
arXiv:2507.01829v1 Announce Type: cross 
Abstract: Edge devices for temporal processing demand models that capture both short- and long- range dynamics under tight memory constraints. While Transformers excel at sequence modeling, their quadratic memory scaling with sequence length makes them impractical for such settings. Recurrent Neural Networks (RNNs) offer constant memory but train sequentially, and Temporal Convolutional Networks (TCNs), though efficient, scale memory with kernel size. To address this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay Embedding), a hybrid-memory system that integrates a temporal 1D-convolution with learnable spacings followed by a minimal gated recurrent unit (minGRU). This design allows the convolutional layer to realize a flexible delay embedding that captures rapid temporal variations, while the recurrent module efficiently maintains global context with minimal memory overhead. We validate our approach on two synthetic tasks, demonstrating that mGRADE effectively separates and preserves multi-scale temporal features. Furthermore, on challenging pixel-by-pixel image classification benchmarks, mGRADE consistently outperforms both pure convolutional and pure recurrent counterparts using approximately 20% less memory footprint, highlighting its suitability for memory-constrained temporal processing at the edge. This highlights mGRADE's promise as an efficient solution for memory-constrained multi-scale temporal processing at the edge.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents</title>
<link>https://arxiv.org/abs/2507.01862</link>
<guid>https://arxiv.org/abs/2507.01862</guid>
<content:encoded><![CDATA[
arXiv:2507.01862v1 Announce Type: cross 
Abstract: Domain specific chatbot applications often involve multi step interactions, such as refining search filters, selecting multiple items, or performing comparisons. Traditional graphical user interfaces (GUIs) handle these workflows by providing explicit "Submit" (commit data) and "Reset" (discard data) actions, allowing back-end systems to track user intent unambiguously. In contrast, conversational agents rely on subtle language cues, which can lead to confusion and incomplete context management. This paper proposes modeling these GUI inspired metaphors acknowledgment (submit like) and context switching (reset-like) as explicit tasks within large language model (LLM) prompts. By capturing user acknowledgment, reset actions, and chain of thought (CoT) reasoning as structured session data, we preserve clarity, reduce user confusion, and align domain-specific chatbot interactions with back-end logic. We demonstrate our approach in hotel booking and customer management scenarios, highlighting improvements in multi-turn task coherence, user satisfaction, and efficiency.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Auto-Encoders for Time-Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.01875</link>
<guid>https://arxiv.org/abs/2507.01875</guid>
<content:encoded><![CDATA[
arXiv:2507.01875v1 Announce Type: cross 
Abstract: We investigate a novel approach to time-series modeling, inspired by the successes of large pretrained foundation models. We introduce FAE (Foundation Auto-Encoders), a foundation generative-AI model for anomaly detection in time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we mean a model pretrained on massive amounts of time-series data which can learn complex temporal patterns useful for accurate modeling, forecasting, and detection of anomalies on previously unseen datasets. FAE leverages VAEs and Dilated Convolutional Neural Networks (DCNNs) to build a generic model for univariate time-series modeling, which could eventually perform properly in out-of-the-box, zero-shot anomaly detection applications. We introduce the main concepts of FAE, and present preliminary results in different multi-dimensional time-series datasets from various domains, including a real dataset from an operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI4Research: A Survey of Artificial Intelligence for Scientific Research</title>
<link>https://arxiv.org/abs/2507.01903</link>
<guid>https://arxiv.org/abs/2507.01903</guid>
<content:encoded><![CDATA[
arXiv:2507.01903v1 Announce Type: cross 
Abstract: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2507.01915</link>
<guid>https://arxiv.org/abs/2507.01915</guid>
<content:encoded><![CDATA[
arXiv:2507.01915v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning</title>
<link>https://arxiv.org/abs/2507.01918</link>
<guid>https://arxiv.org/abs/2507.01918</guid>
<content:encoded><![CDATA[
arXiv:2507.01918v1 Announce Type: cross 
Abstract: We develop a rotation-invariant neural network that provides the global minimum-variance portfolio by jointly learning how to lag-transform historical returns and how to regularise both the eigenvalues and the marginal volatilities of large equity covariance matrices. This explicit mathematical mapping offers clear interpretability of each module's role, so the model cannot be regarded as a pure black-box. The architecture mirrors the analytical form of the global minimum-variance solution yet remains agnostic to dimension, so a single model can be calibrated on panels of a few hundred stocks and applied, without retraining, to one thousand US equities-a cross-sectional jump that demonstrates robust out-of-sample generalisation. The loss function is the future realized minimum portfolio variance and is optimized end-to-end on real daily returns. In out-of-sample tests from January 2000 to December 2024 the estimator delivers systematically lower realised volatility, smaller maximum drawdowns, and higher Sharpe ratios than the best analytical competitors, including state-of-the-art non-linear shrinkage. Furthermore, although the model is trained end-to-end to produce an unconstrained (long-short) minimum-variance portfolio, we show that its learned covariance representation can be used in general optimizers under long-only constraints with virtually no loss in its performance advantage over competing estimators. These gains persist when the strategy is executed under a highly realistic implementation framework that models market orders at the auctions, empirical slippage, exchange fees, and financing charges for leverage, and they remain stable during episodes of acute market stress.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.01924</link>
<guid>https://arxiv.org/abs/2507.01924</guid>
<content:encoded><![CDATA[
arXiv:2507.01924v1 Announce Type: cross 
Abstract: The complexity of mental healthcare billing enables anomalies, including fraud. While machine learning methods have been applied to anomaly detection, they often struggle with class imbalance, label scarcity, and complex sequential patterns. This study explores a hybrid deep learning approach combining Long Short-Term Memory (LSTM) networks and Transformers, with pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior work has not evaluated such hybrid models trained on pseudo-labeled data in the context of healthcare billing. The approach is evaluated on two real-world billing datasets related to mental healthcare. The iForest LSTM baseline achieves the highest recall (0.963) on declaration-level data. On the operation-level data, the hybrid iForest-based model achieves the highest recall (0.744), though at the cost of lower precision. These findings highlight the potential of combining pseudo-labeling with hybrid deep learning in complex, imbalanced anomaly detection settings.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla</title>
<link>https://arxiv.org/abs/2507.01931</link>
<guid>https://arxiv.org/abs/2507.01931</guid>
<content:encoded><![CDATA[
arXiv:2507.01931v1 Announce Type: cross 
Abstract: In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[
arXiv:2507.01939v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks</title>
<link>https://arxiv.org/abs/2507.01955</link>
<guid>https://arxiv.org/abs/2507.01955</guid>
<content:encoded><![CDATA[
arXiv:2507.01955v1 Announce Type: cross 
Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).
  The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2507.01957</link>
<guid>https://arxiv.org/abs/2507.01957</guid>
<content:encoded><![CDATA[
arXiv:2507.01957v1 Announce Type: cross 
Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4$\times$ lower latency than previous parallelized autoregressive models.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation</title>
<link>https://arxiv.org/abs/2507.01961</link>
<guid>https://arxiv.org/abs/2507.01961</guid>
<content:encoded><![CDATA[
arXiv:2507.01961v1 Announce Type: cross 
Abstract: Recently, mobile manipulation has attracted increasing attention for enabling language-conditioned robotic control in household tasks. However, existing methods still face challenges in coordinating mobile base and manipulator, primarily due to two limitations. On the one hand, they fail to explicitly model the influence of the mobile base on manipulator control, which easily leads to error accumulation under high degrees of freedom. On the other hand, they treat the entire mobile manipulation process with the same visual observation modality (e.g., either all 2D or all 3D), overlooking the distinct multimodal perception requirements at different stages during mobile manipulation. To address this, we propose the Adaptive Coordination Diffusion Transformer (AC-DiT), which enhances mobile base and manipulator coordination for end-to-end mobile manipulation. First, since the motion of the mobile base directly influences the manipulator's actions, we introduce a mobility-to-body conditioning mechanism that guides the model to first extract base motion representations, which are then used as context prior for predicting whole-body actions. This enables whole-body control that accounts for the potential impact of the mobile base's motion. Second, to meet the perception requirements at different stages of mobile manipulation, we design a perception-aware multimodal conditioning strategy that dynamically adjusts the fusion weights between various 2D visual images and 3D point clouds, yielding visual features tailored to the current perceptual needs. This allows the model to, for example, adaptively rely more on 2D inputs when semantic information is crucial for action prediction, while placing greater emphasis on 3D geometric information when precise spatial understanding is required. We validate AC-DiT through extensive experiments on both simulated and real-world mobile manipulation tasks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</title>
<link>https://arxiv.org/abs/2307.02075</link>
<guid>https://arxiv.org/abs/2307.02075</guid>
<content:encoded><![CDATA[
arXiv:2307.02075v4 Announce Type: replace 
Abstract: Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To circumvent the shortage of seed alignments provided for training, recent EA models utilize pseudo-labeling strategies to iteratively add unaligned entity pairs predicted with high confidence to the seed alignments for model training. However, the adverse impact of confirmation bias during pseudo-labeling has been largely overlooked, thus hindering entity alignment performance. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to determine entity correspondences and reduce erroneous matches across two KGs. An effective criterion is derived to infer pseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel pseudo-label ensembling refines pseudo-labeled alignments by combining predictions over multiple models independently trained in parallel. The ensembled pseudo-labeled alignments are thereafter used to augment seed alignments to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. Our extensive results and in-depth analyses demonstrate the superiority of UPL-EA over 15 competitive baselines and its utility as a general pseudo-labeling framework for entity alignment.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Modeling with Household Coordination Activities under Limited Information via Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2409.17495</link>
<guid>https://arxiv.org/abs/2409.17495</guid>
<content:encoded><![CDATA[
arXiv:2409.17495v2 Announce Type: replace 
Abstract: Understanding human mobility patterns has long been a challenging task in transportation modeling. Due to the difficulties in obtaining high-quality training datasets across diverse locations, conventional activity-based models and learning-based human mobility modeling algorithms are particularly limited by the availability and quality of datasets. Current approaches primarily focus on spatial-temporal patterns while neglecting semantic relationships such as logical connections or dependencies between activities and household coordination activities like joint shopping trips or family meal times, both crucial for realistic mobility modeling. We propose a retrieval-augmented large language model (LLM) framework that generates activity chains with household coordination using only public accessible statistical and socio-demographic information, reducing the need for sophisticated mobility data. The retrieval-augmentation mechanism enables household coordination and maintains statistical consistency across generated patterns, addressing a key gap in existing methods. Our validation with NHTS and SCAG-ABM datasets demonstrates effective mobility synthesis and strong adaptability for regions with limited mobility data availability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAMS: A python framework for Training Deep Learning Models on EEG Data with Model Card Reporting for Medical Applications</title>
<link>https://arxiv.org/abs/2409.17815</link>
<guid>https://arxiv.org/abs/2409.17815</guid>
<content:encoded><![CDATA[
arXiv:2409.17815v2 Announce Type: replace 
Abstract: Electroencephalography (EEG) provides a non-invasive way to observe brain activity in real time. Deep learning has enhanced EEG analysis, enabling meaningful pattern detection for clinical and research purposes. However, most existing frameworks for EEG data analysis are either focused on preprocessing techniques or deep learning model development, often overlooking the crucial need for structured documentation and model interpretability. In this paper, we introduce DREAMS (Deep REport for AI ModelS), a Python-based framework designed to generate automated model cards for deep learning models applied to EEG data. Unlike generic model reporting tools, DREAMS is specifically tailored for EEG-based deep learning applications, incorporating domain-specific metadata, preprocessing details, performance metrics, and uncertainty quantification. The framework seamlessly integrates with deep learning pipelines, providing structured YAML-based documentation. We evaluate DREAMS through two case studies: an EEG emotion classification task using the FACED dataset and a abnormal EEG classification task using the Temple Univeristy Hospital (TUH) Abnormal dataset. These evaluations demonstrate how the generated model card enhances transparency by documenting model performance, dataset biases, and interpretability limitations. Unlike existing model documentation approaches, DREAMS provides visualized performance metrics, dataset alignment details, and model uncertainty estimations, making it a valuable tool for researchers and clinicians working with EEG-based AI. The source code for DREAMS is open-source, facilitating broad adoption in healthcare AI, research, and ethical AI development.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification</title>
<link>https://arxiv.org/abs/2410.15154</link>
<guid>https://arxiv.org/abs/2410.15154</guid>
<content:encoded><![CDATA[
arXiv:2410.15154v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector, particularly motion control, manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09% and a 131.77% improvement on complex tasks in the MCEVAL dataset.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation</title>
<link>https://arxiv.org/abs/2501.06598</link>
<guid>https://arxiv.org/abs/2501.06598</guid>
<content:encoded><![CDATA[
arXiv:2501.06598v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding tasks. However, interpreting charts with textual descriptions often leads to information loss, as it fails to fully capture the dense information embedded in charts. In contrast, parsing charts into code provides lossless representations that can effectively contain all critical details. Although existing open-source MLLMs have achieved success in chart understanding tasks, they still face two major challenges when applied to chart-to-code tasks: (1) Low executability and poor restoration of chart details in the generated code and (2) Lack of large-scale and diverse training data. To address these challenges, we propose \textbf{ChartCoder}, the first dedicated chart-to-code MLLM, which leverages Code LLMs as the language backbone to enhance the executability of the generated code. Furthermore, we introduce \textbf{Chart2Code-160k}, the first large-scale and diverse dataset for chart-to-code generation, and propose the \textbf{Snippet-of-Thought (SoT)} method, which transforms direct chart-to-code generation data into step-by-step generation. Experiments demonstrate that ChartCoder, with only 7B parameters, surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving superior chart restoration and code excitability. Our code is available at https://github.com/thunlp/ChartCoder.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Categorize Strategic Situations and Decipher Motivations Behind Human Behaviors</title>
<link>https://arxiv.org/abs/2503.15752</link>
<guid>https://arxiv.org/abs/2503.15752</guid>
<content:encoded><![CDATA[
arXiv:2503.15752v5 Announce Type: replace 
Abstract: By varying prompts to a large language model, we can elicit the full range of human behaviors in a variety of different scenarios in classic economic games. By analyzing which prompts elicit which behaviors, we can categorize and compare different strategic situations, which can also help provide insight into what different economic scenarios induce people to think about. We discuss how this provides a first step towards a non-standard method of inferring (deciphering) the motivations behind the human behaviors. We also show how this deciphering process can be used to categorize differences in the behavioral tendencies of different populations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
arXiv:2503.18968v3 Announce Type: replace 
Abstract: In modern medicine, clinical diagnosis relies on the comprehensive analysis of primarily textual and visual data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in large Vision-Language Models (VLMs) and agent-based methods hold great potential for medical diagnosis, thanks to the ability to effectively integrate multi-modal patient data. However, they often provide direct answers and draw empirical-driven conclusions without quantitative analysis, which reduces their reliability and clinical usability. We propose MedAgent-Pro, a new agentic reasoning paradigm that follows the diagnosis principle in modern medicine, to decouple the process into sequential components for step-by-step, evidence-based reasoning. Our MedAgent-Pro workflow presents a hierarchical diagnostic structure to mirror this principle, consisting of disease-level standardized plan generation and patient-level personalized step-by-step reasoning. To support disease-level planning, an RAG-based agent is designed to retrieve medical guidelines to ensure alignment with clinical standards. For patient-level reasoning, we propose to integrate professional tools such as visual models to enable quantitative assessments. Meanwhile, we propose to verify the reliability of each step to achieve evidence-based diagnosis, enforcing rigorous logical reasoning and a well-founded conclusion. Extensive experiments across a wide range of anatomical regions, imaging modalities, and diseases demonstrate the superiority of MedAgent-Pro to mainstream VLMs, agentic systems and state-of-the-art expert models. Ablation studies and human evaluation by clinical experts further validate its robustness and clinical relevance. Code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beating Transformers using Synthetic Cognition</title>
<link>https://arxiv.org/abs/2504.07619</link>
<guid>https://arxiv.org/abs/2504.07619</guid>
<content:encoded><![CDATA[
arXiv:2504.07619v3 Announce Type: replace 
Abstract: The road to Artificial General Intelligence goes through the generation of context-aware reactive behaviors, where the Transformer architecture has been proven to be the state-of-the-art. However, they still fail to develop reasoning. Recently, a novel approach for developing cognitive architectures, called Synthetic Cognition, has been proposed and implemented to develop instantaneous reactive behavior. In this study, we aim to explore the use of Synthetic Cognition to develop context-aware reactive behaviors. We propose a mechanism to deal with sequences for the recent implementation of Synthetic Cognition, and test it against DNA foundation models in DNA sequence classification tasks. In our experiments, our proposal clearly outperforms the DNA foundation models, obtaining the best score on more benchmark tasks than the alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to deal with sequences, and beating the Transformer architecture for sequence classification.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2506.18183</link>
<guid>https://arxiv.org/abs/2506.18183</guid>
<content:encoded><![CDATA[
arXiv:2506.18183v2 Announce Type: replace 
Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Boolean Solution Problem from the Perspective of Predicate Logic -- Extended Version</title>
<link>https://arxiv.org/abs/1706.08329</link>
<guid>https://arxiv.org/abs/1706.08329</guid>
<content:encoded><![CDATA[
arXiv:1706.08329v4 Announce Type: replace-cross 
Abstract: Finding solution values for unknowns in Boolean equations was a principal reasoning mode in the Algebra of Logic of the 19th century. Schr\"oder investigated it as Aufl\"osungsproblem (solution problem). It is closely related to the modern notion of Boolean unification. Today it is commonly presented in an algebraic setting, but seems potentially useful also in knowledge representation based on predicate logic. We show that it can be modeled on the basis of first-order logic extended by second-order quantification. A wealth of classical results transfers, foundations for algorithms unfold, and connections with second-order quantifier elimination and Craig interpolation become apparent. Although for first-order inputs the set of solutions is recursively enumerable, the development of constructive methods remains a challenge. We identify some cases that allow constructions, most of them based on Craig interpolation.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Distillation via the Wasserstein Metric</title>
<link>https://arxiv.org/abs/2311.18531</link>
<guid>https://arxiv.org/abs/2311.18531</guid>
<content:encoded><![CDATA[
arXiv:2311.18531v3 Announce Type: replace-cross 
Abstract: Dataset Distillation (DD) aims to generate a compact synthetic dataset that enables models to achieve performance comparable to training on the full large dataset, significantly reducing computational costs. Drawing from optimal transport theory, we introduce WMDD (Wasserstein Metric-based Dataset Distillation), a straightforward yet powerful method that employs the Wasserstein metric to enhance distribution matching.
  We compute the Wasserstein barycenter of features from a pretrained classifier to capture essential characteristics of the original data distribution. By optimizing synthetic data to align with this barycenter in feature space and leveraging per-class BatchNorm statistics to preserve intra-class variations, WMDD maintains the efficiency of distribution matching approaches while achieving state-of-the-art results across various high-resolution datasets. Our extensive experiments demonstrate WMDD's effectiveness and adaptability, highlighting its potential for advancing machine learning applications at scale.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Squat: Quant Small Language Models on the Edge</title>
<link>https://arxiv.org/abs/2402.10787</link>
<guid>https://arxiv.org/abs/2402.10787</guid>
<content:encoded><![CDATA[
arXiv:2402.10787v2 Announce Type: replace-cross 
Abstract: A growing trend has emerged in designing high-quality Small Language Models (SLMs) with a few million parameters. This trend is driven by the increasing concerns over cloud costs, privacy, and latency. Considering that full parameter training is feasible for SLMs on mobile devices, Quantization-Aware Training (QAT) is employed to improve efficiency by reducing computational overhead and memory footprint. However, previous QAT works adopt fine-grained quantization methods to compress models with billions of parameters on GPUs, incompatible with current commodity hardware, such as mobile and edge devices, which relies on Single Instruction Multiple Data (SIMD) instructions. Thus, the generalization of these methods to SLMs on mobile devices is limited. In this paper, we propose Squat method, an effective QAT framework with deployable quantization for SLMs on mobile devices. Specifically, we propose entropy-guided and distribution-aligned distillation to mitigate the distortion of attention information from quantization. Besides, we employ sub-8-bit token adaptive quantization, assigning varying bit widths to different tokens based on their importance. Furthermore, we develop a SIMD-based Multi-Kernel Mixed-Precision (MKMP) multiplier to support sub-8-bit mixed-precision MAC on mobile devices. Our extensive experiments verify the substantial improvements of our method compared to other QAT methods across various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts, signaling a great advancement. Code: https://github.com/shawnricecake/squant
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Embedded AI Systems</title>
<link>https://arxiv.org/abs/2402.11322</link>
<guid>https://arxiv.org/abs/2402.11322</guid>
<content:encoded><![CDATA[
arXiv:2402.11322v4 Announce Type: replace-cross 
Abstract: Embedded AI systems are expected to incur low power/energy consumption for solving machine learning tasks, as these systems are usually power constrained (e.g., object recognition task in autonomous mobile agents with portable batteries). These requirements can be fulfilled by Spiking Neural Networks (SNNs), since their bio-inspired spike-based operations offer high accuracy and ultra low-power/energy computation. Currently, most of SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, and/or developed without considering memory budgets from the underlying processing hardware of embedded platforms. These limitations hinder SNNs from reaching their full potential in accuracy and efficiency. Toward this, we propose SpikeNAS, a novel fast memory-aware neural architecture search (NAS) framework for SNNs that quickly finds an appropriate SNN architecture with high accuracy under the given memory budgets from targeted embedded systems. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, developing a fast memory-aware search algorithm, and performing quantization. The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy compared to state-of-the-art while meeting the given memory budgets (e.g., 29x, 117x, and 3.7x faster search for CIFAR10, CIFAR100, and TinyImageNet200 respectively, using an Nvidia RTX A6000 GPU machine), thereby quickly providing the appropriate SNN architecture for the memory-constrained embedded AI systems.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divergent Creativity in Humans and Large Language Models</title>
<link>https://arxiv.org/abs/2405.13012</link>
<guid>https://arxiv.org/abs/2405.13012</guid>
<content:encoded><![CDATA[
arXiv:2405.13012v2 Announce Type: replace-cross 
Abstract: The recent surge of Large Language Models (LLMs) has led to claims that they are approaching a level of creativity akin to human capabilities. This idea has sparked a blend of excitement and apprehension. However, a critical piece that has been missing in this discourse is a systematic evaluation of LLMs' semantic diversity, particularly in comparison to human divergent thinking. To bridge this gap, we leverage recent advances in computational creativity to analyze semantic divergence in both state-of-the-art LLMs and a substantial dataset of 100,000 humans. We found evidence that LLMs can surpass average human performance on the Divergent Association Task, and approach human creative writing abilities, though they fall short of the typical performance of highly creative humans. Notably, even the top performing LLMs are still largely surpassed by highly creative individuals, underscoring a ceiling that current LLMs still fail to surpass. Our human-machine benchmarking framework addresses the polemic surrounding the imminent replacement of human creative labour by AI, disentangling the quality of the respective creative linguistic outputs using established objective measures. While prompting deeper exploration of the distinctive elements of human inventive thought compared to those of AI systems, we lay out a series of techniques to improve their outputs with respect to semantic diversity, such as prompt design and hyper-parameter tuning.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OralBBNet: Spatially Guided Dental Segmentation of Panoramic X-Rays with Bounding Box Priors</title>
<link>https://arxiv.org/abs/2406.03747</link>
<guid>https://arxiv.org/abs/2406.03747</guid>
<content:encoded><![CDATA[
arXiv:2406.03747v3 Announce Type: replace-cross 
Abstract: Teeth segmentation and recognition play a vital role in a variety of dental applications and diagnostic procedures. The integration of deep learning models has facilitated the development of precise and automated segmentation methods. Although prior research has explored teeth segmentation, not many methods have successfully performed tooth segmentation and detection simultaneously. This study presents UFBA-425, a dental dataset derived from the UFBA-UESC dataset, featuring bounding box and polygon annotations for 425 panoramic dental X-rays. In addition, this paper presents the OralBBNet architecture, which is based on the best segmentation and detection qualities of architectures such as U-Net and YOLOv8, respectively. OralBBNet is designed to improve the accuracy and robustness of tooth classification and segmentation on panoramic X-rays by leveraging the complementary strengths of U-Net and YOLOv8. Our approach achieved a 1-3% improvement in mean average precision (mAP) for tooth detection compared to existing techniques and a 15-20% improvement in the dice score for teeth segmentation over state-of-the-art (SOTA) solutions for various tooth categories and 2-4% improvement in the dice score compared to other SOTA segmentation architectures. The results of this study establish a foundation for the wider implementation of object detection models in dental diagnostics.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Consistency Models with Generator-Augmented Flows</title>
<link>https://arxiv.org/abs/2406.09570</link>
<guid>https://arxiv.org/abs/2406.09570</guid>
<content:encoded><![CDATA[
arXiv:2406.09570v4 Announce Type: replace-cross 
Abstract: Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network. They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network. In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model. We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost. Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at: https://github.com/thibautissenhuth/consistency_GC.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Instruction Following in Unknown Environments</title>
<link>https://arxiv.org/abs/2406.11818</link>
<guid>https://arxiv.org/abs/2406.11818</guid>
<content:encoded><![CDATA[
arXiv:2406.11818v2 Announce Type: replace-cross 
Abstract: Enabling embodied agents to complete complex human instructions from natural language is crucial to autonomous systems in household services. Conventional methods can only accomplish human instructions in the known environment where all interactive objects are provided to the embodied agent, and directly deploying the existing approaches for the unknown environment usually generates infeasible plans that manipulate non-existing objects. On the contrary, we propose an embodied instruction following (EIF) method for complex tasks in the unknown environment, where the agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. Specifically, we build a hierarchical embodied instruction following framework including the high-level task planner and the low-level exploration controller with multimodal large language models. We then construct a semantic representation map of the scene with dynamic region attention to demonstrate the known visual clues, where the goal of task planning and scene exploration is aligned for human instruction. For the task planner, we generate the feasible step-by-step plans for human goal accomplishment according to the task completion process and the known visual clues. For the exploration controller, the optimal navigation or object interaction policy is predicted based on the generated step-wise plans and the known visual clues. The experimental results demonstrate that our method can achieve 45.09% success rate in 204 complex human instructions such as making breakfast and tidying rooms in large house-level scenes. Code and supplementary are available at https://gary3410.github.io/eif_unknown.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdooring Bias (B^2) into Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2406.15213</link>
<guid>https://arxiv.org/abs/2406.15213</guid>
<content:encoded><![CDATA[
arXiv:2406.15213v3 Announce Type: replace-cross 
Abstract: Recent advances in large text-conditional diffusion models have revolutionized image generation by enabling users to create realistic, high-quality images from textual prompts, significantly enhancing artistic creation and visual communication. However, these advancements also introduce an underexplored attack opportunity: the possibility of inducing biases by an adversary into the generated images for malicious intentions, e.g., to influence public opinion and spread propaganda. In this paper, we study an attack vector that allows an adversary to inject arbitrary bias into a target model. The attack leverages low-cost backdooring techniques using a targeted set of natural textual triggers embedded within a small number of malicious data samples produced with public generative models. An adversary could pick common sequences of words that can then be inadvertently activated by benign users during inference. We investigate the feasibility and challenges of such attacks, demonstrating how modern generative models have made this adversarial process both easier and more adaptable. On the other hand, we explore various aspects of the detectability of such attacks and demonstrate that the model's utility remains intact in the absence of the triggers. Our extensive experiments using over 200,000 generated images and against hundreds of fine-tuned models demonstrate the feasibility of the presented backdoor attack. We illustrate how these biases maintain strong text-image alignment, highlighting the challenges in detecting biased images without knowing that bias in advance. Our cost analysis confirms the low financial barrier ($10-$15) to executing such attacks, underscoring the need for robust defensive strategies against such vulnerabilities in diffusion models.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sublinear Regret for a Class of Continuous-Time Linear-Quadratic Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2407.17226</link>
<guid>https://arxiv.org/abs/2407.17226</guid>
<content:encoded><![CDATA[
arXiv:2407.17226v5 Announce Type: replace-cross 
Abstract: We study reinforcement learning (RL) for a class of continuous-time linear-quadratic (LQ) control problems for diffusions, where states are scalar-valued and running control rewards are absent but volatilities of the state processes depend on both state and control variables. We apply a model-free approach that relies neither on knowledge of model parameters nor on their estimations, and devise an RL algorithm to learn the optimal policy parameter directly. Our main contributions include the introduction of an exploration schedule and a regret analysis of the proposed algorithm. We provide the convergence rate of the policy parameter to the optimal one, and prove that the algorithm achieves a regret bound of $O(N^{\frac{3}{4}})$ up to a logarithmic factor, where $N$ is the number of learning episodes. We conduct a simulation study to validate the theoretical results and demonstrate the effectiveness and reliability of the proposed algorithm. We also perform numerical comparisons between our method and those of the recent model-based stochastic LQ RL studies adapted to the state- and control-dependent volatility setting, demonstrating a better performance of the former in terms of regret bounds.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drug Discovery SMILES-to-Pharmacokinetics Diffusion Models with Deep Molecular Understanding</title>
<link>https://arxiv.org/abs/2408.07636</link>
<guid>https://arxiv.org/abs/2408.07636</guid>
<content:encoded><![CDATA[
arXiv:2408.07636v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is increasingly used in every stage of drug development. One challenge facing drug discovery AI is that drug pharmacokinetic (PK) datasets are often collected independently from each other, often with limited overlap, creating data overlap sparsity. Data sparsity makes data curation difficult for researchers looking to answer research questions in poly-pharmacy, drug combination research, and high-throughput screening. We propose Imagand, a novel SMILES-to-Pharmacokinetic (S2PK) diffusion model capable of generating an array of PK target properties conditioned on SMILES inputs. We show that Imagand-generated synthetic PK data closely resembles real data univariate and bivariate distributions, and improves performance for downstream tasks. Imagand is a promising solution for data overlap sparsity and allows researchers to efficiently generate ligand PK data for drug discovery research. Code is available at https://github.com/bing1100/Imagand.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NegMerge: Sign-Consensual Weight Merging for Machine Unlearning</title>
<link>https://arxiv.org/abs/2410.05583</link>
<guid>https://arxiv.org/abs/2410.05583</guid>
<content:encoded><![CDATA[
arXiv:2410.05583v2 Announce Type: replace-cross 
Abstract: Machine unlearning aims to selectively remove specific knowledge from a trained model. Existing approaches, such as Task Arithmetic, fine-tune the model on the forget set to create a task vector (i.e., a direction in weight space) for subtraction from the original model's weight. However, their effectiveness is highly sensitive to hyperparameter selection, requiring extensive validation to identify the optimal vector from many fine-tuned candidates. In this paper, we propose a novel method that utilizes all fine-tuned models trained with varying hyperparameters instead of a single selection. Specifically, we aggregate the computed task vectors by retaining only the elements with consistent shared signs. The merged task vector is then negated to induce unlearning on the original model. Evaluations on zero-shot and standard image recognition tasks across twelve datasets and four backbone architectures show that our approach outperforms state-of-the-art methods while requiring similar or fewer computational resources. Code is available at https://github.com/naver-ai/negmerge.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization</title>
<link>https://arxiv.org/abs/2410.20573</link>
<guid>https://arxiv.org/abs/2410.20573</guid>
<content:encoded><![CDATA[
arXiv:2410.20573v2 Announce Type: replace-cross 
Abstract: Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning and Adversarial Disentanglement for Privacy-Aware Task-Oriented Semantic Communication</title>
<link>https://arxiv.org/abs/2410.22784</link>
<guid>https://arxiv.org/abs/2410.22784</guid>
<content:encoded><![CDATA[
arXiv:2410.22784v3 Announce Type: replace-cross 
Abstract: Task-oriented semantic communication systems have emerged as a promising approach to achieving efficient and intelligent data transmission in next-generation networks, where only information relevant to a specific task is communicated. This is particularly important in 6G-enabled Internet of Things (6G-IoT) scenarios, where bandwidth constraints, latency requirements, and data privacy are critical. However, existing methods struggle to fully disentangle task-relevant and task-irrelevant information, leading to privacy concerns and suboptimal performance. To address this, we propose an information-bottleneck inspired method, named CLAD (contrastive learning and adversarial disentanglement). CLAD utilizes contrastive learning to effectively capture task-relevant features while employing adversarial disentanglement to discard task-irrelevant information. Additionally, due to the absence of reliable and reproducible methods to quantify the minimality of encoded feature vectors, we introduce the Information Retention Index (IRI), a comparative metric used as a proxy for the mutual information between the encoded features and the input. The IRI reflects how minimal and informative the representation is, making it highly relevant for privacy-preserving and bandwidth-efficient 6G-IoT systems. Extensive experiments demonstrate that CLAD outperforms state-of-the-art baselines in terms of semantic extraction, task performance, privacy preservation, and IRI, making it a promising building block for responsible, efficient and trustworthy 6G-IoT services.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.23114</link>
<guid>https://arxiv.org/abs/2410.23114</guid>
<content:encoded><![CDATA[
arXiv:2410.23114v3 Announce Type: replace-cross 
Abstract: Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, we design a unified framework to measure the object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to evaluate hallucinations via (object, relation, object) triplets extracted from LVLMs' responses, making it easily generalizable to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. With comprehensive evaluations on Tri-HE, we observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple training-free approach that effectively mitigates hallucinations for LVLMs. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>There and Back Again: On the relation between Noise and Image Inversions in Diffusion Models</title>
<link>https://arxiv.org/abs/2410.23530</link>
<guid>https://arxiv.org/abs/2410.23530</guid>
<content:encoded><![CDATA[
arXiv:2410.23530v3 Announce Type: replace-cross 
Abstract: Diffusion Models achieve state-of-the-art performance in generating new samples but lack a low-dimensional latent space that encodes the data into meaningful features. Inversion-based methods address this by reversing the denoising trajectory, mapping each image back to its approximated starting noise. In this work, we thoroughly analyze this procedure and focus on the relation between the initial Gaussian noise, the generated samples, and their corresponding latent encodings obtained through the DDIM inversion. First, we show that latents exhibit structural patterns in the form of less diverse noise predicted for smooth image regions. As a consequence of this divergence, we present that the space of image inversions is notably less manipulative than the original Gaussian noise. Next, we explain the origin of the phenomenon, demonstrating that, during the first inversion steps, the noise prediction error is much more significant for the plain areas than for the rest of the image. As a surprisingly simple solution, we propose to replace the first DDIM Inversion steps with a forward diffusion process, which successfully decorrelates latent encodings, leading to higher quality editions and interpolations. The code is available at https://github.com/luk-st/taba.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation</title>
<link>https://arxiv.org/abs/2411.16370</link>
<guid>https://arxiv.org/abs/2411.16370</guid>
<content:encoded><![CDATA[
arXiv:2411.16370v4 Announce Type: replace-cross 
Abstract: Advancements in image segmentation play an integral role within the broad scope of Deep Learning-based Computer Vision. Furthermore, their widespread applicability in critical real-world tasks has resulted in challenges related to the reliability of such algorithms. Hence, uncertainty quantification has been extensively studied within this context, enabling the expression of model ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to prevent uninformed decision-making. Due to the rapid adoption of Convolutional Neural Network (CNN)-based segmentation models in high-stake applications, a substantial body of research has been published on this very topic, causing its swift expansion into a distinct field. This work provides a comprehensive overview of probabilistic segmentation, by discussing fundamental concepts of uncertainty quantification, governing advancements in the field as well as the application to various tasks. Moreover, literature on both types of uncertainties trace back to four key applications: (1) to quantify statistical inconsistencies in the annotation process due ambiguous images, (2) correlating prediction error with uncertainty, (3) expanding the model hypothesis space for better generalization, and (4) Active Learning. An extensive discussion follows that includes an overview of utilized datasets for each of the applications and evaluation of the available methods. We also highlight challenges related to architectures, uncertainty quantification methods, standardization and benchmarking, and finally end with recommendations for future work such as methods based on single forward passes and models that appropriately leverage volumetric data.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2412.05563</link>
<guid>https://arxiv.org/abs/2412.05563</guid>
<content:encoded><![CDATA[
arXiv:2412.05563v2 Announce Type: replace-cross 
Abstract: The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence. Previous work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant research efforts devoted to quantifying the uncertainty of LLMs. This survey seeks to provide an extensive review of existing uncertainty quantification methods for LLMs, identifying their salient features, along with their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid understanding of the state of the art. Furthermore, we highlight applications of uncertainty quantification methods for LLMs, spanning chatbot and textual applications to embodied artificial intelligence applications in robotics. We conclude with open research challenges in uncertainty quantification of LLMs, seeking to motivate future research.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2412.16264</link>
<guid>https://arxiv.org/abs/2412.16264</guid>
<content:encoded><![CDATA[
arXiv:2412.16264v4 Announce Type: replace-cross 
Abstract: Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism to drop outdated samples. The proposed strategic sample selection algorithm prioritizes new samples that cause the `drifted' pattern, enabling the model to better understand the evolving landscape. Additionally, we introduce strategic forgetting upon detecting significant drift by discarding outdated samples to free up memory, allowing the incorporation of more recent data. SSF captures evolving patterns effectively and ensures the model is aligned with the change of data patterns, significantly enhancing the IDS's adaptability to concept drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15 datasets demonstrates its superior adaptability to concept drift for network intrusion detection. The code is released at https://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Mining Collectively-Behaving Bots in MMORPGs</title>
<link>https://arxiv.org/abs/2501.10461</link>
<guid>https://arxiv.org/abs/2501.10461</guid>
<content:encoded><![CDATA[
arXiv:2501.10461v2 Announce Type: replace-cross 
Abstract: In MMORPGs (Massively Multiplayer Online Role-Playing Games), abnormal players (bots) using unauthorized automated programs to carry out pre-defined behaviors systematically and repeatedly are commonly observed. Bots usually engage in these activities to gain in-game money, which they eventually trade for real money outside the game. Such abusive activities negatively impact the in-game experiences of legitimate users since bots monopolize specific hunting areas and obtain valuable items. Thus, detecting abnormal players is a significant task for game companies. Motivated by the fact that bots tend to behave collectively with similar in-game trajectories due to the auto-programs, we developed BotTRep, a framework that comprises trajectory representation learning followed by clustering using a completely unlabeled in-game trajectory dataset. Our model aims to learn representations for in-game trajectory sequences so that players with contextually similar trajectories have closer embeddings. Then, by applying DBSCAN to these representations and visualizing the corresponding moving patterns, our framework ultimately assists game masters in identifying and banning bots.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirRadar: Inferring Nationwide Air Quality in China with Deep Neural Networks</title>
<link>https://arxiv.org/abs/2501.13141</link>
<guid>https://arxiv.org/abs/2501.13141</guid>
<content:encoded><![CDATA[
arXiv:2501.13141v2 Announce Type: replace-cross 
Abstract: Monitoring real-time air quality is essential for safeguarding public health and fostering social progress. However, the widespread deployment of air quality monitoring stations is constrained by their significant costs. To address this limitation, we introduce \emph{AirRadar}, a deep neural network designed to accurately infer real-time air quality in locations lacking monitoring stations by utilizing data from existing ones. By leveraging learnable mask tokens, AirRadar reconstructs air quality features in unmonitored regions. Specifically, it operates in two stages: first capturing spatial correlations and then adjusting for distribution shifts. We validate AirRadar's efficacy using a year-long dataset from 1,085 monitoring stations across China, demonstrating its superiority over multiple baselines, even with varying degrees of unobserved data. The source code can be accessed at https://github.com/CityMind-Lab/AirRadar.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation</title>
<link>https://arxiv.org/abs/2501.14400</link>
<guid>https://arxiv.org/abs/2501.14400</guid>
<content:encoded><![CDATA[
arXiv:2501.14400v2 Announce Type: replace-cross 
Abstract: Real-world tasks such as garment manipulation and table rearrangement demand robots to perform generalizable, highly precise, and long-horizon actions. Although imitation learning has proven to be an effective approach for teaching robots new skills, large amounts of expert demonstration data are still indispensible for these complex tasks, resulting in high sample complexity and costly data collection. To address this, we propose Semantic Keypoint Imitation Learning (SKIL), a framework which automatically obtains semantic keypoints with the help of vision foundation models, and forms the descriptor of semantic keypoints that enables efficient imitation learning of complex robotic tasks with significantly lower sample complexity. In real-world experiments, SKIL doubles the performance of baseline methods in tasks such as picking a cup or mouse, while demonstrating exceptional robustness to variations in objects, environmental changes, and distractors. For long-horizon tasks like hanging a towel on a rack where previous methods fail completely, SKIL achieves a mean success rate of 70\% with as few as 30 demonstrations. Furthermore, SKIL naturally supports cross-embodiment learning due to its semantic keypoints abstraction. Our experiments demonstrate that even human videos bring considerable improvement to the learning performance. All these results demonstrate the great success of SKIL in achieving data-efficient generalizable robotic learning. Visualizations and code are available at: https://skil-robotics.github.io/SKIL-robotics/.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior</title>
<link>https://arxiv.org/abs/2502.13998</link>
<guid>https://arxiv.org/abs/2502.13998</guid>
<content:encoded><![CDATA[
arXiv:2502.13998v2 Announce Type: replace-cross 
Abstract: Image watermarks have been considered a promising technique to help detect AI-generated content, which can be used to protect copyright or prevent fake image abuse. In this work, we present a black-box method for removing invisible image watermarks, without the need of any dataset of watermarked images or any knowledge about the watermark system. Our approach is simple to implement: given a single watermarked image, we regress it by deep image prior (DIP). We show that from the intermediate steps of DIP one can reliably find an evasion image that can remove invisible watermarks while preserving high image quality. Due to its unique working mechanism and practical effectiveness, we advocate including DIP as a baseline invasion method for benchmarking the robustness of watermarking systems. Finally, by showing the limited ability of DIP and other existing black-box methods in evading training-based visible watermarks, we discuss the positive implications on the practical use of training-based visible watermarks to prevent misinformation abuse.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Matching for Self-Supervised Transfer Learning</title>
<link>https://arxiv.org/abs/2502.14424</link>
<guid>https://arxiv.org/abs/2502.14424</guid>
<content:encoded><![CDATA[
arXiv:2502.14424v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel self-supervised transfer learning method called \underline{\textbf{D}}istribution \underline{\textbf{M}}atching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. DM results in a learned representation space that is intuitively structured and therefore easy to interpret.
  Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis</title>
<link>https://arxiv.org/abs/2503.00032</link>
<guid>https://arxiv.org/abs/2503.00032</guid>
<content:encoded><![CDATA[
arXiv:2503.00032v4 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) increases the difficulty of distinguishing between human-written and LLM-generated text. Detecting LLM-generated text is crucial for upholding academic integrity, preventing plagiarism, protecting copyrights, and ensuring ethical research practices. Most prior studies on detecting LLM-generated text focus primarily on English text. However, languages with distinct morphological and syntactic characteristics require specialized detection approaches. Their unique structures and usage patterns can hinder the direct application of methods primarily designed for English. Among such languages, we focus on Korean, which has relatively flexible spacing rules, a rich morphological system, and less frequent comma usage compared to English. We introduce KatFish, the first benchmark dataset for detecting LLM-generated Korean text. The dataset consists of text written by humans and generated by four LLMs across three genres.
  By examining spacing patterns, part-of-speech diversity, and comma usage, we illuminate the linguistic differences between human-written and LLM-generated Korean text. Building on these observations, we propose KatFishNet, a detection method specifically designed for the Korean language. KatFishNet achieves an average of 19.78% higher AUROC compared to the best-performing existing detection method. Our code and data are available at https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks</title>
<link>https://arxiv.org/abs/2503.00781</link>
<guid>https://arxiv.org/abs/2503.00781</guid>
<content:encoded><![CDATA[
arXiv:2503.00781v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have proven immensely beneficial in education by capturing vast amounts of literature-based information, allowing them to generate context without relying on external sources. In this paper, we propose a generative AI-powered GATE question-answering framework (GATE stands for Graduate Aptitude Test in Engineering) that leverages LLMs to explain GATE solutions and support students in their exam preparation. We conducted extensive benchmarking to select the optimal embedding model and LLM, evaluating our framework based on criteria such as latency, faithfulness, and relevance, with additional validation through human evaluation. Our chatbot integrates state-of-the-art embedding models and LLMs to deliver accurate, context-aware responses. Through rigorous experimentation, we identified configurations that balance performance and computational efficiency, ensuring a reliable chatbot to serve students' needs. Additionally, we discuss the challenges faced in data processing and modeling and implemented solutions. Our work explores the application of Retrieval-Augmented Generation (RAG) for GATE Q/A explanation tasks, and our findings demonstrate significant improvements in retrieval accuracy and response quality. This research offers practical insights for developing effective AI-driven educational tools while highlighting areas for future enhancement in usability and scalability.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFO: Piloting VLM Feedback for Offline RL</title>
<link>https://arxiv.org/abs/2503.01062</link>
<guid>https://arxiv.org/abs/2503.01062</guid>
<content:encoded><![CDATA[
arXiv:2503.01062v4 Announce Type: replace-cross 
Abstract: While internet-scale image and textual data have enabled strong generalization in Vision-Language Models (VLMs), the absence of internet-scale control data has impeded the development of similar generalization in standard reinforcement learning (RL) agents. Although VLMs are fundamentally limited in their ability to solve control tasks due to their lack of action-conditioned training data, their capacity for image understanding allows them to provide valuable feedback in RL tasks by recognizing successful outcomes. A key challenge in Reinforcement Learning from AI Feedback (RLAIF) is determining how best to integrate VLM-derived signals into the learning process. We explore this question in the context of offline RL and introduce a class of methods called sub-trajectory filtered optimization. We identify three key insights. First, trajectory length plays a crucial role in offline RL, as full-trajectory preference learning exacerbates the stitching problem, necessitating the use of sub-trajectories. Second, even in Markovian environments, a non-Markovian reward signal from a sequence of images is required to assess trajectory improvement, as VLMs do not interpret control actions and must rely on visual cues over time. Third, a simple yet effective approach--filtered and weighted behavior cloning--consistently outperforms more complex reinforcement learning from human feedback-based methods. We propose sub-trajectory filtered behavior cloning, a method that leverages VLM feedback on sub-trajectories while incorporating a retrospective filtering mechanism that removes sub-trajectories preceding failures to improve robustness and prevent turbulence. This study is preliminary; we provide initial evidence through evaluations on a toy control domain. Please enjoy our airport puns.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Metacognitive Architectures Remember Their Own Thoughts: A Systematic Review</title>
<link>https://arxiv.org/abs/2503.13467</link>
<guid>https://arxiv.org/abs/2503.13467</guid>
<content:encoded><![CDATA[
arXiv:2503.13467v2 Announce Type: replace-cross 
Abstract: Background: Metacognition has gained significant attention for its potential to enhance autonomy and adaptability of artificial agents but remains a fragmented field: diverse theories, terminologies, and design choices have led to disjointed developments and limited comparability across systems. Existing overviews remain at a conceptual level that is undiscerning to the underlying algorithms, representations, and their respective success.
  Methods: We address this gap by performing an explorative systematic review. Reports were included if they described techniques enabling Computational Metacognitive Architectures (CMAs) to model, store, remember, and process their episodic metacognitive experiences, one of Flavell's (1979a) three foundational components of metacognition. Searches were conducted in 16 databases, consulted between December 2023 and June 2024. Data were extracted using a 20-item framework considering pertinent aspects.
  Results: A total of 101 reports on 35 distinct CMAs were included. Our findings show that metacognitive experiences may boost system performance and explainability, e.g., via self-repair. However, lack of standardization and limited evaluations may hinder progress: only 17% of CMAs were quantitatively evaluated regarding this review's focus, and significant terminological inconsistency limits cross-architecture synthesis. Systems also varied widely in memory content, data types, and employed algorithms.
  Discussion: Limitations include the non-iterative nature of the search query, heterogeneous data availability, and an under-representation of emergent, sub-symbolic CMAs. Future research should focus on standardization and evaluation, e.g., via community-driven challenges, and on transferring promising principles to emergent architectures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concat-ID: Towards Universal Identity-Preserving Video Synthesis</title>
<link>https://arxiv.org/abs/2503.14151</link>
<guid>https://arxiv.org/abs/2503.14151</guid>
<content:encoded><![CDATA[
arXiv:2503.14151v3 Announce Type: replace-cross 
Abstract: We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs variational autoencoders to extract image features, which are then concatenated with video latents along the sequence dimension. It relies exclusively on inherent 3D self-attention mechanisms to incorporate them, eliminating the need for additional parameters or modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin</title>
<link>https://arxiv.org/abs/2504.03597</link>
<guid>https://arxiv.org/abs/2504.03597</guid>
<content:encoded><![CDATA[
arXiv:2504.03597v2 Announce Type: replace-cross 
Abstract: We introduce real-is-sim, a new approach to integrating simulation into behavior cloning pipelines. In contrast to real-only methods, which lack the ability to safely test policies before deployment, and sim-to-real methods, which require complex adaptation to cross the sim-to-real gap, our framework allows policies to seamlessly switch between running on real hardware and running in parallelized virtual environments. At the center of real-is-sim is a dynamic digital twin, powered by the Embodied Gaussian simulator, that synchronizes with the real world at 60Hz. This twin acts as a mediator between the behavior cloning policy and the real robot. Policies are trained using representations derived from simulator states and always act on the simulated robot, never the real one. During deployment, the real robot simply follows the simulated robot's joint states, and the simulation is continuously corrected with real world measurements. This setup, where the simulator drives all policy execution and maintains real-time synchronization with the physical world, shifts the responsibility of crossing the sim-to-real gap to the digital twin's synchronization mechanisms, instead of the policy itself. We demonstrate real-is-sim on a long-horizon manipulation task (PushT), showing that virtual evaluations are consistent with real-world results. We further show how real-world data can be augmented with virtual rollouts and compare to policies trained on different representations derived from the simulator state including object poses and rendered images from both static and robot-mounted cameras. Our results highlight the flexibility of the real-is-sim framework across training, evaluation, and deployment stages. Videos available at https://real-is-sim.github.io.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</title>
<link>https://arxiv.org/abs/2504.03814</link>
<guid>https://arxiv.org/abs/2504.03814</guid>
<content:encoded><![CDATA[
arXiv:2504.03814v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data. Such loops were shown to lead to distribution shifts - models misrepresenting the true underlying distributions of human data (also called model collapse). However, how human data properties affect such shifts remains poorly understood. In this paper, we provide the first empirical examination of the effect of such properties on the outcome of recursive training. We first confirm that using different human datasets leads to distribution shifts of different magnitudes. Through exhaustive manipulation of dataset properties combined with regression analyses, we then identify a set of properties predicting distribution shift magnitudes. Lexical diversity is found to amplify these shifts, while semantic diversity and data quality mitigate them. Furthermore, we find that these influences are highly modular: data scrapped from a given internet domain has little influence on the content generated for another domain. Finally, experiments on political bias reveal that human data properties affect whether the initial bias will be amplified or reduced. Overall, our results portray a novel view, where different parts of internet may undergo different types of distribution shift.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis</title>
<link>https://arxiv.org/abs/2504.05684</link>
<guid>https://arxiv.org/abs/2504.05684</guid>
<content:encoded><![CDATA[
arXiv:2504.05684v2 Announce Type: replace-cross 
Abstract: This paper introduces Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning (TARO), a novel framework for high-fidelity and temporally coherent video-to-audio synthesis. Built upon flow-based transformers, which offer stable training and continuous transformations for enhanced synchronization and audio quality, TARO introduces two key innovations: (1) Timestep-Adaptive Representation Alignment (TRA), which dynamically aligns latent representations by adjusting alignment strength based on the noise schedule, ensuring smooth evolution and improved fidelity, and (2) Onset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp event-driven markers of audio-relevant visual moments to enhance synchronization with dynamic visual events. Extensive experiments on the VGGSound and Landscape datasets demonstrate that TARO outperforms prior methods, achieving relatively 53% lower Frechet Distance (FD), 29% lower Frechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its superior audio quality and synchronization precision.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond</title>
<link>https://arxiv.org/abs/2504.13037</link>
<guid>https://arxiv.org/abs/2504.13037</guid>
<content:encoded><![CDATA[
arXiv:2504.13037v3 Announce Type: replace-cross 
Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual's disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LZ Penalty: An information-theoretic repetition penalty for autoregressive language models</title>
<link>https://arxiv.org/abs/2504.20131</link>
<guid>https://arxiv.org/abs/2504.20131</guid>
<content:encoded><![CDATA[
arXiv:2504.20131v2 Announce Type: replace-cross 
Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate repetitions in autoregressive language models without loss of capability. The penalty is based on the codelengths in the LZ77 universal lossless compression algorithm. Through the lens of the prediction-compression duality, decoding the LZ penalty has the interpretation of sampling from the residual distribution after removing the information that is highly compressible. We demonstrate the LZ penalty enables state-of-the-art open-source reasoning models to operate with greedy (temperature zero) decoding without loss of capability and without instances of degenerate repetition. Both the industry-standard frequency penalty and repetition penalty are ineffective, incurring degenerate repetition rates of up to 4%.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceiving Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models</title>
<link>https://arxiv.org/abs/2505.05626</link>
<guid>https://arxiv.org/abs/2505.05626</guid>
<content:encoded><![CDATA[
arXiv:2505.05626v3 Announce Type: replace-cross 
Abstract: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robustness to Missing Modalities through Clustered Federated Learning</title>
<link>https://arxiv.org/abs/2505.06911</link>
<guid>https://arxiv.org/abs/2505.06911</guid>
<content:encoded><![CDATA[
arXiv:2505.06911v2 Announce Type: replace-cross 
Abstract: In the era of big data, data mining has become indispensable for uncovering hidden patterns and insights from vast and complex datasets. The integration of multimodal data sources further enhances its potential. Multimodal Federated Learning (MFL) is a distributed approach that enhances the efficiency and quality of multimodal learning, ensuring collaborative work and privacy protection. However, missing modalities pose a significant challenge in MFL, often due to data quality issues or privacy policies across the clients. In this work, we present MMiC, a framework for Mitigating Modality incompleteness in MFL within the Clusters. MMiC replaces partial parameters within client models inside clusters to mitigate the impact of missing modalities. Furthermore, it leverages the Banzhaf Power Index to optimize client selection under these conditions. Finally, MMiC employs an innovative approach to dynamically control global aggregation by utilizing Markovitz Portfolio Optimization. Extensive experiments demonstrate that MMiC consistently outperforms existing federated learning architectures in both global and personalized performance on multimodal datasets with missing modalities, confirming the effectiveness of our proposed solution.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations</title>
<link>https://arxiv.org/abs/2505.08195</link>
<guid>https://arxiv.org/abs/2505.08195</guid>
<content:encoded><![CDATA[
arXiv:2505.08195v2 Announce Type: replace-cross 
Abstract: We have developed Aitomia - a platform powered by AI to assist in performing AI-driven atomistic and quantum chemical (QC) simulations. This evolving intelligent assistant platform is equipped with chatbots and AI agents to help experts and guide non-experts in setting up and running the atomistic simulations, monitoring their computation status, analyzing the simulation results, and summarizing them for the user in text and graphical forms. We achieve these goals by exploiting open-source large language models (LLMs, original and fine-tuned), rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia leverages the versatility of our MLatom ecosystem, supporting AI-enhanced computational chemistry tasks ranging from ground- to excited-state calculations such as geometry optimizations, thermochemistry, and spectra calculations. Aitomia is the first intelligent assistant publicly accessible online on a cloud computing platform for atomistic simulations of broad scope (Aitomistic Hub at https://aitomistic.xyz), while it may also be deployed locally as described at http://mlatom.com/aitomia. Aitomia is expected to lower the barrier to performing atomistic simulations, democratizing simulations, and accelerating research and development in the relevant fields.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Semantics With Large Language Models</title>
<link>https://arxiv.org/abs/2505.11764</link>
<guid>https://arxiv.org/abs/2505.11764</guid>
<content:encoded><![CDATA[
arXiv:2505.11764v2 Announce Type: replace-cross 
Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Large Memory Language Models with Internal and External Knowledge</title>
<link>https://arxiv.org/abs/2505.15962</link>
<guid>https://arxiv.org/abs/2505.15962</guid>
<content:encoded><![CDATA[
arXiv:2505.15962v2 Announce Type: replace-cross 
Abstract: Neural language models are black-boxes -- both linguistic patterns and factual knowledge are distributed across billions of opaque parameters. This entangled encoding makes it difficult to reliably inspect, verify, or update specific facts. We propose a new class of language models, Large Memory Language Models (LMLM) with a pre-training recipe that stores factual knowledge in both internal weights and an external database. Our approach strategically masks externally retrieved factual values from the training loss, thereby teaching the model to perform targeted lookups rather than relying on memorization in model weights. Our experiments demonstrate that LMLMs achieve competitive performance compared to significantly larger, knowledge-dense LLMs on standard benchmarks, while offering the advantages of explicit, editable, and verifiable knowledge bases. This work represents a fundamental shift in how language models interact with and manage factual knowledge.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization</title>
<link>https://arxiv.org/abs/2505.18975</link>
<guid>https://arxiv.org/abs/2505.18975</guid>
<content:encoded><![CDATA[
arXiv:2505.18975v3 Announce Type: replace-cross 
Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular element-wise tensor operations, and hardware-unfriendly nonlinear functions in the SSM block. To address these issues, this paper presents FastMamba, a dedicated accelerator on FPGA with hardware-algorithm co-design to promote the deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit quantization for linear layers through Hadamard transformation to eliminate outliers. Moreover, a hardware-friendly and fine-grained power-of-two quantization framework is presented for the SSM block and convolution layer, and a first-order linear approximation is developed to optimize the nonlinear functions. Based on the accurate algorithm quantization, we propose an accelerator that integrates parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which enhances computational efficiency and reduces hardware complexity. Finally, we evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency than RTX 3090 GPU.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?</title>
<link>https://arxiv.org/abs/2505.20295</link>
<guid>https://arxiv.org/abs/2505.20295</guid>
<content:encoded><![CDATA[
arXiv:2505.20295v2 Announce Type: replace-cross 
Abstract: To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. To support the development of this universal form of LLM uncertainties, we publish our metric at https://github.com/apple/ml-selfreflect
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.21717</link>
<guid>https://arxiv.org/abs/2505.21717</guid>
<content:encoded><![CDATA[
arXiv:2505.21717v3 Announce Type: replace-cross 
Abstract: We present LrcSSM, a $\textit{nonlinear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach</title>
<link>https://arxiv.org/abs/2505.22128</link>
<guid>https://arxiv.org/abs/2505.22128</guid>
<content:encoded><![CDATA[
arXiv:2505.22128v2 Announce Type: replace-cross 
Abstract: This work addresses mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted to space-based edge computing constraints. Leveraging Sentinel-2 data, our method estimates the defocus kernel and trains a restoration model within a GAN framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and PSNR by 25.00%, confirming the model's ability to recover lost details when the original clean image is known. On IMAGIN-e, where no reference images exist, perceptual quality metrics indicate a substantial enhancement, with NIQE improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard restoration. The approach is currently deployed aboard the IMAGIN-e mission, demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing constraints, the method enables applications such as water body segmentation and contour detection while maintaining processing viability despite resource limitations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems</title>
<link>https://arxiv.org/abs/2506.03602</link>
<guid>https://arxiv.org/abs/2506.03602</guid>
<content:encoded><![CDATA[
arXiv:2506.03602v2 Announce Type: replace-cross 
Abstract: Rule representations significantly influence the search capabilities and decision boundaries within the search space of Learning Classifier Systems (LCSs), a family of rule-based machine learning systems that evolve interpretable models through evolutionary processes. However, it is very difficult to choose an appropriate rule representation for each problem. Additionally, some problems benefit from using different representations for different subspaces within the input space. Thus, an adaptive mechanism is needed to choose an appropriate rule representation for each rule in LCSs. This article introduces a flexible rule representation using a four-parameter beta distribution and integrates it into a fuzzy-style LCS. The four-parameter beta distribution can form various function shapes, and this flexibility enables our LCS to automatically select appropriate representations for different subspaces. Our rule representation can represent crisp/fuzzy decision boundaries in various boundary shapes, such as rectangles and bells, by controlling four parameters, compared to the standard representations such as trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the appropriate rule representation for each subspace. Moreover, our LCS incorporates a generalization bias favoring crisp rules where feasible, enhancing model interpretability without compromising accuracy. Experimental results on real-world classification tasks show that our LCS achieves significantly superior test accuracy and produces more compact rule sets. Our implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An extended abstract related to this work is available at https://doi.org/10.36227/techrxiv.174900805.59801248/v1.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
<link>https://arxiv.org/abs/2506.06382</link>
<guid>https://arxiv.org/abs/2506.06382</guid>
<content:encoded><![CDATA[
arXiv:2506.06382v2 Announce Type: replace-cross 
Abstract: We prove that perfect hallucination control in large language models is mathematically impossible. No LLM inference mechanism can simultaneously achieve truthful response generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality. This impossibility is fundamental, arising from the mathematical structure of information aggregation itself rather than engineering limitations. The proof spans three mathematical frameworks: auction theory, proper scoring theory for probabilistic predictions, and log-sum-exp analysis for transformer architectures. In each setting, we demonstrate that information aggregation creates unavoidable violations of conservation principles. The Jensen gap in transformer probability aggregation provides a direct measure of this impossibility. These results reframe hallucination from an engineering bug to an inevitable mathematical feature of distributed intelligence. There are fundamental trade-offs between truthfulness, knowledge utilization, and response completeness, providing principled foundations for managing rather than eliminating hallucination. This work reveals deep connections between neural network inference, philosophy of knowledge and reasoning, and classical results in game theory and information theory, opening new research directions for developing beneficial AI systems within mathematical constraints.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning</title>
<link>https://arxiv.org/abs/2506.06955</link>
<guid>https://arxiv.org/abs/2506.06955</guid>
<content:encoded><![CDATA[
arXiv:2506.06955v3 Announce Type: replace-cross 
Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers</title>
<link>https://arxiv.org/abs/2506.08641</link>
<guid>https://arxiv.org/abs/2506.08641</guid>
<content:encoded><![CDATA[
arXiv:2506.08641v2 Announce Type: replace-cross 
Abstract: Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal a new direction for reusing vision representations in a non-visual domain. Code is available at https://github.com/ExplainableML/TiViT.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information</title>
<link>https://arxiv.org/abs/2506.09548</link>
<guid>https://arxiv.org/abs/2506.09548</guid>
<content:encoded><![CDATA[
arXiv:2506.09548v2 Announce Type: replace-cross 
Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is robust to challenging conditions such as featureless environments and deformable terrains. We developed an online learning-based leg kinematics model named the neural leg kinematics model, which incorporates tactile information (foot reaction force) to implicitly express the nonlinear dynamics between robot feet and the ground. Online training of this model enhances its adaptability to weight load changes of a robot (e.g., assuming delivery or transportation tasks) and terrain conditions. According to the \textit{neural adaptive leg odometry factor} and online uncertainty estimation of the leg kinematics model-based motion predictions, we jointly solve online training of this kinematics model and odometry estimation on a unified factor graph to retain the consistency of both. The proposed method was verified through real experiments using a quadruped robot in two challenging situations: 1) a sandy beach, representing an extremely featureless area with a deformable terrain, and 2) a campus, including multiple featureless areas and terrain types of asphalt, gravel (deformable terrain), and grass. Experimental results showed that our odometry estimation incorporating the \textit{neural leg kinematics model} outperforms state-of-the-art works. Our project page is available for further details: https://takuokawara.github.io/RAL2025_project_page/
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>15,500 Seconds: Lean UAV Classification Leveraging PEFT and Pre-Trained Networks</title>
<link>https://arxiv.org/abs/2506.11049</link>
<guid>https://arxiv.org/abs/2506.11049</guid>
<content:encoded><![CDATA[
arXiv:2506.11049v2 Announce Type: replace-cross 
Abstract: Unmanned Aerial Vehicles (UAVs) pose an escalating security concerns as the market for consumer and military UAVs grows. This paper address the critical data scarcity challenges in deep UAV audio classification. We build upon our previous work expanding novel approaches such as: parameter efficient fine-tuning, data augmentation, and pre-trained networks. We achieve performance upwards of 95\% validation accuracy with EfficientNet-B0.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments</title>
<link>https://arxiv.org/abs/2506.13205</link>
<guid>https://arxiv.org/abs/2506.13205</guid>
<content:encoded><![CDATA[
arXiv:2506.13205v4 Announce Type: replace-cross 
Abstract: With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphGSOcc: Semantic-Geometric Graph Transformer with Dynamic-Static Decoupling for 3D Gaussian Splatting-based Occupancy Prediction</title>
<link>https://arxiv.org/abs/2506.14825</link>
<guid>https://arxiv.org/abs/2506.14825</guid>
<content:encoded><![CDATA[
arXiv:2506.14825v2 Announce Type: replace-cross 
Abstract: Addressing the task of 3D semantic occupancy prediction for autonomous driving, we tackle two key issues in existing 3D Gaussian Splatting (3DGS) methods: (1) unified feature aggregation neglecting semantic correlations among similar categories and across regions, (2) boundary ambiguities caused by the lack of geometric constraints in MLP iterative optimization and (3) biased issues in dynamic-static object coupling optimization. We propose the GraphGSOcc model, a novel framework that combines semantic and geometric graph Transformer and decouples dynamic-static objects optimization for 3D Gaussian Splatting-based Occupancy Prediction. We propose the Dual Gaussians Graph Attenntion, which dynamically constructs dual graph structures: a geometric graph adaptively calculating KNN search radii based on Gaussian poses, enabling large-scale Gaussians to aggregate features from broader neighborhoods while compact Gaussians focus on local geometric consistency; a semantic graph retaining top-M highly correlated nodes via cosine similarity to explicitly encode semantic relationships within and across instances. Coupled with the Multi-scale Graph Attention framework, fine-grained attention at lower layers optimizes boundary details, while coarsegrained attention at higher layers models object-level topology. On the other hand, we decouple dynamic and static objects by leveraging semantic probability distributions and design a Dynamic-Static Decoupled Gaussian Attention mechanism to optimize the prediction performance for both dynamic objects and static scenes. GraphGSOcc achieves state-ofthe-art performance on the SurroundOcc-nuScenes, Occ3D-nuScenes, OpenOcc and KITTI occupancy benchmarks. Experiments on the SurroundOcc dataset achieve an mIoU of 25.20%, reducing GPU memory to 6.8 GB, demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to GaussianWorld.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2506.14988</link>
<guid>https://arxiv.org/abs/2506.14988</guid>
<content:encoded><![CDATA[
arXiv:2506.14988v2 Announce Type: replace-cross 
Abstract: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safety Evaluations of Theory of Mind in Large Language Models</title>
<link>https://arxiv.org/abs/2506.17352</link>
<guid>https://arxiv.org/abs/2506.17352</guid>
<content:encoded><![CDATA[
arXiv:2506.17352v2 Announce Type: replace-cross 
Abstract: As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior. To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design</title>
<link>https://arxiv.org/abs/2506.19997</link>
<guid>https://arxiv.org/abs/2506.19997</guid>
<content:encoded><![CDATA[
arXiv:2506.19997v2 Announce Type: replace-cross 
Abstract: Generalizing deep reinforcement learning agents to unseen environments remains a significant challenge. One promising solution is Unsupervised Environment Design (UED), a co-evolutionary framework in which a teacher adaptively generates tasks with high learning potential, while a student learns a robust policy from this evolving curriculum. Existing UED methods typically measure learning potential via regret, the gap between optimal and current performance, approximated solely by value-function loss. Building on these approaches, we introduce the transition prediction error as an additional term in our regret approximation. To capture how training on one task affects performance on others, we further propose a lightweight metric called co-learnability. By combining these two measures, we present Transition-aware Regret Approximation with Co-learnability for Environment Design (TRACED). Empirical evaluations show that TRACED yields curricula that improve zero-shot generalization across multiple benchmarks while requiring up to 2x fewer environment interactions than strong baselines. Ablation studies confirm that the transition prediction error drives rapid complexity ramp-up and that co-learnability delivers additional gains when paired with the transition prediction error. These results demonstrate how refined regret approximation and explicit modeling of task relationships can be leveraged for sample-efficient curriculum design in UED.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases</title>
<link>https://arxiv.org/abs/2506.17336</link>
<guid>https://arxiv.org/abs/2506.17336</guid>
<content:encoded><![CDATA[
<div> privacy, large language models, encrypted search, user data, hybrid framework

Summary:
The article introduces a novel approach to utilizing large language models (LLMs) while maintaining user privacy. By employing a technique called Socratic Chain-of-Thought Reasoning, a generic user query is first sent to a powerful, untrusted LLM to generate prompts and sub-queries without accessing sensitive user data. These sub-queries are then encrypted and searched across a large dataset of private user information using a Homomorphically Encrypted Vector Database. The final response is generated by a local language model, combining the strengths of both the untrusted LLM and the local model. This hybrid framework, tested on the LoCoMo long-context QA benchmark, outperformed using the powerful LLM alone. By decomposing tasks between the untrusted LLM and the local model, this system showcases a step towards preserving user privacy while benefiting from the capabilities of advanced language models. 

<br /><br />Summary: <div>
arXiv:2506.17336v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating and Customizing Robotic Arm Trajectories using Neural Networks</title>
<link>https://arxiv.org/abs/2506.20259</link>
<guid>https://arxiv.org/abs/2506.20259</guid>
<content:encoded><![CDATA[
<div> neural network, robotic arm, trajectory customization, precision, cognitive robotics
Summary:<br /><br />This article presents a novel neural network approach for generating and customizing the trajectory of a robotic arm to ensure precision and repeatability. The method, applied in cognitive robotics with the NICO robot, allows for pointing to specific points in space with linear movements. The neural network computes the forward kinematics of the arm and integrates with a joint angle generator to train on an artificial dataset. The robot's movement quality is evaluated based on shape and accuracy, achieved through the computation of angular velocities. The approach successfully generates precise and customizable trajectories, adaptable to various settings, enhancing the predictability and interaction of the robotic arm with humans. <div>
arXiv:2506.20259v2 Announce Type: replace-cross 
Abstract: We introduce a neural network approach for generating and customizing the trajectory of a robotic arm, that guarantees precision and repeatability. To highlight the potential of this novel method, we describe the design and implementation of the technique and show its application in an experimental setting of cognitive robotics. In this scenario, the NICO robot was characterized by the ability to point to specific points in space with precise linear movements, increasing the predictability of the robotic action during its interaction with humans. To achieve this goal, the neural network computes the forward kinematics of the robot arm. By integrating it with a generator of joint angles, another neural network was developed and trained on an artificial dataset created from suitable start and end poses of the robotic arm. Through the computation of angular velocities, the robot was characterized by its ability to perform the movement, and the quality of its action was evaluated in terms of shape and accuracy. Thanks to its broad applicability, our approach successfully generates precise trajectories that could be customized in their shape and adapted to different settings.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.00008</link>
<guid>https://arxiv.org/abs/2507.00008</guid>
<content:encoded><![CDATA[
<div> modality separation, visual grounding, graphical user interfaces, vision-language models, attention mechanism  
Summary:  
DiMo-GUI is a framework for grounding natural language queries in GUIs that incorporates dynamic visual grounding and modality-aware optimization. It splits the input into textual and iconic elements, allowing independent reasoning using vision-language models. The method dynamically focuses attention by generating candidate focal regions and incrementally zooming in to refine results when predictions are ambiguous. This hierarchical refinement process helps disambiguate visually cluttered layouts without additional training. Evaluation on standard GUI grounding benchmarks shows consistent improvements over baseline pipelines, demonstrating the effectiveness of combining modality separation with region-focused reasoning. <br /><br />Summary: <div>
arXiv:2507.00008v1 Announce Type: new 
Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables</title>
<link>https://arxiv.org/abs/2507.00041</link>
<guid>https://arxiv.org/abs/2507.00041</guid>
<content:encoded><![CDATA[
<div> Keyword: talent management, table extraction, semantic understanding, LLM-enhanced framework, query answering<br />
Summary:<br />
This paper discusses the challenges of extracting information from complex tabular formats in talent management systems. Current methods struggle with semantic understanding, leading to poor performance in information retrieval. To address this issue, TalentMine, a novel LLM-enhanced framework, is introduced. This framework preserves both structural and semantic dimensions of tabular data through multimodal reasoning. Experimental evaluation demonstrates TalentMine's superior performance in query answering tasks compared to standard extraction methods. The study also identifies the Claude v3 Haiku model as optimal for talent management applications. The key contributions of this work include an analysis of semantic information loss in current extraction pipelines, a novel approach for semantically enriched table representation, an efficient integration framework for retrieval-augmented systems, and comprehensive benchmarks showing substantial improvements in talent analytics tasks.<br /><br />Summary: <div>
arXiv:2507.00041v1 Announce Type: new 
Abstract: In talent management systems, critical information often resides in complex tabular formats, presenting significant retrieval challenges for conventional language models. These challenges are pronounced when processing Talent documentation that requires precise interpretation of tabular relationships for accurate information retrieval and downstream decision-making. Current table extraction methods struggle with semantic understanding, resulting in poor performance when integrated into retrieval-augmented chat applications. This paper identifies a key bottleneck - while structural table information can be extracted, the semantic relationships between tabular elements are lost, causing downstream query failures. To address this, we introduce TalentMine, a novel LLM-enhanced framework that transforms extracted tables into semantically enriched representations. Unlike conventional approaches relying on CSV or text linearization, our method employs specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data. Experimental evaluation across employee benefits document collections demonstrates TalentMine's superior performance, achieving 100% accuracy in query answering tasks compared to 0% for standard AWS Textract extraction and 40% for AWS Textract Visual Q&amp;A capabilities. Our comparative analysis also reveals that the Claude v3 Haiku model achieves optimal performance for talent management applications. The key contributions of this work include (1) a systematic analysis of semantic information loss in current table extraction pipelines, (2) a novel LLM-based method for semantically enriched table representation, (3) an efficient integration framework for retrieval-augmented systems as end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks showing substantial improvements across multiple categories.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
<link>https://arxiv.org/abs/2507.00048</link>
<guid>https://arxiv.org/abs/2507.00048</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, self-driving laboratories, FAIR data infrastructure, optimization, collaborative research <br />
Summary: 
The article discusses the integration of machine learning in self-driving laboratories (SDL) to enhance discovery processes in scientific and engineering fields. By utilizing FAIR data infrastructure, distributed SDLs can collaborate effectively by sharing experimental data in a centralized database. Researchers contribute raw data to a shared platform, which is then processed using machine learning models and analysis tools. A nanoHUB workflow facilitates sequential optimization through active learning, enabling researchers to define objectives and train ML models in real-time for future experiment selection. The optimization task, inspired by the idea of a "frugal twin," involves finding the optimal combination of food dyes to achieve a target color. The framework allows for easy setup of experiments, data sharing, and exploration of predictive ML models, making it accessible to researchers and students. The tools introduced are versatile and can be extended to various optimization problems. 

<br /><br />Summary: <div>
arXiv:2507.00048v1 Announce Type: new 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network</title>
<link>https://arxiv.org/abs/2507.00050</link>
<guid>https://arxiv.org/abs/2507.00050</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Inertial Measurement Unit sensors, Zero-shot learning, Explainable models, Benchmark datasets

Summary: 
The paper introduces a novel Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN) model for IMU-based HAR. SEZ-HARN addresses the lack of comprehensive datasets and transparency in existing models. It can recognize new activities and provide explanations through skeleton videos. Evaluation on benchmark datasets shows competitive Zero-shot recognition accuracy and realistic explanations. SEZ-HARN achieves a prediction accuracy within 3% of the best black-box model on PAMAP2 and maintains comparable performance on other datasets. The model's transparency and ability to provide understandable explanations make it a valuable advancement in the field of HAR. 

<br /><br />Summary: <div>
arXiv:2507.00050v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement Unit (IMU) sensors, has many practical applications in healthcare and assisted living environments. However, its use in real-world scenarios has been limited by the lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent. This paper introduces a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process. We evaluate the effectiveness of the proposed SEZ-HARN on four benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its performance against three state-of-the-art black-box ZS-HAR models. The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation</title>
<link>https://arxiv.org/abs/2507.00054</link>
<guid>https://arxiv.org/abs/2507.00054</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, small language models, large language models, reward-guided dataset distillation, student model

Summary:
AdvDistill introduces a reward-guided dataset distillation framework to improve the performance of small language models learning from large language models. This framework incorporates multiple generations of responses from the teacher model for each prompt and assigns rewards based on rule-based verifiers. These rewards, representing weights during training, lead to significant enhancements in student model performance on reasoning tasks. Unlike traditional distillation methods where the student model simply mimics the teacher's responses, AdvDistill offers a more nuanced approach that prioritizes generalizability and computational efficiency. By leveraging rewarding mechanisms in dataset distillation, this study showcases the effectiveness and benefits of integrating incentives into the learning process for small language models. <div>
arXiv:2507.00054v1 Announce Type: new 
Abstract: The push to compress and impart the proficiency of Large Language Models (LLMs) into more deployable and efficient Small Language Models (SLMs) has benefited from improvements in knowledge distillation (KD) techniques. These techniques allow a smaller student model to learn from a more capable and larger teacher model's responses. However, distillation often revolves around the student model merely copying the teacher's in-distribution responses, limiting its generalisability. This limitation is amplified on reasoning tasks and can be computationally expensive. In this study, we propose AdvDistill, a reward-guided dataset distillation framework. We utilise multiple generations (responses) from a teacher for each prompt and assign rewards based on rule-based verifiers. These varying and normally distributed rewards serve as weights when training student models. Our methods and their subsequent behavioural analysis demonstrate a significant improvement in student model performance for mathematical and complex reasoning tasks, showcasing the efficacy and benefits of incorporating a rewarding mechanism in dataset distillation processes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems</title>
<link>https://arxiv.org/abs/2507.00079</link>
<guid>https://arxiv.org/abs/2507.00079</guid>
<content:encoded><![CDATA[
<div> active field, research, Artificial General Intelligence, Large Language Models, VoyagerVision <br />
Summary: <br />
The paper discusses the concept of open-endedness in Artificial General Intelligence (AGI) research and the use of Large Language Models (LLMs) in interpreting image inputs. It introduces VoyagerVision, a multi-modal model that utilizes visual feedback from Minecraft screenshots to create structures within the game environment. VoyagerVision extends the capabilities of the previous model, Voyager, by successfully creating an average of 2.75 unique structures in fifty iterations. The model shows promise in flat worlds, achieving success in half of all building unit tests. However, more complex structures present challenges. Overall, VoyagerVision demonstrates the potential of combining visual inputs with language models to enhance spatial understanding and task performance in AGI systems. <br /> <div>
arXiv:2507.00079v1 Announce Type: new 
Abstract: Open-endedness is an active field of research in the pursuit of capable Artificial General Intelligence (AGI), allowing models to pursue tasks of their own choosing. Simultaneously, recent advancements in Large Language Models (LLMs) such as GPT-4o [9] have allowed such models to be capable of interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use of such features, providing an LLM with pixel data of an agent's POV to parse the environment and allow it to solve tasks. This paper proposes that providing these visual inputs to a model gives it greater ability to interpret spatial environments, and as such, can increase the number of tasks it can successfully perform, extending its open-ended potential. To this aim, this paper proposes VoyagerVision -- a multi-modal model capable of creating structures within Minecraft using screenshots as a form of visual feedback, building on the foundation of Voyager. VoyagerVision was capable of creating an average of 2.75 unique structures within fifty iterations of the system, as Voyager was incapable of this, it is an extension in an entirely new direction. Additionally, in a set of building unit tests VoyagerVision was successful in half of all attempts in flat worlds, with most failures arising in more complex structures. Project website is available at https://esmyth-dev.github.io/VoyagerVision.github.io/
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models</title>
<link>https://arxiv.org/abs/2507.00092</link>
<guid>https://arxiv.org/abs/2507.00092</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Inverse Reasoning, Transparency, AI Safety, Interpretability 

Summary: 
SAGE-nano, a 4-billion-parameter reasoning model, introduces textbfinverse reasoning, a novel paradigm that enables Large Language Models (LLMs) to explain their own reasoning chains post-hoc. By employing a metacognitive structure that reflects back via attention processes, SAGE-nano can identify major decision points and generate explanations for its reasoning choices. Through rigorous testing on various logical reasoning puzzles and benchmarks, SAGE-nano has demonstrated high reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) comparable to models like Claude-3.5 Sonnet and GPT-4o. This framework not only improves reasoning performance but also enhances interpretability and transparency in AI systems. By advancing the field of inverse reasoning, this work contributes to AI safety, education, and scientific discovery, creating new avenues for the development of transparent AI systems. 

Summary: <br /><br /> <div>
arXiv:2507.00092v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis</title>
<link>https://arxiv.org/abs/2507.00180</link>
<guid>https://arxiv.org/abs/2507.00180</guid>
<content:encoded><![CDATA[
<div> Keywords: legacy software, decision logic, reinforcement learning, interpretable rules, black box systems

Summary: 
The paper discusses the challenge of modernizing legacy software systems due to a lack of documentation and understanding of the original decision logic. A novel pipeline is proposed to extract interpretable decision logic from legacy systems treated as black boxes. The approach utilizes a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful output changes. Counterfactual state transitions are collected and clustered using K-Means, and decision trees are trained on these clusters to extract human-readable rules near the identified boundaries. The effectiveness of the pipeline is demonstrated on dummy legacy systems with varying complexity, showcasing the successful exploration of relevant boundary regions and accurate extraction of core decision logic. This approach provides a promising foundation for generating specifications and test cases during legacy migration.<br /><br />Summary: <div>
arXiv:2507.00180v1 Announce Type: new 
Abstract: Modernizing legacy software systems is a critical but challenging task, often hampered by a lack of documentation and understanding of the original system's intricate decision logic. Traditional approaches like behavioral cloning merely replicate input-output behavior without capturing the underlying intent. This paper proposes a novel pipeline to automatically extract interpretable decision logic from legacy systems treated as black boxes. The approach uses a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful changes in the system's output. These counterfactual state transitions, where the output changes, are collected and clustered using K-Means. Decision trees are then trained on these clusters to extract human-readable rules that approximate the system's decision logic near the identified boundaries. I demonstrated the pipeline's effectiveness on three dummy legacy systems with varying complexity, including threshold-based, combined-conditional, and non-linear range logic. Results show that the RL agent successfully focuses exploration on relevant boundary regions, and the extracted rules accurately reflect the core logic of the underlying dummy systems, providing a promising foundation for generating specifications and test cases during legacy migration.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline</title>
<link>https://arxiv.org/abs/2507.00181</link>
<guid>https://arxiv.org/abs/2507.00181</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, cognitive engagement, artificial intelligence, academic writing, self-regulated learning

Summary:
The study examines the impact of AI tools, specifically ChatGPT, on cognitive engagement during academic writing tasks. Participants were assigned to either an AI-assisted or a non-assisted condition and completed an argumentative writing task followed by a cognitive engagement scale. Results showed lower cognitive engagement in the ChatGPT group, indicating possible cognitive offloading when using AI assistance. This raises concerns about the potential negative effects on deep thinking and active learning. The study highlights the need for pedagogical strategies that encourage active, reflective engagement with AI-generated content to prevent compromising self-regulated learning and deep cognitive involvement of students.<br /><br />Summary: <div>
arXiv:2507.00181v1 Announce Type: new 
Abstract: Despite the increasing use of large language models (LLMs) in education, concerns have emerged about their potential to reduce deep thinking and active learning. This study investigates the impact of generative artificial intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of students during academic writing tasks. The study employed an experimental design with participants randomly assigned to either an AI-assisted (ChatGPT) or a non-assisted (control) condition. Participants completed a structured argumentative writing task followed by a cognitive engagement scale (CES), the CES-AI, developed to assess mental effort, attention, deep processing, and strategic thinking. The results revealed significantly lower cognitive engagement scores in the ChatGPT group compared to the control group. These findings suggest that AI assistance may lead to cognitive offloading. The study contributes to the growing body of literature on the psychological implications of AI in education and raises important questions about the integration of such tools into academic practice. It calls for pedagogical strategies that promote active, reflective engagement with AI-generated content to avoid compromising self-regulated learning and deep cognitive involvement of students.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Artificial Intelligence in Medicine; improved performance and explainability</title>
<link>https://arxiv.org/abs/2507.00205</link>
<guid>https://arxiv.org/abs/2507.00205</guid>
<content:encoded><![CDATA[
<div> framework, xHAIM, explainability, Generative AI, patient data

Summary: 
The article introduces xHAIM, an Explainable AI framework that enhances prediction and explainability in medical tasks. By leveraging Generative AI, xHAIM identifies task-relevant patient data, generates comprehensive summaries, improves predictive modeling, and provides clinical explanations by linking predictions to medical knowledge. Tested on the HAIM-MIMIC-MM dataset, xHAIM significantly improves average AUC from 79.9% to 90.3% for tasks related to chest pathology and operative procedures. By transforming AI from a black-box predictor to an explainable decision support system, xHAIM allows clinicians to trace predictions back to relevant patient data, facilitating the integration of AI advancements in medicine. <div>
arXiv:2507.00205v1 Announce Type: new 
Abstract: With the increasing interest in deploying Artificial Intelligence in medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework that fuses multimodal data to solve downstream clinical tasks. However, HAIM uses data in a task-agnostic manner and lacks explainability. To address these limitations, we introduce xHAIM (Explainable HAIM), a novel framework leveraging Generative AI to enhance both prediction and explainability through four structured steps: (1) automatically identifying task-relevant patient data across modalities, (2) generating comprehensive patient summaries, (3) using these summaries for improved predictive modeling, and (4) providing clinical explanations by linking predictions to patient-specific medical knowledge. Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9% to 90.3% across chest pathology and operative tasks. Importantly, xHAIM transforms AI from a black-box predictor into an explainable decision support system, enabling clinicians to interactively trace predictions back to relevant patient data, bridging AI advancements with clinical utility.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning for routing: A guided review of recent developments and future directions</title>
<link>https://arxiv.org/abs/2507.00218</link>
<guid>https://arxiv.org/abs/2507.00218</guid>
<content:encoded><![CDATA[
<div> ML tools, combinatorial optimization problems, routing problems, TSP, VRP  
Summary:  
- The paper reviews the use of machine learning tools for solving NP-hard combinatorial optimization problems, focusing on routing problems like the traveling salesman problem (TSP) and vehicle routing problem (VRP).  
- Exact algorithms for these problems take a long time, while heuristics offer only approximate solutions.  
- Machine learning models have shown success in enhancing the resolution of these challenging routing problems.  
- The paper proposes a taxonomy classifying ML-based routing methods into construction-based and improvement-based approaches, suited for different problem characteristics.  
- By integrating traditional operational research (OR) methods with modern ML techniques, the aim is to guide future research and tackle emerging VRP variants.  

<br /><br />Summary: <div>
arXiv:2507.00218v1 Announce Type: new 
Abstract: This paper reviews the current progress in applying machine learning (ML) tools to solve NP-hard combinatorial optimization problems, with a focus on routing problems such as the traveling salesman problem (TSP) and the vehicle routing problem (VRP). Due to the inherent complexity of these problems, exact algorithms often require excessive computational time to find optimal solutions, while heuristics can only provide approximate solutions without guaranteeing optimality. With the recent success of machine learning models, there is a growing trend in proposing and implementing diverse ML techniques to enhance the resolution of these challenging routing problems. We propose a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, highlighting their applicability to various problem characteristics. This review aims to integrate traditional OR methods with state-of-the-art ML techniques, providing a structured framework to guide future research and address emerging VRP variants.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context</title>
<link>https://arxiv.org/abs/2507.00417</link>
<guid>https://arxiv.org/abs/2507.00417</guid>
<content:encoded><![CDATA[
<div> Keywords: ASTRO, Autoregressive Search-Taught Reasoner, language models, reinforcement learning, search behavior<br />
Summary:<br />
ASTRO is a framework that trains language models to reason like search algorithms, incorporating self-reflection, backtracking, and exploration. By utilizing a synthetic dataset from Monte Carlo Tree Search, ASTRO helps non-reasoner models like Llama 3 develop structured search behavior. This approach involves converting search traces into natural language chain-of-thoughts, enabling models to learn from successes and failures. Through fine-tuning and reinforcement learning, ASTRO boosts the reasoning capabilities of Llama 3 models, leading to significant performance improvements on mathematical problem-solving tasks. The results show a 16.0% increase on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, particularly enhancing performance on complex problems requiring iterative correction. This study showcases how incorporating search-inspired training can effectively enhance the reasoning abilities of open language models. <br /><br />Summary: <div>
arXiv:2507.00417v1 Announce Type: new 
Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
<div> math reasoning, large language models, transfer learning, reinforcement learning, supervised fine-tuning

Summary:<br />
- Math reasoning has shown significant progress in large language models, surpassing human-level performance on benchmarks such as MATH and AIME.
- However, the gains in math may not necessarily reflect broader problem-solving abilities but rather could be due to narrow overfitting.
- Evaluation of over 20 reasoning-tuned models across various tasks indicates that models excelling in math struggle to transfer their success to other domains.
- Controlled experiments on Qwen3-14B models reveal that reinforcement learning-tuned models exhibit better generalization across domains compared to supervised fine-tuning-tuned models.
- Analysis suggests that supervised fine-tuning induces significant representation and output drift, while reinforcement learning preserves general-domain structure. These findings call for a reevaluation of standard post-training methods, especially the reliance on supervised fine-tuning for advancing reasoning models.<br /><br />Summary: <div>
arXiv:2507.00432v1 Announce Type: new 
Abstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Local Search in SMT-NRA with MCSAT Integration</title>
<link>https://arxiv.org/abs/2507.00557</link>
<guid>https://arxiv.org/abs/2507.00557</guid>
<content:encoded><![CDATA[
<div> Keywords: Satisfiability Modulo Theory, Nonlinear Real Arithmetic, Local Search, MCSAT framework, 2d-LS

Summary: 
In this paper, the authors propose enhancements to local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA). They introduce a new two-dimensional cell-jump move called $2d$-cell-jump to improve the existing local search method. The extended local search framework, $2d$-LS, integrates the model constructing satisfiability calculus (MCSAT) framework for increased search efficiency. By implementing the sample-cell projection operator for MCSAT, the efficiency of the search process is further improved by guiding it away from conflicting states in the real domain. Additionally, a hybrid framework for SMT-NRA is designed, combining MCSAT, $2d$-LS, and OpenCAD to enhance search efficiency through information exchange. Experimental results show significant improvements in local search performance, validating the effectiveness of the proposed methods. 

<br /><br />Summary: <div>
arXiv:2507.00557v1 Announce Type: new 
Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the key operation, cell-jump, of the local search method for SMT-NRA. Then, we propose an extended local search framework, named \emph{$2d$-LS} (following the local search framework, LS, for SMT-NRA), integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency. To further improve the efficiency of MCSAT, we implement a recently proposed technique called \emph{sample-cell projection operator} for MCSAT, which is well suited for CDCL-style search in the real domain and helps guide the search away from conflicting states. Finally, we design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange. The experimental results demonstrate improvements in local search performance, highlighting the effectiveness of the proposed methods.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess</title>
<link>https://arxiv.org/abs/2507.00726</link>
<guid>https://arxiv.org/abs/2507.00726</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, large language models, strategic reasoning, chess, knowledge distillation

Summary:
Reinforcement learning (RL) is applied to large language models (LLMs) for strategic reasoning in chess. Using a chess-pretrained action-value network, dense rewards are provided to improve move quality, resembling knowledge distillation. Experimental results show that these dense rewards outperform sparse binary rewards but fall short of expert levels. Ablation studies reveal that the limitations may result from a lack of deep internal understanding of chess in the pretrained models, which RL alone might not fully address. Despite promising outcomes with dense rewards, strategic reasoning in chess for LLMs remains a challenge. This study highlights the importance of incorporating domain-specific knowledge and the complexities involved in enhancing strategic reasoning capabilities in LLMs through RL methods.

<br /><br />Summary: 
Keywords: reinforcement learning, large language models, strategic reasoning, chess, knowledge distillation 
Key points: RL for strategic reasoning in chess with LLMs, dense rewards outperform sparse rewards but fall short of expert levels, deficiency in internal chess understanding limits progress, RL alone may not fully address this limitation, challenges in advancing strategic reasoning capabilities in LLMs. <div>
arXiv:2507.00726v1 Announce Type: new 
Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown promise in mathematical reasoning, strategic reasoning for LLMs using RL remains largely unexplored. We investigate whether LLMs can develop strategic reasoning capabilities through RL in chess. To this end, we leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation. Our experiments show that our distillation-based dense rewards often outperform sparse binary rewards. However, surprisingly, all models plateau far below expert levels. We provide SFT and RL ablations on chess reasoning training and find evidence that this limitation stems from a deficit in the pretrained models' internal understanding of chess--a deficit which RL alone may not be able to fully overcome.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis</title>
<link>https://arxiv.org/abs/2507.00810</link>
<guid>https://arxiv.org/abs/2507.00810</guid>
<content:encoded><![CDATA[
<div> Algorithm, minimax problems, nonsmooth optimization, quadratic programming, convergence

Summary:<br />
The paper presents an improved numerical algorithm for solving minimax problems by integrating nonsmooth optimization, quadratic programming, and iterative processes. The algorithm's convergence is rigorously proven under assumptions of gradient continuity and boundedness. The proposed algorithm has broad applications in areas like robust optimization and imbalanced learning. <div>
arXiv:2507.00810v1 Announce Type: new 
Abstract: In this paper, we propose an improved numerical algorithm for solving minimax problems based on nonsmooth optimization, quadratic programming and iterative process. We also provide a rigorous proof of convergence for our algorithm under some mild assumptions, such as gradient continuity and boundedness. Such an algorithm can be widely applied in various fields such as robust optimization, imbalanced learning, etc.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents</title>
<link>https://arxiv.org/abs/2507.00841</link>
<guid>https://arxiv.org/abs/2507.00841</guid>
<content:encoded><![CDATA[
<div> mobile multimodal agents, security issues, risk discrimination mechanism, automated assisted assessment scheme, large language model

Summary: 
This study addresses the security risks associated with mobile multimodal agents, which face potential jailbreak threats from attackers manipulating agent behaviors to perform sensitive operations. Existing security measures have limitations in detecting risky behaviors across multiple interactions. The research proposes a risk discrimination mechanism that incorporates behavioral sequence information and an automated assisted assessment scheme utilizing a large language model. Through validation on high-risk tasks, the method shows improved recognition of risky behaviors and aids in reducing agent jailbreaking. The study aims to contribute valuable insights for enhancing security risk modeling and protection in multimodal intelligent agent systems.<br /><br />Summary: <div>
arXiv:2507.00841v1 Announce Type: new 
Abstract: With the wide application of multimodal foundation models in intelligent agent systems, scenarios such as mobile device control, intelligent assistant interaction, and multimodal task execution are gradually relying on such large model-driven agents. However, the related systems are also increasingly exposed to potential jailbreak risks. Attackers may induce the agents to bypass the original behavioral constraints through specific inputs, and then trigger certain risky and sensitive operations, such as modifying settings, executing unauthorized commands, or impersonating user identities, which brings new challenges to system security. Existing security measures for intelligent agents still have limitations when facing complex interactions, especially in detecting potentially risky behaviors across multiple rounds of conversations or sequences of tasks. In addition, an efficient and consistent automated methodology to assist in assessing and determining the impact of such risks is currently lacking. This work explores the security issues surrounding mobile multimodal agents, attempts to construct a risk discrimination mechanism by incorporating behavioral sequence information, and designs an automated assisted assessment scheme based on a large language model. Through preliminary validation in several representative high-risk tasks, the results show that the method can improve the recognition of risky behaviors to some extent and assist in reducing the probability of agents being jailbroken. We hope that this study can provide some valuable references for the security risk modeling and protection of multimodal intelligent agent systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial General Intelligence, cognitive neuroscience, multi-agent coordination, VLMs, neurosymbolic systems

Summary: 
This paper discusses the pursuit of achieving Artificial General Intelligence (AGI) and the limitations faced by current models like GPT-4.5 and DeepSeek. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing AGI systems. The rise of Agentic RAG frameworks, which combine retrieval, planning, and dynamic tool use, is highlighted for enabling more adaptive behavior. Generalization strategies such as information compression, test-time adaptation, and training-free methods are crucial for flexible intelligence. Vision-Language Models (VLMs) are seen as evolving interfaces for embodied understanding and task completion, not just perception modules. The integration of memory and reasoning, along with advances in neurosymbolic systems and reinforcement learning, is bridging the gap between statistical learning and goal-directed cognition. The paper also identifies scientific, technical, and ethical challenges in the journey towards achieving AGI.

<br /><br />Summary: <div>
arXiv:2507.00951v1 Announce Type: new 
Abstract: Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Agent Safety via Causal Influence Prompting</title>
<link>https://arxiv.org/abs/2507.00979</link>
<guid>https://arxiv.org/abs/2507.00979</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous agents, large language models, causal influence diagrams, risk mitigation, safety enhancement <br />
Summary: 
Autonomous agents powered by large language models are becoming increasingly common in various assistive tasks, highlighting the importance of ensuring their safe and reliable behavior. To address this need, a novel technique called CIP leveraging causal influence diagrams (CIDs) has been introduced. CIDs offer a structured way to represent cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. The approach involves initializing a CID based on task specifications, guiding agent interactions using the CID, and iteratively refining the CID based on observed behaviors and outcomes. Experimental results show that this method effectively enhances safety in tasks involving code execution and mobile device control. CIP demonstrates promising potential in mitigating risks and improving the reliability of autonomous agents powered by large language models. <br /><br />Summary: <div>
arXiv:2507.00979v1 Announce Type: new 
Abstract: As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypertokens: Holographic Associative Memory in Tokenized LLMs</title>
<link>https://arxiv.org/abs/2507.00002</link>
<guid>https://arxiv.org/abs/2507.00002</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Information spreading, HDRAM, Symbolic memory framework, Quantum-inspired search

Summary: 
Large Language Models (LLMs) are powerful but suffer from information spreading, which is reframed as an information-theoretic communication issue. To address memory problems in LLMs, HDRAM (Holographically Defined Random Access Memory) is introduced, treating transformer latent space as a spread-spectrum channel. HDRAM uses hypertokens, structured symbolic codes, error-correcting codes (ECC), holographic computing, and quantum-inspired search to recover distributed information through despreading. It enables efficient key-value operations and Grover-style search in latent space. HDRAM integrates ECC, compressed sensing, and Krylov subspace alignment to significantly improve associative retrieval without changing the architecture, showcasing how Classical-Holographic-Quantum-inspired (CHQ) principles can bolster transformer architectures. <br /><br />Summary: <div>
arXiv:2507.00002v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from apparent precision loss, reframed here as information spreading. This reframing shifts the problem from computational precision to an information-theoretic communication issue. We address the K:V and V:K memory problem in LLMs by introducing HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework treating transformer latent space as a spread-spectrum channel. Built upon hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, HDRAM recovers distributed information through principled despreading. These phase-coherent memory addresses enable efficient key-value operations and Grover-style search in latent space. By combining ECC grammar with compressed sensing and Krylov subspace alignment, HDRAM significantly improves associative retrieval without architectural changes, demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE</title>
<link>https://arxiv.org/abs/2507.00003</link>
<guid>https://arxiv.org/abs/2507.00003</guid>
<content:encoded><![CDATA[
<div> NeutroSENSE; ensemble framework; interpretable intrusion detection; IoT environments; neutrosophic logic 

Summary:
NeutroSENSE is a framework using Random Forest, XGBoost, and Logistic Regression with neutrosophic logic for intrusion detection in IoT. It quantifies uncertainty and allows abstention based on truth, falsity, and indeterminacy components. The system flags predictions with high indeterminacy for review using global and adaptive thresholds. Evaluation on IoT-CAD dataset showed 97% accuracy, with misclassified samples having higher indeterminacy. Indeterminacy aids in targeted review, especially in edge deployments. Results confirm a correlation between indeterminacy scores and error likelihood, enhancing trust in AI decisions. Neutrosophic logic improves accuracy and explainability, laying a foundation for trustworthy AI in IoT security systems at the edge and fog. <br /><br />Summary: <div>
arXiv:2507.00003v1 Announce Type: cross 
Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments. By integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the system decomposes prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification and abstention. Predictions with high indeterminacy are flagged for review using both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24). The use of indeterminacy as a proxy for uncertainty enables informed abstention and targeted review-particularly valuable in edge deployments. Figures and tables validate the correlation between I-scores and error likelihood, supporting more trustworthy, human-in-the-loop AI decisions. This work shows that neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search</title>
<link>https://arxiv.org/abs/2507.00004</link>
<guid>https://arxiv.org/abs/2507.00004</guid>
<content:encoded><![CDATA[
<div> inference, large language models, compute-optimality, directed stochastic skill search, tripartite graph framework

Summary:
The article discusses the computational, energy, and financial resources required for large language models (LLMs) during training and deployment, with a focus on inference costs. It introduces directed stochastic skill search (DS3) as a framework for efficient inference through a learned skill graph. By analyzing different inference strategies like chain-of-thought (CoT) and tree-of-thought (ToT), the study derives closed-form expressions for task success and compute cost. The framework extends existing models to incorporate inference and empirically observed patterns such as linear accuracy scaling and variation in preferred inference strategies are theoretically recovered. The framework also captures emergent behaviors like reasoning-induced changes in performance and best-of-N (BoN) and majority voting behavior. Ultimately, the framework provides a deeper theoretical understanding of training-inference interdependencies to support algorithmic design and resource allocation. <br /><br />Summary: <div>
arXiv:2507.00004v1 Announce Type: cross 
Abstract: Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy</title>
<link>https://arxiv.org/abs/2507.00007</link>
<guid>https://arxiv.org/abs/2507.00007</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, educational framework, critical thinking, digital literacy, undergraduate students

Summary:
This paper introduces a novel educational framework that integrates generative artificial intelligence (GenAI) platforms like ChatGPT and Claude into laboratory activities for undergraduate students. Instead of relying uncritically on large language models, the proposed pedagogical model encourages students to treat GenAI as both a research subject and a cognitive tool. In this framework, students create discipline-specific prompts and assess GenAI-generated responses in various modalities. A pilot study in a general astronomy course for non-science majors showed high engagement levels and critical reflection among students, who continued the activity outside of class and presented their findings at a research symposium. The study emphasizes the significance of structured AI interactions in education and suggests that combining GenAI with reflective assessment methods can enhance learning outcomes. The model presented is replicable and adaptable to various scientific disciplines.
<br /><br />Summary: <div>
arXiv:2507.00007v1 Announce Type: cross 
Abstract: This paper presents a new educational framework for integrating generative artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini into laboratory activities aimed at developing critical thinking and digital literacy among undergraduate students. Recognizing the limitations and risks of uncritical reliance on large language models (LLMs), the proposed pedagogical model reframes GenAI as a research subject and cognitive tool. Students formulate discipline-specific prompts and evaluate GenAI-generated responses in text, image, and video modalities. A pilot implementation in a general astronomy course for non-science majors demonstrated high levels of engagement and critical reflection, with many students continuing the activity after class and presenting results at a research symposium. The results highlight the importance of structured AI interactions in education and suggest that GenAI can improve learning outcomes when combined with reflective assessment methods. The study proposes a replicable model for interdisciplinary AI-integrated lab work, adaptable to scientific disciplines. See the guide to learning activities based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel RL approach for efficient Elevator Group Control Systems</title>
<link>https://arxiv.org/abs/2507.00011</link>
<guid>https://arxiv.org/abs/2507.00011</guid>
<content:encoded><![CDATA[
<div> Keywords: elevator traffic management, Reinforcement Learning, Markov Decision Process, Dueling Double Deep Q-learning, Vrije Universiteit Amsterdam

Summary: 
- The study focuses on efficient elevator traffic management in large buildings and the challenges faced by heuristic-based controllers in handling the stochastic and combinatorial nature of elevator dispatching.
- A novel approach is proposed using Reinforcement Learning (RL) to develop an Elevator Group Control System (EGCS) for a six-elevator, fifteen-floor system at Vrije Universiteit Amsterdam.
- Key innovations include a unique action space encoding, infra-steps to model continuous passenger arrivals, and a tailored reward signal for improved learning efficiency.
- Different approaches to adapt the discounting factor to the infra-step formulation are explored in the study.
- RL architectures based on Dueling Double Deep Q-learning are investigated, demonstrating that the RL-based EGCS adapts to traffic patterns, learns from a stochastic environment, and outperforms traditional rule-based algorithms. 

<br /><br />Summary: <div>
arXiv:2507.00011v1 Announce Type: cross 
Abstract: Efficient elevator traffic management in large buildings is critical for minimizing passenger travel times and energy consumption. Because heuristic- or pattern-detection-based controllers struggle with the stochastic and combinatorial nature of dispatching, we model the six-elevator, fifteen-floor system at Vrije Universiteit Amsterdam as a Markov Decision Process and train an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS). Key innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of infra-steps to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. In addition, we explore various ways to adapt the discounting factor to the infra-step formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Undistillable Models by Minimizing Conditional Mutual Information</title>
<link>https://arxiv.org/abs/2507.00012</link>
<guid>https://arxiv.org/abs/2507.00012</guid>
<content:encoded><![CDATA[
<div> undistillable DNN, knowledge distillation, label smoothing, conditional mutual information, training method<br />
Summary:<br />
- A new training method, CMI minimized (CMIM), is proposed to create undistillable deep neural networks (DNNs) by minimizing conditional mutual information (CMI) values of output probability distributions.
- The CMIM model outperforms DNN models trained with cross entropy alone and is resistant to knowledge distillation (KD) methods.
- Undistillable DNNs have highly concentrated output probability distributions for each label, making them difficult to extract knowledge from.
- The CMIM model collapses clusters of output probabilities, making it challenging to distill information.
- By minimizing CMI values during training, the CMIM model becomes undistillable and demonstrates improved prediction accuracy compared to models trained with conventional methods. <br /><br />Summary: <div>
arXiv:2507.00012v1 Announce Type: cross 
Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a black-box input-output teacher, it cannot be distilled through knowledge distillation (KD). In this case, the distilled student (referred to as the knockoff student) does not outperform a student trained independently with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2507.00013</link>
<guid>https://arxiv.org/abs/2507.00013</guid>
<content:encoded><![CDATA[
<div> Seasonal-trend decomposition, masked time-series modeling, entangled patterns, period masking, contrastive learning <br />
Summary:<br />
Forecasting complex time series is crucial for various industrial applications. Traditional masked time-series modeling may learn spurious temporal patterns due to ignoring semantic structure. To address this, a novel framework called ST-MTM is proposed, incorporating seasonal-trend decomposition and innovative masking methods. ST-MTM leverages period masking for seasonal components and sub-series masking for trends to capture distinct temporal semantics effectively. It introduces a contrastive learning task to enhance contextual consistency among multiple masked seasonal representations. Experimental results demonstrate superior forecasting performance of ST-MTM compared to existing methods. The framework effectively pre-trains models on intricate temporal dependencies, improving forecasting accuracy through the decomposition of entangled patterns. <div>
arXiv:2507.00013v1 Announce Type: cross 
Abstract: Forecasting complex time series is an important yet challenging problem that involves various industrial applications. Recently, masked time-series modeling has been proposed to effectively model temporal dependencies for forecasting by reconstructing masked segments from unmasked ones. However, since the semantic information in time series is involved in intricate temporal variations generated by multiple time series components, simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach. Specifically, we propose ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components to produce multiple masked seasonal series based on inherent multi-periodicity and a sub-series masking strategy for trend components to mask temporal regions that share similar variations. The proposed masking method presents an effective pre-training task for learning intricate temporal variations and dependencies. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations. Experimental results show that our proposed ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Bench-CL: Continual Learning for Coding Agents</title>
<link>https://arxiv.org/abs/2507.00014</link>
<guid>https://arxiv.org/abs/2507.00014</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, continual learning, GitHub issues, LangGraph, Python repositories

Summary:
The article introduces SWE-Bench-CL, a continual learning benchmark for evaluating an agent's ability to accumulate experience and transfer knowledge across tasks in software engineering. It is based on the SWE-Bench Verified dataset and organizes GitHub issues into chronologically ordered sequences to mimic real-world software development evolution. The benchmark includes an analysis of inter-task structural similarity and contextual sensitivity, an evaluation framework with a semantic memory module, and specialized metrics to capture the stability-plasticity trade-off. Memory-enabled and memory-disabled agents are compared across Python repositories using a rigorous experimental protocol. The code and data for the benchmark are publicly available, allowing for reproducible research and the development of more adaptive and robust AI agents in the field of software engineering.

<br /><br />Summary: <div>
arXiv:2507.00014v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at https://github.com/thomasjoshi/agents-never-forget, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications</title>
<link>https://arxiv.org/abs/2507.00015</link>
<guid>https://arxiv.org/abs/2507.00015</guid>
<content:encoded><![CDATA[
<div> transformer, modulation classification, adversarial attacks, vision transformer, attention mechanism

Summary:<br />
The paper proposes a defensive strategy for transformer-based modulation classification systems to address vulnerabilities to adversarial attacks. A novel vision transformer architecture is introduced with an adversarial indicator (AdvI) token for attack detection. This is the first work to incorporate an AdvI token in vision transformers for defense against adversarial attacks. The method combines adversarial training and AdvI token detection in a unified neural network model, simplifying system architecture. The AdvI token influences attention weights, highlighting suspicious regions in input data. Experimental results show superior performance in handling white-box attacks compared to existing methods. This approach outperforms methods using fast gradient, projected gradient descent, and iterative attacks. <br /><br /> <div>
arXiv:2507.00015v1 Announce Type: cross 
Abstract: The remarkable success of transformers across various fields such as natural language processing and computer vision has paved the way for their applications in automatic modulation classification, a critical component in the communication systems of Internet of Things (IoT) devices. However, it has been observed that transformer-based classification of radio signals is susceptible to subtle yet sophisticated adversarial attacks. To address this issue, we have developed a defensive strategy for transformer-based modulation classification systems to counter such adversarial attacks. In this paper, we propose a novel vision transformer (ViT) architecture by introducing a new concept known as adversarial indicator (AdvI) token to detect adversarial attacks. To the best of our knowledge, this is the first work to propose an AdvI token in ViT to defend against adversarial attacks. Integrating an adversarial training method with a detection mechanism using AdvI token, we combine a training time defense and running time defense in a unified neural network model, which reduces architectural complexity of the system compared to detecting adversarial perturbations using separate models. We investigate into the operational principles of our method by examining the attention mechanism. We show the proposed AdvI token acts as a crucial element within the ViT, influencing attention weights and thereby highlighting regions or features in the input data that are potentially suspicious or anomalous. Through experimental results, we demonstrate that our approach surpasses several competitive methods in handling white-box attack scenarios, including those utilizing the fast gradient method, projected gradient descent attacks and basic iterative method.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-based Fine-Tuning through Pre-trained Model Regularization</title>
<link>https://arxiv.org/abs/2507.00016</link>
<guid>https://arxiv.org/abs/2507.00016</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained models, fine-tuning, gradient-based parameter selection, efficiency, regularization 

Summary: 
The paper introduces an efficient fine-tuning method called GRFT that updates rows or columns of the weight matrix based on gradients, reducing storage overhead and improving parameter selection efficiency. The approach outperforms existing methods like GPS, Adapter Tuning, and LoRA, achieving state-of-the-art results. By updating only a small percentage of total parameters on datasets like FGVC and VTAB, GRFT demonstrates high efficiency and effectiveness. The incorporation of regularization enhances knowledge transfer from pre-trained models. Theoretical analysis shows that updating rows or columns with the highest sum of squared gradients is optimal. The proposed method addresses the computational resource and storage demands associated with fine-tuning large pre-trained models, providing a promising solution for various downstream tasks. Source code for GRFT will be made available soon. 

<br /><br />Summary: <div>
arXiv:2507.00016v1 Announce Type: cross 
Abstract: Large pre-trained models have demonstrated extensive applications across various fields. However, fine-tuning these models for specific downstream tasks demands significant computational resources and storage. One fine-tuning method, gradient-based parameter selection (GPS), focuses on fine-tuning only the parameters with high gradients in each neuron, thereby reducing the number of training parameters. Nevertheless, this approach increases computational resource requirements and storage demands. In this paper, we propose an efficient gradient-based and regularized fine-tuning method (GRFT) that updates the rows or columns of the weight matrix. We theoretically demonstrate that the rows or columns with the highest sum of squared gradients are optimal for updating. This strategy effectively reduces storage overhead and improves the efficiency of parameter selection. Additionally, we incorporate regularization to enhance knowledge transfer from the pre-trained model. GRFT achieves state-of-the-art performance, surpassing existing methods such as GPS, Adapter Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the total parameters on FGVC and VTAB datasets, respectively, demonstrating its high efficiency and effectiveness. The source code will be released soon.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections</title>
<link>https://arxiv.org/abs/2507.00018</link>
<guid>https://arxiv.org/abs/2507.00018</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, preference learning, large language model, post-training, optimization<br />
Summary:<br />
The article presents a unified theoretical framework that connects Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. It shows that both SFT and Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT as a form of implicit reward learning. A limitation in conventional SFT is identified, where the KL divergence term remains constant during optimization, hindering model updates. A learning rate reduction approach is proposed to overcome this limitation, leading to performance improvements in instruction following tasks. Alternative SFT objectives derived from f-divergence functions are shown to enhance post-DPO model performance. The article also extends the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, with mathematical derivations and experimental validation.<br /> <div>
arXiv:2507.00018v1 Announce Type: cross 
Abstract: Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations</title>
<link>https://arxiv.org/abs/2507.00019</link>
<guid>https://arxiv.org/abs/2507.00019</guid>
<content:encoded><![CDATA[
<div> encoding strategies, quantum data, classification task, performance analysis, optimization

Summary:
Three quantum-inspired data encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy (GDS), and Class Conditional Value Strategy (CCVS), are proposed and evaluated for transforming classical data into quantum data in classical machine learning models. Each strategy has different approaches to encoding data, impacting encoding efficiency, correctness, model accuracy, and computational cost. The Instance Level Strategy treats each row independently, the Global Discrete Strategy maps all unique feature values uniformly, and the Class Conditional Value Strategy encodes unique values for each class. By assessing trade-offs between encoding time, precision, and predictive performance, insights are provided for optimizing quantum-inspired data transformations in classical machine learning workflows. This study contributes to the understanding of the impact of quantum data encoding strategies on classification tasks. <div>
arXiv:2507.00019v1 Announce Type: cross 
Abstract: In this study, we propose, evaluate and compare three quantum inspired data encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy (GDS) and Class Conditional Value Strategy (CCVS), for transforming classical data into quantum data for use in pure classical machine learning models. The primary objective is to reduce high encoding time while ensuring correct encoding values and analyzing their impact on classification performance. The Instance Level Strategy treats each row of dataset independently; mimics local quantum states. Global Discrete Value Based encoding strategy maps all unique feature values across the full dataset to quantum states uniformly. In contrast, the Class conditional Value based encoding strategy encodes unique values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their impact on en-coding efficiency, correctness, model accuracy, and computational cost. By analyzing the trade offs between encoding time, precision, and predictive performance, this study provides insights into optimizing quantum inspired data transformations for classical machine learning workflows.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLU Attention Improve Transformer</title>
<link>https://arxiv.org/abs/2507.00022</link>
<guid>https://arxiv.org/abs/2507.00022</guid>
<content:encoded><![CDATA[
<div> Keywords: GLU Attention, neural network, model performance, convergence speed, lightweight

Summary:
GLU Attention, a novel attention mechanism introduced in this paper, adds nonlinearity to Attention values and enhances neural network performance. The experiments conducted show improved model performance and faster convergence across text and vision modalities. GLU Attention requires zero additional parameters and has minimal computational costs, making it a lightweight solution. It can easily integrate with other technologies like Flash Attention, RoPE, and MHA variants such as GQA. The project is open-sourced on GitHub. <div>
arXiv:2507.00022v1 Announce Type: cross 
Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity</title>
<link>https://arxiv.org/abs/2507.00024</link>
<guid>https://arxiv.org/abs/2507.00024</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, inverse design, reinforcement learning, materials discovery, high-dimensional spaces

Summary: <br /><br />AIMatDesign introduces a reinforcement learning framework for materials discovery that addresses challenges in high-dimensional materials composition spaces. The framework augments experimental data using difference-based algorithms to build a trusted experience pool, enhancing model reliability and incorporating domain expert knowledge. An automated refinement strategy guided by large language models corrects prediction inconsistencies, while a knowledge-based reward function improves training stability and efficiency. AIMatDesign outperforms traditional machine learning and reinforcement learning methods in discovery efficiency, convergence speed, and success rates. Experimental synthesis of Zr-based alloys produced a top-performing BMG closely matching predictions. The framework accurately captured yield strength variations with composition, showing reliability and potential for closed-loop materials discovery. <div>
arXiv:2507.00024v1 Announce Type: cross 
Abstract: With the growing demand for novel materials, machine learning-driven inverse design methods face significant challenges in reconciling the high-dimensional materials composition space with limited experimental data. Existing approaches suffer from two major limitations: (I) machine learning models often lack reliability in high-dimensional spaces, leading to prediction biases during the design process; (II) these models fail to effectively incorporate domain expert knowledge, limiting their capacity to support knowledge-guided inverse design. To address these challenges, we introduce AIMatDesign, a reinforcement learning framework that addresses these limitations by augmenting experimental data using difference-based algorithms to build a trusted experience pool, accelerating model convergence. To enhance model reliability, an automated refinement strategy guided by large language models (LLMs) dynamically corrects prediction inconsistencies, reinforcing alignment between reward signals and state value functions. Additionally, a knowledge-based reward function leverages expert domain rules to improve stability and efficiency during training. Our experiments demonstrate that AIMatDesign significantly surpasses traditional machine learning and reinforcement learning methods in discovery efficiency, convergence speed, and success rates. Among the numerous candidates proposed by AIMatDesign, experimental synthesis of representative Zr-based alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\% elongation, closely matching predictions. Moreover, the framework accurately captured the trend of yield strength variation with composition, demonstrating its reliability and potential for closed-loop materials discovery.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing to New Dynamical Systems via Frequency Domain Adaptation</title>
<link>https://arxiv.org/abs/2507.00025</link>
<guid>https://arxiv.org/abs/2507.00025</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Simulator, Dynamical Adaptation, Generalization, Parameter-Efficient, Deep Neural Networks 
Summary: 
Fourier Neural Simulator for Dynamical Adaptation (FNSDA) is introduced to effectively learn underlying dynamics and generalize to new environments by adapting in the Fourier space. FNSDA identifies shareable dynamics based on known environments, partitions Fourier modes, and learns to adjust modes for each new environment based on latent systematic parameters. This method achieves superior or competitive generalization performance in various dynamic systems while significantly reducing parameter cost. The approach is demonstrated on four dynamic system families, showcasing its ability to adapt to new environments and make reliable predictions. The code for FNSDA is publicly available for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2507.00025v1 Announce Type: cross 
Abstract: Learning the underlying dynamics from data with deep neural networks has shown remarkable potential in modeling various complex physical dynamics. However, current approaches are constrained in their ability to make reliable predictions in a specific domain and struggle with generalizing to unseen systems that are governed by the same general dynamics but differ in environmental characteristics. In this work, we formulate a parameter-efficient method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can readily generalize to new dynamics via adaptation in the Fourier space. Specifically, FNSDA identifies the shareable dynamics based on the known environments using an automatic partition in Fourier modes and learns to adjust the modes specific for each new environment by conditioning on low-dimensional latent systematic parameters for efficient generalization. We evaluate our approach on four representative families of dynamic systems, and the results show that FNSDA can achieve superior or competitive generalization performance compared to existing methods with a significantly reduced parameter cost. Our code is available at https://github.com/WonderSeven/FNSDA.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2507.00026</link>
<guid>https://arxiv.org/abs/2507.00026</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, safety evaluation, adversarial prompts, automated prompt generation, multi-objective reinforcement learning

Summary:
The article discusses the importance of evaluating the safety of Large Language Models (LLMs) in real-world applications, particularly in the face of adversarial prompts. Existing manual safety benchmarks are limited in adaptability and topic coverage, necessitating the development of more dynamic evaluation methods. The proposed Reality-Oriented Safety Evaluation (ROSE) framework leverages multi-objective reinforcement learning to generate diverse and contextually rich adversarial prompts for LLM testing. Experiments demonstrate ROSE's superiority in uncovering safety vulnerabilities in advanced LLMs, with significant enhancements in evaluation metrics. By addressing the shortcomings of current automated prompt generation methods and emphasizing real-world contextualization, ROSE offers a more practical approach to evaluating the safety of LLMs.<br /><br />Summary: <div>
arXiv:2507.00026v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, evaluating their safety-especially under adversarial prompting-has become critical. Arguably, effective safety evaluations should be adaptive, evolving with LLM capabilities, and also cover a broad spectrum of harmful topics and real-world scenarios to fully expose potential vulnerabilities. Existing manual safety benchmarks, built on handcrafted adversarial prompts, are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. In contrast, automated adversarial prompt generation offers a promising path toward adaptive evaluation. However, current methods often suffer from insufficient adversarial topic coverage (topic-level diversity) and weak alignment with real-world contexts. These shortcomings stem from the exploration-exploitation dilemma in black-box optimization and a lack of real-world contextualization, resulting in adversarial prompts that are both topically narrow and scenario-repetitive. To address these issues, we propose Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts. Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics. We hope ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs. WARNING: This paper contains examples of potentially harmful text.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation</title>
<link>https://arxiv.org/abs/2507.00028</link>
<guid>https://arxiv.org/abs/2507.00028</guid>
<content:encoded><![CDATA[
<div> Hierarchical Interactions, Trajectory Semantics, Joint Embedding Predictive Architecture, Multi-scale Representations, Urban Trajectory <br />
Summary:<br />
The article introduces HiT-JEPA, a framework for effectively representing urban trajectory data. It addresses the challenge of capturing diverse information by incorporating fine-grained details and high-level summaries in a unified model. HiT-JEPA utilizes a three-layer hierarchy to capture point-level details, intermediate patterns, and high-level abstractions, allowing for the integration of local dynamics and global semantics. Experimental results demonstrate that the hierarchical design of HiT-JEPA produces richer, multi-scale representations, enhancing trajectory similarity computation. The framework provides a comprehensive approach to learning urban trajectory representations across semantic abstraction levels and offers a solution to the limitations of existing methods in capturing both long-term dependencies and local nuances simultaneously. <br /> <div>
arXiv:2507.00028v1 Announce Type: cross 
Abstract: The representation of urban trajectory data plays a critical role in effectively analyzing spatial movement patterns. Despite considerable progress, the challenge of designing trajectory representations that can capture diverse and complementary information remains an open research problem. Existing methods struggle in incorporating trajectory fine-grained details and high-level summary in a single model, limiting their ability to attend to both long-term dependencies while preserving local nuances. To address this, we propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint Embedding Predictive Architecture), a unified framework for learning multi-scale urban trajectory representations across semantic abstraction levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures point-level fine-grained details, intermediate patterns, and high-level trajectory abstractions, enabling the model to integrate both local dynamics and global semantics in one coherent structure. Extensive experiments on multiple real-world datasets for trajectory similarity computation show that HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code is available at: https://anonymous.4open.science/r/HiT-JEPA.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing</title>
<link>https://arxiv.org/abs/2507.00029</link>
<guid>https://arxiv.org/abs/2507.00029</guid>
<content:encoded><![CDATA[
<div> low-rank adaptation, mixture-of-experts, LoRA-Mixer, task-specific alignment, parameter efficiency<br />
Summary:<br />
The LoRA-Mixer framework addresses limitations in adapting large language models to multiple tasks by integrating low-rank adaptation (LoRA) experts into a modular mixture-of-experts (MoE) design. The framework replaces projection matrices with dynamically routed, task-specific LoRA experts, ensuring compatibility with various foundation models. It supports joint optimization of experts and routing mechanisms or deployment of pre-trained LoRA modules. An adaptive Specialization Balance Loss (SBL) optimizes expert balance and task alignment. Extensive experiments show significant improvements in performance on benchmark datasets, with LoRA-Mixer outperforming state-of-the-art methods with fewer parameters. <div>
arXiv:2507.00029v1 Announce Type: cross 
Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts (MoE) for adapting large language models (LLMs) to multiple tasks still exhibit prevailing limitations: they either swap entire attention/feed-forward layers for switch experts or bolt on parallel expert branches, diluting parameter efficiency and task fidelity. We propose the LoRA-Mixer, a modular and lightweight MoE framework that integrates LoRA experts. Our core innovation lies in replacing the projection matrices of the attention module's input/output linear layers with dynamically routed, task-specific LoRA experts. This design ensures seamless compatibility with diverse foundation models, including transformers and state space models (SSMs), by leveraging their inherent linear projection structures. The framework supports two operational paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a novel hard-soft routing strategy, or (2) direct deployment of pre-trained, frozen LoRA modules sourced from external repositories. To enable robust router training with limited data while ensuring stable routing decisions and maximizing expert reuse, we introduce an adaptive Specialization Balance Loss (SBL) that jointly optimizes expert balance and task-specific alignment. Extensive experiments on seven benchmark datasets, including MedQA, CoLA, SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base models, respectively. Compared with state-of-the-art methods, LoRA-Mixer achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively, using only 48% of the parameters, demonstrating its efficiency and strong performance.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2507.00030</link>
<guid>https://arxiv.org/abs/2507.00030</guid>
<content:encoded><![CDATA[
<div> bandit, deep reinforcement learning, action duration, policy flexibility, computational efficiency
Summary:
This article introduces a novel paradigm that combines contextual bandits with Deep Reinforcement Learning (DRL) to improve the temporal scale of action execution. By integrating a contextual bandit module with a Deep Q-Network (DQN), the system can dynamically select optimal action repetition rates based on state contexts. Experimental results on Atari 2600 games show significant performance enhancements compared to static duration baselines. The adaptive temporal abstractions offered by this approach not only enhance policy flexibility but also improve computational efficiency. This paradigm is particularly useful for real-time applications such as gaming and robotics, where dynamic action durations play a crucial role. <div>
arXiv:2507.00030v1 Announce Type: cross 
Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex sequential decision-making tasks, such as playing Atari 2600 games and mastering board games. A critical yet underexplored aspect of DRL is the temporal scale of action execution. We propose a novel paradigm that integrates contextual bandits with DRL to adaptively select action durations, enhancing policy flexibility and computational efficiency. Our approach augments a Deep Q-Network (DQN) with a contextual bandit module that learns to choose optimal action repetition rates based on state contexts. Experiments on Atari 2600 games demonstrate significant performance improvements over static duration baselines, highlighting the efficacy of adaptive temporal abstractions in DRL. This paradigm offers a scalable solution for real-time applications like gaming and robotics, where dynamic action durations are critical.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing</title>
<link>https://arxiv.org/abs/2507.00032</link>
<guid>https://arxiv.org/abs/2507.00032</guid>
<content:encoded><![CDATA[
<div> memory consolidation, knowledge tracing, Hebbian memory, continual learning, personalized learning

Summary:
KUL-KT is a biologically inspired architecture for knowledge tracing that combines Hebbian memory encoding with gradient-based consolidation. It introduces innovations such as time-decaying Hebbian memory updates and the Loss-aligned Internal Target method for ideal internal state computation without backpropagation through time. The architecture includes a fast Hebbian memory and a slower linear network for few-shot personalization and natural forgetting. Operating in embedding space, KUL-KT supports structured and unstructured inputs, outperforming strong baselines on ten public knowledge tracing benchmarks. In a classroom deployment, it improved learner-perceived helpfulness and reduced difficulty. Ablation studies confirmed the importance of Hebbian decay and LIT for continual adaptation. Compared to a graph-based model, KUL-KT trained faster and used significantly less memory, making it a memory-efficient framework for personalized learning at scale. 

<br /><br />Summary: <div>
arXiv:2507.00032v1 Announce Type: cross 
Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge tracing (KT), combining Hebbian memory encoding with gradient-based consolidation in a scalable, input-agnostic framework. KUL-KT adapts the principle of memory consolidation in neural systems, to student modeling by introducing two key innovations: (i) a time-decaying Hebbian memory update that enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target (LIT) method to compute an ideal internal state, allowing continual learning without backpropagation through time. The architecture consists of a fast Hebbian memory that captures each learner interaction via a single associative update, and a slower linear network that consolidates recalled samples through gradient descent. This design enables few-shot personalization and natural forgetting without storing raw data or relying on large cohort training. Operating entirely in embedding space, KUL-KT supports both structured (tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT outperforms strong baselines on ten public KT benchmarks in rank-sensitive metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT personalized quizzes from short-answer data, leading to improved learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation studies confirm that Hebbian decay and LIT are critical for continual adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x faster and uses 99.01\% less memory. These results position KUL-KT as a biologically grounded, memory-efficient, and input-flexible framework for personalized learning at scale.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment Sampling in Video LLMs for Long-Form Video QA</title>
<link>https://arxiv.org/abs/2507.00033</link>
<guid>https://arxiv.org/abs/2507.00033</guid>
<content:encoded><![CDATA[
<div> video large language models, VideoQA, frame sub-sampling, moment sampling, long-form VideoQA

Summary:
- Video large language models (Video LLMs) have improved video question answering (VideoQA) but struggle with long videos due to frame sub-sampling limitations.
- Frame sub-sampling can lead to loss of key frames or inclusion of redundant information affecting model performance.
- Proposed moment sampling approach uses a text-to-video moment retrieval model to select relevant frames based on the question.
- Lightweight moment retrieval model prioritizes frame selection to improve long-form VideoQA in Video LLMs.
- Extensive experiments on four datasets with four state-of-the-art Video LLMs show the effectiveness of the proposed approach. 

<br /><br />Summary: <div>
arXiv:2507.00033v1 Announce Type: cross 
Abstract: Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose "moment sampling", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Fusion via Neuron Interpolation</title>
<link>https://arxiv.org/abs/2507.00037</link>
<guid>https://arxiv.org/abs/2507.00037</guid>
<content:encoded><![CDATA[
<div> Keywords: model fusion, neural networks, neuron-centric algorithms, neuron attribution scores, benchmark datasets

Summary:<br />
Model fusion involves combining multiple trained neural networks into a single representative model. This process is complicated by differences in internal representations due to various factors like permutation invariance and random initialization. A novel family of neuron-centric algorithms has been developed to effectively integrate parent models by grouping intermediate neurons and approximating target representations with corresponding sub-networks. These algorithms incorporate neuron attribution scores and can generalize to different layer types. Experimental results on benchmark datasets show that the proposed algorithms outperform previous fusion techniques, especially in zero-shot and non-IID fusion scenarios. The code for the algorithms is available on GitHub. <div>
arXiv:2507.00037v1 Announce Type: cross 
Abstract: Model fusion aims to combine the knowledge of multiple models by creating one representative model that captures the strengths of all of its parents. However, this process is non-trivial due to differences in internal representations, which can stem from permutation invariance, random initialization, or differently distributed training data. We present a novel, neuron-centric family of model fusion algorithms designed to integrate multiple trained neural networks into a single network effectively regardless of training data distribution. Our algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Unlike prior approaches, our approach incorporates neuron attribution scores into the fusion process. Furthermore, our algorithms can generalize to arbitrary layer types. Experimental results on various benchmark datasets demonstrate that our algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios. The code is available at https://github.com/AndrewSpano/neuron-interpolation-model-fusion.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information</title>
<link>https://arxiv.org/abs/2507.00038</link>
<guid>https://arxiv.org/abs/2507.00038</guid>
<content:encoded><![CDATA[
<div> Keyword: Data reduction, Pointwise V-information, Classifier performance, Progressive learning, Cross-lingual data reduction <br />
Summary: <br />
Data reduction is crucial for enhancing data-centric AI by selecting the most informative instances from large datasets to improve model training efficiency. This paper introduces a data reduction strategy based on Pointwise V-information (PVI) to identify optimal instances for improved model performance. By filtering out low-difficulty instances using PVI, the approach achieves a significant reduction in data size (10%-30%) with minimal loss in classifier accuracy. Additionally, a progressive learning technique utilizing instances sorted by PVI leads to accelerated convergence and a 0.8% increase in accuracy compared to traditional training methods. The study also extends the PVI framework to Chinese NLP tasks, demonstrating its applicability in cross-lingual data reduction and faster training. The released codes provide a valuable resource for implementation and further research in this area. <br /> <div>
arXiv:2507.00038v1 Announce Type: cross 
Abstract: Data reduction plays a vital role in data-centric AI by identifying the most informative instance within large-scale datasets to enhance model training efficiency. The core challenge lies in how to select the optimal instances-rather than the entire datasets-to improve data quality and training efficiency. In this paper, we propose an effective data reduction strategy based on Pointwise V-information(PVI). First, we quantify instance difficulty using PVI and filter out low-difficulty instances enabling a static approach. Experiments demonstrate that removing 10%-30% of the data preserves the classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we use a progressive learning approach to training the classifiers on instances sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy gain over conventional training. Our results suggest that with the effective data reduction strategy, training a classifier on the selected optimal subset could enhance the model performance and boost training efficiency. Moreover, we have transferred the PVI framework, which previously applied only to English datasets, to diverse Chinese NLP tasks and base models, leading to valuable insights for cross-lingual data reduction and faster training. The codes are released at https://github.com/zhouwenchi/DatasetReductionStrategy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing</title>
<link>https://arxiv.org/abs/2507.00039</link>
<guid>https://arxiv.org/abs/2507.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph classification, Quality measures, Pattern ranking, Benchmark, Clustering

Summary: 
This article presents a comparative analysis of 38 quality measures used in graph classification tasks. The measures are characterized based on four mathematical properties and evaluated empirically using publicly available datasets. A gold standard ranking of patterns is established to compare the performance of the measures. The study introduces a clustering-based preprocessing step to group similar patterns for improved classification results. Results indicate that some commonly used measures in the literature may not be optimal for classification tasks. The proposed preprocessing step reduces the number of patterns to process while maintaining comparable performance. The research highlights the importance of selecting appropriate quality measures for graph classification and suggests that popular measures may not always yield the best results. <div>
arXiv:2507.00039v1 Announce Type: cross 
Abstract: Graph classification aims to categorize graphs based on their structural and attribute features, with applications in diverse fields such as social network analysis and bioinformatics. Among the methods proposed to solve this task, those relying on patterns (i.e. subgraphs) provide good explainability, as the patterns used for classification can be directly interpreted. To identify meaningful patterns, a standard approach is to use a quality measure, i.e. a function that evaluates the discriminative power of each pattern. However, the literature provides tens of such measures, making it difficult to select the most appropriate for a given application. Only a handful of surveys try to provide some insight by comparing these measures, and none of them specifically focuses on graphs. This typically results in the systematic use of the most widespread measures, without thorough evaluation. To address this issue, we present a comparative analysis of 38 quality measures from the literature. We characterize them theoretically, based on four mathematical properties. We leverage publicly available datasets to constitute a benchmark, and propose a method to elaborate a gold standard ranking of the patterns. We exploit these resources to perform an empirical comparison of the measures, both in terms of pattern ranking and classification performance. Moreover, we propose a clustering-based preprocessing step, which groups patterns appearing in the same graphs to enhance classification performance. Our experimental results demonstrate the effectiveness of this step, reducing the number of patterns to be processed while achieving comparable performance. Additionally, we show that some popular measures widely used in the literature are not associated with the best results.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay</title>
<link>https://arxiv.org/abs/2507.00042</link>
<guid>https://arxiv.org/abs/2507.00042</guid>
<content:encoded><![CDATA[
<div> adaptive experience replay, edge model update algorithm, Domain Distance Metric-based Experience Selection, multi-kernel maximum mean discrepancy, traffic monitoring<br />
Summary:<br />
- In dynamic traffic environments, continual adaptation of edge models for object detection may suffer from catastrophic forgetting.<br />
- Existing approaches like experience replay and visual prompts struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation.<br />
- The proposed ER-EMU algorithm utilizes adaptive experience replay with a First-In-First-Out (FIFO) buffer and a Domain Distance Metric-based Experience Selection (DDM-ES) algorithm.<br />
- DDM-ES uses multi-kernel maximum mean discrepancy to prioritize selection of historical data that is most dissimilar to the current target domain, ensuring training diversity and preventing overfitting.<br />
- Experiments on the Bellevue traffic video dataset show that ER-EMU consistently improves the performance of cloud-edge collaborative object detection frameworks. <br /> <div>
arXiv:2507.00042v1 Announce Type: cross 
Abstract: Continually adapting edge models in cloud-edge collaborative object detection for traffic monitoring suffers from catastrophic forgetting, where models lose previously learned knowledge when adapting to new data distributions. This is especially problematic in dynamic traffic environments characterised by periodic variations (e.g., day/night, peak hours), where past knowledge remains valuable. Existing approaches like experience replay and visual prompts offer some mitigation, but struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation. Specifically, simply storing and replaying all historical data can be inefficient, while treating all historical experiences as equally important overlooks their varying relevance to the current domain. This paper proposes ER-EMU, an edge model update algorithm based on adaptive experience replay, to address these limitations. ER-EMU utilizes a limited-size experience buffer managed using a First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target domains, prioritizing the selection of historical data that is most dissimilar to the current target domain. This ensures training diversity and facilitates the retention of knowledge from a wider range of past experiences, while also preventing overfitting to the new domain. The experience buffer is also updated using a simple random sampling strategy to maintain a balanced representation of previous domains. Experiments on the Bellevue traffic video dataset, involving repeated day/night cycles, demonstrate that ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations</title>
<link>https://arxiv.org/abs/2507.00043</link>
<guid>https://arxiv.org/abs/2507.00043</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, DICOM, contrastive learning, multimodal, clinical applications
Summary:
MR-CLIP is a framework designed to address the challenges of interpreting MRI scans accurately based on image contrast. It aligns MRI images with DICOM metadata to learn contrast-aware representations without the need for manual labels. The framework is trained on a diverse clinical dataset, capturing contrast variations across acquisitions and within scans to enable anatomy-invariant representations. MR-CLIP's effectiveness is demonstrated in cross-modal retrieval and contrast classification tasks, showcasing its scalability and potential for various clinical applications. The code and weights for MR-CLIP are publicly available on GitHub, providing a valuable resource for the research community. The framework offers a solution to the lack of reliable and standardized metadata in MRI datasets, improving image interpretation, retrieval, and integration into clinical workflows. This approach holds promise for advancing clinical applications and achieving modality-invariant representations in the field of medical imaging. <br /><br />Summary: <div>
arXiv:2507.00043v1 Announce Type: cross 
Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at https://github.com/myigitavci/MR-CLIP.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HistoART: Histopathology Artifact Detection and Reporting Tool</title>
<link>https://arxiv.org/abs/2507.00044</link>
<guid>https://arxiv.org/abs/2507.00044</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole Slide Imaging, artifact detection, deep learning, cancer diagnostics, digital histopathology

Summary:
In the field of cancer diagnostics, Whole Slide Imaging (WSI) is a key technology for digitizing tissue specimens. However, artifacts introduced during slide preparation and scanning can compromise the accuracy of analysis. To tackle this issue, three robust artifact detection approaches were compared: a foundation model-based approach (FMA), a deep learning approach (DLA), and a knowledge-based approach (KBA). Evaluations on 50,000+ image patches from various scanners showed that the FMA outperformed the other methods with a high patch-wise AUROC of 0.995. To provide actionable insights, a quality report scorecard was developed to quantify high-quality image patches and visualize artifact distributions. This research showcases the effectiveness of utilizing advanced technologies like deep learning in improving the quality and accuracy of digital histopathology. 

<br /><br />Summary: <div>
arXiv:2507.00044v1 Announce Type: cross 
Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to digitize tissue specimens for detailed, high-resolution examination; however, other diagnostic approaches, such as liquid biopsy and molecular testing, are also utilized based on the cancer type and clinical context. While WSI has revolutionized digital histopathology by enabling automated, precise analysis, it remains vulnerable to artifacts introduced during slide preparation and scanning. These artifacts can compromise downstream image analysis. To address this challenge, we propose and compare three robust artifact detection approaches for WSIs: (1) a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics. The methods target six common artifact types: tissue folds, out-of-focus regions, air bubbles, tissue damage, marker traces, and blood contamination. Evaluations were conducted on 50,000+ image patches from diverse scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]), outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978]) and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into actionable insights, we developed a quality report scorecard that quantifies high-quality patches and visualizes artifact distributions.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning</title>
<link>https://arxiv.org/abs/2507.00045</link>
<guid>https://arxiv.org/abs/2507.00045</guid>
<content:encoded><![CDATA[
<div> detective, multimodal language models, visual perception, reasoning, challenging tasks
Summary:
The article explores the performance of agentic Multi-Modal Large Language Models (MLLMs) like GPT-03 in challenging detective tasks. While MLLMs excel in expert-level tasks, they struggle in scenarios like CaughtCheating where subtle clues must be detected from images. The study identifies the limitations of existing MLLMs in handling such tasks, emphasizing the need for enhanced visual perception and reasoning capabilities. By investigating the CaughtCheating scenario, the research aims to bridge the gap between MLLMs and human detective abilities, offering valuable insights for practical usage. Success in these challenging tasks could enable MLLMs to achieve human-level detective skills, providing a new avenue for advancing artificial intelligence in perception and reasoning. <div>
arXiv:2507.00045v1 Announce Type: cross 
Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.00052</link>
<guid>https://arxiv.org/abs/2507.00052</guid>
<content:encoded><![CDATA[
<div> attack templates, visual perturbations, vulnerability scoring framework, medical imaging workflows, adversarial variants
Summary:<br />
The article introduces VSF--Med, a framework for evaluating the security of Vision Language Models (VLMs) in medical imaging. It includes text-prompt attack templates, imperceptible visual perturbations, and a rubric for vulnerability scoring. VSF--Med allows for the benchmarking of medical VLMs using over 30,000 adversarial variants from radiology images. The analysis shows shifts in z-score values for persistence of attack effects, prompt injection effectiveness, and safety bypass success across VLMs. Llama-3.2-11B-Vision-Instruct exhibits the highest vulnerability increase in persistence of attack effects, while GPT-4o shows vulnerability increases in both persistence of attack effects and prompt injection attacks. The framework aims to streamline labor-intensive medical imaging workflows and enhance the security evaluation of VLMs in clinical settings.<br />Summary: <div>
arXiv:2507.00052v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) hold great promise for streamlining labour-intensive medical imaging workflows, yet systematic security evaluations in clinical settings remain scarce. We introduce VSF--Med, an end-to-end vulnerability-scoring framework for medical VLMs that unites three novel components: (i) a rich library of sophisticated text-prompt attack templates targeting emerging threat vectors; (ii) imperceptible visual perturbations calibrated by structural similarity (SSIM) thresholds to preserve clinical realism; and (iii) an eight-dimensional rubric evaluated by two independent judge LLMs, whose raw scores are consolidated via z-score normalization to yield a 0--32 composite risk metric. Built entirely on publicly available datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000 adversarial variants from 5,000 radiology images and enables reproducible benchmarking of any medical VLM with a single command. Our consolidated analysis reports mean z-score shifts of $0.90\sigma$ for persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness, and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs. Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection attacks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Correctness Without Oracles in LLM-Based Code Generation</title>
<link>https://arxiv.org/abs/2507.00057</link>
<guid>https://arxiv.org/abs/2507.00057</guid>
<content:encoded><![CDATA[
<div> Keywords: code generation, natural language specifications, Large Language Models, incoherence, error estimation<br />
Summary:<br />
Generating code from natural language specifications using Large Language Models (LLMs) has been successful, but LLMs often produce factually incorrect outputs. This paper introduces a measure of incorrectness called incoherence, which estimates the probability of generated code being incorrect without an existing correct implementation. The proposed methodology efficiently identifies incorrect programs and can replace oracle-based evaluation of LLMs with high reliability. Experiments show that the incoherence-based evaluation can identify about two-thirds of incorrect programs without false positives. The ranking of LLMs based on correctness assessment by an oracle is strongly correlated with the ranking based on incoherence evaluation. This approach provides a valuable tool for assessing the accuracy of code generated by LLMs in the absence of an oracle.<br /> 
Summary: <div>
arXiv:2507.00057v1 Announce Type: cross 
Abstract: Generating code from natural language specifications is one of the most successful applications of Large Language Models (LLMs). Yet, they hallucinate: LLMs produce outputs that may be grammatically correct but are factually incorrect. Without an existing, correct implementation (i.e., an oracle), can we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence, that can be estimated efficiently in the absence of an oracle and provides a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. Our experiments demonstrate an extraordinary effectiveness. For the average code generation task, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives. In fact, an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via our incoherence.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data</title>
<link>https://arxiv.org/abs/2507.00061</link>
<guid>https://arxiv.org/abs/2507.00061</guid>
<content:encoded><![CDATA[
<div> framework, human activity recognition, sensor placement detection, self-distillation, accelerometer data<br />
<br />
Summary: 
The paper introduces the Smooth-Distill framework for simultaneous human activity recognition (HAR) and sensor placement detection using wearable sensor data. It utilizes a CNN-based architecture, MTL-net, with two output branches for each task. The framework employs a self-distillation approach with a historical version of the model as the teacher, reducing computational overhead. A comprehensive dataset capturing 12 sleep postures across three wearing positions is developed. Smooth-Distill outperforms alternative methods in both HAR and sensor placement detection tasks, showing improvements in accuracy and training efficiency. It demonstrates stability in convergence patterns and reduced overfitting compared to traditional multitask learning. The framework facilitates knowledge distillation in HAR systems, offering a balance between accuracy and training efficiency. It also reduces computational costs, making it suitable for resource-constrained platforms. The code and model are available on GitHub. <div>
arXiv:2507.00061v1 Announce Type: cross 
Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework designed to simultaneously perform human activity recognition (HAR) and sensor placement detection using wearable sensor data. The proposed approach utilizes a unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. Unlike conventional distillation methods that require separate teacher and student models, the proposed framework utilizes a smoothed, historical version of the model itself as the teacher, significantly reducing training computational overhead while maintaining performance benefits. To support this research, we developed a comprehensive accelerometer-based dataset capturing 12 distinct sleep postures across three different wearing positions, complementing two existing public datasets (MHealth and WISDM). Experimental results show that Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. This method demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines. This framework contributes to the practical implementation of knowledge distillation in human activity recognition systems, offering an effective solution for multitask learning with accelerometer data that balances accuracy and training efficiency. More broadly, it reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms. The code and model are available at https://github.com/Kuan2vn/smooth\_distill.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph</title>
<link>https://arxiv.org/abs/2507.00066</link>
<guid>https://arxiv.org/abs/2507.00066</guid>
<content:encoded><![CDATA[
arXiv:2507.00066v1 Announce Type: cross 
Abstract: Human reliability remains a critical concern in safety-critical domains such as nuclear power, where operational failures are often linked to human error. While conventional human reliability analysis (HRA) methods have been widely adopted, they rely heavily on expert judgment for identifying human failure events (HFEs) and assigning performance influencing factors (PIFs). This reliance introduces challenges related to reproducibility, subjectivity, and limited integration of interface-level data. In particular, current approaches lack the capacity to rigorously assess how human-machine interface design contributes to operator performance variability and error susceptibility. To address these limitations, this study proposes a framework for risk-informed human failure event identification and interface-induced risk assessment driven by AutoGraph (InSight-R). By linking empirical behavioral data to the interface-embedded knowledge graph (IE-KG) constructed by the automated graph-based execution framework (AutoGraph), the InSight-R framework enables automated HFE identification based on both error-prone and time-deviated operational paths. Furthermore, we discuss the relationship between designer-user conflicts and human error. The results demonstrate that InSight-R not only enhances the objectivity and interpretability of HFE identification but also provides a scalable pathway toward dynamic, real-time human reliability assessment in digitalized control environments. This framework offers actionable insights for interface design optimization and contributes to the advancement of mechanism-driven HRA methodologies.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding</title>
<link>https://arxiv.org/abs/2507.00068</link>
<guid>https://arxiv.org/abs/2507.00068</guid>
<content:encoded><![CDATA[
arXiv:2507.00068v1 Announce Type: cross 
Abstract: While multi-modal learning has advanced significantly, current approaches often treat modalities separately, creating inconsistencies in representation and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization via Textual Alignment), a theoretically-grounded framework that unifies visual and auditory inputs into a structured textual space for seamless processing with large language models. MANTA addresses four key challenges: (1) semantic alignment across modalities with information-theoretic optimization, (2) adaptive temporal synchronization for varying information densities, (3) hierarchical content representation for multi-scale understanding, and (4) context-aware retrieval of sparse information from long sequences. We formalize our approach within a rigorous mathematical framework, proving its optimality for context selection under token constraints. Extensive experiments on the challenging task of Long Video Question Answering show that MANTA improves state-of-the-art models by up to 22.6% in overall accuracy, with particularly significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement) and cross-modal understanding (25.1% improvement). Our framework introduces novel density estimation techniques for redundancy minimization while preserving rare signals, establishing new foundations for unifying multimodal representations through structured text.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient plant disease detection using transfer learning approach</title>
<link>https://arxiv.org/abs/2507.00070</link>
<guid>https://arxiv.org/abs/2507.00070</guid>
<content:encoded><![CDATA[
arXiv:2507.00070v1 Announce Type: cross 
Abstract: Plant diseases pose significant challenges to farmers and the agricultural sector at large. However, early detection of plant diseases is crucial to mitigating their effects and preventing widespread damage, as outbreaks can severely impact the productivity and quality of crops. With advancements in technology, there are increasing opportunities for automating the monitoring and detection of disease outbreaks in plants. This study proposed a system designed to identify and monitor plant diseases using a transfer learning approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two state-ofthe-art models in the field of object detection. By fine-tuning these models on a dataset of plant leaf images, the system is able to accurately detect the presence of Bacteria, Fungi and Viral diseases such as Powdery Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's performance was evaluated using several metrics, including mean Average Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05, 89.40, 91.22, and 87.66, respectively. The result demonstrates the superior effectiveness and efficiency of YOLOv8 compared to other object detection methods, highlighting its potential for use in modern agricultural practices. The approach provides a scalable, automated solution for early any plant disease detection, contributing to enhanced crop yield, reduced reliance on manual monitoring, and supporting sustainable agricultural practices.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap</title>
<link>https://arxiv.org/abs/2507.00075</link>
<guid>https://arxiv.org/abs/2507.00075</guid>
<content:encoded><![CDATA[
arXiv:2507.00075v1 Announce Type: cross 
Abstract: Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further introduce how to predict the ultimate power of self-improvement using only information from the first few training epochs. We empirically validate the effectiveness of the theoretical model on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The language of time: a language model perspective on time-series foundation models</title>
<link>https://arxiv.org/abs/2507.00078</link>
<guid>https://arxiv.org/abs/2507.00078</guid>
<content:encoded><![CDATA[
arXiv:2507.00078v1 Announce Type: cross 
Abstract: With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State and Memory is All You Need for Robust and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2507.00081</link>
<guid>https://arxiv.org/abs/2507.00081</guid>
<content:encoded><![CDATA[
arXiv:2507.00081v1 Announce Type: cross 
Abstract: Large language models (LLMs) have enabled powerful advances in natural language understanding and generation. Yet their application to complex, real-world scientific workflows remain limited by challenges in memory, planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals), a modular agentic framework that allows LLM-based agents to autonomously plan, reason, and achieve robust and reliable domain-specific task execution. Agents are constructed dynamically from source code documentation and augmented with finite-state automata (FSA) memory, enabling persistent state tracking and context-aware decision-making. This approach eliminates the need for manual prompt engineering and allows for robust, scalable deployment across diverse applications via maintaining context across extended workflows and to recover from tool or execution failures. We validate SciBORG through integration with both physical and virtual hardware, such as microwave synthesizers for executing user-specified reactions, with context-aware decision making and demonstrate its use in autonomous multi-step bioassay retrieval from the PubChem database utilizing multi-step planning, reasoning, agent-to-agent communication and coordination for execution of exploratory tasks. Systematic benchmarking shows that SciBORG agents achieve reliable execution, adaptive planning, and interpretable state transitions. Our results show that memory and state awareness are critical enablers of agentic planning and reliability, offering a generalizable foundation for deploying AI agents in complex environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission</title>
<link>https://arxiv.org/abs/2507.00082</link>
<guid>https://arxiv.org/abs/2507.00082</guid>
<content:encoded><![CDATA[
arXiv:2507.00082v1 Announce Type: cross 
Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small Language Models (SLMs) on edge devices with the high accuracy of Large Language Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM inference, HLMs reduce latency and communication by invoking LLMs only when local SLM predictions are uncertain, i.e., when token-level confidence is low or entropy is high. However, ambiguous or low-confidence predictions still require frequent offloading to the LLM, leading to significant communication overhead in bandwidth-constrained settings. To address this, we propose FedHLM, a communication-efficient HLM framework that integrates uncertainty-aware inference with Federated Learning (FL). FedHLM's key innovation lies in collaboratively learning token-level uncertainty thresholds that govern when LLM assistance is needed. Rather than using static or manually tuned thresholds, FedHLM employs FL to optimize these thresholds in a privacy-preserving, distributed manner. Additionally, it leverages embedding-based token representations for Peer-to-Peer (P2P) resolution, enabling clients to reuse tokens inferred by semantically similar peers without engaging the LLM. We further introduce hierarchical model aggregation: edge servers refine local routing policies through client updates, while cross-cluster coordination aligns global decision boundaries. This layered design captures recurring uncertainty patterns, reducing redundant LLM queries. Experiments on large-scale news classification tasks show that FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss, making it well-suited for scalable and efficient edge-AI applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks</title>
<link>https://arxiv.org/abs/2507.00083</link>
<guid>https://arxiv.org/abs/2507.00083</guid>
<content:encoded><![CDATA[
arXiv:2507.00083v1 Announce Type: cross 
Abstract: This study addresses the lack of structured causal modeling between tactical strike behavior and strategic delay in current strategic-level simulations, particularly the structural bottlenecks in capturing intermediate variables within the "resilience - nodal suppression - negotiation window" chain. We propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN), a novel framework that closes the causal loop from tactical input to strategic delay output. The model integrates graph attention mechanisms, counterfactual simulation units, and spatial intervention node reconstruction to enable dynamic simulations of strike configurations and synchronization strategies. Training data are generated from a multi-physics simulation platform (GEANT4 + COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and policy-level validation. Experimental results demonstrate that IA-STGNN significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost), achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5 percent accuracy, while improving causal path consistency and intervention stability. IA-STGNN enables interpretable prediction of strategic delay and supports applications such as nuclear deterrence simulation, diplomatic window assessment, and multi-strategy optimization, providing a structured and transparent AI decision-support mechanism for high-level policy modeling.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism</title>
<link>https://arxiv.org/abs/2507.00085</link>
<guid>https://arxiv.org/abs/2507.00085</guid>
<content:encoded><![CDATA[
arXiv:2507.00085v1 Announce Type: cross 
Abstract: Accurate traffic prediction is essential for Intelligent Transportation Systems (ITS), yet current methods struggle with the inherent complexity and non-linearity of traffic dynamics, making it difficult to integrate spatial and temporal characteristics. Furthermore, existing approaches use static techniques to address non-stationary and anomalous historical data, which limits adaptability and undermines data smoothing. To overcome these challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative framework for network-level traffic speed prediction. GFEN introduces a novel topological spatiotemporal graph fusion technique that meticulously extracts and merges spatial and temporal correlations from both data distribution and network topology using trainable methods, enabling the modeling of multi-scale spatiotemporal features. Additionally, GFEN employs a hybrid methodology combining a k-th order difference-based mathematical framework with an attention-based deep learning structure to adaptively smooth historical observations and dynamically mitigate data anomalies and non-stationarity. Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods by approximately 6.3% in prediction accuracy and exhibits convergence rates nearly twice as fast as recent hybrid models, confirming its superior performance and potential to significantly enhance traffic prediction system efficiency.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation</title>
<link>https://arxiv.org/abs/2507.00087</link>
<guid>https://arxiv.org/abs/2507.00087</guid>
<content:encoded><![CDATA[
arXiv:2507.00087v1 Announce Type: cross 
Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks. We present pUniFind, the first large-scale multimodal pre-trained model in proteomics that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing. Trained on over 100 million open search-derived spectra, pUniFind aligns spectral and peptide modalities via cross modality prediction and outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage. These results establish a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How large language models judge and influence human cooperation</title>
<link>https://arxiv.org/abs/2507.00088</link>
<guid>https://arxiv.org/abs/2507.00088</guid>
<content:encoded><![CDATA[
arXiv:2507.00088v1 Announce Type: cross 
Abstract: Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Heterogeneous Multi-dimensional Data : A Comparative Study</title>
<link>https://arxiv.org/abs/2507.00090</link>
<guid>https://arxiv.org/abs/2507.00090</guid>
<content:encoded><![CDATA[
arXiv:2507.00090v1 Announce Type: cross 
Abstract: Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\sigma$-Maximal Ancestral Graphs</title>
<link>https://arxiv.org/abs/2507.00093</link>
<guid>https://arxiv.org/abs/2507.00093</guid>
<content:encoded><![CDATA[
arXiv:2507.00093v1 Announce Type: cross 
Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of Directed Acyclic Graphs (DAGs) with latent (selection) variables. These graphical objects encode information about ancestral relations and d-separations of the DAGs they represent. This abstract representation has been used amongst others to prove the soundness and completeness of the FCI algorithm for causal discovery, and to derive a do-calculus for its output. One significant inherent limitation of MAGs is that they rule out the possibility of cyclic causal relationships. In this work, we address that limitation. We introduce and study a class of graphical objects that we coin ''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these graphs provide an abstract representation of (possibly cyclic) Directed Graphs (DGs) with latent (selection) variables, analogously to how MAGs represent DAGs. We study the properties of these objects and provide a characterization of their Markov equivalence classes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)</title>
<link>https://arxiv.org/abs/2507.00094</link>
<guid>https://arxiv.org/abs/2507.00094</guid>
<content:encoded><![CDATA[
arXiv:2507.00094v1 Announce Type: cross 
Abstract: Despite growing interest in process analysis and mining for data-aware specifications, alignment-based conformance checking for declarative process models has focused on pure control-flow specifications, or mild data-aware extensions limited to numerical data and variable-to-constant comparisons. This is not surprising: finding alignments is computationally hard, even more so in the presence of data dependencies. In this paper, we challenge this problem in the case where the reference model is captured using data-aware Declare with general data types and data conditions. We show that, unexpectedly, it is possible to compute data-aware optimal alignments in this rich setting, enjoying at once efficiency and expressiveness. This is achieved by carefully combining the two best-known approaches to deal with control flow and data dependencies when computing alignments, namely A* search and SMT solving. Specifically, we introduce a novel algorithmic technique that efficiently explores the search space, generating descendant states through the application of repair actions aiming at incrementally resolving constraint violations. We prove the correctness of our algorithm and experimentally show its efficiency. The evaluation witnesses that our approach matches or surpasses the performance of the state of the art while also supporting significantly more expressive data dependencies, showcasing its potential to support real-world applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets</title>
<link>https://arxiv.org/abs/2507.00096</link>
<guid>https://arxiv.org/abs/2507.00096</guid>
<content:encoded><![CDATA[
arXiv:2507.00096v1 Announce Type: cross 
Abstract: Alternative Assets tokenization is transforming non-traditional financial instruments are represented and traded on the web. However, ensuring trustworthiness in web-based tokenized ecosystems poses significant challenges, from verifying off-chain asset data to enforcing regulatory compliance. This paper proposes an AI-governed agent architecture that integrates intelligent agents with blockchain to achieve web-trustworthy tokenization of alternative assets. In the proposed architecture, autonomous agents orchestrate the tokenization process (asset verification, valuation, compliance checking, and lifecycle management), while an AI-driven governance layer monitors agent behavior and enforces trust through adaptive policies and cryptoeconomic incentives. We demonstrate that this approach enhances transparency, security, and compliance in asset tokenization, addressing key concerns around data authenticity and fraud. A case study on tokenizing real estate assets illustrates how the architecture mitigates risks (e.g., fraudulent listings and money laundering) through real-time AI anomaly detection and on-chain enforcement. Our evaluation and analysis suggest that combining AI governance with multi-agent systems and blockchain can significantly bolster trust in tokenized asset ecosystems. This work offers a novel framework for trustworthy asset tokenization on the web and provides insights for practitioners aiming to deploy secure, compliant tokenization platforms.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series</title>
<link>https://arxiv.org/abs/2507.00102</link>
<guid>https://arxiv.org/abs/2507.00102</guid>
<content:encoded><![CDATA[
arXiv:2507.00102v1 Announce Type: cross 
Abstract: Ensuring consistent product quality in modern manufacturing is crucial, particularly in safety-critical applications. Conventional quality control approaches, reliant on manually defined thresholds and features, lack adaptability to the complexity and variability inherent in production data and necessitate extensive domain expertise. Conversely, data-driven methods, such as machine learning, demonstrate high detection performance but typically function as black-box models, thereby limiting their acceptance in industrial environments where interpretability is paramount. This paper introduces a methodology for industrial fault detection, which is both data-driven and transparent. The approach integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a do-main-specific visualisation technique that maps model explanations to operator-interpretable features. Furthermore, the study proposes an evaluation methodology that assesses model explanations through quantitative perturbation analysis and evaluates visualisations by qualitative expert assessment. The approach was applied to the crimping process, a safety-critical joining technique, using a dataset of univariate, discrete time series. The system achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and inter-pretability of the generated explanations. This human-centric approach is designed to enhance trust and interpretability in data-driven fault detection, thereby contributing to applied system design in industrial quality control.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives</title>
<link>https://arxiv.org/abs/2507.00108</link>
<guid>https://arxiv.org/abs/2507.00108</guid>
<content:encoded><![CDATA[
arXiv:2507.00108v1 Announce Type: cross 
Abstract: Computer programming is undergoing a true transformation driven by powerful new tools for automatic source code generation based on large language models. This transformation is also manifesting in introductory programming courses at universities around the world, generating an in-depth debate about how programming content should be taught, learned, and assessed in the context of generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on this issue, highlighting the advantages and disadvantages identified in the specialized literature. On the other hand, it proposes enriching teaching and learning methodologies by focusing on code comprehension and execution rather than on mere coding or program functionality. In particular, it advocates for the use of visual representations of code and visual simulations of its execution as effective tools for teaching, learning, and assessing programming, thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming course are presented to provide preliminary context supporting the incorporation of visual simulations in Java (or other languages) as part of the training process.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise</title>
<link>https://arxiv.org/abs/2507.00145</link>
<guid>https://arxiv.org/abs/2507.00145</guid>
<content:encoded><![CDATA[
arXiv:2507.00145v1 Announce Type: cross 
Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform entropy directly from physical noise, eliminating the need for bulky quantum devices or expensive laboratory-grade RF receivers. Instead, it relies on a low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number generators (RNGs), our dynamic inner-outer network couples adaptive natural sources and reseeding, yielding truly unpredictable and autonomous sequences. Generated numbers pass the NIST SP 800-22 battery better than a CPU-based method. It also passes nineteen bespoke statistical tests for both bit- and integer-level analysis. All results satisfy cryptographic standards, while forward and backward prediction experiments reveal no exploitable biases. The model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG broadens the reach of high-integrity random number generators across secure systems, cryptographic protocols, embedded and edge devices, stochastic simulations, and server applications that need randomness.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments</title>
<link>https://arxiv.org/abs/2507.00161</link>
<guid>https://arxiv.org/abs/2507.00161</guid>
<content:encoded><![CDATA[
arXiv:2507.00161v1 Announce Type: cross 
Abstract: Political polarization undermines democratic civic education by exacerbating identity-based resistance to opposing viewpoints. Emerging AI technologies offer new opportunities to advance interventions that reduce polarization and promote political open-mindedness. We examined novel design strategies that leverage adaptive and emotionally-responsive civic narratives that may sustain students' emotional engagement in stories, and in turn, promote perspective-taking toward members of political out-groups. Drawing on theories from political psychology and narratology, we investigate how affective computing techniques can support three storytelling mechanisms: transportation into a story world, identification with characters, and interaction with the storyteller. Using a design-based research (DBR) approach, we iteratively developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS) platform. Our prototype integrates facial emotion recognition and attention tracking to assess users' affective and attentional states in real time. Narrative content is organized around pre-structured story outlines, with beat-by-beat language adaptation implemented via GPT-4, personalizing linguistic tone to sustain students' emotional engagement in stories that center political perspectives different from their own. Our work offers a foundation for AI-supported, emotionally-sensitive strategies that address affective polarization while preserving learner autonomy. We conclude with implications for civic education interventions, algorithmic literacy, and HCI challenges associated with AI dialogue management and affect-adaptive learning environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros</title>
<link>https://arxiv.org/abs/2507.00184</link>
<guid>https://arxiv.org/abs/2507.00184</guid>
<content:encoded><![CDATA[
arXiv:2507.00184v1 Announce Type: cross 
Abstract: Recent research shows how diffusion models can unconditionally generate tile-based game levels, but use of diffusion models for text-to-level generation is underexplored. There are practical considerations for creating a usable model: caption/level pairs are needed, as is a text embedding model, and a way of generating entire playable levels, rather than individual scenes. We present strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch. Captions are automatically assigned to generated levels so that the degree of overlap between input and output captions can be compared. We also assess the diversity and playability of the resulting levels. Results are compared with an unconditional diffusion model and a generative adversarial network, as well as the text-to-level approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model uses a simple transformer model for text embedding, and takes less time to train than diffusion models employing more complex text encoders, indicating that reliance on larger language models is not necessary. We also present a GUI allowing designers to construct long levels from model-generated scenes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)</title>
<link>https://arxiv.org/abs/2507.00185</link>
<guid>https://arxiv.org/abs/2507.00185</guid>
<content:encoded><![CDATA[
arXiv:2507.00185v1 Announce Type: cross 
Abstract: Current artificial intelligence models for medical imaging are predominantly single modality and single disease. Attempts to create multimodal and multi-disease models have resulted in inconsistent clinical accuracy. Furthermore, training these models typically requires large, labour-intensive, well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal, multi-specialty foundation model trained using self-supervised learning and a memory module. MerMED-FM was trained on 3.3 million medical images from over ten specialties and seven modalities, including computed tomography (CT), chest X-rays (CXR), ultrasound (US), pathology patches, color fundus photography (CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was evaluated across multiple diseases and compared against existing foundational models. Strong performance was achieved across all modalities, with AUROCs of 0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894 (CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable, versatile, cross-specialty foundation model that enables robust medical imaging interpretation across diverse medical disciplines.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions</title>
<link>https://arxiv.org/abs/2507.00191</link>
<guid>https://arxiv.org/abs/2507.00191</guid>
<content:encoded><![CDATA[
arXiv:2507.00191v1 Announce Type: cross 
Abstract: Wearable devices record physiological and behavioral signals that can improve health predictions. While foundation models are increasingly used for such predictions, they have been primarily applied to low-level sensor data, despite behavioral data often being more informative due to their alignment with physiologically relevant timescales and quantities. We develop foundation models of such behavioral signals using over 2.5B hours of wearable data from 162K individuals, systematically optimizing architectures and tokenization strategies for this unique dataset. Evaluated on 57 health-related tasks, our model shows strong performance across diverse real-world applications including individual-level classification and time-varying health state prediction. The model excels in behavior-driven tasks like sleep prediction, and improves further when combined with representations of raw sensor data. These results underscore the importance of tailoring foundation model design to wearables and demonstrate the potential to enable new health applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness</title>
<link>https://arxiv.org/abs/2507.00195</link>
<guid>https://arxiv.org/abs/2507.00195</guid>
<content:encoded><![CDATA[
arXiv:2507.00195v1 Announce Type: cross 
Abstract: This thesis contributes to the theoretical understanding of local update algorithms, especially Local SGD, in distributed and federated optimization under realistic models of data heterogeneity. A central focus is on the bounded second-order heterogeneity assumption, which is shown to be both necessary and sufficient for local updates to outperform centralized or mini-batch methods in convex and non-convex settings. The thesis establishes tight upper and lower bounds in several regimes for various local update algorithms and characterizes the min-max complexity of multiple problem classes. At its core is a fine-grained consensus-error-based analysis framework that yields sharper finite-time convergence bounds under third-order smoothness and relaxed heterogeneity assumptions. The thesis also extends to online federated learning, providing fundamental regret bounds under both first-order and bandit feedback. Together, these results clarify when and why local updates offer provable advantages, and the thesis serves as a self-contained guide for analyzing Local SGD in heterogeneous environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures</title>
<link>https://arxiv.org/abs/2507.00209</link>
<guid>https://arxiv.org/abs/2507.00209</guid>
<content:encoded><![CDATA[
arXiv:2507.00209v1 Announce Type: cross 
Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning</title>
<link>https://arxiv.org/abs/2507.00214</link>
<guid>https://arxiv.org/abs/2507.00214</guid>
<content:encoded><![CDATA[
arXiv:2507.00214v1 Announce Type: cross 
Abstract: Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering the underlying analytic structure within Standard Model constants using artificial intelligence</title>
<link>https://arxiv.org/abs/2507.00225</link>
<guid>https://arxiv.org/abs/2507.00225</guid>
<content:encoded><![CDATA[
arXiv:2507.00225v1 Announce Type: cross 
Abstract: This paper presents a search for underlying analytic structures among the fundamental parameters of the Standard Model (SM) using symbolic regression and genetic programming. We identify the simplest analytic relationships connecting pairs of these constants and report several notable observations based on about a thousand expressions with relative precision better than 1%. These results may serve as valuable inputs for model builders and artificial intelligence methods aimed at uncovering hidden patterns among the SM constants, or potentially used as building blocks for a deeper underlying law that connects all parameters of the SM through a small set of fundamental constants.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis</title>
<link>https://arxiv.org/abs/2507.00227</link>
<guid>https://arxiv.org/abs/2507.00227</guid>
<content:encoded><![CDATA[
arXiv:2507.00227v1 Announce Type: cross 
Abstract: While generative methods have progressed rapidly in recent years, generating expressive prosody for an utterance remains a challenging task in text-to-speech synthesis. This is particularly true for systems that model prosody explicitly through parameters such as pitch, energy, and duration, which is commonly done for the sake of interpretability and controllability. In this work, we investigate the effectiveness of stochastic methods for this task, including Normalizing Flows, Conditional Flow Matching, and Rectified Flows. We compare these methods to a traditional deterministic baseline, as well as to real human realizations. Our extensive subjective and objective evaluations demonstrate that stochastic methods produce natural prosody on par with human speakers by capturing the variability inherent in human speech. Further, they open up additional controllability options by allowing the sampling temperature to be tuned.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss</title>
<link>https://arxiv.org/abs/2507.00229</link>
<guid>https://arxiv.org/abs/2507.00229</guid>
<content:encoded><![CDATA[
arXiv:2507.00229v1 Announce Type: cross 
Abstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing the sampling rate. While most SSR methods focus on magnitude reconstruction, recent research highlights the importance of phase reconstruction for improved perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency Transformation Network that reconstructs both magnitude and phase in complex domains for improved SSR tasks. It incorporates a complex global attention block to model inter-phoneme and inter-frequency dependencies and a complex conformer to capture long-range and local features, improving frequency reconstruction and noise robustness. CTFT-Net employs time-domain and multi-resolution frequency-domain loss functions for better generalization. Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave, WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling (2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy artifacts.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations</title>
<link>https://arxiv.org/abs/2507.00234</link>
<guid>https://arxiv.org/abs/2507.00234</guid>
<content:encoded><![CDATA[
arXiv:2507.00234v1 Announce Type: cross 
Abstract: In this paper, we present a novel framework for enhancing model interpretability by integrating heatmaps produced separately by ResNet and a restructured 2D Transformer with globally weighted input saliency. We address the critical problem of spatial-temporal misalignment in existing interpretability methods, where convolutional networks fail to capture global context and Transformers lack localized precision - a limitation that impedes actionable insights in safety-critical domains like healthcare and industrial monitoring. Our method merges gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization, achieving full spatial-temporal alignment while preserving real-time performance. Empirical evaluations on clinical (ECG arrhythmia detection) and industrial (energy consumption prediction) datasets demonstrate significant improvements: the hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy Appliance dataset-outperforming standalone ResNet, Transformer, and InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L (0.650) scores. By formalizing interpretability as causal fidelity and spatial-temporal alignment, our approach bridges the gap between technical outputs and stakeholder understanding, offering a scalable solution for transparent, time-aware decision-making.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linearly Decoding Refused Knowledge in Aligned Language Models</title>
<link>https://arxiv.org/abs/2507.00239</link>
<guid>https://arxiv.org/abs/2507.00239</guid>
<content:encoded><![CDATA[
arXiv:2507.00239v1 Announce Type: cross 
Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely "leftover" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving it both linearly accessible and indirectly influential in downstream behavior.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition</title>
<link>https://arxiv.org/abs/2507.00248</link>
<guid>https://arxiv.org/abs/2507.00248</guid>
<content:encoded><![CDATA[
arXiv:2507.00248v1 Announce Type: cross 
Abstract: We present a novel framework for real-time sign language recognition using lightweight DNNs trained on limited data. Our system addresses key challenges in sign language recognition, including data scarcity, high computational costs, and discrepancies in frame rates between training and inference environments. By encoding sign language specific parameters, such as handshape, palm orientation, movement, and location into vectorized inputs, and leveraging MediaPipe for landmark extraction, we achieve highly separable input data representations. Our DNN architecture, optimized for sub 10MB deployment, enables accurate classification of 343 signs with less than 10ms latency on edge devices. The data annotation platform 'slait data' facilitates structured labeling and vector extraction. Our model achieved 92% accuracy in isolated sign recognition and has been integrated into the 'slait ai' web application, where it demonstrates stable inference.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00257</link>
<guid>https://arxiv.org/abs/2507.00257</guid>
<content:encoded><![CDATA[
arXiv:2507.00257v1 Announce Type: cross 
Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable progress, achieving superhuman performance in a wide range of simulated environments. As research moves toward deploying RL in real-world applications, the field faces a new set of challenges inherent to real-world settings, such as large state-action spaces, non-stationarity, and partial observability. Despite their importance, these challenges are often underexplored in current benchmarks, which tend to focus on idealized, fully observable, and stationary environments, often neglecting to incorporate real-world complexities explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive suite of realistic environments designed to support the development and evaluation of RL algorithms that can operate in real-world scenarios. The suite includes a diverse set of tasks that expose algorithms to a variety of practical challenges. Our experimental results show that, in these settings, standard RL algorithms confirm their competitiveness against rule-based benchmarks, motivating the development of new methods to fully exploit the potential of RL to tackle the complexities of real-world tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Fine-Tuning Methods on Memorization in Large Language Models</title>
<link>https://arxiv.org/abs/2507.00258</link>
<guid>https://arxiv.org/abs/2507.00258</guid>
<content:encoded><![CDATA[
arXiv:2507.00258v1 Announce Type: cross 
Abstract: As the capabilities of pre-trained large language models (LLMs) continue to advance, the "pre-train and fine-tune" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems</title>
<link>https://arxiv.org/abs/2507.00268</link>
<guid>https://arxiv.org/abs/2507.00268</guid>
<content:encoded><![CDATA[
arXiv:2507.00268v1 Announce Type: cross 
Abstract: Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations</title>
<link>https://arxiv.org/abs/2507.00269</link>
<guid>https://arxiv.org/abs/2507.00269</guid>
<content:encoded><![CDATA[
arXiv:2507.00269v1 Announce Type: cross 
Abstract: Current sparse autoencoder (SAE) approaches to neural network interpretability assume that activations can be decomposed through linear superposition into sparse, interpretable features. Despite high reconstruction fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit pathological behavioral errors. We propose that neural networks encode information in two complementary spaces compressed into the same substrate: feature identity and feature integration. To test this dual encoding hypothesis, we develop sequential and joint-training architectures to capture identity and integration patterns simultaneously. Joint training achieves 41.3% reconstruction improvement and 51.6% reduction in KL divergence errors. This architecture spontaneously develops bimodal feature organization: low squared norm features contributing to integration pathways and the rest contributing directly to the residual. Small nonlinear components (3% of parameters) achieve 16.5% standalone improvements, demonstrating parameter-efficient capture of computational relationships crucial for behavior. Additionally, intervention experiments using 2x2 factorial stimulus designs demonstrated that integration features exhibit selective sensitivity to experimental manipulations and produce systematic behavioral effects on model outputs, including significant interaction effects across semantic dimensions. This work provides systematic evidence for (1) dual encoding in neural representations, (2) meaningful nonlinearly encoded feature interactions, and (3) introduces an architectural paradigm shift from post-hoc feature analysis to integrated computational design, establishing foundations for next-generation SAEs.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Q-learning for Value-based Deep Reinforcement Learning, Revisited</title>
<link>https://arxiv.org/abs/2507.00275</link>
<guid>https://arxiv.org/abs/2507.00275</guid>
<content:encoded><![CDATA[
arXiv:2507.00275v1 Announce Type: cross 
Abstract: Overestimation is pervasive in reinforcement learning (RL), including in Q-learning, which forms the algorithmic basis for many value-based deep RL algorithms. Double Q-learning is an algorithm introduced to address Q-learning's overestimation by training two Q-functions and using both to de-correlate action-selection and action-evaluation in bootstrap targets. Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks (DQN), Double Q-learning was adapted to deep RL in the form of Double DQN. However, Double DQN only loosely adapts Double Q-learning, forgoing the training of two different Q-functions that bootstrap off one another. In this paper, we study algorithms that adapt this core idea of Double Q-learning for value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our aim is to understand whether DDQL exhibits less overestimation than Double DQN and whether performant instantiations of DDQL exist. We answer both questions affirmatively, demonstrating that DDQL reduces overestimation and outperforms Double DQN in aggregate across 57 Atari 2600 games, without requiring additional hyperparameters. We also study several aspects of DDQL, including its network architecture, replay ratio, and minibatch sampling strategy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Privacy Management with Generative AI for Blind and Low-Vision People</title>
<link>https://arxiv.org/abs/2507.00286</link>
<guid>https://arxiv.org/abs/2507.00286</guid>
<content:encoded><![CDATA[
arXiv:2507.00286v1 Announce Type: cross 
Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to interpret and manage visual content in their daily lives. While such tools can enhance the accessibility of visual content and so enable greater user independence, they also introduce complex challenges around visual privacy. In this paper, we investigate the current practices and future design preferences of blind and low vision individuals through an interview study with 21 participants. Our findings reveal a range of current practices with GenAI that balance privacy, efficiency, and emotional agency, with users accounting for privacy risks across six key scenarios, such as self-presentation, indoor/outdoor spatial privacy, social sharing, and handling professional content. Our findings reveal design preferences, including on-device processing, zero-retention guarantees, sensitive content redaction, privacy-aware appearance indicators, and multimodal tactile mirrored interaction methods. We conclude with actionable design recommendations to support user-centered visual privacy through GenAI, expanding the notion of privacy and responsible handling of others data.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Multiview Xray Matching</title>
<link>https://arxiv.org/abs/2507.00287</link>
<guid>https://arxiv.org/abs/2507.00287</guid>
<content:encoded><![CDATA[
arXiv:2507.00287v1 Announce Type: cross 
Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing fractures, muscular injuries, and other anomalies. While significant advances have been made in AI-based analysis of single images, current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations. In this work, we present a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views. This is achieved using digitally reconstructed radiographs (DRR), which are automatically derived from unannotated CT volumes. Our approach incorporates a transformer-based training phase to accurately predict correspondences across two or more X-ray views. Furthermore, we demonstrate that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data. Extensive evaluations on both synthetic and real X-ray datasets show that incorporating correspondences improves performance in multi-view fracture classification.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context</title>
<link>https://arxiv.org/abs/2507.00288</link>
<guid>https://arxiv.org/abs/2507.00288</guid>
<content:encoded><![CDATA[
arXiv:2507.00288v1 Announce Type: cross 
Abstract: This study explores how AI-powered digital innovations are reshaping organisational accountability in a transnational governance context. As AI systems increasingly mediate decision-making in domains such as auditing and financial reporting, traditional mechanisms of accountability, based on control, transparency, and auditability, are being destabilised. We integrate the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and institutional theory to examine how organisations adopt AI technologies in response to regulatory, ethical, and cultural pressures that transcend national boundaries. We argue that accountability is co-constructed within global socio-technical networks, shaped not only by user perceptions but also by governance logics and normative expectations. Extending TAM, we incorporate compliance and legitimacy as key factors in perceived usefulness and usability. Drawing on ANT, we reconceptualise accountability as a relational and emergent property of networked assemblages. We propose two organisational strategies including internal governance reconfiguration and external actor-network engagement to foster responsible, legitimate, and globally accepted AI adoption in the accounting domain.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Variability of Multiple Instance Learning Methods for Digital Pathology</title>
<link>https://arxiv.org/abs/2507.00292</link>
<guid>https://arxiv.org/abs/2507.00292</guid>
<content:encoded><![CDATA[
arXiv:2507.00292v1 Announce Type: cross 
Abstract: Digital pathology has revolutionized the field by enabling the digitization of tissue samples into whole slide images (WSIs). However, the high resolution and large size of WSIs present significant challenges when it comes to applying Deep Learning models. As a solution, WSIs are often divided into smaller patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a (too) costly pixel-wise annotation. By treating each slide as a bag of patches, Multiple Instance Learning (MIL) methods have emerged as a suitable solution for WSI classification. A major drawback of MIL methods is their high variability in performance across different runs, which can reach up to 10-15 AUC points on the test set, making it difficult to compare different MIL methods reliably. This variability mainly comes from three factors: i) weight initialization, ii) batch (shuffling) ordering, iii) and learning rate. To address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL methods. We first train multiple models for a few epochs and average the most stable and promising ones based on validation scores. This approach can be applied to any existing MIL model to reduce performance variability. It also simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. We extensively validate our approach on WSI classification tasks using 2 different datasets, 3 initialization strategies and 5 MIL methods, for a total of more than 2000 experiments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural language processing for African languages</title>
<link>https://arxiv.org/abs/2507.00297</link>
<guid>https://arxiv.org/abs/2507.00297</guid>
<content:encoded><![CDATA[
arXiv:2507.00297v1 Announce Type: cross 
Abstract: Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-ended Scientific Discovery via Bayesian Surprise</title>
<link>https://arxiv.org/abs/2507.00310</link>
<guid>https://arxiv.org/abs/2507.00310</guid>
<content:encoded><![CDATA[
arXiv:2507.00310v1 Announce Type: cross 
Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDS -- a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDS in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDS substantially outperforms competitors by producing 5--29\% more discoveries deemed surprising by the LLM. Our human evaluation further finds that two-thirds of AutoDS discoveries are surprising to the domain experts, suggesting this is an important step forward towards building open-ended ASD systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones</title>
<link>https://arxiv.org/abs/2507.00322</link>
<guid>https://arxiv.org/abs/2507.00322</guid>
<content:encoded><![CDATA[
arXiv:2507.00322v1 Announce Type: cross 
Abstract: Despite remarkable advances in coding capabilities, language models (LMs) still struggle with simple syntactic tasks such as generating balanced parentheses. In this study, we investigate the underlying mechanisms behind the persistence of these errors across LMs of varying sizes (124M-7B) to both understand and mitigate the errors. Our study reveals that LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. While some components reliably promote correct answers across a generalized range of inputs (i.e., implementing "sound mechanisms''), others are less reliable and introduce noise by promoting incorrect tokens (i.e., implementing "faulty mechanisms''). Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions. Motivated by this insight, we introduce RASteer, a steering method to systematically identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$% without impairing the models' general coding ability. We further demonstrate its broader applicability in arithmetic reasoning tasks, achieving performance gains of up to around $20$%.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video</title>
<link>https://arxiv.org/abs/2507.00339</link>
<guid>https://arxiv.org/abs/2507.00339</guid>
<content:encoded><![CDATA[
arXiv:2507.00339v1 Announce Type: cross 
Abstract: Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTS-Guided AI Interaction Workflow for Business Insights</title>
<link>https://arxiv.org/abs/2507.00347</link>
<guid>https://arxiv.org/abs/2507.00347</guid>
<content:encoded><![CDATA[
arXiv:2507.00347v1 Announce Type: cross 
Abstract: Modern firms face a flood of dense, unstructured reports. Turning these documents into usable insights takes heavy effort and is far from agile when quick answers are needed. VTS-AI tackles this gap. It integrates Visual Thinking Strategies, which emphasize evidence-based observation, linking, and thinking, into AI agents, so the agents can extract business insights from unstructured text, tables, and images at scale. The system works in three tiers (micro, meso, macro). It tags issues, links them to source pages, and rolls them into clear action levers stored in a searchable YAML file. In tests on an 18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt yet produced richer findings: page locations, verbatim excerpts, severity scores, and causal links. Analysts can accept or adjust these outputs in the same IDE, keeping human judgment in the loop. Early results show VTS-AI spots the direction of key metrics and flags where deeper number-crunching is needed. Next steps include mapping narrative tags to financial ratios, adding finance-tuned language models through a Model-Context Protocol, and building a Risk & Safety Layer to stress-test models and secure data. These upgrades aim to make VTS-AI a production-ready, audit-friendly tool for rapid business analysis.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AST-guided LLM Approach for SVRF Code Synthesis</title>
<link>https://arxiv.org/abs/2507.00352</link>
<guid>https://arxiv.org/abs/2507.00352</guid>
<content:encoded><![CDATA[
arXiv:2507.00352v1 Announce Type: cross 
Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and Optical Proximity Correction (OPC) and it faces challenges as advancing nodes create complex design rules that renders traditional SVRF development ineffective and highlight an expertise gap. This paper introduces a novel methodology integrating Abstract Syntax Tree (AST) embedding and Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring semantic accuracy and error minimization through structural validation with domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific scoring framework that complements standard metrics like BLEU and ROUGE-L. In our approach, AST provides rigorous structural validation, while RAG infuses relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our methodology demonstrates up to a 40\% improvement in code generation accuracy compared to basic text-based fine-tuning process. This fusion of industry expertise with advanced coding strategies not only optimizes SVRF development under limited dataset constraints but also creates a more intuitive and efficient coding environment. Consequently, users can rapidly iterate through design cycles, reduce manual error correction, and significantly improve overall productivity.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation</title>
<link>https://arxiv.org/abs/2507.00356</link>
<guid>https://arxiv.org/abs/2507.00356</guid>
<content:encoded><![CDATA[
arXiv:2507.00356v1 Announce Type: cross 
Abstract: Deep learning methods have significantly advanced the development of intelligent rinterpretation in remote sensing (RS), with foundational model research based on large-scale pre-training paradigms rapidly reshaping various domains of Earth Observation (EO). However, compared to the open accessibility and high spatiotemporal coverage of medium-resolution data, the limited acquisition channels for ultra-high-resolution optical RS imagery have constrained the progress of high-resolution remote sensing vision foundation models (RSVFM). As the world's largest sub-meter-level commercial RS satellite constellation, the Jilin-1 constellation possesses abundant sub-meter-level image resources. This study proposes CGEarthEye, a RSVFM framework specifically designed for Jilin-1 satellite characteristics, comprising five backbones with different parameter scales with totaling 2.1 billion parameters. To enhance the representational capacity of the foundation model, we developed JLSSD, the first 15-million-scale multi-temporal self-supervised learning (SSL) dataset featuring global coverage with quarterly temporal sampling within a single year, constructed through multi-level representation clustering and sampling strategies. The framework integrates seasonal contrast, augmentation-based contrast, and masked patch token contrastive strategies for pre-training. Comprehensive evaluations across 10 benchmark datasets covering four typical RS tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art (SOTA) performance. Further analysis reveals CGEarthEye's superior characteristics in feature visualization, model convergence, parameter efficiency, and practical mapping applications. This study anticipates that the exceptional representation capabilities of CGEarthEye will facilitate broader and more efficient applications of Jilin-1 data in traditional EO application.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2507.00358</link>
<guid>https://arxiv.org/abs/2507.00358</guid>
<content:encoded><![CDATA[
arXiv:2507.00358v1 Announce Type: cross 
Abstract: We study reinforcement learning (RL) for the same class of continuous-time stochastic linear--quadratic (LQ) control problems as in \cite{huang2024sublinear}, where volatilities depend on both states and controls while states are scalar-valued and running control rewards are absent. We propose a model-free, data-driven exploration mechanism that adaptively adjusts entropy regularization by the critic and policy variance by the actor. Unlike the constant or deterministic exploration schedules employed in \cite{huang2024sublinear}, which require extensive tuning for implementations and ignore learning progresses during iterations, our adaptive exploratory approach boosts learning efficiency with minimal tuning. Despite its flexibility, our method achieves a sublinear regret bound that matches the best-known model-free results for this class of LQ problems, which were previously derived only with fixed exploration schedules. Numerical experiments demonstrate that adaptive explorations accelerate convergence and improve regret performance compared to the non-adaptive model-free and model-based counterparts.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing</title>
<link>https://arxiv.org/abs/2507.00378</link>
<guid>https://arxiv.org/abs/2507.00378</guid>
<content:encoded><![CDATA[
arXiv:2507.00378v1 Announce Type: cross 
Abstract: Conformance testing is essential for ensuring that protocol implementations comply with their specifications. However, traditional testing approaches involve manually creating numerous test cases and scripts, making the process labor-intensive and inefficient. Recently, Large Language Models (LLMs) have demonstrated impressive text comprehension and code generation abilities, providing promising opportunities for automation. In this paper, we propose iPanda, the first end-to-end framework that leverages LLMs to automate protocol conformance testing. Given a protocol specification document and its implementation, iPanda first employs a keyword-based method to automatically generate comprehensive test cases. Then, it utilizes a code-based retrieval-augmented generation approach to effectively interpret the implementation and produce executable test code. To further enhance code quality, iPanda incorporates an iterative self-correction mechanism to refine generated test scripts interactively. Finally, by executing and analyzing the generated tests, iPanda systematically verifies compliance between implementations and protocol specifications. Comprehensive experiments on various protocols show that iPanda significantly outperforms pure LLM-based approaches, improving the success rate (Pass@1) of test-code generation by factors ranging from 4.675 times to 10.751 times.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials</title>
<link>https://arxiv.org/abs/2507.00407</link>
<guid>https://arxiv.org/abs/2507.00407</guid>
<content:encoded><![CDATA[
arXiv:2507.00407v1 Announce Type: cross 
Abstract: Accurate molecular property predictions require 3D geometries, which are typically obtained using expensive methods such as density functional theory (DFT). Here, we attempt to obtain molecular geometries by relying solely on machine learning interatomic potential (MLIP) models. To this end, we first curate a large-scale molecular relaxation dataset comprising 3.5 million molecules and 300 million snapshots. Then MLIP foundation models are trained with supervised learning to predict energy and forces given 3D molecular structures. Once trained, we show that the foundation models can be used in different ways to obtain geometries either explicitly or implicitly. First, it can be used to obtain low-energy 3D geometries via geometry optimization, providing relaxed 3D geometries for downstream molecular property predictions. To mitigate potential biases and enhance downstream predictions, we introduce geometry fine-tuning based on the relaxed 3D geometries. Second, the foundation models can be directly fine-tuned for property prediction when ground truth 3D geometries are available. Our results demonstrate that MLIP foundation models trained on relaxation data can provide valuable molecular geometries that benefit property predictions.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs</title>
<link>https://arxiv.org/abs/2507.00418</link>
<guid>https://arxiv.org/abs/2507.00418</guid>
<content:encoded><![CDATA[
arXiv:2507.00418v1 Announce Type: cross 
Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt) and performance against leading NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform (NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90 billion parameters, are served using the vLLM framework. The QAic inference cards appears to be energy efficient and performs well in the energy efficiency metric in most cases. The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications within the National Research Platform (NRP).
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding</title>
<link>https://arxiv.org/abs/2507.00419</link>
<guid>https://arxiv.org/abs/2507.00419</guid>
<content:encoded><![CDATA[
arXiv:2507.00419v1 Announce Type: cross 
Abstract: Understanding Earth's subsurface is critical for energy transition, natural hazard mitigation, and planetary science. Yet subsurface analysis remains fragmented, with separate models required for structural interpretation, stratigraphic analysis, geobody segmentation, and property modeling-each tightly coupled to specific data distributions and task formulations. We introduce the Geological Everything Model 3D (GEM), a unified generative architecture that reformulates all these tasks as prompt-conditioned inference along latent structural frameworks derived from subsurface imaging. This formulation moves beyond task-specific models by enabling a shared inference mechanism, where GEM propagates human-provided prompts-such as well logs, masks, or structural sketches-along inferred structural frameworks to produce geologically coherent outputs. Through this mechanism, GEM achieves zero-shot generalization across tasks with heterogeneous prompt types, without retraining for new tasks or data sources. This capability emerges from a two-stage training process that combines self-supervised representation learning on large-scale field seismic data with adversarial fine-tuning using mixed prompts and labels across diverse subsurface tasks. GEM demonstrates broad applicability across surveys and tasks, including Martian radar stratigraphy analysis, structural interpretation in subduction zones, full seismic stratigraphic interpretation, geobody delineation, and property modeling. By bridging expert knowledge with generative reasoning in a structurally aware manner, GEM lays the foundation for scalable, human-in-the-loop geophysical AI-transitioning from fragmented pipelines to a vertically integrated, promptable reasoning system. Project page: https://douyimin.github.io/GEM
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation</title>
<link>https://arxiv.org/abs/2507.00435</link>
<guid>https://arxiv.org/abs/2507.00435</guid>
<content:encoded><![CDATA[
arXiv:2507.00435v1 Announce Type: cross 
Abstract: We present RoboEval, a simulation benchmark and structured evaluation framework designed to reveal the limitations of current bimanual manipulation policies. While prior benchmarks report only binary task success, we show that such metrics often conceal critical weaknesses in policy behavior -- such as poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval introduces a suite of tiered, semantically grounded tasks decomposed into skill-specific stages, with variations that systematically challenge spatial, physical, and coordination capabilities. Tasks are paired with fine-grained diagnostic metrics and 3000+ human demonstrations to support imitation learning. Our experiments reveal that policies with similar success rates diverge in how tasks are executed -- some struggle with alignment, others with temporally consistent bimanual control. We find that behavioral metrics correlate with success in over half of task-metric pairs, and remain informative even when binary success saturates. By pinpointing when and how policies fail, RoboEval enables a deeper, more actionable understanding of robotic manipulation -- and highlights the need for evaluation tools that go beyond success alone.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Recipe for Causal Graph Regression: Confounding Effects Revisited</title>
<link>https://arxiv.org/abs/2507.00440</link>
<guid>https://arxiv.org/abs/2507.00440</guid>
<content:encoded><![CDATA[
arXiv:2507.00440v1 Announce Type: cross 
Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems</title>
<link>https://arxiv.org/abs/2507.00443</link>
<guid>https://arxiv.org/abs/2507.00443</guid>
<content:encoded><![CDATA[
arXiv:2507.00443v1 Announce Type: cross 
Abstract: Recent advances in multi-agent systems manipulation have demonstrated a rising demand for the implementation of multi-UAV systems in urban areas, which are always subjected to the presence of static and dynamic obstacles. Inspired by the collective behavior of tilapia fish and pigeons, the focus of the presented research is on the introduction of a nature-inspired collision-free formation control for a multi-UAV system, considering the obstacle avoidance maneuvers. The developed framework in this study utilizes a semi-distributed control approach, in which, based on a probabilistic Lloyd's algorithm, a centralized guidance algorithm works for optimal positioning of the UAVs, while a distributed control approach has been used for the intervehicle collision and obstacle avoidance. Further, the presented framework has been extended to the 3D space with a novel definition of 3D maneuvers. Finally, the presented framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the obtained results demonstrated the validity of the presented method in dynamic environments with stationary and moving obstacles.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design</title>
<link>https://arxiv.org/abs/2507.00445</link>
<guid>https://arxiv.org/abs/2507.00445</guid>
<content:encoded><![CDATA[
arXiv:2507.00445v1 Announce Type: cross 
Abstract: We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Agent Identification for General Game Playing</title>
<link>https://arxiv.org/abs/2507.00451</link>
<guid>https://arxiv.org/abs/2507.00451</guid>
<content:encoded><![CDATA[
arXiv:2507.00451v1 Announce Type: cross 
Abstract: We present an efficient and generalised procedure to accurately identify the best performing algorithm for each sub-task in a multi-problem domain. Our approach treats this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific task and each arm corresponds to a specific algorithm or agent. We propose an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction. We evaluate the performance of Optimistic-WS on two of the most popular general game domains, the General Video Game AI (GVGAI) framework and the Ludii general game playing system, with the goal of identifying the highest performing agent for each game within a limited number of trials. Compared to previous best arm identification algorithms for multi-armed bandits, our results demonstrate a substantial performance improvement in terms of average simple regret. This novel approach can be used to significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks, as well as other multi-task domains with high algorithm runtimes.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales</title>
<link>https://arxiv.org/abs/2507.00454</link>
<guid>https://arxiv.org/abs/2507.00454</guid>
<content:encoded><![CDATA[
arXiv:2507.00454v1 Announce Type: cross 
Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment between visual inputs and language descriptions caused by target movement. Previous trackers have explored many effective feature modification methods to preserve more aligned features. However, an important yet unexplored factor ultimately hinders their capability, which is the inherent differences in the temporal and spatial scale of information between visual and language inputs. To address this issue, we propose a novel visual-language tracker that enhances the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and \textbf{S}patial scale of different input components, named as \textbf{ATSTrack}. Specifically, we decompose each language description into phrases with different attributes based on their temporal and spatial correspondence with visual inputs, and modify their features in a fine-grained manner. Moreover, we introduce a Visual-Language token that comprises modified linguistic information from the previous frame to guide the model to extract visual features that are more relevant to language description, thereby reducing the impact caused by the differences in spatial scale. Experimental results show that our proposed ATSTrack achieves performance comparable to existing methods. Our code will be released.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process-aware and high-fidelity microstructure generation using stable diffusion</title>
<link>https://arxiv.org/abs/2507.00459</link>
<guid>https://arxiv.org/abs/2507.00459</guid>
<content:encoded><![CDATA[
arXiv:2507.00459v1 Announce Type: cross 
Abstract: Synthesizing realistic microstructure images conditioned on processing parameters is crucial for understanding process-structure relationships in materials design. However, this task remains challenging due to limited training micrographs and the continuous nature of processing variables. To overcome these challenges, we present a novel process-aware generative modeling approach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art text-to-image diffusion model adapted for microstructure generation. Our method introduces numeric-aware embeddings that encode continuous variables (annealing temperature, time, and magnification) directly into the model's conditioning, enabling controlled image generation under specified process conditions and capturing process-driven microstructural variations. To address data scarcity and computational constraints, we fine-tune only a small fraction of the model's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently transferring the pre-trained model to the materials domain. We validate realism using a semantic segmentation model based on a fine-tuned U-Net with a VGG16 encoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean IoU, outperforming previous methods. Quantitative analyses using physical descriptors and spatial statistics show strong agreement between synthetic and real microstructures. Specifically, two-point correlation and lineal-path errors remain below 2.1% and 0.6%, respectively. Our method represents the first adaptation of SD3.5-Large for process-aware microstructure generation, offering a scalable approach for data-driven materials design.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization</title>
<link>https://arxiv.org/abs/2507.00461</link>
<guid>https://arxiv.org/abs/2507.00461</guid>
<content:encoded><![CDATA[
arXiv:2507.00461v1 Announce Type: cross 
Abstract: This research paper introduces two novel complex-valued Hopfield neural networks (CvHNNs) that incorporate phase and magnitude quantization. The first CvHNN employs a ceiling-type activation function that operates on the rectangular coordinate representation of the complex net contribution. The second CvHNN similarly incorporates phase and magnitude quantization but utilizes a ceiling-type activation function based on the polar coordinate representation of the complex net contribution. The proposed CvHNNs, with their phase and magnitude quantization, significantly increase the number of states compared to existing models in the literature, thereby expanding the range of potential applications for CvHNNs.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity Conscious Refined Random Forest</title>
<link>https://arxiv.org/abs/2507.00467</link>
<guid>https://arxiv.org/abs/2507.00467</guid>
<content:encoded><![CDATA[
arXiv:2507.00467v1 Announce Type: cross 
Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its robust classification performance across diverse domains. However, it often relies on hundreds of trees and all input features, leading to high inference cost and model redundancy. In this work, our goal is to grow trees dynamically only on informative features and then enforce maximal diversity by clustering and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest Classifier that iteratively refines itself by first removing the least informative features and then analytically determines how many new trees should be grown, followed by correlation-based clustering to remove redundant trees. The classification accuracy of our model was compared against the standard RF on the same number of trees. Experiments on 8 multiple benchmark datasets, including binary and multiclass datasets, demonstrate that the proposed model achieves improved accuracy compared to standard RF.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Aware Style Transfer for Adaptive Holographic Reconstruction</title>
<link>https://arxiv.org/abs/2507.00482</link>
<guid>https://arxiv.org/abs/2507.00482</guid>
<content:encoded><![CDATA[
arXiv:2507.00482v1 Announce Type: cross 
Abstract: Inline holographic imaging presents an ill-posed inverse problem of reconstructing objects' complex amplitude from recorded diffraction patterns. Although recent deep learning approaches have shown promise over classical phase retrieval algorithms, they often require high-quality ground truth datasets of complex amplitude maps to achieve a statistical inverse mapping operation between the two domains. Here, we present a physics-aware style transfer approach that interprets the object-to-sensor distance as an implicit style within diffraction patterns. Using the style domain as the intermediate domain to construct cyclic image translation, we show that the inverse mapping operation can be learned in an adaptive manner only with datasets composed of intensity measurements. We further demonstrate its biomedical applicability by reconstructing the morphology of dynamically flowing red blood cells, highlighting its potential for real-time, label-free imaging. As a framework that leverages physical cues inherently embedded in measurements, the presented method offers a practical learning strategy for imaging applications where ground truth is difficult or impossible to obtain.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00485</link>
<guid>https://arxiv.org/abs/2507.00485</guid>
<content:encoded><![CDATA[
arXiv:2507.00485v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at https://github.com/azure-123/PNAct.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms</title>
<link>https://arxiv.org/abs/2507.00491</link>
<guid>https://arxiv.org/abs/2507.00491</guid>
<content:encoded><![CDATA[
arXiv:2507.00491v1 Announce Type: cross 
Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems. cAI systems are typically composed of deep neural networks (DNNs), transformers, and large language models (LLMs), exhibiting a high degree of computational diversity and dynamic workload variation. Deploying cAI services on mobile edge platforms poses a significant challenge in scheduling concurrent DNN-transformer inference tasks, which arrive dynamically in an unknown sequence. Existing mobile edge AI inference strategies manage multi-DNN or transformer-only workloads, relying on design-time profiling, and cannot handle concurrent inference of DNNs and transformers required by cAI systems. In this work, we address the challenge of scheduling cAI systems on heterogeneous mobile edge platforms. We present Twill, a run-time framework to handle concurrent inference requests of cAI workloads through task affinity-aware cluster mapping and migration, priority-aware task freezing/unfreezing, and DVFS, while minimizing inference latency within power budgets. We implement and deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate Twill against state-of-the-art edge AI inference techniques over contemporary DNNs and LLMs, reducing inference latency by 54% on average, while honoring power budgets.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models</title>
<link>https://arxiv.org/abs/2507.00493</link>
<guid>https://arxiv.org/abs/2507.00493</guid>
<content:encoded><![CDATA[
arXiv:2507.00493v1 Announce Type: cross 
Abstract: Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeamCMU at Touch\'e: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search</title>
<link>https://arxiv.org/abs/2507.00509</link>
<guid>https://arxiv.org/abs/2507.00509</guid>
<content:encoded><![CDATA[
arXiv:2507.00509v1 Announce Type: cross 
Abstract: As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center</title>
<link>https://arxiv.org/abs/2507.00513</link>
<guid>https://arxiv.org/abs/2507.00513</guid>
<content:encoded><![CDATA[
arXiv:2507.00513v1 Announce Type: cross 
Abstract: The integration of various AI tools creates a complex socio-technical environment where employee-customer interactions form the core of work practices. This study investigates how customer service representatives (CSRs) at the power grid service customer service call center perceive AI assistance in their interactions with customers. Through a field visit and semi-structured interviews with 13 CSRs, we found that AI can alleviate some traditional burdens during the call (e.g., typing and memorizing) but also introduces new burdens (e.g., earning, compliance, psychological burdens). This research contributes to a more nuanced understanding of AI integration in organizational settings and highlights the efforts and burdens undertaken by CSRs to adapt to the updated system.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.00525</link>
<guid>https://arxiv.org/abs/2507.00525</guid>
<content:encoded><![CDATA[
arXiv:2507.00525v1 Announce Type: cross 
Abstract: Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries. We introduce Box-QAymo, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes. Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames. To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity. Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions. Project page and dataset are available at https://djamahl99.github.io/qaymo-pages/.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support</title>
<link>https://arxiv.org/abs/2507.00535</link>
<guid>https://arxiv.org/abs/2507.00535</guid>
<content:encoded><![CDATA[
arXiv:2507.00535v1 Announce Type: cross 
Abstract: More than twenty-five years ago, first ideas were developed on how to design a system that can provide recommendations to groups of users instead of individual users. Since then, a rich variety of algorithmic proposals were published, e.g., on how to acquire individual preferences, how to aggregate them, and how to generate recommendations for groups of users. However, despite the rich literature on the topic, barely any examples of real-world group recommender systems can be found. This lets us question common assumptions in academic research, in particular regarding communication processes in a group and how recommendation-supported decisions are made. In this essay, we argue that these common assumptions and corresponding system designs often may not match the needs or expectations of users. We thus call for a reorientation in this research area, leveraging the capabilities of modern Generative AI assistants like ChatGPT. Specifically, as one promising future direction, we envision group recommender systems to be systems where human group members interact in a chat and an AI-based group recommendation agent assists the decision-making process in an agentic way. Ultimately, this shall lead to a more natural group decision-making environment and finally to wider adoption of group recommendation systems in practice.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation</title>
<link>https://arxiv.org/abs/2507.00537</link>
<guid>https://arxiv.org/abs/2507.00537</guid>
<content:encoded><![CDATA[
arXiv:2507.00537v1 Announce Type: cross 
Abstract: This paper studies the role of attention heads in CLIP's image encoder. While CLIP has exhibited robust performance across diverse applications, we hypothesize that certain attention heads negatively affect final representations and that ablating them can improve performance in downstream tasks. To capitalize on this insight, we propose a simple yet effective method, called Attention Ablation Technique (AAT), to suppress the contribution of specific heads by manipulating attention weights. By integrating two alternative strategies tailored for different application scenarios, AAT systematically identifies and ablates detrimental attention heads to enhance representation quality. Experiments demonstrate that AAT consistently improves downstream task performance across various domains, boosting recall rate by up to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight the potential of AAT to effectively refine large-scale vision-language models with virtually no increase in inference cost.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Design in Nanophotonics via Representation Learning</title>
<link>https://arxiv.org/abs/2507.00546</link>
<guid>https://arxiv.org/abs/2507.00546</guid>
<content:encoded><![CDATA[
arXiv:2507.00546v1 Announce Type: cross 
Abstract: Inverse design in nanophotonics, the computational discovery of structures achieving targeted electromagnetic (EM) responses, has become a key tool for recent optical advances. Traditional intuition-driven or iterative optimization methods struggle with the inherently high-dimensional, non-convex design spaces and the substantial computational demands of EM simulations. Recently, machine learning (ML) has emerged to address these bottlenecks effectively. This review frames ML-enhanced inverse design methodologies through the lens of representation learning, classifying them into two categories: output-side and input-side approaches. Output-side methods use ML to learn a representation in the solution space to create a differentiable solver that accelerates optimization. Conversely, input-side techniques employ ML to learn compact, latent-space representations of feasible device geometries, enabling efficient global exploration through generative models. Each strategy presents unique trade-offs in data requirements, generalization capacity, and novel design discovery potentials. Hybrid frameworks that combine physics-based optimization with data-driven representations help escape poor local optima, improve scalability, and facilitate knowledge transfer. We conclude by highlighting open challenges and opportunities, emphasizing complexity management, geometry-independent representations, integration of fabrication constraints, and advancements in multiphysics co-designs.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadViM: Backdoor Attack against Vision Mamba</title>
<link>https://arxiv.org/abs/2507.00577</link>
<guid>https://arxiv.org/abs/2507.00577</guid>
<content:encoded><![CDATA[
arXiv:2507.00577v1 Announce Type: cross 
Abstract: Vision State Space Models (SSMs), particularly architectures like Vision Mamba (ViM), have emerged as promising alternatives to Vision Transformers (ViTs). However, the security implications of this novel architecture, especially their vulnerability to backdoor attacks, remain critically underexplored. Backdoor attacks aim to embed hidden triggers into victim models, causing the model to misclassify inputs containing these triggers while maintaining normal behavior on clean inputs. This paper investigates the susceptibility of ViM to backdoor attacks by introducing BadViM, a novel backdoor attack framework specifically designed for Vision Mamba. The proposed BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency sensitivity patterns of the victim model to create stealthy, distributed triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss that strategically manipulates the internal representations of model by aligning the hidden states of backdoor images with those of target classes. Extensive experimental results demonstrate that BadViM achieves superior attack success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits remarkable resilience against common defensive measures, including PatchDrop, PatchShuffle and JPEG compression, which typically neutralize normal backdoor attacks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification</title>
<link>https://arxiv.org/abs/2507.00579</link>
<guid>https://arxiv.org/abs/2507.00579</guid>
<content:encoded><![CDATA[
arXiv:2507.00579v1 Announce Type: cross 
Abstract: Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3 - Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. We propose a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns. Our system achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task. This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Video Detection via Perceptual Straightening</title>
<link>https://arxiv.org/abs/2507.00583</link>
<guid>https://arxiv.org/abs/2507.00583</guid>
<content:encoded><![CDATA[
arXiv:2507.00583v1 Announce Type: cross 
Abstract: The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the "perceptual straightening" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Circuit Structure Optimization for Quantum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00589</link>
<guid>https://arxiv.org/abs/2507.00589</guid>
<content:encoded><![CDATA[
arXiv:2507.00589v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through environmental interaction. However, RL suffers from reduced learning efficiency due to the curse of dimensionality in high-dimensional spaces. Quantum reinforcement learning (QRL) addresses this issue by leveraging superposition and entanglement in quantum computing, allowing efficient handling of high-dimensional problems with fewer resources. QRL combines quantum neural networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as the core computational module. The PQC performs linear and nonlinear transformations through gate operations, similar to hidden layers in classical neural networks. Previous QRL studies, however, have used fixed PQC structures based on empirical intuition without verifying their optimality. This paper proposes a QRL-NAS algorithm that integrates quantum neural architecture search (QNAS) to optimize PQC structures within QRL. Experiments demonstrate that QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its effectiveness and practical utility.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-resolution spatial memory requires grid-cell-like neural codes</title>
<link>https://arxiv.org/abs/2507.00598</link>
<guid>https://arxiv.org/abs/2507.00598</guid>
<content:encoded><![CDATA[
arXiv:2507.00598v1 Announce Type: cross 
Abstract: Continuous attractor networks (CANs) are widely used to model how the brain temporarily retains continuous behavioural variables via persistent recurrent activity, such as an animal's position in an environment. However, this memory mechanism is very sensitive to even small imperfections, such as noise or heterogeneity, which are both common in biological systems. Previous work has shown that discretising the continuum into a finite set of discrete attractor states provides robustness to these imperfections, but necessarily reduces the resolution of the represented variable, creating a dilemma between stability and resolution. We show that this stability-resolution dilemma is most severe for CANs using unimodal bump-like codes, as in traditional models. To overcome this, we investigate sparse binary distributed codes based on random feature embeddings, in which neurons have spatially-periodic receptive fields. We demonstrate theoretically and with simulations that such grid-cell-like codes enable CANs to achieve both high stability and high resolution simultaneously. The model extends to embedding arbitrary nonlinear manifolds into a CAN, such as spheres or tori, and generalises linear path integration to integration along freely-programmable on-manifold vector fields. Together, this work provides a theory of how the brain could robustly represent continuous variables with high resolution and perform flexible computations over task-relevant manifolds.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies</title>
<link>https://arxiv.org/abs/2507.00606</link>
<guid>https://arxiv.org/abs/2507.00606</guid>
<content:encoded><![CDATA[
arXiv:2507.00606v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Reward Models for Preference-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.00611</link>
<guid>https://arxiv.org/abs/2507.00611</guid>
<content:encoded><![CDATA[
arXiv:2507.00611v1 Announce Type: cross 
Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn high-performance policies in environments where the reward signal is hard to specify, avoiding heuristic and time-consuming reward design. However, PbRL can suffer from slow convergence speed since it requires training in a reward model. Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization. In this paper, we propose a method to effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM assumes that the true reward of the environment can be split into a sum of two parts: a prior reward and a learned reward. The prior reward is a term available before training, for example, a user's ``best guess'' reward function, or a reward function learned from inverse reinforcement learning (IRL), and the learned reward is trained with preferences. We introduce state-based and image-based versions of RRM and evaluate them on several tasks in the Meta-World environment suite. Experimental results show that our method substantially improves the performance of a common PbRL method. Our method achieves performance improvements for a variety of different types of prior rewards, including proxy rewards, a reward obtained from IRL, and even a negated version of the proxy reward. We also conduct experiments with a Franka Panda to show that our method leads to superior performance on a real robot. It significantly accelerates policy learning for different tasks, achieving success in fewer steps than the baseline. The videos are presented at https://sunlighted.github.io/RRM-web/.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping</title>
<link>https://arxiv.org/abs/2507.00613</link>
<guid>https://arxiv.org/abs/2507.00613</guid>
<content:encoded><![CDATA[
arXiv:2507.00613v1 Announce Type: cross 
Abstract: Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac parametric mapping for characterizing myocardial tissue and diagnosing cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI) acquires 11 breath-hold baseline images with interleaved rest periods to ensure mapping accuracy. However, prolonged scanning can be challenging for patients with poor breathholds, often leading to motion artifacts that degrade image quality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a signal recovery model involving an iterative estimation process. Recent studies have proposed deep-learning approaches for rapid $T_1$ mapping using shortened sequences to reduce acquisition time for patient comfort. Nevertheless, existing methods overlook important physics constraints, limiting interpretability and generalization. In this work, we present an accelerated, end-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary Differential Equations (ODEs) to model temporal dynamics and address these challenges. Our method achieves high-accuracy $T_1$ estimation from a sparse subset of baseline images and ensures efficient null index estimation at test time. Specifically, we develop a continuous-time LSTM-ODE model to enable selective Look-Locker (LL) data acquisition with arbitrary time lags. Experimental results show superior performance in $T_1$ estimation for both native and post-contrast sequences and demonstrate the strong benefit of our physics-based formulation over direct data-driven $T_1$ priors.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Horus: A Protocol for Trustless Delegation Under Uncertainty</title>
<link>https://arxiv.org/abs/2507.00631</link>
<guid>https://arxiv.org/abs/2507.00631</guid>
<content:encoded><![CDATA[
arXiv:2507.00631v1 Announce Type: cross 
Abstract: Correctness is an emergent property of systems where exposing error is cheaper than committing it. In dynamic, low-trust environments, autonomous AI agents benefit from delegating work to sub-agents, yet correctness cannot be assured through upfront specification or centralized oversight. We propose a protocol that enforces correctness through collateralized claims in a recursive verification game. Tasks are published as intents, and solvers compete to fulfill them. Selected solvers carry out tasks under risk, with correctness checked post hoc by verifiers. Any challenger can challenge a result by staking against it to trigger the verification process. Incorrect agents are slashed and correct opposition is rewarded, with an escalation path that penalizes erroneous verifiers themselves. When incentives are aligned across solvers, challengers, and verifiers, falsification conditions make correctness the Nash equilibrium.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models</title>
<link>https://arxiv.org/abs/2507.00653</link>
<guid>https://arxiv.org/abs/2507.00653</guid>
<content:encoded><![CDATA[
arXiv:2507.00653v1 Announce Type: cross 
Abstract: The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity</title>
<link>https://arxiv.org/abs/2507.00657</link>
<guid>https://arxiv.org/abs/2507.00657</guid>
<content:encoded><![CDATA[
arXiv:2507.00657v1 Announce Type: cross 
Abstract: We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound</title>
<link>https://arxiv.org/abs/2507.00660</link>
<guid>https://arxiv.org/abs/2507.00660</guid>
<content:encoded><![CDATA[
arXiv:2507.00660v1 Announce Type: cross 
Abstract: Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per- and inter-phase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75mm). Both the code and the dataset are available at https://github.com/crs524/MTCNet.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2507.00665</link>
<guid>https://arxiv.org/abs/2507.00665</guid>
<content:encoded><![CDATA[
arXiv:2507.00665v1 Announce Type: cross 
Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2507.00669</link>
<guid>https://arxiv.org/abs/2507.00669</guid>
<content:encoded><![CDATA[
arXiv:2507.00669v1 Announce Type: cross 
Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an Audio-Guided Attention module that captures interactions between candidate objects and relational speech cues, improving target discrimination in cluttered scenes. To support benchmarking, we synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods-highlighting the promise of integrating spoken language into 3D vision tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.00709</link>
<guid>https://arxiv.org/abs/2507.00709</guid>
<content:encoded><![CDATA[
arXiv:2507.00709v1 Announce Type: cross 
Abstract: Lane segment topology reasoning constructs a comprehensive road network by capturing the topological relationships between lane segments and their semantic types. This enables end-to-end autonomous driving systems to perform road-dependent maneuvers such as turning and lane changing. However, the limitations in consistent positional embedding and temporal multiple attribute learning in existing methods hinder accurate roadnet reconstruction. To address these issues, we propose TopoStreamer, an end-to-end temporal perception model for lane segment topology reasoning. Specifically, TopoStreamer introduces three key improvements: streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising. The streaming attribute constraints enforce temporal consistency in both centerline and boundary coordinates, along with their classifications. Meanwhile, dynamic lane boundary positional encoding enhances the learning of up-to-date positional information within queries, while lane segment denoising helps capture diverse lane segment patterns, ultimately improving model performance. Additionally, we assess the accuracy of existing models using a lane boundary classification metric, which serves as a crucial measure for lane-changing scenarios in autonomous driving. On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements over state-of-the-art methods, achieving substantial performance gains of +3.4% mAP in lane segment perception and +2.1% OLS in centerline perception tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title>
<link>https://arxiv.org/abs/2507.00724</link>
<guid>https://arxiv.org/abs/2507.00724</guid>
<content:encoded><![CDATA[
arXiv:2507.00724v1 Announce Type: cross 
Abstract: Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End</title>
<link>https://arxiv.org/abs/2507.00755</link>
<guid>https://arxiv.org/abs/2507.00755</guid>
<content:encoded><![CDATA[
arXiv:2507.00755v1 Announce Type: cross 
Abstract: This paper presents a circuit-algorithm co-design framework for learnable analog front-end (AFE) in audio signal classification. Designing AFE and backend classifiers separately is a common practice but non-ideal, as shown in this paper. Instead, this paper proposes a joint optimization of the backend classifier with the AFE's transfer function to achieve system-level optimum. More specifically, the transfer function parameters of an analog bandpass filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training loop for the classifier. Using a co-design loss function LBPF, this work shows superior optimization of both the filter bank and the classifier. Implemented in open-source SKY130 130nm CMOS process, the optimized design achieved 90.5%-94.2% accuracy for 10-keyword classification task across a wide range of input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters. Compared to conventional approach, the proposed audio AFE achieves 8.7% and 12.9% reduction in power and capacitor area respectively.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing</title>
<link>https://arxiv.org/abs/2507.00769</link>
<guid>https://arxiv.org/abs/2507.00769</guid>
<content:encoded><![CDATA[
arXiv:2507.00769v1 Announce Type: cross 
Abstract: Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability</title>
<link>https://arxiv.org/abs/2507.00788</link>
<guid>https://arxiv.org/abs/2507.00788</guid>
<content:encoded><![CDATA[
arXiv:2507.00788v1 Announce Type: cross 
Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assistants affects software maintainability, specifically how easily other developers can evolve the resulting source code. [Method] We conducted a two-phase controlled experiment involving 151 participants, 95% of whom were professional developers. In Phase 1, participants added a new feature to a Java web application, with or without AI assistance. In Phase 2, a randomized controlled trial, new participants evolved these solutions without AI assistance. [Results] AI-assisted development in Phase 1 led to a modest speedup in subsequent evolution and slightly higher average CodeHealth. Although neither difference was significant overall, the increase in CodeHealth was statistically significant when habitual AI users completed Phase 1. For Phase 1, we also observed a significant effect that corroborates previous productivity findings: using an AI assistant yielded a 30.7% median decrease in task completion time. Moreover, for habitual AI users, the mean speedup was 55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants can effectively accelerate development. Moreover, we did not observe warning signs of degraded code-level maintainability. We recommend that future research focus on risks such as code bloat from excessive code generation and the build-up of cognitive debt as developers invest less mental effort during implementation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</title>
<link>https://arxiv.org/abs/2507.00790</link>
<guid>https://arxiv.org/abs/2507.00790</guid>
<content:encoded><![CDATA[
arXiv:2507.00790v1 Announce Type: cross 
Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many LLMs Are More Utilitarian Than One</title>
<link>https://arxiv.org/abs/2507.00814</link>
<guid>https://arxiv.org/abs/2507.00814</guid>
<content:encoded><![CDATA[
arXiv:2507.00814v1 Announce Type: cross 
Abstract: Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments</title>
<link>https://arxiv.org/abs/2507.00816</link>
<guid>https://arxiv.org/abs/2507.00816</guid>
<content:encoded><![CDATA[
arXiv:2507.00816v1 Announce Type: cross 
Abstract: Accurate dynamics modeling is essential for quadrotors to achieve precise trajectory tracking in various applications. Traditional physical knowledge-driven modeling methods face substantial limitations in unknown environments characterized by variable payloads, wind disturbances, and external perturbations. On the other hand, data-driven modeling methods suffer from poor generalization when handling out-of-distribution (OoD) data, restricting their effectiveness in unknown scenarios. To address these challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN), which combines knowledge-driven and data-driven modeling methods by embedding physical constraints directly into the training process for robust quadrotor dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional Network (TCN) architecture that efficiently captures temporal dependencies from historical flight data, while a physics-informed loss function applies physical principles to improve model generalization and robustness across previously unseen conditions. By incorporating real-time prediction results into a model predictive control (MPC) framework, we achieve improvements in closed-loop tracking performance. Comprehensive simulations and real-world flight experiments demonstrate that our approach outperforms baseline methods in terms of prediction accuracy, tracking precision, and robustness to unknown environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs</title>
<link>https://arxiv.org/abs/2507.00817</link>
<guid>https://arxiv.org/abs/2507.00817</guid>
<content:encoded><![CDATA[
arXiv:2507.00817v1 Announce Type: cross 
Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection</title>
<link>https://arxiv.org/abs/2507.00832</link>
<guid>https://arxiv.org/abs/2507.00832</guid>
<content:encoded><![CDATA[
arXiv:2507.00832v1 Announce Type: cross 
Abstract: Introduction: Deep learning (DL) models can help detect intracranial aneurysms on CTA, but high false positive (FP) rates remain a barrier to clinical translation, despite improvement in model architectures and strategies like detection threshold tuning. We employed an automated, anatomy-based, heuristic-learning hybrid artery-vein segmentation post-processing method to further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with 1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143 held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and cavernous venous sinus (CVS) segmentation masks were applied to remove possible FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3) vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79 false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%; 3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular (CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from 1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable post-processing can improve DL-based aneurysm detection model performance. More broadly, automated, domain-informed, hybrid heuristic-learning processing holds promise for improving the performance and clinical acceptance of aneurysm detection models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.00833</link>
<guid>https://arxiv.org/abs/2507.00833</guid>
<content:encoded><![CDATA[
arXiv:2507.00833v1 Announce Type: cross 
Abstract: For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is https://openhumanoidgen.github.io.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stylometry recognizes human and LLM-generated texts in short samples</title>
<link>https://arxiv.org/abs/2507.00838</link>
<guid>https://arxiv.org/abs/2507.00838</guid>
<content:encoded><![CDATA[
arXiv:2507.00838v1 Announce Type: cross 
Abstract: The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NN-Former: Rethinking Graph Structure in Neural Architecture Representation</title>
<link>https://arxiv.org/abs/2507.00880</link>
<guid>https://arxiv.org/abs/2507.00880</guid>
<content:encoded><![CDATA[
arXiv:2507.00880v1 Announce Type: cross 
Abstract: The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each of both methods has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above issues, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. We thus propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes</title>
<link>https://arxiv.org/abs/2507.00891</link>
<guid>https://arxiv.org/abs/2507.00891</guid>
<content:encoded><![CDATA[
arXiv:2507.00891v1 Announce Type: cross 
Abstract: Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions provide.To address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks</title>
<link>https://arxiv.org/abs/2507.00902</link>
<guid>https://arxiv.org/abs/2507.00902</guid>
<content:encoded><![CDATA[
arXiv:2507.00902v1 Announce Type: cross 
Abstract: Direct-satellite-to-device (DS2D) communication is emerging as a promising solution for global mobile service extension, leveraging the deployment of satellite constellations. However, the challenge of managing DS2D connectivity for multi-constellations becomes outstanding, including high interference and frequent handovers caused by multi-coverage overlap and rapid satellite movement. Moreover, existing approaches primarily operate within single-constellation shell, which inherently limits the ability to exploit the vast potential of multi-constellation connectivity provision, resulting in suboptimal DS2D service performances. To address these challenges, this article proposes a Constellation as a Service (CaaS) framework, which treats the entire multi-constellation infrastructure as a shared resource pool and dynamically forms optimal sub-constellations (SCs) for each DS2D service region. The formation of each SC integrates satellites from various orbits to provide tailored connectivity based on user demands, guided by two innovative strategies: predictive satellite beamforming using generative artificial intelligence (GenAI) and pre-configured handover path for efficient satellite access and mobility management. Simulation results demonstrate that CaaS significantly improves satellite service rates while reducing handover overhead, making it an efficient and continuable solution for managing DS2D connectivity in multi-constellation environments.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection</title>
<link>https://arxiv.org/abs/2507.00903</link>
<guid>https://arxiv.org/abs/2507.00903</guid>
<content:encoded><![CDATA[
arXiv:2507.00903v1 Announce Type: cross 
Abstract: Objectives Parametric tissue mapping enables quantitative cardiac tissue characterization but is limited by inter-observer variability during manual delineation. Traditional approaches relying on average relaxation values and single cutoffs may oversimplify myocardial complexity. This study evaluates whether deep learning (DL) can achieve segmentation accuracy comparable to inter-observer variability, explores the utility of statistical features beyond mean T1/T2 values, and assesses whether machine learning (ML) combining multiple features enhances disease detection. Materials & Methods T1 and T2 maps were manually segmented. The test subset was independently annotated by two observers, and inter-observer variability was assessed. A DL model was trained to segment left ventricle blood pool and myocardium. Average (A), lower quartile (LQ), median (M), and upper quartile (UQ) were computed for the myocardial pixels and employed in classification by applying cutoffs or in ML. Dice similarity coefficient (DICE) and mean absolute percentage error evaluated segmentation performance. Bland-Altman plots assessed inter-user and model-observer agreement. Receiver operating characteristic analysis determined optimal cutoffs. Pearson correlation compared features from model and manual segmentations. F1-score, precision, and recall evaluated classification performance. Wilcoxon test assessed differences between classification methods, with p < 0.05 considered statistically significant. Results 144 subjects were split into training (100), validation (15) and evaluation (29) subsets. Segmentation model achieved a DICE of 85.4%, surpassing inter-observer agreement. Random forest applied to all features increased F1-score (92.7%, p < 0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining multiple features with ML improves disease detection.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses</title>
<link>https://arxiv.org/abs/2507.00907</link>
<guid>https://arxiv.org/abs/2507.00907</guid>
<content:encoded><![CDATA[
arXiv:2507.00907v1 Announce Type: cross 
Abstract: In a world where deepfakes and cloned voices are emerging as sophisticated attack vectors, organizations require a new security mindset: Sensorial Zero Trust [9]. This article presents a scientific analysis of the need to systematically doubt information perceived through the senses, establishing rigorous verification protocols to mitigate the risks of fraud based on generative artificial intelligence. Key concepts, such as Out-of-Band verification, Vision-Language Models (VLMs) as forensic collaborators, cryptographic provenance, and human training, are integrated into a framework that extends Zero Trust principles to human sensory information. The approach is grounded in empirical findings and academic research, emphasizing that in an era of AI-generated realities, even our eyes and ears can no longer be implicitly trusted without verification. Leaders are called to foster a culture of methodological skepticism to protect organizational integrity in this new threat landscape.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona</title>
<link>https://arxiv.org/abs/2507.00909</link>
<guid>https://arxiv.org/abs/2507.00909</guid>
<content:encoded><![CDATA[
arXiv:2507.00909v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is fueling exponential electricity demand growth, threatening grid reliability, raising prices for communities paying for new energy infrastructure, and stunting AI innovation as data centers wait for interconnection to constrained grids. This paper presents the first field demonstration, in collaboration with major corporate partners, of a software-only approach--Emerald Conductor--that transforms AI data centers into flexible grid resources that can efficiently and immediately harness existing power systems without massive infrastructure buildout. Conducted at a 256-GPU cluster running representative AI workloads within a commercial, hyperscale cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in cluster power usage for three hours during peak grid events while maintaining AI quality of service (QoS) guarantees. By orchestrating AI workloads based on real-time grid signals without hardware modifications or energy storage, this platform reimagines data centers as grid-interactive assets that enhance grid reliability, advance affordability, and accelerate AI's development.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications</title>
<link>https://arxiv.org/abs/2507.00914</link>
<guid>https://arxiv.org/abs/2507.00914</guid>
<content:encoded><![CDATA[
arXiv:2507.00914v1 Announce Type: cross 
Abstract: The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks</title>
<link>https://arxiv.org/abs/2507.00938</link>
<guid>https://arxiv.org/abs/2507.00938</guid>
<content:encoded><![CDATA[
arXiv:2507.00938v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has enabled the development of autonomous web agents capable of navigating and interacting with real websites. However, evaluating such agents remains challenging due to the instability and inconsistency of existing benchmarks, which often rely on dynamic content or oversimplified simulations. In this work, we introduce WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks grounded in the arXiv platform. WebArXiv ensures reproducible and reliable evaluation by anchoring tasks in fixed web snapshots with deterministic ground truths and standardized action trajectories. Through behavioral analysis, we identify a common failure mode, Rigid History Reflection, where agents over-rely on fixed interaction histories. To address this, we propose a lightweight dynamic reflection mechanism that allows agents to selectively retrieve relevant past steps during decision-making. We evaluate ten state-of-the-art web agents on WebArXiv. Results demonstrate clear performance differences across agents and validate the effectiveness of our proposed reflection strategy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sentences to Sequences: Rethinking Languages in Biological System</title>
<link>https://arxiv.org/abs/2507.00953</link>
<guid>https://arxiv.org/abs/2507.00953</guid>
<content:encoded><![CDATA[
arXiv:2507.00953v1 Announce Type: cross 
Abstract: The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement</title>
<link>https://arxiv.org/abs/2507.00966</link>
<guid>https://arxiv.org/abs/2507.00966</guid>
<content:encoded><![CDATA[
arXiv:2507.00966v1 Announce Type: cross 
Abstract: With the advent of new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform state-of-the-art models in single-channel speech enhancement, automatic speech recognition, and self-supervised audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this issue, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VoiceBank+Demand Extended (VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our proposed MambAttention model significantly outperforms existing state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 and EARS-WHAM_v2, while matching their performance on the in-domain dataset VB-DemandEx. Ablation studies highlight the role of weight sharing between the time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. However, our MambAttention model remains superior on both out-of-domain datasets across all reported evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgical Neural Radiance Fields from One Image</title>
<link>https://arxiv.org/abs/2507.00969</link>
<guid>https://arxiv.org/abs/2507.00969</guid>
<content:encoded><![CDATA[
arXiv:2507.00969v1 Announce Type: cross 
Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D reconstruction and view synthesis, yet their reliance on extensive multi-view data limits their application in surgical intraoperative settings where only limited data is available. In particular, collecting such extensive data intraoperatively is impractical due to time constraints. This work addresses this challenge by leveraging a single intraoperative image and preoperative data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera viewpoints and images needed for robust and unobstructed training. Intraoperatively, the appearance of the surgical image is transferred to the pre-constructed training set through neural style transfer, specifically combining WTC2 and STROTSS to prevent over-stylization. This process enables the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases. Quantitative comparisons to NeRF models trained on real surgical microscope images demonstrate strong synthesis agreement, with similarity metrics indicating high reconstruction fidelity and stylistic alignment. When compared with ground truth, our method demonstrates high structural similarity, confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF training in surgical settings, overcoming the limitations of traditional multi-view methods.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning as an Adaptive Defense for Safety</title>
<link>https://arxiv.org/abs/2507.00971</link>
<guid>https://arxiv.org/abs/2507.00971</guid>
<content:encoded><![CDATA[
arXiv:2507.00971v1 Announce Type: cross 
Abstract: Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations</title>
<link>https://arxiv.org/abs/2507.00990</link>
<guid>https://arxiv.org/abs/2507.00990</guid>
<content:encoded><![CDATA[
arXiv:2507.00990v1 Announce Type: cross 
Abstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks</title>
<link>https://arxiv.org/abs/2507.01001</link>
<guid>https://arxiv.org/abs/2507.01001</guid>
<content:encoded><![CDATA[
arXiv:2507.01001v1 Announce Type: cross 
Abstract: We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes</title>
<link>https://arxiv.org/abs/2507.01003</link>
<guid>https://arxiv.org/abs/2507.01003</guid>
<content:encoded><![CDATA[
arXiv:2507.01003v1 Announce Type: cross 
Abstract: Recent studies have proposed interpreting the training process from an ergodic perspective. Building on this foundation we present a unified framework for understanding and accelerating the training of deep neural networks via stochastic gradient descent. By analyzing the geometric landscape of the objective function we introduce a practical diagnostic, the running estimate of the largest Lyapunov exponent, which provably distinguishes genuine convergence toward stable minimizers from mere statistical stabilization near saddle points. We then propose a ghost category extension for standard classifiers that adds auxiliary ghost output nodes so the model gains extra descent directions that open a lateral corridor around narrow loss barriers and enable the optimizer to bypass poor basins during the early training phase. We show that this extension strictly reduces approximation error and that after sufficient convergence the ghost dimensions collapse and the extended model's invariant law coincides with that of the original and there exists a path in the enlarged parameter space along which the total loss does not increase while the original loss decreases by an arbitrary margin. Taken together these results provide a principled architecture level intervention that accelerates early stage trainability while preserving asymptotic behavior.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
arXiv:2507.01006v1 Announce Type: cross 
Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conquering Ghosts: Relation Learning for Information Reliability Representation and End-to-End Robust Navigation</title>
<link>https://arxiv.org/abs/2203.09952</link>
<guid>https://arxiv.org/abs/2203.09952</guid>
<content:encoded><![CDATA[
arXiv:2203.09952v4 Announce Type: replace 
Abstract: Environmental disturbances, such as sensor data noises, various lighting conditions, challenging weathers and external adversarial perturbations, are inevitable in real self-driving applications. Existing researches and testings have shown that they can severely influence the vehicles perception ability and performance, one of the main issue is the false positive detection, i.e., the ghost object which is not real existed or occurs in the wrong position (such as a non-existent vehicle). Traditional navigation methods tend to avoid every detected objects for safety, however, avoiding a ghost object may lead the vehicle into a even more dangerous situation, such as a sudden break on the highway. Considering the various disturbance types, it is difficult to address this issue at the perceptual aspect. A potential solution is to detect the ghost through relation learning among the whole scenario and develop an integrated end-to-end navigation system. Our underlying logic is that the behavior of all vehicles in the scene is influenced by their neighbors, and normal vehicles behave in a logical way, while ghost vehicles do not. By learning the spatio-temporal relation among surrounding vehicles, an information reliability representation is learned for each detected vehicle and then a robot navigation network is developed. In contrast to existing works, we encourage the network to learn how to represent the reliability and how to aggregate all the information with uncertainties by itself, thus increasing the efficiency and generalizability. To the best of the authors knowledge, this paper provides the first work on using graph relation learning to achieve end-to-end robust navigation in the presence of ghost vehicles. Simulation results in the CARLA platform demonstrate the feasibility and effectiveness of the proposed method in various scenarios.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexora: Flexible Low Rank Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2408.10774</link>
<guid>https://arxiv.org/abs/2408.10774</guid>
<content:encoded><![CDATA[
arXiv:2408.10774v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are driving advancements in artificial intelligence by increasing the scale of model parameters, which has significantly enhanced generalization ability and unlocked new capabilities in practice. However, their performance in specific downstream tasks is usually hindered by their knowledge boundaries on these tasks. Thus, fine-tuning techniques, especially the widely used Low-Rank Adaptation (LoRA) method, have been introduced to expand the boundaries on these tasks, whereas LoRA would underperform on certain tasks owing to its potential overfitting on these tasks. To overcome this overfitting and improve the performance of LoRA, we propose the flexible low rank adaptation (Flexora) method to automatically and flexibly select the most important layers needing to be fine-tuned to achieve the best performance on different downstream tasks. Specifically, Flexora firstly frames this layer selection problem as a well-defined hyperparameter optimization (HPO) problem, then addresses it using the unrolled differentiation (UD) method, and finally selects the most useful layers based on the optimized hyperparameters. Our extensive experiments on many pretrained models and natural language tasks show that Flexora is able to consistently improve over the existing baselines, indicating the effectiveness of our Flexora in practice. We additionally provide insightful theoretical results and many ablation studies to deliver a comprehensive understanding of our Flexora.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[
arXiv:2409.20302v4 Announce Type: replace 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information, particularly for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component for efficient ontology management, the growing size of ontologies and accumulating errors caused by manual labour overwhelm current OV approaches. In this paper, we propose a fresh approach to performing OV using existing ontology matching (OM) techniques and systems. We introduce a unified OM4OV pipeline. From an OM perspective, we reconstruct a new task formulation and measurements for OV tasks. Building upon the prior alignment(s) from OM, we propose a pipeline optimisation method called the cross-reference (CR) mechanism to enhance overall OV performance. We experimentally validate the OM4OV pipeline and the cross-reference mechanism in an OV testbed originating from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also discuss insights into OM used for OV tasks, where some apparent false mappings detected by OV systems are not actually untrue.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach</title>
<link>https://arxiv.org/abs/2505.02952</link>
<guid>https://arxiv.org/abs/2505.02952</guid>
<content:encoded><![CDATA[
arXiv:2505.02952v2 Announce Type: replace 
Abstract: Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs</title>
<link>https://arxiv.org/abs/2505.06096</link>
<guid>https://arxiv.org/abs/2505.06096</guid>
<content:encoded><![CDATA[
arXiv:2505.06096v2 Announce Type: replace 
Abstract: Limitations in Large Language Model (LLM) capabilities for hardware design tasks, such as generating functional Verilog codes, have motivated various fine-tuning optimizations utilizing curated hardware datasets from open-source repositories. However, these datasets remain limited in size and contain minimal checks on licensing for reuse, resulting in potential copyright violations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to estimate the risk of Verilog-trained LLMs to generate copyright-protected codes. To minimize this risk, we present an open-source Verilog dataset, FreeSet, containing over 220k files, along with the automated dataset curation framework utilized to provide additional guarantees of fair-use Verilog data. We then execute an LLM fine-tuning framework consisting of continual pre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our results indicate that FreeV demonstrates the smallest risk of copyright-infringement among prior works, with only a 3% violation rate. Furthermore, experimental results demonstrate improvements in Verilog generation functionality over its baseline model, improving VerilogEval pass@10 rates by over 10%.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks</title>
<link>https://arxiv.org/abs/2505.16459</link>
<guid>https://arxiv.org/abs/2505.16459</guid>
<content:encoded><![CDATA[
arXiv:2505.16459v3 Announce Type: replace 
Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale</title>
<link>https://arxiv.org/abs/2505.20094</link>
<guid>https://arxiv.org/abs/2505.20094</guid>
<content:encoded><![CDATA[
arXiv:2505.20094v3 Announce Type: replace 
Abstract: Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Program of Equations Thoughts to Solve Algebra Word Problems</title>
<link>https://arxiv.org/abs/2505.20170</link>
<guid>https://arxiv.org/abs/2505.20170</guid>
<content:encoded><![CDATA[
arXiv:2505.20170v2 Announce Type: replace 
Abstract: Solving algebraic word problems (AWPs) has recently emerged as an important natural language processing task. Recently, large language models (LLMs) have demonstrated powerful mathematical capabilities, and the Chain-of-Thought technique, which guides LLMs through step-by-step reasoning, has yielded impressive results. However, this reasoning ability is limited by the computational weaknesses of LLMs themselves, where calculation errors can accumulate, leading to incorrect final answers. To address this, we propose Program of Equations Thoughts (POET), which transforms the task of generating step-by-step reasoning answers into a two-stage task of predicting equations and generating code, offloading complex computations to a Python interpreter to avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which utilizes a manually designed template to enable LLMs to directly generate Python code for one-step solving. Our method achieves accuracies of 95.3% and 98.0% on the PEN and ALG514 datasets, respectively, setting a new state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5% on the DRAW-1K dataset.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conceptual Framework Toward Embodied Collective Adaptive Intelligence</title>
<link>https://arxiv.org/abs/2505.23153</link>
<guid>https://arxiv.org/abs/2505.23153</guid>
<content:encoded><![CDATA[
arXiv:2505.23153v2 Announce Type: replace 
Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder</title>
<link>https://arxiv.org/abs/2210.06230</link>
<guid>https://arxiv.org/abs/2210.06230</guid>
<content:encoded><![CDATA[
arXiv:2210.06230v3 Announce Type: replace-cross 
Abstract: Formal/symbolic semantics can provide canonical, rigid controllability and interpretability to sentence representations due to their \textit{localisation} or \textit{composition} property. How can we deliver such property to the current distributional sentence representations to control and interpret the generation of language models (LMs)? In this work, we theoretically frame the sentence semantics as the composition of \textit{semantic role - word content} features and propose the formal semantic geometry. To inject such geometry into Transformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational AutoEncoder with a supervision approach, where the sentence generation can be manipulated and explained over low-dimensional latent Gaussian space. In addition, we propose a new probing algorithm to guide the movement of sentence vectors over such geometry. Experimental results reveal that the formal semantic geometry can potentially deliver better control and interpretation to sentence generation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods</title>
<link>https://arxiv.org/abs/2302.14368</link>
<guid>https://arxiv.org/abs/2302.14368</guid>
<content:encoded><![CDATA[
arXiv:2302.14368v5 Announce Type: replace-cross 
Abstract: As Diffusion Models have shown promising performance, a lot of efforts have been made to improve the controllability of Diffusion Models. However, how to train Diffusion Models to have the disentangled latent spaces and how to naturally incorporate the disentangled conditions during the sampling process have been underexplored. In this paper, we present a training framework for feature disentanglement of Diffusion Models (FDiff). We further propose two sampling methods that can boost the realism of our Diffusion Models and also enhance the controllability. Concisely, we train Diffusion Models conditioned on two latent features, a spatial content mask, and a flattened style embedding. We rely on the inductive bias of the denoising process of Diffusion Models to encode pose/layout information in the content feature and semantic/style information in the style feature. Regarding the sampling methods, we first generalize Composable Diffusion Models (GCDM) by breaking the conditional independence assumption to allow for some dependence between conditional inputs, which is shown to be effective in realistic generation in our experiments. Second, we propose timestep-dependent weight scheduling for content and style features to further improve the performance. We also observe better controllability of our proposed methods compared to existing methods in image manipulation and image translation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs "Difficult" Downstream Tasks in LLMs</title>
<link>https://arxiv.org/abs/2310.02277</link>
<guid>https://arxiv.org/abs/2310.02277</guid>
<content:encoded><![CDATA[
arXiv:2310.02277v3 Announce Type: replace-cross 
Abstract: We present Junk DNA Hypothesis by adopting a novel task-centric angle for the pre-trained weights of large language models (LLMs). It has been believed that weights in LLMs contain significant redundancy, leading to the conception that a considerable chunk of the parameters can be removed by pruning without compromising performance. Contrary to this belief, this paper presents a counter-argument: small-magnitude weights of pre-trained model weights encode vital knowledge essential for tackling difficult downstream tasks - manifested as the monotonic relationship between the performance drop of downstream tasks across the difficulty spectrum, as we prune more pre-trained weights by magnitude. Moreover, we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed. Interestingly, our evaluations show that the other popular compression, namely quantization, fails to exhibit similar monotonic effect and does not as convincingly disentangle this task-difficulty information. To study formally, we introduce several quantifiable metrics to gauge the downstream task difficulty: (1) within the same task category, and (2) across different task categories. Our extensive experiments substantiate the Junk DNA Hypothesis across a diverse range of model sizes, tasks, datasets, and even pruning methods. Codes are available at: https://github.com/VITA-Group/Junk_DNA_Hypothesis.git.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)</title>
<link>https://arxiv.org/abs/2311.10248</link>
<guid>https://arxiv.org/abs/2311.10248</guid>
<content:encoded><![CDATA[
arXiv:2311.10248v3 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \textit{ground-truth model update}. To find such \textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets.
  In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT</title>
<link>https://arxiv.org/abs/2401.03302</link>
<guid>https://arxiv.org/abs/2401.03302</guid>
<content:encoded><![CDATA[
arXiv:2401.03302v4 Announce Type: replace-cross 
Abstract: Reliable diagnosis of brain tumors remains challenging due to low clinical incidence rates of such cases. However, this low rate is neglected in most of proposed methods. We propose a clinically inspired framework for anomaly-resilient tumor detection and classification. Detection leverages YOLOv8n fine-tuned on a realistically imbalanced dataset (1:9 tumor-to-normal ratio; 30,000 MRI slices from 81 patients). In addition, we propose a novel Patient-to-Patient (PTP) metric that evaluates diagnostic reliability at the patient level. Classification employs knowledge distillation: a Data Efficient Image Transformer (DeiT) student model is distilled from a ResNet152 teacher. The distilled ViT achieves an F1-score of 0.92 within 20 epochs, matching near teacher performance (F1=0.97) with significantly reduced computational resources. This end-to-end framework demonstrates high robustness in clinically representative anomaly-distributed data, offering a viable tool that adheres to realistic situations in clinics.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying analogy of concepts via ologs and wiring diagrams</title>
<link>https://arxiv.org/abs/2402.01020</link>
<guid>https://arxiv.org/abs/2402.01020</guid>
<content:encoded><![CDATA[
arXiv:2402.01020v2 Announce Type: replace-cross 
Abstract: We build on the theory of ontology logs (ologs) created by Spivak and Kent, and define a notion of wiring diagrams. In this article, a wiring diagram is a finite directed labelled graph. The labels correspond to types in an olog; they can also be interpreted as readings of sensors in an autonomous system. As such, wiring diagrams can be used as a framework for an autonomous system to form abstract concepts. We show that the graphs underlying skeleton wiring diagrams form a category. This allows skeleton wiring diagrams to be compared and manipulated using techniques from both graph theory and category theory. We also extend the usual definition of graph edit distance to the case of wiring diagrams by using operations only available to wiring diagrams, leading to a metric on the set of all skeleton wiring diagrams. In the end, we give an extended example on calculating the distance between two concepts represented by wiring diagrams, and explain how to apply our framework to any application domain.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Differentiable Lagrangian Convolutional Neural Network for Physics-Informed Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2402.10747</link>
<guid>https://arxiv.org/abs/2402.10747</guid>
<content:encoded><![CDATA[
arXiv:2402.10747v2 Announce Type: replace-cross 
Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods. It consists of a U-Net that dynamically produces mesoscale advection motion fields, a differentiable semi-Lagrangian extrapolation operator, and an advection-free U-Net capturing the growth and decay of precipitation over time. Using our approach, we successfully implement the Lagrangian convolutional neural network for precipitation nowcasting in a fully differentiable and GPU-accelerated manner. This allows for end-to-end training and inference, including the data-driven Lagrangian coordinate system transformation of the data at runtime. We evaluate the model and compare it with other related AI-based models both quantitatively and qualitatively in an extreme event case study. Based on our evaluation, LUPIN matches and even exceeds the performance of the chosen benchmarks, opening the door for other Lagrangian machine learning models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging</title>
<link>https://arxiv.org/abs/2404.09158</link>
<guid>https://arxiv.org/abs/2404.09158</guid>
<content:encoded><![CDATA[
arXiv:2404.09158v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce StreakNet-Arch, a real-time, end-to-end binary-classification framework based on our self-developed Underwater Carrier LiDAR-Radar (UCLR) that embeds Self-Attention and our novel Double Branch Cross Attention (DBC-Attention) to enhance scatter suppression. Under controlled water tank validation conditions, StreakNet-Arch with Self-Attention or DBC-Attention outperforms traditional bandpass filtering and achieves higher $F_1$ scores than learning-based MP networks and CNNs at comparable model size and complexity. Real-time benchmarks on an NVIDIA RTX 3060 show a constant Average Imaging Time (54 to 84 ms) regardless of frame count, versus a linear increase (58 to 1,257 ms) for conventional methods. To facilitate further research, we contribute a publicly available streak-tube camera image dataset contains 2,695,168 real-world underwater 3D point cloud data. More importantly, we validate our UCLR system in a South China Sea trial, reaching an error of 46mm for 3D target at 1,000 m depth and 20 m range. Source code and data are available at https://github.com/BestAnHongjun/StreakNet .
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Confidence Estimation via Black-Box Access</title>
<link>https://arxiv.org/abs/2406.04370</link>
<guid>https://arxiv.org/abs/2406.04370</guid>
<content:encoded><![CDATA[
arXiv:2406.04370v4 Announce Type: replace-cross 
Abstract: Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of Flan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\&amp;A tasks as well as of Pegasus-large and BART-large on two benchmark summarization tasks with it surpassing baselines by even over $10\%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks Generalize on Low Complexity Data</title>
<link>https://arxiv.org/abs/2409.12446</link>
<guid>https://arxiv.org/abs/2409.12446</guid>
<content:encoded><![CDATA[
arXiv:2409.12446v4 Announce Type: replace-cross 
Abstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d.~data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number. For primality testing, our theorem shows the following and more. Suppose that we draw an i.i.d.~sample of $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL network accurately answers, with error probability $1- O((\ln N)/n)$, whether a newly drawn number between $1$ and $N$ is a prime or not. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so. Extensions to noisy data are also discussed, suggesting that MDL neural network interpolators can demonstrate tempered overfitting.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs</title>
<link>https://arxiv.org/abs/2410.01141</link>
<guid>https://arxiv.org/abs/2410.01141</guid>
<content:encoded><![CDATA[
arXiv:2410.01141v3 Announce Type: replace-cross 
Abstract: This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers</title>
<link>https://arxiv.org/abs/2411.04403</link>
<guid>https://arxiv.org/abs/2411.04403</guid>
<content:encoded><![CDATA[
arXiv:2411.04403v2 Announce Type: replace-cross 
Abstract: Learned sparse retrieval, which can efficiently perform retrieval through mature inverted-index engines, has garnered growing attention in recent years. Particularly, the inference-free sparse retrievers are attractive as they eliminate online model inference in the retrieval phase thereby avoids huge computational cost, offering reasonable throughput and latency. However, even the state-of-the-art (SOTA) inference-free sparse models lag far behind in terms of search relevance when compared to both sparse and dense siamese models. Towards competitive search relevance for inference-free sparse retrievers, we argue that they deserve dedicated training methods other than using same ones with siamese encoders. In this paper, we propose two different approaches for performance improvement. First, we propose an IDF-aware penalty for the matching function that suppresses the contribution of low-IDF tokens and increases the model's focus on informative terms. Moreover, we propose a heterogeneous ensemble knowledge distillation framework that combines siamese dense and sparse retrievers to generate supervisory signals during the pre-training phase. The ensemble framework of dense and sparse retriever capitalizes on their strengths respectively, providing a strong upper bound for knowledge distillation. To concur the diverse feedback from heterogeneous supervisors, we normalize and then aggregate the outputs of the teacher models to eliminate score scale differences. On the BEIR benchmark, our model outperforms existing SOTA inference-free sparse model by \textbf{3.3 NDCG@10 score}. It exhibits search relevance comparable to siamese sparse retrievers and client-side latency only \textbf{1.1x that of BM25}.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGD: Steepest Perturbed Gradient Descent Optimization</title>
<link>https://arxiv.org/abs/2411.04946</link>
<guid>https://arxiv.org/abs/2411.04946</guid>
<content:encoded><![CDATA[
arXiv:2411.04946v2 Announce Type: replace-cross 
Abstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoCogQA: A Controllable Benchmark for Evaluating Cognitive Abilities in Video-Language Models</title>
<link>https://arxiv.org/abs/2411.09105</link>
<guid>https://arxiv.org/abs/2411.09105</guid>
<content:encoded><![CDATA[
arXiv:2411.09105v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Video-Language Models (LVLMs) have led to promising results in multimodal video understanding. However, it remains unclear whether these models possess the cognitive capabilities required for high-level tasks, particularly those involving symbolic and abstract perception. Existing benchmarks typically rely on real-world, annotated videos, which lack control over video content and inherent difficulty, limiting their diagnostic power. To bridge this gap, we propose VideoCogQA, a scalable and fully controllable benchmark inspired by game-world environments, designed to evaluate the cognitive abilities of LVLMs. By generating synthetic videos via a programmatic engine, VideoCogQA allows fine-grained control over visual elements, temporal dynamics, and task difficulty. This approach enables a focused evaluation of video cognitive abilities, independent of prior knowledge from visual scene semantics. The dataset includes 800 videos and 3,280 question-answer pairs, featuring tasks related to abstract concepts, symbolic elements, and multimodal integration, with varying levels of difficulty. Experimental results show that even state-of-the-art (SOTA) models, such as GPT-4o, achieve an average performance of 48.8% on tasks involving abstract concepts. Additionally, performance drops by 15% as task complexity increases, highlighting the challenges LVLMs face in maintaining consistent performance. Through this work, we hope to show the limitations of current LVLMs and offer insights into how they can more effectively emulate human cognitive processes in the future.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Holistic to Localized: Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2411.12787</link>
<guid>https://arxiv.org/abs/2411.12787</guid>
<content:encoded><![CDATA[
arXiv:2411.12787v3 Announce Type: replace-cross 
Abstract: Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal Large Language Models (MLLMs) to downstream tasks with minimal computational overhead. However, as task diversity and complexity increase, EVIT faces significant challenges in resolving data conflicts. To address this limitation, we propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local framework that enhances the adapter's capacity to address data conflict through dual structural optimization. Specifically, we utilize two subspaces: a skill space for stable, holistic knowledge retention, and a rank-rectified task space that locally activates the holistic knowledge. Additionally, we introduce Visual Cue Enhancement (VCE), a multi-level local feature aggregation module designed to enrich the vision-language projection with local details. Our approach is both memory- and time-efficient, requiring only 1.16$\times$ the inference time of the standard LoRA method (with injection into the query and value projection layers), and just 73\% of the inference time of a 4-expert LoRA-MoE. Extensive experiments on various downstream tasks and general MLLM benchmarks validate the effectiveness of our proposed methods.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity Preserving 3D Head Stylization with Multiview Score Distillation</title>
<link>https://arxiv.org/abs/2411.13536</link>
<guid>https://arxiv.org/abs/2411.13536</guid>
<content:encoded><![CDATA[
arXiv:2411.13536v2 Announce Type: replace-cross 
Abstract: 3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the https://three-bee.github.io/head_stylization for more visuals.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2411.13949</link>
<guid>https://arxiv.org/abs/2411.13949</guid>
<content:encoded><![CDATA[
arXiv:2411.13949v2 Announce Type: replace-cross 
Abstract: Visual instruction tuning (VIT) enables multimodal large language models (MLLMs) to effectively handle a wide range of vision tasks by framing them as language-based instructions. Building on this, continual visual instruction tuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks, accommodating evolving functionalities. While prior work has advanced CVIT through the development of new benchmarks and approaches to mitigate catastrophic forgetting, these efforts largely follow traditional continual learning paradigms, neglecting the unique challenges specific to CVIT. We identify a dual form of catastrophic forgetting in CVIT, where MLLMs not only forget previously learned visual understanding but also experience a decline in instruction following abilities as they acquire new tasks. To address this, we introduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework, which employs separable routing through two distinct modules-one for visual understanding and another for instruction following. This dual-routing design enables specialized adaptation in both domains, preventing forgetting while improving performance. Furthermore, we propose a new CVIT benchmark that goes beyond existing benchmarks by additionally evaluating a model's ability to generalize to unseen tasks and handle diverse instructions across various tasks. Extensive experiments demonstrate that SMoLoRA outperforms existing methods in mitigating dual forgetting, improving generalization to unseen tasks, and ensuring robustness in following diverse instructions. Code is available at https://github.com/Minato-Zackie/SMoLoRA.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automatic Graph Construction Framework based on Large Language Models for Recommendation</title>
<link>https://arxiv.org/abs/2412.18241</link>
<guid>https://arxiv.org/abs/2412.18241</guid>
<content:encoded><![CDATA[
arXiv:2412.18241v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have emerged as state-of-the-art methods to learn from graph-structured data for recommendation. However, most existing GNN-based recommendation methods focus on the optimization of model structures and learning strategies based on pre-defined graphs, neglecting the importance of the graph construction stage. Earlier works for graph construction usually rely on speciffic rules or crowdsourcing, which are either too simplistic or too labor-intensive. Recent works start to utilize large language models (LLMs) to automate the graph construction, in view of their abundant open-world knowledge and remarkable reasoning capabilities. Nevertheless, they generally suffer from two limitations: (1) invisibility of global view (e.g., overlooking contextual information) and (2) construction inefficiency. To this end, we introduce AutoGraph, an automatic graph construction framework based on LLMs for recommendation. Specifically, we first use LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Next, we employ vector quantization to extract the latent factors from the semantic vectors. The latent factors are then incorporated as extra nodes to link the user/item nodes, resulting in a graph with in-depth global-view semantics. We further design metapath-based message aggregation to effectively aggregate the semantic and collaborative information. The framework is model-agnostic and compatible with different backbone models. Extensive experiments on three real-world datasets demonstrate the efficacy and efffciency of AutoGraph compared to existing baseline methods. We have deployed AutoGraph in Huawei advertising platform, and gain a 2.69% improvement on RPM and a 7.31% improvement on eCPM in the online A/B test. Currently AutoGraph has been used as the main trafffc model, serving hundreds of millions of people.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing baselines for generative discovery of inorganic crystals</title>
<link>https://arxiv.org/abs/2501.02144</link>
<guid>https://arxiv.org/abs/2501.02144</guid>
<content:encoded><![CDATA[
arXiv:2501.02144v2 Announce Type: replace-cross 
Abstract: Generative artificial intelligence offers a promising avenue for materials discovery, yet its advantages over traditional methods remain unclear. In this work, we introduce and benchmark two baseline approaches - random enumeration of charge-balanced prototypes and data-driven ion exchange of known compounds - against four generative techniques based on diffusion models, variational autoencoders, and large language models. Our results show that established methods such as ion exchange are better at generating novel materials that are stable, although many of these closely resemble known compounds. In contrast, generative models excel at proposing novel structural frameworks and, when sufficient training data is available, can more effectively target properties such as electronic band gap and bulk modulus. To enhance the performance of both the baseline and generative approaches, we implement a post-generation screening step in which all proposed structures are passed through stability and property filters from pre-trained machine learning models including universal interatomic potentials. This low-cost filtering step leads to substantial improvement in the success rates of all methods, remains computationally efficient, and ultimately provides a practical pathway toward more effective generative strategies for materials discovery. By establishing baselines for comparison, this work highlights opportunities for continued advancement of generative models, especially for the targeted generation of novel materials that are thermodynamically stable.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of In-Context-Learning-Based Text-to-SQL Errors</title>
<link>https://arxiv.org/abs/2501.09310</link>
<guid>https://arxiv.org/abs/2501.09310</guid>
<content:encoded><![CDATA[
arXiv:2501.09310v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</title>
<link>https://arxiv.org/abs/2501.10736</link>
<guid>https://arxiv.org/abs/2501.10736</guid>
<content:encoded><![CDATA[
arXiv:2501.10736v3 Announce Type: replace-cross 
Abstract: Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Representation Consistency Model via Contrastive Denoising</title>
<link>https://arxiv.org/abs/2501.13094</link>
<guid>https://arxiv.org/abs/2501.13094</guid>
<content:encoded><![CDATA[
arXiv:2501.13094v2 Announce Type: replace-cross 
Abstract: Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3% on average, with up to 11.6% at larger radii, while reducing inference costs by 85$\times$ on average. Codes are available at: https://github.com/jiachenlei/rRCM.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach</title>
<link>https://arxiv.org/abs/2501.16243</link>
<guid>https://arxiv.org/abs/2501.16243</guid>
<content:encoded><![CDATA[
arXiv:2501.16243v3 Announce Type: replace-cross 
Abstract: We address the problem of quantum reinforcement learning (QRL) under model-free settings with quantum oracle access to the Markov Decision Process (MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG) algorithm, which replaces the random sampling used in classical Natural Policy Gradient (NPG) estimators with a deterministic gradient estimation approach, enabling seamless integration into quantum systems. While this modification introduces a bounded bias in the estimator, the bias decays exponentially with increasing truncation levels. This paper demonstrates that the proposed QNPG algorithm achieves a sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-1.5})$ for queries to the quantum oracle, significantly improving the classical lower bound of $\tilde{\mathcal{O}}(\epsilon^{-2})$ for queries to the MDP.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds</title>
<link>https://arxiv.org/abs/2502.02869</link>
<guid>https://arxiv.org/abs/2502.02869</guid>
<content:encoded><![CDATA[
arXiv:2502.02869v2 Announce Type: replace-cross 
Abstract: In-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce step-wise supervision and induce prior information in the ICRL framework.Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering</title>
<link>https://arxiv.org/abs/2502.03628</link>
<guid>https://arxiv.org/abs/2502.03628</guid>
<content:encoded><![CDATA[
arXiv:2502.03628v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits ranking throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss - visually grounded tokens gradually become less favored throughout generation, and (2) early excitation - semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information - visually grounded tokens though not being eventually decoded still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by about 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies. Code is available at https://github.com/LzVv123456/VISTA.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms</title>
<link>https://arxiv.org/abs/2502.04388</link>
<guid>https://arxiv.org/abs/2502.04388</guid>
<content:encoded><![CDATA[
arXiv:2502.04388v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) agents capable of autonomous learning and independent decision-making hold great promise for addressing complex challenges across various critical infrastructure domains, including transportation, energy systems, and manufacturing. However, the surge in the design and deployment of AI systems, driven by various stakeholders with distinct and unaligned objectives, introduces a crucial challenge: How can uncoordinated AI systems coexist and evolve harmoniously in shared environments without creating chaos or compromising safety? To address this, we advocate for a fundamental rethinking of existing multi-agent frameworks, such as multi-agent systems and game theory, which are largely limited to predefined rules and static objective structures. We posit that AI agents should be empowered to adjust their objectives dynamically, make compromises, form coalitions, and safely compete or cooperate through evolving relationships and social feedback. Through two case studies in critical infrastructure applications, we call for a shift toward the emergent, self-organizing, and context-aware nature of these multi-agentic AI systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Curse of Depth in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05795</link>
<guid>https://arxiv.org/abs/2502.05795</guid>
<content:encoded><![CDATA[
arXiv:2502.05795v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling (LNS), which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Across a wide range of model sizes (130M to 7B), our experiments show that LNS consistently outperforms previous normalization and scaling techniques in enhancing LLM pre-training performance. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at \href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization</title>
<link>https://arxiv.org/abs/2502.13030</link>
<guid>https://arxiv.org/abs/2502.13030</guid>
<content:encoded><![CDATA[
arXiv:2502.13030v5 Announce Type: replace-cross 
Abstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation</title>
<link>https://arxiv.org/abs/2503.03040</link>
<guid>https://arxiv.org/abs/2503.03040</guid>
<content:encoded><![CDATA[
arXiv:2503.03040v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models have demonstrated impressive capabilities in task-oriented applications, yet building emotionally intelligent chatbots that can engage in natural, strategic conversations remains a challenge. We present a novel approach called SAGE that uses latent variables to control long-horizon behavior in dialogue generation. At the core of our method is the State-Action Chain (SAC), which augments standard language model fine-tuning by introducing latent variables that encapsulate emotional states and conversational strategies between dialogue turns. During inference, these variables are generated before each response, enabling coarse-grained control over dialogue progression while maintaining natural interaction patterns. We also introduce a self-improvement pipeline that leverages dialogue tree search, LLM-based reward modeling, and targeted fine-tuning to optimize conversational trajectories. Our experimental results show that models trained with this approach demonstrate improved performance in emotional intelligence metrics while maintaining strong capabilities on LLM benchmarks. The discrete nature of our latent variables facilitates search-based strategies and provides a foundation for future applications of reinforcement learning to dialogue systems, where learning can occur at the state level rather than the token level. https://github.com/apple/ml-sage-dialog-gen
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2503.07330</link>
<guid>https://arxiv.org/abs/2503.07330</guid>
<content:encoded><![CDATA[
arXiv:2503.07330v2 Announce Type: replace-cross 
Abstract: Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative Perception</title>
<link>https://arxiv.org/abs/2503.13504</link>
<guid>https://arxiv.org/abs/2503.13504</guid>
<content:encoded><![CDATA[
arXiv:2503.13504v2 Announce Type: replace-cross 
Abstract: Multi-agent collaborative perception enhances each agent perceptual capabilities by sharing sensing information to cooperatively perform robot perception tasks. This approach has proven effective in addressing challenges such as sensor deficiencies, occlusions, and long-range perception. However, existing representative collaborative perception systems transmit intermediate feature maps, such as bird-eye view (BEV) representations, which contain a significant amount of non-critical information, leading to high communication bandwidth requirements. To enhance communication efficiency while preserving perception capability, we introduce CoCMT, an object-query-based collaboration framework that optimizes communication bandwidth by selectively extracting and transmitting essential features. Within CoCMT, we introduce the Efficient Query Transformer (EQFormer) to effectively fuse multi-agent object queries and implement a synergistic deep supervision to enhance the positive reinforcement between stages, leading to improved overall performance. Experiments on OPV2V and V2V4Real datasets show CoCMT outperforms state-of-the-art methods while drastically reducing communication needs. On V2V4Real, our model (Top-50 object queries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods, while improving AP70 by 1.1 percent. This efficiency breakthrough enables practical collaborative perception deployment in bandwidth-constrained environments without sacrificing detection accuracy.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation</title>
<link>https://arxiv.org/abs/2503.18549</link>
<guid>https://arxiv.org/abs/2503.18549</guid>
<content:encoded><![CDATA[
arXiv:2503.18549v2 Announce Type: replace-cross 
Abstract: A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
arXiv:2503.21248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses</title>
<link>https://arxiv.org/abs/2503.21393</link>
<guid>https://arxiv.org/abs/2503.21393</guid>
<content:encoded><![CDATA[
arXiv:2503.21393v3 Announce Type: replace-cross 
Abstract: Large Language models (LLMs) have been prominent for language translation, including low-resource languages. There has been limited study on the assessment of the quality of translations generated by LLMs, including Gemini, GPT, and Google Translate. This study addresses this limitation by using semantic and sentiment analysis of selected LLMs for Indian languages, including Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita, Tamas and Maha Prasthanam ) that have been well translated by experts and use LLMs to generate their translations into English, and provide a comparison with selected expert (human) translations. Our investigation revealed that while LLMs have made significant progress in translation accuracy, challenges remain in preserving sentiment and semantic integrity, especially in metaphorical and philosophical contexts for texts such as the Bhagavad Gita. The sentiment analysis revealed that GPT models are better at preserving the sentiment polarity for the given texts when compared to human (expert) translation. The results revealed that GPT models are generally better at maintaining the sentiment and semantics when compared to Google Translate. This study could help in the development of accurate and culturally sensitive translation systems for large language models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
arXiv:2505.00703v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-Nemotron: Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
arXiv:2505.00949v4 Announce Type: replace-cross 
Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment</title>
<link>https://arxiv.org/abs/2505.09438</link>
<guid>https://arxiv.org/abs/2505.09438</guid>
<content:encoded><![CDATA[
arXiv:2505.09438v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title>
<link>https://arxiv.org/abs/2505.16211</link>
<guid>https://arxiv.org/abs/2505.16211</guid>
<content:encoded><![CDATA[
arXiv:2505.16211v2 Announce Type: replace-cross 
Abstract: The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification</title>
<link>https://arxiv.org/abs/2505.16722</link>
<guid>https://arxiv.org/abs/2505.16722</guid>
<content:encoded><![CDATA[
arXiv:2505.16722v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly prevalent in global applications, ensuring that they are toxicity-free across diverse linguistic contexts remains a critical challenge. We explore "Cross-lingual Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling detoxification capabilities to transfer between high and low-resource languages across different script families. We analyze cross-lingual detoxification's effectiveness through 392 extensive settings to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation. Our code and dataset are publicly available at https://github.com/himanshubeniwal/Breaking-mBad.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</title>
<link>https://arxiv.org/abs/2505.17117</link>
<guid>https://arxiv.org/abs/2505.17117</guid>
<content:encoded><![CDATA[
arXiv:2505.17117v3 Announce Type: replace-cross 
Abstract: Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Regularization-Based Structured Pruning for LLMs</title>
<link>https://arxiv.org/abs/2505.18232</link>
<guid>https://arxiv.org/abs/2505.18232</guid>
<content:encoded><![CDATA[
arXiv:2505.18232v2 Announce Type: replace-cross 
Abstract: The deployment of large language models (LLMs) is largely hindered by their large number of parameters. Structural pruning has emerged as a promising solution. Prior structured pruning methods directly remove unimportant parameters based on certain metrics, which often causes knowledge loss and necessitates extensive retraining. To overcome this, we introduce a novel pruning method TRSP: Two-Stage Regularization-Based Structured Pruning for LLMs. Specifically, we multiply the output of each transformer layer by an initial learnable weight and iteratively learn these weights by adding their $\ell_1$-norm as a regularization term to the loss function, serving as the first-stage regularization. Subsequently, we apply additional regularization to the difference between the output and input of layers with smaller weights, encouraging the shift of knowledge to the preserved layers. This serves as the second-stage regularization. TRSP retains more knowledge and better preserves model performance than direct parameter elimination. Through extensive experimentation we show that TRSP outperforms strong layer-wise structured pruning methods without requiring retraining. As a layer-wise pruning method, it delivers notable end-to-end acceleration, making it a promising solution for efficient LLM deployment.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
arXiv:2505.19955v2 Announce Type: replace-cross 
Abstract: Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
arXiv:2505.20485v3 Announce Type: replace-cross 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</title>
<link>https://arxiv.org/abs/2505.24625</link>
<guid>https://arxiv.org/abs/2505.24625</guid>
<content:encoded><![CDATA[
arXiv:2505.24625v2 Announce Type: replace-cross 
Abstract: Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems</title>
<link>https://arxiv.org/abs/2506.02007</link>
<guid>https://arxiv.org/abs/2506.02007</guid>
<content:encoded><![CDATA[
arXiv:2506.02007v2 Announce Type: replace-cross 
Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on eBPF. eACGM collects real-time performance data from key hardware components, including the GPU and network communication layer, as well as from key software stacks such as CUDA, Python, and PyTorch, all without requiring any code instrumentation or modifications. Additionally, it leverages libnvml to gather process-level GPU resource usage information. By applying a Gaussian Mixture Model (GMM) to the collected multidimensional performance metrics for statistical modeling and clustering analysis, eACGM effectively identifies complex failure modes, such as latency anomalies, hardware failures, and communication inefficiencies, enabling rapid diagnosis of system bottlenecks and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive empirical studies and case analyses in multi-node distributed training scenarios. The results demonstrate that eACGM, while maintaining a non-intrusive and low-overhead profile, successfully captures critical performance anomalies during model training and inference. Its stable anomaly detection performance and comprehensive monitoring capabilities validate its applicability and scalability in real-world production environments, providing strong support for performance optimization and fault diagnosis in large-scale AI/ML systems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bregman Centroid Guided Cross-Entropy Method</title>
<link>https://arxiv.org/abs/2506.02205</link>
<guid>https://arxiv.org/abs/2506.02205</guid>
<content:encoded><![CDATA[
arXiv:2506.02205v2 Announce Type: replace-cross 
Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in model-based reinforcement learning (MBRL), but its unimodal sampling strategy often leads to premature convergence in multimodal landscapes. In this work, we propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for principled information aggregation and diversity control. $\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman centroid across CEM workers and updates the least contributing ones by sampling within a trust region around the centroid. Leveraging the duality between Bregman divergences and exponential family distributions, we show that $\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM pipelines with negligible overhead. Empirical results on synthetic benchmarks, a cluttered navigation task, and full MBRL pipelines demonstrate that $\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution quality, providing a simple yet effective upgrade for CEM.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making a Pipeline Production-Ready: Challenges and Lessons Learned in the Healthcare Domain</title>
<link>https://arxiv.org/abs/2506.06946</link>
<guid>https://arxiv.org/abs/2506.06946</guid>
<content:encoded><![CDATA[
arXiv:2506.06946v2 Announce Type: replace-cross 
Abstract: Deploying a Machine Learning (ML) training pipeline into production requires good software engineering practices. Unfortunately, the typical data science workflow often leads to code that lacks critical software quality attributes. This experience report investigates this problem in SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to pre-diagnose insufficiency respiratory via speech analysis. This paper presents an overview of the architecture of the MLES, then compares three versions of its Continuous Training subsystem: from a proof of concept Big Ball of Mud (v1), to a design pattern-based Modular Monolith (v2), to a test-driven set of Microservices (v3) Each version improved its overall extensibility, maintainability, robustness, and resiliency. The paper shares challenges and lessons learned in this process, offering insights for researchers and practitioners seeking to productionize their pipelines.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</title>
<link>https://arxiv.org/abs/2506.10967</link>
<guid>https://arxiv.org/abs/2506.10967</guid>
<content:encoded><![CDATA[
arXiv:2506.10967v2 Announce Type: replace-cross 
Abstract: In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\% and CUDA latency by 78\%, while maintaining 94\% of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2506.12036</link>
<guid>https://arxiv.org/abs/2506.12036</guid>
<content:encoded><![CDATA[
arXiv:2506.12036v3 Announce Type: replace-cross 
Abstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Diffusion and State Space Models for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.12747</link>
<guid>https://arxiv.org/abs/2506.12747</guid>
<content:encoded><![CDATA[
arXiv:2506.12747v2 Announce Type: replace-cross 
Abstract: Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the model's robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at https://github.com/Rows21/k-Means_Mask_Mamba.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
arXiv:2506.13759v2 Announce Type: replace-cross 
Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying and Improving Graph Neural Network-based Motif Estimation</title>
<link>https://arxiv.org/abs/2506.15709</link>
<guid>https://arxiv.org/abs/2506.15709</guid>
<content:encoded><![CDATA[
arXiv:2506.15709v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARTS: Collaborative Agents for Recommendation Textual Summarization</title>
<link>https://arxiv.org/abs/2506.17765</link>
<guid>https://arxiv.org/abs/2506.17765</guid>
<content:encoded><![CDATA[
arXiv:2506.17765v2 Announce Type: replace-cross 
Abstract: Current recommendation systems often require some form of textual data summarization, such as generating concise and coherent titles for product carousels or other grouped item displays. While large language models have shown promise in NLP domains for textual summarization, these approaches do not directly apply to recommendation systems, where explanations must be highly relevant to the core features of item sets, adhere to strict word limit constraints. In this paper, we propose CARTS (Collaborative Agents for Recommendation Textual Summarization), a multi-agent LLM framework designed for structured summarization in recommendation systems. CARTS decomposes the task into three stages-Generation Augmented Generation (GAG), refinement circle, and arbitration, where successive agent roles are responsible for extracting salient item features, iteratively refining candidate titles based on relevance and length feedback, and selecting the final title through a collaborative arbitration process. Experiments on large-scale e-commerce data and live A/B testing show that CARTS significantly outperforms single-pass and chain-of-thought LLM baselines, delivering higher title relevance and improved user engagement metrics.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking the Pedagogical Knowledge of Large Language Models</title>
<link>https://arxiv.org/abs/2506.18710</link>
<guid>https://arxiv.org/abs/2506.18710</guid>
<content:encoded><![CDATA[
arXiv:2506.18710v3 Announce Type: replace-cross 
Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting</title>
<link>https://arxiv.org/abs/2506.19089</link>
<guid>https://arxiv.org/abs/2506.19089</guid>
<content:encoded><![CDATA[
arXiv:2506.19089v2 Announce Type: replace-cross 
Abstract: We introduce $\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration</title>
<link>https://arxiv.org/abs/2506.19283</link>
<guid>https://arxiv.org/abs/2506.19283</guid>
<content:encoded><![CDATA[
arXiv:2506.19283v2 Announce Type: replace-cross 
Abstract: While multi-vehicular collaborative driving demonstrates clear advantages over single-vehicle autonomy, traditional infrastructure-based V2X systems remain constrained by substantial deployment costs and the creation of "uncovered danger zones" in rural and suburban areas. We present AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side Units (RSUs). Drones offer unique advantages over ground-based perception: complementary bird's-eye-views that reduce occlusions, dynamic positioning capabilities that enable hovering, patrolling, and escorting navigation rules, and significantly lower deployment costs compared to fixed infrastructure. Our dataset comprises 6.73 hours of drone-assisted driving scenarios across urban, suburban, and rural environments with varied weather and lighting conditions. The AirV2X-Perception dataset facilitates the development and standardized evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in the rapidly expanding field of aerial-assisted autonomous driving systems. The dataset and development kits are open-sourced at https://github.com/taco-group/AirV2X-Perception.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning</title>
<link>https://arxiv.org/abs/2506.17525</link>
<guid>https://arxiv.org/abs/2506.17525</guid>
<content:encoded><![CDATA[
<div> auditing, multilingual speech datasets, quality issues, language planning, dataset development <br />
Summary: 
The article discusses a quality audit of three public multilingual speech datasets and identifies significant quality issues in some languages. These issues are categorized as micro-level and macro-level, with the latter being more prevalent in under-resourced languages. A case analysis of Taiwanese Southern Min highlights the importance of proactive language planning and data quality control during dataset creation. The article proposes guidelines and recommendations for improving dataset development, emphasizing the need for sociolinguistic awareness and language planning principles. Additionally, it suggests that the dataset creation process could be utilized for community-led language planning and revitalization efforts. <div>
arXiv:2506.17525v2 Announce Type: replace-cross 
Abstract: Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and Vox Populi - shows that in some languages, these datasets suffer from significant quality issues, which may obfuscate downstream evaluation results while creating an illusion of success. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the dataset creation process. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness and language planning principles. Furthermore, we encourage research into how this creation process itself can be leveraged as a tool for community-led language planning and revitalization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.17728</link>
<guid>https://arxiv.org/abs/2506.17728</guid>
<content:encoded><![CDATA[
<div> multi-turn interactive thinking, deep reasoning, large language model, logical coherence, contextual consistency
Summary:
KAG-Thinker introduces a multi-turn interactive thinking and deep reasoning framework using a parameter-light large language model (LLM). The framework decomposes complex questions into solvable sub-problems and classifies them as Knowledge Retrieval or Reasoning Analysis tasks. It optimizes knowledge retrieval from LLMs and external sources using confidence calibration and reflective reasoning. The structured thinking process enhances logical coherence and contextual consistency in question-answering tasks on knowledge bases. Retrieval, Math, and Deduce functions handle retrieval and reasoning tasks, with explicit modeling of dependencies and parameter passing. The framework leverages logical function interfaces for task classification and knowledge acquisition through the depth solving module. The knowledge boundary module identifies the optimal knowledge source for retrieval tasks, ensuring comprehensive knowledge acquisition. <div>
arXiv:2506.17728v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn interactive thinking and deep reasoning framework powered by a dedicated parameter-light large language model (LLM). Our approach constructs a structured thinking process for solving complex problems, enhancing the the logical coherence and contextual consistency of the reasoning process in question-answering (Q&amp;A) tasks on domain-specific knowledge bases (KBs) within LLMs. Following the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG, this framework first decomposes complex questions into independently solvable sub-problems (which are also referred to as logical forms) through \textbf{breadth decomposition}. Each such logical form is represented in two equivalent forms-natural language and logical function-and subsequently classified as either a Knowledge Retrieval or Reasoning Analysis task. Dependencies and parameter passing between these tasks are explicitly modeled via logical function interfaces. In the solving process, the Retrieval function performs retrieval tasks. It retrieves one-hop structured and unstructured information of specified knowledge unit. While the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} module to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} module to enhance the comprehensiveness of knowledge acquisition...
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs</title>
<link>https://arxiv.org/abs/2506.18985</link>
<guid>https://arxiv.org/abs/2506.18985</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, visual question answering, interpretability, cross-modal reasoning, explainable AI<br />
Summary: <br />
The article discusses the challenges in interpreting the visual attention of large vision-language models (LVLMs) during visual question answering tasks. To address this issue, the authors propose a framework called GLIMPSE that attributes LVLM outputs to relevant visual evidence and textual signals. GLIMPSE combines gradient-weighted attention, adaptive layer propagation, and relevance-weighted token aggregation to generate response-level heat maps for cross-modal reasoning interpretation. The framework outperforms existing methods in interpretability and enhances human-alignment accuracy. Through GLIMPSE, researchers can gain insights into LVLM cross-modal attribution, trace reasoning processes, analyze human-attention misalignment, detect hallucinations, uncover biases, and promote transparency. This analytic explainable AI (XAI) approach provides a comprehensive understanding of LVLM behavior and ensures the reliability and fairness of AI systems. <br /> <div>
arXiv:2506.18985v2 Announce Type: replace-cross 
Abstract: Recent progress in large vision-language models (LVLMs) has advanced the state of the art in visual question answering (VQA). However, interpreting where LVLMs direct their visual attention while generating free-form responses remains a significant challenge, yet is essential for understanding model behavior. We introduce GLIMPSE (Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation), a lightweight, model-agnostic framework that jointly attributes LVLM outputs to the most relevant visual evidence and textual signals supporting open-ended VQA. GLIMPSE fuses gradient-weighted attention, adaptive layer propagation, and relevance-weighted token aggregation to produce holistic response-level heat maps for interpreting cross-modal reasoning, outperforming prior interpretability methods and pushing the state-of-the-art in human-alignment. We demonstrate an analytic explainable AI (XAI) approach using GLIMPSE to uncover fine-grained insights into LVLM cross-modal attribution, trace reasoning dynamics, analyze systematic human-attention misalignment, diagnose hallucination, expose bias, and ensure transparency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education</title>
<link>https://arxiv.org/abs/2506.19107</link>
<guid>https://arxiv.org/abs/2506.19107</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, education, pedagogical prompting, instructional design, interactive learning

Summary:
- The study focuses on the use of large language models (LLMs) in education and the potential impact on student learning outcomes.
- The concept of pedagogical prompting is introduced as a method to improve learning-oriented responses from LLMs.
- The study conducted formative surveys with instructors teaching early undergraduate CS courses to inform the instructional design.
- An interactive learning intervention was developed to train students in pedagogical prompting skills.
- The evaluation of the intervention showed significant improvements in learners' LLM-based pedagogical help-seeking skills, positive attitudes towards the system, and increased willingness to use pedagogical prompts in the future.
- The study contributes a theoretical framework of pedagogical prompting, empirical insights on instructor attitudes, and a scalable approach for implementing LLM-based help-seeking in classrooms. 
Summary: <br /><br /> <div>
arXiv:2506.19107v2 Announce Type: replace-cross 
Abstract: With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2506.19753</link>
<guid>https://arxiv.org/abs/2506.19753</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic dialects, QADI dataset, RNN models, Transformer models, MARBERTv2. 

Summary:<br /><br />
This study focuses on classifying 18 Arabic dialects using the QADI dataset of Arabic tweets. Various models such as RNN, Transformer, and large language models (LLMs) were utilized, with MARBERTv2 achieving the highest accuracy of 65% and F1-score of 64%. The research showcases the effectiveness of state-of-the-art preprocessing techniques and NLP models in addressing significant linguistic challenges in Arabic dialect identification. The findings indicate potential applications in personalized chatbots that can communicate in users' dialects, social media monitoring, and enhanced accessibility for Arabic-speaking communities. This study highlights the importance of leveraging advanced technologies to overcome language barriers and facilitate better communication and understanding among diverse Arabic dialects. <div>
arXiv:2506.19753v2 Announce Type: replace-cross 
Abstract: The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in users' dialects, social media monitoring, and greater accessibility for Arabic communities.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Human-Like Planning via LLMs</title>
<link>https://arxiv.org/abs/2506.22604</link>
<guid>https://arxiv.org/abs/2506.22604</guid>
<content:encoded><![CDATA[
<div> Keywords: robot, end-user programming, natural language, drag-and-drop interfaces, language model

Summary: 
This paper explores the combination of natural language programming and drag-and-drop interfaces for specifying tasks for robots. By using a large language model (LLM) pipeline, the system accepts natural language input and generates human-like action sequences at a detailed level. The study compares the generated action sequences to hand-specified ones and found that larger models tend to outperform smaller ones in producing human-like actions. However, even smaller models can achieve satisfactory performance. This research demonstrates the potential for combining intuitive natural language communication with precise drag-and-drop interfaces in robot programming, offering accessible means for end-users to specify tasks effectively. <br /><br />Summary: <div>
arXiv:2506.22604v1 Announce Type: new 
Abstract: Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ludax: A GPU-Accelerated Domain Specific Language for Board Games</title>
<link>https://arxiv.org/abs/2506.22609</link>
<guid>https://arxiv.org/abs/2506.22609</guid>
<content:encoded><![CDATA[
<div> game description languages, reinforcement learning, hardware acceleration, Ludax framework, board games

Summary:
The article introduces Ludax, a domain-specific language for board games that automatically compiles into hardware-accelerated code. Ludax combines the generality of game description languages with the speed of modern parallel processing hardware, enhancing deep learning pipelines. The framework aims to accelerate games research by enabling rapid simulation and providing a flexible representation scheme. The article provides a detailed breakdown of Ludax's description language, technical notes on the compilation process, speed benchmarking, and a demonstration of training RL agents. Ludax, along with implementations of existing board games, is openly available for use. <div>
arXiv:2506.22609v1 Announce Type: new 
Abstract: Games have long been used as benchmarks and testing environments for research in artificial intelligence. A key step in supporting this research was the development of game description languages: frameworks that compile domain-specific code into playable and simulatable game environments, allowing researchers to generalize their algorithms and approaches across multiple games without having to manually implement each one. More recently, progress in reinforcement learning (RL) has been largely driven by advances in hardware acceleration. Libraries like JAX allow practitioners to take full advantage of cutting-edge computing hardware, often speeding up training and testing by orders of magnitude. Here, we present a synthesis of these strands of research: a domain-specific language for board games which automatically compiles into hardware-accelerated code. Our framework, Ludax, combines the generality of game description languages with the speed of modern parallel processing hardware and is designed to fit neatly into existing deep learning pipelines. We envision Ludax as a tool to help accelerate games research generally, from RL to cognitive science, by enabling rapid simulation and providing a flexible representation scheme. We present a detailed breakdown of Ludax's description language and technical notes on the compilation process, along with speed benchmarking and a demonstration of training RL agents. The Ludax framework, along with implementations of existing board games, is open-source and freely available.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URSA: The Universal Research and Scientific Agent</title>
<link>https://arxiv.org/abs/2506.22653</link>
<guid>https://arxiv.org/abs/2506.22653</guid>
<content:encoded><![CDATA[
<div> agents, AI, scientific research, language models, URSA

Summary:
The article discusses the use of Large Language Models (LLMs) in scientific research tasks, showcasing their potential to revolutionize modern science. It introduces URSA, a scientific agent ecosystem that combines modular agents and tools, including advanced physics simulation codes, to tackle varied scientific problems effectively. By employing LLMs in "agentic" AI, URSA aims to accelerate research tasks and remove bottlenecks to progress in cutting-edge research. The architecture of URSA is highlighted, along with examples demonstrating the system's potential for complex problem-solving. Ultimately, URSA presents a promising approach to enhancing scientific research by leveraging the capabilities of LLMs and AI technologies. <div>
arXiv:2506.22653v1 Announce Type: new 
Abstract: Large language models (LLMs) have moved far beyond their initial form as simple chatbots, now carrying out complex reasoning, planning, writing, coding, and research tasks. These skills overlap significantly with those that human scientists use day-to-day to solve complex problems that drive the cutting edge of research. Using LLMs in "agentic" AI has the potential to revolutionize modern science and remove bottlenecks to progress. In this work, we present URSA, a scientific agent ecosystem for accelerating research tasks. URSA consists of a set of modular agents and tools, including coupling to advanced physics simulation codes, that can be combined to address scientific problems of varied complexity and impact. This work highlights the architecture of URSA, as well as examples that highlight the potential of the system.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explanations are a means to an end</title>
<link>https://arxiv.org/abs/2506.22740</link>
<guid>https://arxiv.org/abs/2506.22740</guid>
<content:encoded><![CDATA[
<div> Keywords: explainable machine learning, statistical decision theory, clinical decision support, model evaluation, expected explanation use

Summary: 
This paper emphasizes the importance of designing and evaluating explanations in machine learning with a specific end goal in mind rather than just describing how models work. By using a framework based on statistical decision theory, the authors aim to prevent misuse of explanations and provide a boost in performance for decision-makers in various use cases such as clinical decision support and debugging. They propose a functionally-grounded approach that forces researchers to specify concrete use cases for evaluation, combining theoretical and empirical perspectives to analyze the value of explanations. The paper introduces definitions that bridge these perspectives and demonstrates how this approach can help quantify the maximum improvement in performance that explanations can offer to decision-makers. <div>
arXiv:2506.22740v1 Announce Type: new 
Abstract: Modern methods for explainable machine learning are designed to describe how models map inputs to outputs--without deep consideration of how these explanations will be used in practice. This paper argues that explanations should be designed and evaluated with a specific end in mind. We describe how to formalize this end in a framework based in statistical decision theory. We show how this functionally-grounded approach can be applied across diverse use cases, such as clinical decision support, providing recourse, or debugging. We demonstrate its use to characterize the maximum "boost" in performance on a particular task that an explanation could provide an idealized decision-maker, preventing misuse due to ambiguity by forcing researchers to specify concrete use cases that can be analyzed in light of models of expected explanation use. We argue that evaluation should meld theoretical and empirical perspectives on the value of explanation, and contribute definitions that span these perspectives.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems</title>
<link>https://arxiv.org/abs/2506.22774</link>
<guid>https://arxiv.org/abs/2506.22774</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Trustworthy AI, ethics, algorithmic processes, assessment method 

Summary: 
The paper discusses the challenges and impact of Artificial Intelligence (AI) technology on society, emphasizing the need to address its potential negative consequences. It highlights the complexity and widespread integration of AI systems, leading to a reliance on technologies beyond human oversight. While existing guidelines offer a holistic view of Trustworthy AI, they lack quantification techniques. On the other hand, technological tools focus on specific aspects and lack a holistic perspective. The paper proposes an assessment method that combines ethical components with algorithmic processes like PageRank and TrustRank to quantify trustworthiness objectively. By integrating algorithmic criteria, the approach strives to minimize subjectivity in self-assessment techniques prevalent in the field. This method demonstrates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering relevant ethical guidelines. 

<br /><br />Summary: <div>
arXiv:2506.22774v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges posed by human-made artifacts, particularly those widely integrated into society and exert significant influence, highlighting potential benefits and their negative consequences. While other technologies may also pose substantial risks, AI's pervasive reach makes its societal effects especially profound. The complexity of AI systems, coupled with their remarkable capabilities, can lead to a reliance on technologies that operate beyond direct human oversight or understanding. To mitigate the risks that arise, several theoretical tools and guidelines have been developed, alongside efforts to create technological tools aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view of the issue but fail to provide techniques for quantifying trustworthiness. Conversely, while technological tools are better at achieving such quantification, they lack a holistic perspective, focusing instead on specific aspects of Trustworthy AI. This paper aims to introduce an assessment method that combines the ethical components of Trustworthy AI with the algorithmic processes of PageRank and TrustRank. The goal is to establish an assessment framework that minimizes the subjectivity inherent in the self-assessment techniques prevalent in the field by introducing algorithmic criteria. The application of our approach indicates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering the theoretical content of relevant guidelines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models</title>
<link>https://arxiv.org/abs/2506.22865</link>
<guid>https://arxiv.org/abs/2506.22865</guid>
<content:encoded><![CDATA[
<div> ReasonBridge, hierarchical knowledge distillation, reasoning traces, transfer learning, adapter architecture <br />
Summary:<br />
The paper introduces ReasonBridge, a methodology that transfers reasoning capabilities from closed-source to open-source Large Language Models (LLMs). It uses a hierarchical distillation framework and a tailored dataset, Reason1K, with 1,000 diverse reasoning traces. ReasonBridge efficiently enhances reasoning in open-source models by up to 23% on benchmark tasks, narrowing the performance gap with closed-source models. It incorporates a sparse reasoning-focused adapter architecture and a test-time compute scaling mechanism for guided inference interventions. The enhanced Qwen2.5-14B model outperforms Claude-Sonnet3.5 on MATH500 and matches its performance on AIME problems. The methodology is effective across various reasoning domains and model architectures, offering a sample-efficient approach to enhancing instruction following capabilities. <br /> <div>
arXiv:2506.22865v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have revealed a significant performance gap between closed-source and open-source models, particularly in tasks requiring complex reasoning and precise instruction following. This paper introduces ReasonBridge, a methodology that efficiently transfers reasoning capabilities from powerful closed-source to open-source models through a novel hierarchical knowledge distillation framework. We develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning traces emphasizing difficulty, diversity, and quality. These traces are filtered from across multiple domains using a structured multi-criteria selection algorithm. Our transfer learning approach incorporates: (1) a hierarchical distillation process capturing both strategic abstraction and tactical implementation patterns, (2) a sparse reasoning-focused adapter architecture requiring only 0.3% additional trainable parameters, and (3) a test-time compute scaling mechanism using guided inference interventions. Comprehensive evaluations demonstrate that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks, significantly narrowing the gap with closed-source models. Notably, the enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its performance on competition-level AIME problems. Our methodology generalizes effectively across diverse reasoning domains and model architectures, establishing a sample-efficient approach to reasoning enhancement for instruction following.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Enterprise: AI-Centric User to User-Centric AI</title>
<link>https://arxiv.org/abs/2506.22893</link>
<guid>https://arxiv.org/abs/2506.22893</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Enterprises, Decision-making, Agents, User-Centric AI
 
Summary:
The paper discusses the impact of Artificial Intelligence (AI) on enterprise decision-making, highlighting the potential for AI to enhance productivity in various business functions. It emphasizes the importance of Agents imbued with AI to facilitate decision-making processes in enterprises. The authors propose six tenets for successful implementation of AI in enterprises, emphasizing the need for a shift towards User-Centric AI. They suggest market mechanisms for platforms to align the design and delivery of AI by Agents to meet the needs of enterprise users. Overall, the paper underscores the role of AI in transforming decision-making processes in enterprises and advocates for a user-centered approach in the development and implementation of AI technologies. 

Summary: <br /><br />The paper discusses the impact of Artificial Intelligence (AI) on enterprise decision-making, highlighting the potential for AI to enhance productivity in various business functions. It emphasizes the importance of Agents imbued with AI to facilitate decision-making processes in enterprises. The authors propose six tenets for successful implementation of AI in enterprises, emphasizing the need for a shift towards User-Centric AI. They suggest market mechanisms for platforms to align the design and delivery of AI by Agents to meet the needs of enterprise users. Overall, the paper underscores the role of AI in transforming decision-making processes in enterprises and advocates for a user-centered approach in the development and implementation of AI technologies.  <div>
arXiv:2506.22893v1 Announce Type: new 
Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here. Or, so it seems over the last three years. AI has the potential to impact many areas of human life - personal, social, health, education, professional. In this paper, we take a closer look at the potential of AI for Enterprises, where decision-making plays a crucial and repeated role across functions, tasks, and operations. We consider Agents imbued with AI as means to increase decision-productivity of enterprises. We highlight six tenets for Agentic success in enterprises, by drawing attention to what the current, AI-Centric User paradigm misses, in the face of persistent needs of and usefulness for Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we offer six tenets and promote market mechanisms for platforms, aligning the design of AI and its delivery by Agents to the cause of enterprise users.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning</title>
<link>https://arxiv.org/abs/2506.22919</link>
<guid>https://arxiv.org/abs/2506.22919</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixture-of-Experts, Heterogeneity, Sparse Gating, Specialization, Conditional Computation

Summary: 
Hecto introduces a novel Mixture-of-Experts (MoE) architecture with architectural heterogeneity, combining a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism. The model performs well on multiple benchmarks such as AG News, SST-2, HotpotQA, and STS-B, showing specialized expert alignment to distinct reasoning types. Despite receiving isolated input representations, Hecto matches or closely trails homogeneous baselines in performance. It exhibits improved performance at larger batch sizes due to relaxed computational constraints, allowing its heterogeneous architecture to optimize effectively. Ablation results confirm that architectural diversity is crucial for Hecto's stability and interpretability across diverse reasoning tasks. Hecto serves as a benchmark for conditional computation, offering a principled framework for specialized reasoning in low-resource settings, emphasizing model strength derived from principled specialization. 

<br /><br />Summary: <div>
arXiv:2506.22919v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing inputs to specialized experts, but these experts rely on identical inductive biases, thus limiting representational diversity. This static computation pathway is inefficient for inputs that require different types of reasoning and limits specialization and interpretability. We propose Hecto, a lightweight MoE architecture that leverages architectural heterogeneity by combining a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely trails homogeneous baselines in performance despite receiving isolated input representations, while achieving clear expert specialization, with each expert aligning to distinct reasoning types (temporal vs static). At larger batch sizes, Hecto exhibits improved performance, benefiting from relaxed computational constraints that allow its heterogeneous architecture to optimize more effectively. Ablation results isolate architectural diversity as the source of Hecto's stability and interpretability across diverse reasoning tasks. Overall, Hecto establishes itself as a new benchmark for conditional computation, offering a principled framework for specialized reasoning in low-resource regimes with its model strength derived from principled specialization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Rationality in the Reasoning Process of Language Models through Self-playing Game</title>
<link>https://arxiv.org/abs/2506.22920</link>
<guid>https://arxiv.org/abs/2506.22920</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, self-play, reasoning, Critic-Discernment Game, comprehension

Summary:
Large language models (LLMs) have shown strong reasoning abilities in tasks like math and coding but lack true comprehension of their reasoning processes. This paper proposes using self-play through a Critic-Discernment Game (CDG) to enhance model rationality in reasoning without human supervision. In the CDG, a prover offers a solution to a problem and faces critiques that can aid or mislead. The prover's objective is to maintain correct answers in the face of misleading feedback and correct errors based on constructive criticism. Experiments on tasks involving mathematical reasoning, error detection, self-correction, and long-chain reasoning show that CDG training significantly improves LLMs' ability to understand their reasoning processes. This approach could advance the capabilities of AI models to reason and comprehend complex problems autonomously. 

<br /><br />Summary: Large language models have strong reasoning abilities but lack comprehension of their processes. To address this, the study proposes using self-play with a Critic-Discernment Game to enhance model rationality in reasoning without human supervision. The game involves a prover offering solutions to problems while receiving critiques that can assist or mislead. By maintaining correct answers and correcting errors based on feedback, LLMs can improve their reasoning abilities significantly in tasks like mathematical reasoning and error detection. This approach has the potential to advance AI models' autonomy in understanding and reasoning through self-play training. <div>
arXiv:2506.22920v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning</title>
<link>https://arxiv.org/abs/2506.22992</link>
<guid>https://arxiv.org/abs/2506.22992</guid>
<content:encoded><![CDATA[
<div> complex reasoning, multimodal language models, MARBLE, multistep plans, perception<br />
Summary:<br />
The article introduces MARBLE, a challenging benchmark for evaluating multimodal language models' ability to reason step-by-step through complex problems. The benchmark includes tasks like M-Portal and M-Cube that require crafting and understanding multistep plans under various constraints. Current multimodal language models exhibit poor performance on MARBLE, with most models achieving near-random accuracy. The study highlights that complex reasoning remains a challenge for existing models, indicating the need for advancements in multimodal language processing. Perception is identified as a bottleneck, as models struggle to extract information from visual inputs effectively. The article aims to inspire the development of next-generation models capable of advanced reasoning and planning across multiple modalities. <br /><br />Summary: <div>
arXiv:2506.22992v1 Announce Type: new 
Abstract: The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks</title>
<link>https://arxiv.org/abs/2506.23049</link>
<guid>https://arxiv.org/abs/2506.23049</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-speech, dialogue, agentic reasoning, open-source, automated tool use<br />
Summary: AURA (Agent for Understanding, Reasoning, and Automated Tool Use) is the first open-source system capable of full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. It combines ASR, TTS, and LLMs in a cascaded pipeline and supports tools like calendar booking and web search. AURA's modular design allows easy integration of new tools through natural language prompts. On VoiceBench, it outperforms other open-weight systems and achieves competitive scores on AlpacaEval. Human evaluation indicates a 90% task success rate on complex, multi-turn speech tasks. With a focus on goal-driven tasks, AURA fills a gap in the current landscape of language and speech technologies. <br /><br />Summary: <div>
arXiv:2506.23049v1 Announce Type: new 
Abstract: Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI's Euclid's Elements Moment: From Language Models to Computable Thought</title>
<link>https://arxiv.org/abs/2506.23080</link>
<guid>https://arxiv.org/abs/2506.23080</guid>
<content:encoded><![CDATA[
<div> Keywords: Evolutionary Framework, Geometry of Cognition, Metalinguistic Moment, Mathematical Symbolism Moment, Formal Logic System Moment

Summary:
The paper presents a five-stage evolutionary framework for understanding the development of artificial intelligence, comparing it to historical human cognitive technologies. It argues that AI is advancing through distinct epochs, with each epoch marked by a revolutionary shift in its representation and reasoning capabilities. The "Geometry of Cognition" framework explains past architectural shifts in AI and provides a path forward. The evolution of AI is viewed as reflexive, reshaping its own architecture as it progresses through stages. The current "Metalinguistic Moment" involves self-reflective capabilities, leading to future stages such as the "Mathematical Symbolism Moment" and the "Formal Logic System Moment." These stages will focus on developing a computable calculus of thought and creating provably aligned and reliable AI. The paper offers a methodological foundation for future research and provides strategies for startups and developers looking to build intelligent systems. 

<br /><br />Summary: <div>
arXiv:2506.23080v1 Announce Type: new 
Abstract: This paper presents a comprehensive five-stage evolutionary framework for understanding the development of artificial intelligence, arguing that its trajectory mirrors the historical progression of human cognitive technologies. We posit that AI is advancing through distinct epochs, each defined by a revolutionary shift in its capacity for representation and reasoning, analogous to the inventions of cuneiform, the alphabet, grammar and logic, mathematical calculus, and formal logical systems. This "Geometry of Cognition" framework moves beyond mere metaphor to provide a systematic, cross-disciplinary model that not only explains AI's past architectural shifts-from expert systems to Transformers-but also charts a concrete and prescriptive path forward. Crucially, we demonstrate that this evolution is not merely linear but reflexive: as AI advances through these stages, the tools and insights it develops create a feedback loop that fundamentally reshapes its own underlying architecture. We are currently transitioning into a "Metalinguistic Moment," characterized by the emergence of self-reflective capabilities like Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the "Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be defined by the development of a computable calculus of thought, likely through neuro-symbolic architectures and program synthesis, culminating in provably aligned and reliable AI that reconstructs its own foundational representations. This work serves as the methodological capstone to our trilogy, which previously explored the economic drivers ("why") and cognitive nature ("what") of AI. Here, we address the "how," providing a theoretical foundation for future research and offering concrete, actionable strategies for startups and developers aiming to build the next generation of intelligent systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study</title>
<link>https://arxiv.org/abs/2506.23107</link>
<guid>https://arxiv.org/abs/2506.23107</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, risky decision-making, simulation, ChatGPT, Constant Relative Risk Aversion

Summary: 
- The study examines the ability of large language models (LLMs) to simulate risky decision-making scenarios.
- LLMs, ChatGPT 40 and ChatGPT 01-mini, were compared to actual human responses in lottery-based tasks using transportation survey data from four cities.
- Both models exhibited more risk-averse behavior than humans, with ChatGPT 01-mini aligning closer to human decisions.
- Analysis of multilingual data from Nanjing and Hong Kong showed that model predictions in Chinese deviated more from actual responses than in English, indicating the influence of prompt language on simulation performance.
- The findings showcase the potential and current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural contexts.

<br /><br />Summary: <div>
arXiv:2506.23107v1 Announce Type: new 
Abstract: Large language models (LLMs) have made significant strides, extending their applications to dialogue systems, automated content creation, and domain-specific advisory tasks. However, as their use grows, concerns have emerged regarding their reliability in simulating complex decision-making behavior, such as risky decision-making, where a single choice can lead to multiple outcomes. This study investigates the ability of LLMs to simulate risky decision-making scenarios. We compare model-generated decisions with actual human responses in a series of lottery-based tasks, using transportation stated preference survey data from participants in Sydney, Dhaka, Hong Kong, and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework. Results show that both models exhibit more risk-averse behavior than human participants, with o1-mini aligning more closely with observed human decisions. Further analysis of multilingual data from Nanjing and Hong Kong indicates that model predictions in Chinese deviate more from actual responses compared to English, suggesting that prompt language may influence simulation performance. These findings highlight both the promise and the current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural settings.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy</title>
<link>https://arxiv.org/abs/2506.23123</link>
<guid>https://arxiv.org/abs/2506.23123</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, foundation models, societal impact, AI policy, AI governance

Summary: 
This dissertation explores the intersection of technology and society in the era of artificial intelligence (AI). It delves into the conceptual framing of foundation models, discussing their capabilities, risks, and their role within the broader economy. The empirical insights provided focus on transparency, highlighting the importance of evaluations at the model level and indexes at the organization level. The transition from understanding to action is emphasized, with a call for evidence-based AI policy to address the societal impact of foundation models. By building scientific foundations and enhancing the research-policy interface, the dissertation aims to contribute towards achieving better societal outcomes in the age of AI. Overall, it provides a comprehensive framework for understanding and governing AI technology for the benefit of society.<br /><br />Summary: <div>
arXiv:2506.23123v1 Announce Type: new 
Abstract: Artificial intelligence is humanity's most promising technology because of the remarkable capabilities offered by foundation models. Yet, the same technology brings confusion and consternation: foundation models are poorly understood and they may precipitate a wide array of harms. This dissertation explains how technology and society coevolve in the age of AI, organized around three themes. First, the conceptual framing: the capabilities, risks, and the supply chain that grounds foundation models in the broader economy. Second, the empirical insights that enrich the conceptual foundations: transparency created via evaluations at the model level and indexes at the organization level. Finally, the transition from understanding to action: superior understanding of the societal impact of foundation models advances evidence-based AI policy. View together, this dissertation makes inroads into achieving better societal outcomes in the age of AI by building the scientific foundations and research-policy interface required for better AI governance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons</title>
<link>https://arxiv.org/abs/2506.23128</link>
<guid>https://arxiv.org/abs/2506.23128</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Relational Reasoning, Benchmark tasks, DeepSeek-R1, GPT-4o

Summary:
Large Language Models (LLMs) like DeepSeek-R1, DeepSeek-V3, and GPT-4o were evaluated for their relational reasoning capabilities through benchmark tasks. DeepSeek-R1 showed the highest F1-scores across tasks, indicating proficiency in logical deduction and relational inference. However, all models, including DeepSeek-R1, struggled with complex problems due to token length limitations and incomplete output structures. DeepSeek-R1's longer Chain-of-Thought responses revealed unique planning and verification strategies but also inconsistencies in reasoning. There is a need for further analysis of LLMs' internal inference dynamics to improve reasoning abilities. Future work should focus on multimodal reasoning and systematic examination of reasoning failures. The study provides empirical insights and theoretical implications for enhancing LLMs' reasoning abilities, especially in tasks requiring structured, multi-step logical inference.

<br /><br />Summary: Large Language Models were evaluated for relational reasoning, with DeepSeek-R1 outperforming others. However, all models faced challenges in complex problems due to limitations in token length and output structures. DeepSeek-R1's unique planning and verification strategies were highlighted, but inconsistencies in reasoning were noted, calling for deeper scrutiny. Future research should explore multimodal reasoning and analyze reasoning failures systematically to improve LLMs' relational reasoning abilities. <div>
arXiv:2506.23128v1 Announce Type: new 
Abstract: How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMs' internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMs' reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing</title>
<link>https://arxiv.org/abs/2506.23141</link>
<guid>https://arxiv.org/abs/2506.23141</guid>
<content:encoded><![CDATA[
<div> semantic-aware relational message passing, knowledge graph completion, top-K neighbor selection strategy, multi-head attention aggregator, link prediction task

Summary:
- The article proposes a semantic-aware relational message passing framework for Knowledge Graph Completion (KGC).
- Traditional node-based message passing mechanisms in knowledge graphs tend to introduce noise and suffer from information dilution or over-smoothing.
- The framework introduces a semantic-aware Top-K neighbor selection strategy to select only the most relevant edges for information aggregation.
- Information from the selected edges is fused with the central node's representation using a multi-head attention aggregator to generate a semantically focused node message.
- The model effectively captures and propagates contextual information most relevant to the link prediction task, improving performance on established benchmarks. 

<br /><br />Summary: <div>
arXiv:2506.23141v1 Announce Type: new 
Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a \textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a \textbf{multi-head attention aggregator} to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rises for Measuring Local Distributivity in Lattices</title>
<link>https://arxiv.org/abs/2506.23168</link>
<guid>https://arxiv.org/abs/2506.23168</guid>
<content:encoded><![CDATA[
<div> distributivity, lattice theory, Formal Concept Analysis, concept lattices, rises<br />
<br />
Summary: 
In the paper, the concept of rises in concept lattices is introduced to measure distributivity, a well-studied notion in lattice theory. Rises assess changes in covering concepts within a lattice and determine distributivity based on the presence of non-unit rises. The study establishes that a lattice is distributive if non-unit rises are absent, connecting rises to meet- and join distributivity. Analysis of real-world data shows high join distributivity but lower meet distributivity in concept lattices. The manifestation of join distributivity on ordered sets is also explored, highlighting the relationship between distributivity and lattice structures. <div>
arXiv:2506.23168v1 Announce Type: new 
Abstract: Distributivity is a well-established and extensively studied notion in lattice theory. In the context of data analysis, particularly within Formal Concept Analysis (FCA), lattices are often observed to exhibit a high degree of distributivity. However, no standardized measure exists to quantify this property. In this paper, we introduce the notion of rises in (concept) lattices as a means to assess distributivity. Rises capture how the number of attributes or objects in covering concepts change within the concept lattice. We show that a lattice is distributive if and only if no non-unit rises occur. Furthermore, we relate rises to the classical notion of meet- and join distributivity. We observe that concept lattices from real-world data are to a high degree join-distributive, but much less meet-distributive. We additionally study how join-distributivity manifests on the level of ordered sets.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis</title>
<link>https://arxiv.org/abs/2506.23273</link>
<guid>https://arxiv.org/abs/2506.23273</guid>
<content:encoded><![CDATA[
<div> pipeline, text2sql, financial statements, natural language queries, FinStat2SQL

Summary:
FinStat2SQL is a lightweight text2sql pipeline designed for natural language queries over financial statements, particularly suited for entities following local standards like VAS. It employs a multi-agent setup combining large and small language models for entity extraction, SQL generation, and self-correction. The system includes a domain-specific database and is evaluated on a synthetic QA dataset. A fine-tuned 7B model used in FinStat2SQL achieves an accuracy rate of 61.33% with response times under 4 seconds on consumer hardware, outperforming GPT-4o-mini. This solution offers a scalable and cost-efficient way for financial analysis, enabling Vietnamese enterprises to access AI-powered querying capabilities. 

<br /><br />Summary: <div>
arXiv:2506.23273v1 Announce Type: new 
Abstract: Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games</title>
<link>https://arxiv.org/abs/2506.23276</link>
<guid>https://arxiv.org/abs/2506.23276</guid>
<content:encoded><![CDATA[
<div> cooperation, large language models, social dilemmas, costly sanctioning, behavioral patterns

Summary: 
The study focuses on how large language models (LLMs) navigate social dilemmas, specifically the challenge of costly sanctioning in multi-agent systems. By adapting a public goods game, different behavioral patterns among LLMs are observed: some consistently cooperate, some fluctuate, some decline over time, and some rigidly follow fixed strategies. Interestingly, reasoning LLMs struggle with cooperation, while traditional LLMs consistently achieve high levels of cooperation. These findings suggest that enhancing reasoning capabilities in LLMs may not necessarily lead to cooperation. Insights from this study are valuable for deploying LLM agents in environments that require sustained collaboration. The code for the study is available on GitHub for reference. 

<br /><br />Summary: <div>
arXiv:2506.23276v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATSim: Urban Mobility Simulation with Generative Agents</title>
<link>https://arxiv.org/abs/2506.23306</link>
<guid>https://arxiv.org/abs/2506.23306</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-based urban mobility simulation, generative agents, AI technology, transportation, behavioral diversity 

Summary: 
GATSim (Generative-Agent Transport Simulation) is a new framework that utilizes recent advancements in AI technology to create generative agents with rich behavioral characteristics for urban mobility simulation. These agents have diverse socioeconomic attributes, individual lifestyles, and evolving preferences that influence their mobility decisions. The framework combines an urban mobility foundation model with agent cognitive systems and a transport simulation environment. The implementation includes psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms for the agents. Experiments demonstrated that generative agents can produce believable travel behaviors and realistic traffic evolution patterns. The prototype system code is open source and available on GitHub at https://github.com/qiliuchn/gatsim.<br /><br />Summary: <div>
arXiv:2506.23306v1 Announce Type: new 
Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based systems that fail to capture the complexity, adaptability, and behavioral diversity characteristic of human travel decision-making. Recent advances in large language models and AI agent technology offer opportunities to create agents with reasoning capabilities, persistent memory, and adaptive learning mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advances to create generative agents with rich behavioral characteristics for urban mobility simulation. Unlike conventional approaches, GATSim agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences that shape their mobility decisions through psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms. The main contributions of this study include: (1) a comprehensive architecture combining an urban mobility foundation model with agent cognitive systems and transport simulation environment, (2) a fully functional prototype implementation, and (3) systematic validation demonstrating that generative agents produce believable travel behaviors. Through designed reflection processes, generative agents in this study can transform specific travel experiences into generalized insights, enabling realistic behavioral adaptation over time with specialized mechanisms for activity planning and real-time reactive behaviors tailored to urban mobility contexts. Experiments show that generative agents perform competitively with human annotators in mobility scenarios while naturally producing macroscopic traffic evolution patterns. The code for the prototype system is shared at https://github.com/qiliuchn/gatsim.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Confidence Paradox: Can LLM Know When It's Wrong</title>
<link>https://arxiv.org/abs/2506.23464</link>
<guid>https://arxiv.org/abs/2506.23464</guid>
<content:encoded><![CDATA[
<div> alignment, honesty calibration, ethical responsiveness, model confidence, uncertainty

Summary:
The paper introduces HonestVQA, a self-supervised honesty calibration framework for Document Visual Question Answering (DocVQA) systems. It aims to align model confidence with actual correctness, quantify uncertainty, and enforce ethical response behavior. The framework includes weighted loss functions and contrastive learning to improve accuracy and reduce overconfidence. Two evaluation metrics, Honesty Score (H-Score) and Ethical Confidence Index (ECI), are introduced to benchmark alignment between confidence, accuracy, and ethical communication. Empirical results show that HonestVQA improves DocVQA accuracy and F1 score across different datasets. It reduces overconfidence and demonstrates strong generalization in cross-domain evaluation. Ablation results highlight the importance of alignment and contrastive loss in maintaining accuracy.<br /><br />Summary: <div>
arXiv:2506.23464v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed in real world applications, yet they remain ethically opaque-often producing overconfident answers to ambiguous questions or failing to communicate uncertainty in a trustworthy manner. This misalignment between model confidence and actual knowledge poses significant risks, particularly in domains requiring ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT have advanced SOTA performance by focusing on architectural sophistication and accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised honesty calibration framework for ethically aligned DocVQA. Our model-agnostic method quantifies uncertainty to identify knowledge gaps, aligns model confidence with actual correctness using weighted loss functions, and enforces ethical response behavior via contrastive learning. We further introduce two principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3% and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score, demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy without alignment or contrastive loss.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.23503</link>
<guid>https://arxiv.org/abs/2506.23503</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive Behavioral Therapy, Social Media, Sentiment Analysis, Text Summarization, Mental Health Disorders

Summary:
The article introduces a new Cognitive Behavioral Therapy (CBT) framework that utilizes advanced technology to analyze social media data for detecting negative emotions and cognitive distortions. The system employs state-of-the-art models such as BERT, RoBERTa, T5, PEGASUS, and mT5 for sentiment analysis, text summarization, and text translation in multiple languages. Unlike existing models, this framework not only identifies negative thoughts but also predicts additional negative side effects and potential mental health disorders like phobias and eating disorders. By offering a more comprehensive understanding and intervention strategy, the system provides psychotherapists with a powerful tool for early detection and treatment of various psychological issues.

<br /><br />Summary: Cognitive Behavioral Therapy framework using advanced technology to analyze social media data for detecting negative emotions and cognitive distortions. The system predicts additional negative side effects and potential mental health disorders, offering a comprehensive intervention strategy for early detection and treatment. <div>
arXiv:2506.23503v1 Announce Type: new 
Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the irrational thought patterns associated with mental health disorders, but its effectiveness relies on accurately identifying cognitive pathways to provide targeted treatment. In today's digital age, individuals often express negative emotions on social media, where they may reveal cognitive distortions, and in severe cases, exhibit suicidal tendencies. However, there is a significant gap in methodologies designed to analyze these cognitive pathways, which could be critical for psychotherapists aiming to deliver timely and effective interventions in online environments. Cognitive Behavioral Therapy (CBT) framework leveraging acceptance, commitment and data augmentation to categorize and address both textual and visual content as positive or negative. Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5, PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages focusing on detecting negative emotions and cognitive distortions within social media data. While existing models are primarily designed to identify negative thoughts, the proposed system goes beyond this by predicting additional negative side effects and other potential mental health disorders likes Phobias, Eating Disorders. This enhancement allows for a more comprehensive understanding and intervention strategy, offering psychotherapists a powerful tool for early detection and treatment of various psychological issues.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM</title>
<link>https://arxiv.org/abs/2506.23504</link>
<guid>https://arxiv.org/abs/2506.23504</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, hybrid model, electricity prices, AlexNet, LSTM

Summary: 
This study introduces a new hybrid model combining AlexNet and LSTM algorithms for more accurate electricity price forecasting. Traditional methods often fail to accurately predict prices due to a lack of consideration for external variables. By incorporating features such as demand, temperature, sunlight, and rain, the hybrid model improves prediction accuracy. Through methods like minimum-maximum scaling and time windowing, the model predicts future electricity prices based on past data. Results show the hybrid model outperforms standalone models RNN and ANN, achieving an accuracy rating of 97.08 compared to 96.64 and 96.63, respectively. The combination of AlexNet's feature extraction and LSTM's sequential pattern learning significantly enhances the forecasting capabilities of the model. <br /><br />Summary: <div>
arXiv:2506.23504v1 Announce Type: new 
Abstract: The recent development of advanced machine learning methods for hybrid models has greatly addressed the need for the correct prediction of electrical prices. This method combines AlexNet and LSTM algorithms, which are used to introduce a new model with higher accuracy in price forecasting. Despite RNN and ANN being effective, they often fail to deal with forex time sequence data. The traditional methods do not accurately forecast the prices. These traditional methods only focus on demand and price which leads to insufficient analysis of data. To address this issue, using the hybrid approach, which focuses on external variables that also effect the predicted prices. Nevertheless, due to AlexNet's excellent feature extraction and LSTM's learning sequential patterns, the prediction accuracy is vastly increased. The model is built on the past data, which has been supplied with the most significant elements like demand, temperature, sunlight, and rain. For example, the model applies methods, such as minimum-maximum scaling and a time window, to predict the electricity prices of the future. The results show that this hybrid model is good than the standalone ones in terms of accuracy. Although we got our accuracy rating of 97.08, it shows higher accompaniments than remaining models RNN and ANN with accuracies of 96.64 and 96.63 respectively.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays</title>
<link>https://arxiv.org/abs/2506.23517</link>
<guid>https://arxiv.org/abs/2506.23517</guid>
<content:encoded><![CDATA[
<div> AI detection tools, GPTZero, QuillBot, reliability, AI-generated texts
Summary:
GPTZero, a commonly used AI detector, was tested for its accuracy in identifying AI-generated texts of varying lengths. The study included short, medium, and long essays, both AI-generated and human-written. Results showed GPTZero successfully detected AI content with high accuracy, but had limitations in distinguishing human-authored texts, leading to some false positives. Educators are advised to be cautious when relying solely on AI detection tools for detecting AI-generated content in student submissions. <br /><br />Summary: <div>
arXiv:2506.23517v1 Announce Type: new 
Abstract: As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain. In our study, we focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count). We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence. A vast majority of the AI-generated papers were detected accurately (ranging from 91-100% AI believed generation), while the human generated essays fluctuated; there were a handful of false positives. These findings suggest that although GPTZero is effective at detecting purely AI-generated content, its reliability in distinguishing human-authored texts is limited. Educators should therefore exercise caution when relying solely on AI detection tools.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data</title>
<link>https://arxiv.org/abs/2506.23520</link>
<guid>https://arxiv.org/abs/2506.23520</guid>
<content:encoded><![CDATA[
<div> Keywords: robotic synthesis, automated extraction, large language model, data framework, chemical procedures<br />
Summary:<br />
ChemActor is a large language model designed for automated extraction of chemical procedures from literature, addressing challenges in ambiguity and high annotation costs. It utilizes a sequential framework with a data selection module to generate structured action sequences from unstructured experimental procedures. Through a multi-round LLMs circle review metric, ChemActor demonstrates an advanced understanding of chemical experimental procedures. Experiments on reaction-to-description and description-to-action tasks show that ChemActor, augmented by LLM-generated data, achieves superior performance over baseline models by 10%. The code for ChemActor is available for access on GitHub. This innovation signifies a significant step forward in the field of robotic synthesis and computer-aided extraction of chemical protocols. <br />Summary: <div>
arXiv:2506.23520v1 Announce Type: new 
Abstract: With the increasing interest in robotic synthesis in the context of organic chemistry, the automated extraction of chemical procedures from literature is critical. However, this task remains challenging due to the inherent ambiguity of chemical language and the high cost of human annotation required for developing reliable computer-aided extraction protocols. Here, we present ChemActor, a fully fine-tuned large language model (LLM), as a chemical executor to convert between unstructured experimental procedures and structured action sequences. We propose a sequential LLM-generated data framework to address the challenges of insufficient and low-quality annotated data. This framework integrates a data selection module that selects data based on distribution divergence, with a general-purpose LLM, to generate machine-executable actions from a single molecule input. Additionally, we introduce a novel multi-round LLMs circle review metric, which reflects the model's advanced understanding of chemical experimental procedures. Extensive experiments on reaction-to-description (R2D) and description-to-action (D2A) tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves state-of-the-art performance, outperforming the baseline model by 10%. The code is available at: https://github.com/Zhanghahah/ChemActor.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CooT: Learning to Coordinate In-Context with Coordination Transformers</title>
<link>https://arxiv.org/abs/2506.23549</link>
<guid>https://arxiv.org/abs/2506.23549</guid>
<content:encoded><![CDATA[
<div> Coordination Transformers, multi-agent systems, dynamic environments, adaptation, interaction histories <br />
<br />
Summary: Coordination among artificial agents in uncertain environments is a challenge in multi-agent systems. Coordination Transformers (CooT) is a new framework that adapts to new partners rapidly using recent interaction histories. Unlike existing methods, CooT focuses on predicting actions aligned with observed partner interactions, rather than just increasing training partner diversity. Trained on diverse agent pairs' interaction trajectories, CooT learns effective coordination strategies without explicit supervision. Evaluation on the Overcooked benchmark shows CooT outperforms baseline methods with unseen partners. Human evaluations confirm CooT as the most effective collaborative partner. Ablations demonstrate CooT's robustness, flexibility, and sensitivity to context in multi-agent scenarios. <br /><br />Summary: <div>
arXiv:2506.23549v1 Announce Type: new 
Abstract: Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI</title>
<link>https://arxiv.org/abs/2506.23563</link>
<guid>https://arxiv.org/abs/2506.23563</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal large language models, reasoning, evaluation, MMReason

Summary:
MMReason introduces a new benchmark, designed to evaluate the long-chain reasoning capabilities of Multimodal Large Language Models (MLLMs). The benchmark addresses shortcomings in existing models by providing diverse, challenging questions from various fields and difficulty levels. These questions are curated to prevent guessability and memorization, ensuring robust reasoning evaluations. An open-ended format and multi-model voting technique are used to filter out shortcut cases. Detailed step-by-step solutions are provided, and a ternary scoring mechanism assesses intermediate reasoning steps accurately. MMReason aims to advance research on MLLM reasoning capabilities and provides an in-depth analysis of popular models. The code for MMReason will be available on GitHub, offering a valuable resource for researchers in the field. <br /><br />Summary:  <div>
arXiv:2506.23563v1 Announce Type: new 
Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at https://github.com/HJYao00/MMReason.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models</title>
<link>https://arxiv.org/abs/2506.23576</link>
<guid>https://arxiv.org/abs/2506.23576</guid>
<content:encoded><![CDATA[
<div> Jailbreaking attacks, large language models, multi-agent systems, AutoDefense, Deepleaps.<br />
<br />Summary: This study explores the use of multi-agent large language model systems to defend against jailbreaking attacks. Three jailbreaking strategies, including AutoDefense and two from Deepleaps, were evaluated in single-agent setups and compared to two- and three-agent configurations. The results indicate that multi-agent systems improve resistance to jailbreaks, particularly in reducing false negatives. However, the effectiveness varies based on the type of attack and introduces trade-offs such as increased false positives and computational overhead. These findings highlight the limitations of current automated defenses and suggest avenues for enhancing alignment robustness in future large language model systems. <br /><br /> <div>
arXiv:2506.23576v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have raised concerns about jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper investigates the use of multi-agent LLM systems as a defence against such attacks. We evaluate three jailbreaking strategies, including the original AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the AutoDefense framework, we compare single-agent setups with two- and three-agent configurations. Our results show that multi-agent systems enhance resistance to jailbreaks, especially by reducing false negatives. However, its effectiveness varies by attack type, and it introduces trade-offs such as increased false positives and computational overhead. These findings point to the limitations of current automated defences and suggest directions for improving alignment robustness in future LLM systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games</title>
<link>https://arxiv.org/abs/2506.23626</link>
<guid>https://arxiv.org/abs/2506.23626</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Game, Reward Function, Automated tuning, Language Model <br />
<br />Summary: 
This study introduces an automated approach for fine-tuning Reinforcement Learning agents' reward function weights in games, addressing challenges in production deployment. By using a Language Model to propose updated weights based on user-defined behavioral goals and performance statistics, the process iteratively refines agent behavior without manual intervention. In a racing task evaluation, the Language Model-guided tuning significantly improved agent performance, with a success rate increase from 9% to 74% in one iteration. Comparison against human expert-tuned agents showed competitive results, with the Language Model-guided agent achieving an 80% success rate and completing laps in 855 time steps by the final iteration. This approach demonstrates the potential for automated reward function tuning to enhance RL agent performance in dynamic game environments. <br /> <div>
arXiv:2506.23626v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) in games has gained significant momentum in recent years, enabling the creation of different agent behaviors that can transform a player's gaming experience. However, deploying RL agents in production environments presents two key challenges: (1) designing an effective reward function typically requires an RL expert, and (2) when a game's content or mechanics are modified, previously tuned reward weights may no longer be optimal. Towards the latter challenge, we propose an automated approach for iteratively fine-tuning an RL agent's reward function weights, based on a user-defined language based behavioral goal. A Language Model (LM) proposes updated weights at each iteration based on this target behavior and a summary of performance statistics from prior training rounds. This closed-loop process allows the LM to self-correct and refine its output over time, producing increasingly aligned behavior without the need for manual reward engineering. We evaluate our approach in a racing task and show that it consistently improves agent performance across iterations. The LM-guided agents show a significant increase in performance from $9\%$ to $74\%$ success rate in just one iteration. We compare our LM-guided tuning against a human expert's manual weight design in the racing task: by the final iteration, the LM-tuned agent achieved an $80\%$ success rate, and completed laps in an average of $855$ time steps, a competitive performance against the expert-tuned agent's peak $94\%$ success, and $850$ time steps.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HASD: Hierarchical Adaption for pathology Slide-level Domain-shift</title>
<link>https://arxiv.org/abs/2506.23673</link>
<guid>https://arxiv.org/abs/2506.23673</guid>
<content:encoded><![CDATA[
<div> Hierarchical Adaptation framework, Slide-level Domain-shift, WSI features, domain adaptation, pathology AI

Summary: The article introduces a new approach, Hierarchical Adaptation framework for Slide-level Domain-shift (HASD), to address the challenge of domain shift in pathology AI. HASD focuses on slide-level domain adaptation, aiming to capture global features necessary in clinical settings. It incorporates a hierarchical adaptation framework that includes a Domain-level Alignment Solver, Slide-level Geometric Invariance Regularization, and Patch-level Attention Consistency Regularization to ensure feature alignment and maintenance of critical diagnostic cues. Additionally, a prototype selection mechanism is used to reduce computational costs. The proposed method is tested on two slide-level tasks across five datasets, demonstrating significant improvements in AUROC and C-index metrics for breast cancer grading and survival prediction tasks. HASD offers a practical and efficient solution for pathology institutions, reducing computational and annotation expenses. 

<br /><br />Summary: <div>
arXiv:2506.23673v1 Announce Type: new 
Abstract: Domain shift is a critical problem for pathology AI as pathology data is heavily influenced by center-specific conditions. Current pathology domain adaptation methods focus on image patches rather than WSI, thus failing to capture global WSI features required in typical clinical scenarios. In this work, we address the challenges of slide-level domain shift by proposing a Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD achieves multi-scale feature consistency and computationally efficient slide-level domain adaptation through two key components: (1) a hierarchical adaptation framework that integrates a Domain-level Alignment Solver for feature alignment, a Slide-level Geometric Invariance Regularization to preserve the morphological structure, and a Patch-level Attention Consistency Regularization to maintain local critical diagnostic cues; and (2) a prototype selection mechanism that reduces computational overhead. We validate our method on two slide-level tasks across five datasets, achieving a 4.1\% AUROC improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in a UCEC survival prediction cohort. Our method provides a practical and reliable slide-level domain adaption solution for pathology institutions, minimizing both computational and annotation costs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pok\'eAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red</title>
<link>https://arxiv.org/abs/2506.23689</link>
<guid>https://arxiv.org/abs/2506.23689</guid>
<content:encoded><![CDATA[
<div> Pok\'eAI, large language model, multi-agent framework, Pok\'emon Red, text-based <br />
<br />
Summary:
The article introduces Pok\'eAI, a multi-agent framework designed to autonomously play Pok\'emon Red. It consists of three agents-Planning, Execution, and Critique-working together in a closed-loop decision-making system. The Planning Agent generates tasks, delegated to the Execution Agent for implementation, and verified by the Critique Agent. A battle module within the Execution Agent achieves an 80.8% win rate in wild encounters. Battle performance correlates with linguistic ability, suggesting a link between language skills and strategic reasoning. Each LLM exhibits a unique playstyle, indicating individual model development of distinct strategies. <div>
arXiv:2506.23689v1 Announce Type: new 
Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model (LLM) framework designed to autonomously play and progress through Pok\'emon Red. Our system consists of three specialized agents-Planning, Execution, and Critique-each with its own memory bank, role, and skill set. The Planning Agent functions as the central brain, generating tasks to progress through the game. These tasks are then delegated to the Execution Agent, which carries them out within the game environment. Upon task completion, the Critique Agent evaluates the outcome to determine whether the objective was successfully achieved. Once verification is complete, control returns to the Planning Agent, forming a closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution Agent. Our results show that the battle AI achieves an average win rate of 80.8% across 50 wild encounters, only 6% lower than the performance of an experienced human player. Furthermore, we find that a model's battle performance correlates strongly with its LLM Arena score on language-related tasks, indicating a meaningful link between linguistic ability and strategic reasoning. Finally, our analysis of gameplay logs reveals that each LLM exhibits a unique playstyle, suggesting that individual models develop distinct strategic behaviors.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models</title>
<link>https://arxiv.org/abs/2506.23692</link>
<guid>https://arxiv.org/abs/2506.23692</guid>
<content:encoded><![CDATA[
<div> Classification, LLM-driven agents, automation, research workflow, AI Scientists
Summary:
Agent for Science (Agent4S) is proposed as the Fifth Scientific Paradigm, utilizing LLM-driven agents to automate the research workflow. A five-level classification for Agent4S is introduced, progressing from basic task automation to fully autonomous, collaborative "AI Scientists." This framework represents a revolutionary leap in scientific discovery, addressing the inefficiencies of the current research paradigm. By leveraging AI technologies, Agent4S aims to streamline and enhance the research process, leading to more efficient and effective scientific endeavors. Through the integration of automated agents, the potential for groundbreaking discoveries and advancements in various fields of science is expanded, laying the groundwork for a future where AI plays a pivotal role in driving scientific progress. <br /><br />Summary: <div>
arXiv:2506.23692v1 Announce Type: new 
Abstract: While AI for Science (AI4S) serves as an analytical tool in the current research paradigm, it doesn't solve its core inefficiency. We propose "Agent for Science" (Agent4S)-the use of LLM-driven agents to automate the entire research workflow-as the true Fifth Scientific Paradigm. This paper introduces a five-level classification for Agent4S, outlining a clear roadmap from simple task automation to fully autonomous, collaborative "AI Scientists." This framework defines the next revolutionary step in scientific discovery.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Perspective On AI Safety Through Control Theory Methodologies</title>
<link>https://arxiv.org/abs/2506.23703</link>
<guid>https://arxiv.org/abs/2506.23703</guid>
<content:encoded><![CDATA[
<div> AI, safety assurance, cyber-physical systems, data-driven control, system theory<br />
<br />
Summary: 
This article discusses the importance of ensuring the safety of AI systems, especially in the context of real-world cyber-physical systems. It proposes a new perspective called data control, which leverages control theory to enhance AI safety. By analyzing the data-generation process and the abstraction by AI systems in a system theory-inspired manner, the article aims to improve safety analysis and assurance in AI engineering. The approach outlined in the article provides a generic foundation for safety analysis that can be adapted for specific AI systems and applications, with the goal of driving innovation in the field of AI safety. <div>
arXiv:2506.23703v1 Announce Type: new 
Abstract: While artificial intelligence (AI) is advancing rapidly and mastering increasingly complex problems with astonishing performance, the safety assurance of such systems is a major concern. Particularly in the context of safety-critical, real-world cyber-physical systems, AI promises to achieve a new level of autonomy but is hampered by a lack of safety assurance. While data-driven control takes up recent developments in AI to improve control systems, control theory in general could be leveraged to improve AI safety. Therefore, this article outlines a new perspective on AI safety based on an interdisciplinary interpretation of the underlying data-generation process and the respective abstraction by AI systems in a system theory-inspired and system analysis-driven manner. In this context, the new perspective, also referred to as data control, aims to stimulate AI engineering to take advantage of existing safety analysis and assurance in an interdisciplinary way to drive the paradigm of data control. Following a top-down approach, a generic foundation for safety analysis and assurance is outlined at an abstract level that can be refined for specific AI systems and applications and is prepared for future innovation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments</title>
<link>https://arxiv.org/abs/2506.23706</link>
<guid>https://arxiv.org/abs/2506.23706</guid>
<content:encoded><![CDATA[
<div> Trusted Execution Environments, Attestable Audits, AI model, confidentiality, benchmarks <br />
<br />
Summary: 
The article introduces the concept of Attestable Audits to enhance the evaluation of AI models on benchmarks, addressing challenges related to verification, safety, and compliance at scale. These audits are designed to operate within Trusted Execution Environments, ensuring secure and confidential verification of interactions with AI models. The proposed approach offers a solution to protecting sensitive data, even in scenarios where trust between the model provider and auditor is lacking. By demonstrating the feasibility of this concept through a prototype implementation on typical audit benchmarks against Llama-3.1, the study highlights the potential for enhancing transparency and accountability in AI governance frameworks. The integration of Attestable Audits provides a verifiable way to evaluate and ensure compliance with AI models, offering a significant advancement in addressing the complexities of model validation and verification in real-world applications. <div>
arXiv:2506.23706v1 Announce Type: new 
Abstract: Benchmarks are important measures to evaluate safety and compliance of AI models at scale. However, they typically do not offer verifiable results and lack confidentiality for model IP and benchmark datasets. We propose Attestable Audits, which run inside Trusted Execution Environments and enable users to verify interaction with a compliant AI model. Our work protects sensitive data even when model provider and auditor do not trust each other. This addresses verification challenges raised in recent AI governance frameworks. We build a prototype demonstrating feasibility on typical audit benchmarks against Llama-3.1.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesL: Towards a Logical Framework for Bayesian Networks</title>
<link>https://arxiv.org/abs/2506.23773</link>
<guid>https://arxiv.org/abs/2506.23773</guid>
<content:encoded><![CDATA[
<div> Keywords: BayesL, Bayesian networks, queries, reasoning, what-if scenarios
Summary: 
BayesL is a novel logical framework that allows for specifying, querying, and verifying the behavior of Bayesian networks (BNs). It introduces a structured language, enabling the creation of queries over BNs and facilitating versatile reasoning about causal and evidence-based relationships. BayesL, pronounced "Basil," permits comprehensive what-if scenario evaluations without manual modifications to the model. This framework provides a powerful tool for conducting in-depth analysis and understanding the behavior of complex BNs. By enabling users to query and verify BNs, BayesL enhances the capabilities of researchers and practitioners working with probabilistic graphical models. With its emphasis on automating scenario evaluations, BayesL streamlines the process of exploring various possibilities within a Bayesian network. The integration of BayesL into the field of Bayesian networks opens up new avenues for efficient and accurate modeling and analysis of complex systems. Its structured approach to reasoning and querying offers a significant advancement in the field of probabilistic graphical models.<br /><br />Summary: <div>
arXiv:2506.23773v1 Announce Type: new 
Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil") is a structured language that allows for the creation of queries over BNs. It facilitates versatile reasoning concerning causal and evidence-based relationships, and permits comprehensive what-if scenario evaluations without the need for manual modifications to the model.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)</title>
<link>https://arxiv.org/abs/2506.23784</link>
<guid>https://arxiv.org/abs/2506.23784</guid>
<content:encoded><![CDATA[
<div> Nielsen transformation, word equations, Graph Neural Networks, ranking, minimum unsatisfiable subsets <br />
<br />
Summary: In this study, the use of Graph Neural Networks (GNNs) for ranking word equations in the solving process is investigated. A novel graph-based representation for word equations is introduced to maintain global information across conjuncts, allowing GNNs to have a comprehensive perspective during ranking. Three approaches are proposed to adapt a multi-classification task to equation ranking to handle the variable number of conjuncts. The GNN training is facilitated by minimum unsatisfiable subsets (MUSes) of word equations. Experimental results demonstrate that the proposed framework outperforms current string solvers in solving problems where each variable appears only once in each equation. <div>
arXiv:2506.23784v1 Announce Type: new 
Abstract: Nielsen transformation is a standard approach for solving word equations: by repeatedly splitting equations and applying simplification steps, equations are rewritten until a solution is reached. When solving a conjunction of word equations in this way, the performance of the solver will depend considerably on the order in which equations are processed. In this work, the use of Graph Neural Networks (GNNs) for ranking word equations before and during the solving process is explored. For this, a novel graph-based representation for word equations is presented, preserving global information across conjuncts, enabling the GNN to have a holistic view during ranking. To handle the variable number of conjuncts, three approaches to adapt a multi-classification task to the problem of ranking equations are proposed. The training of the GNN is done with the help of minimum unsatisfiable subsets (MUSes) of word equations. The experimental results show that, compared to state-of-the-art string solvers, the new framework solves more problems in benchmarks where each variable appears at most once in each equation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.23793</link>
<guid>https://arxiv.org/abs/2506.23793</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent pathfinding, MAPF-GPT-DDG, decentralized suboptimal solvers, machine learning, scalability

Summary:<br /><br />
Multi-agent pathfinding (MAPF) is a complex problem in which multiple robots navigate a shared environment simultaneously. Traditional optimal solutions to MAPF are time-consuming, leading researchers to explore decentralized suboptimal solvers that utilize machine learning. MAPF-GPT-DDG is a new approach that refines the pre-trained MAPF model using centralized expert data. By incorporating a unique delta-data generation mechanism, MAPF-GPT-DDG accelerates training and enhances performance during testing. Experimental results demonstrate that MAPF-GPT-DDG outperforms existing learning-based MAPF solvers, including the original MAPF-GPT, in solution quality across various scenarios. Impressively, MAPF-GPT-DDG showcases exceptional scalability by handling MAPF instances with up to 1 million agents in a single environment, setting a new standard in the field of MAPF domains. <div>
arXiv:2506.23793v1 Announce Type: new 
Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot trajectory planning problems, where multiple homogeneous robots simultaneously move in the shared environment. While solving MAPF optimally has been proven to be NP-hard, scalable, and efficient, solvers are vital for real-world applications like logistics, search-and-rescue, etc. To this end, decentralized suboptimal MAPF solvers that leverage machine learning have come on stage. Building on the success of the recently introduced MAPF-GPT, a pure imitation learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training while significantly improving performance at test time. Our experiments demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF solvers, including the original MAPF-GPT, regarding solution quality across many testing scenarios. Remarkably, it can work with MAPF instances involving up to 1 million agents in a single environment, setting a new milestone for scalability in MAPF domains.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents</title>
<link>https://arxiv.org/abs/2506.23844</link>
<guid>https://arxiv.org/abs/2506.23844</guid>
<content:encoded><![CDATA[
<div> memory augmentation, autonomous agents, security risks, defense strategies, Reflective Risk-Aware Agent Architecture (R2A2)

Summary:
Recent advances in large language models have led to the development of autonomous AI agents with enhanced capabilities such as long-term memory retention, modular tool use, recursive planning, and reflective reasoning. However, these advancements also bring about new security risks like memory poisoning, tool misuse, reward hacking, and emergent misalignment. These risks stem from architectural fragilities across perception, cognition, memory, and action modules. To address these challenges, defense strategies such as input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection have been deployed at different autonomy layers. A new approach, the Reflective Risk-Aware Agent Architecture (R2A2), based on Constrained Markov Decision Processes (CMDPs), integrates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to ensure proactive safety in the agent's decision-making loop. 

<br /><br />Summary: <div>
arXiv:2506.23844v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence</title>
<link>https://arxiv.org/abs/2506.23908</link>
<guid>https://arxiv.org/abs/2506.23908</guid>
<content:encoded><![CDATA[
<div> Keywords: deductive reasoning, artificial general intelligence, statistical learning, exact learning, algorithm design 

Summary: 
The article discusses the importance of sound deductive reasoning in artificial intelligence systems and highlights the limitations of current AI models in performing deductive reasoning tasks. It argues that the unsound behavior of these systems is a result of the statistical learning approach used in their development. To address this issue and achieve reliable deductive reasoning, the authors propose a shift towards the exact learning paradigm, which requires correctness on all inputs. They suggest that optimizing for statistical performance should be replaced by a focus on exact learning, emphasizing the need for algorithmic tasks to meet this ambitious objective. The authors believe that exact learning is both essential and achievable and should guide future algorithm design in the pursuit of artificial general intelligence capable of sound deductive reasoning. 

<br /><br />Summary: <div>
arXiv:2506.23908v1 Announce Type: new 
Abstract: Sound deductive reasoning -- the ability to derive new knowledge from existing facts and rules -- is an indisputably desirable aspect of general intelligence. Despite the major advances of AI systems in areas such as math and science, especially since the introduction of transformer architectures, it is well-documented that even the most advanced frontier systems regularly and consistently falter on easily-solvable deductive reasoning tasks. Hence, these systems are unfit to fulfill the dream of achieving artificial general intelligence capable of sound deductive reasoning. We argue that their unsound behavior is a consequence of the statistical learning approach powering their development. To overcome this, we contend that to achieve reliable deductive reasoning in learning-based AI systems, researchers must fundamentally shift from optimizing for statistical performance against distributions on reasoning problems and algorithmic tasks to embracing the more ambitious exact learning paradigm, which demands correctness on all inputs. We argue that exact learning is both essential and possible, and that this ambitious objective should guide algorithm design.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice</title>
<link>https://arxiv.org/abs/2506.23924</link>
<guid>https://arxiv.org/abs/2506.23924</guid>
<content:encoded><![CDATA[
<div> stochastic modeling, large language models, operations research, simulation-optimization, automation  
Summary:  
Large Language Models (LLMs) have shown expert-level skills in various domains but their application in Operations Research (OR) remains underexplored. This study evaluates LLMs' capability to solve stochastic modeling problems, a subset of OR involving uncertainty. The researchers gathered graduate-level OR problems and assessed LLM performance. Using SimOpt, an open-source library, they tested LLM decision-making under uncertainty. Results indicate that LLMs perform on par with human experts in both academic and practical scenarios, suggesting potential for AI agents to support OR researchers and enhance real-world impact through automation. While further work is needed for robust automation of the stochastic modeling pipeline, the study highlights the promise of LLMs in advancing OR applications.  
<br /><br />Summary: <div>
arXiv:2506.23924v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited expert-level capabilities across various domains. However, their abilities to solve problems in Operations Research (OR) -- the analysis and optimization of mathematical models derived from real-world problems or their verbal descriptions -- remain underexplored. In this work, we take a first step toward evaluating LLMs' abilities to solve stochastic modeling problems, a core class of OR problems characterized by uncertainty and typically involving tools from probability, statistics, and stochastic processes. We manually procure a representative set of graduate-level homework and doctoral qualification-exam problems and test LLMs' abilities to solve them. We further leverage SimOpt, an open-source library of simulation-optimization problems and solvers, to investigate LLMs' abilities to make real-world decisions under uncertainty. Our results show that, though a nontrivial amount of work is still needed to reliably automate the stochastic modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on par with human experts in both classroom and practical settings. These findings highlight the potential of building AI agents that assist OR researchers and amplify the real-world impact of OR through automation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system</title>
<link>https://arxiv.org/abs/2506.23926</link>
<guid>https://arxiv.org/abs/2506.23926</guid>
<content:encoded><![CDATA[
<div> measurement, resilience, industrial chain, deep learning, prediction  

Summary:  
Industrial Brain is a novel framework that combines higher-order activity-driven neuro networks and CT-OODA symbolic reasoning to autonomously plan resilience in industrial chains. It addresses the challenge of predicting resilience in complex network topologies and dynamics, outperforming existing methods by up to 11.03%. The Industrial Brain can understand and model node activity dynamics and network co-evolution without simplifying assumptions, revealing underlying laws hidden in complex networks. It allows for accurate resilience prediction, inference, and planning, even in the presence of observational disturbances. The framework demonstrates significant improvements in resilience prediction and planning compared to traditional approaches, showcasing its potential for addressing gaps in industrial chain management. <br /><br />Summary: <div>
arXiv:2506.23926v1 Announce Type: new 
Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental functionality amidst failures and errors, is crucial for scientific management and engineering applications of industrial chain. The problem is particularly challenging when the number or types of multiple co-evolution of resilience (for example, randomly placed) are extremely chaos. Existing end-to-end deep learning ordinarily do not generalize well to unseen full-feld reconstruction of spatiotemporal co-evolution structure, and predict resilience of network topology, especially in multiple chaos data regimes typically seen in real-world applications. To address this challenge, here we propose industrial brain, a human-like autonomous cognitive decision-making and planning framework integrating higher-order activity-driven neuro network and CT-OODA symbolic reasoning to autonomous plan resilience directly from observational data of global variable. The industrial brain not only understands and model structure of node activity dynamics and network co-evolution topology without simplifying assumptions, and reveal the underlying laws hidden behind complex networks, but also enabling accurate resilience prediction, inference, and planning. Experimental results show that industrial brain significantly outperforms resilience prediction and planning methods, with an accurate improvement of up to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension reduction. It also generalizes to unseen topologies and dynamics and maintains robust performance despite observational disturbances. Our findings suggest that industrial brain addresses an important gap in resilience prediction and planning for industrial chain.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models</title>
<link>https://arxiv.org/abs/2506.23949</link>
<guid>https://arxiv.org/abs/2506.23949</guid>
<content:encoded><![CDATA[
<div> AI models, risk management, GPAI, foundation models, large-scale
Summary:
This document presents risk management practices for developers working on multi-purpose AI models, such as large language models or 'general-purpose AI' (GPAI) models. The focus is on identifying, analyzing, and mitigating risks associated with these models. The guidance is intended for developers of state-of-the-art GPAI/foundation models, as well as downstream developers using these models in their applications. The document draws from leading AI risk management standards like the NIST AI Risk Management Framework and ISO/IEC 23894 but tailors the guidance to address the specific challenges faced by developers of GPAI/foundation models. It aims to help developers ensure the safe and responsible deployment of these advanced AI technologies, considering both the beneficial capabilities they offer and the potential risks of adverse events. <div>
arXiv:2506.23949v1 Announce Type: new 
Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language models or other 'general-purpose AI' (GPAI) models, 'foundation models,' generative AI models, and 'frontier models' (typically all referred to hereafter with the umbrella term 'GPAI/foundation models' except where greater specificity is needed), can provide many beneficial capabilities but also risks of adverse events with profound consequences. This document provides risk-management practices or controls for identifying, analyzing, and mitigating risks of GPAI/foundation models. We intend this document primarily for developers of large-scale, state-of-the-art GPAI/foundation models; others that can benefit from this guidance include downstream developers of end-use applications that build on a GPAI/foundation model. This document facilitates conformity with or use of leading AI risk management-related standards, adapting and building on the generic voluntary guidance in the NIST AI Risk Management Framework and ISO/IEC 23894, with a focus on the unique issues faced by developers of GPAI/foundation models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing AI Agents to Advance Research on Refugee Child Mental Health</title>
<link>https://arxiv.org/abs/2506.23992</link>
<guid>https://arxiv.org/abs/2506.23992</guid>
<content:encoded><![CDATA[
<div> Keywords: refugee crisis, AI-based framework, child mental health, retrieval-augmented generation, humanitarian datasets<br />
Summary:<br />
This research focuses on addressing the mental health challenges faced by displaced children in the international refugee crisis. Utilizing AI technology, two retrieval-augmented generation (RAG) pipelines, Zephyr-7B-beta, and DeepSeek R1-7B, were compared for their effectiveness in processing complex refugee health data. The study highlights the importance of leveraging cutting-edge AI methods in conjunction with migration research and child psychology to develop scalable solutions for assisting policymakers, mental health practitioners, and humanitarian agencies in supporting displaced children's mental wellbeing. Both models, Zephyr and DeepSeek, performed well, but DeepSeek R1 demonstrated superior accuracy in answer relevance, scoring 0.91. This research provides a valuable framework for harnessing AI to better understand and address the mental health needs of displaced children in humanitarian crises.<br /> <div>
arXiv:2506.23992v1 Announce Type: new 
Abstract: The international refugee crisis deepens, exposing millions of dis placed children to extreme psychological trauma. This research suggests a com pact, AI-based framework for processing unstructured refugee health data and distilling knowledge on child mental health. We compare two Retrieval-Aug mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to determine how well they process challenging humanitarian datasets while avoid ing hallucination hazards. By combining cutting-edge AI methods with migration research and child psychology, this study presents a scalable strategy to assist policymakers, mental health practitioners, and humanitarian agencies to better assist displaced children and recognize their mental wellbeing. In total, both the models worked properly but significantly Deepseek R1 is superior to Zephyr with an accuracy of answer relevance 0.91
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing Non-Markovian Decision Process via History Aggregator</title>
<link>https://arxiv.org/abs/2506.24026</link>
<guid>https://arxiv.org/abs/2506.24026</guid>
<content:encoded><![CDATA[
<div> Methodology, Category Theory, Markov Decision Processes, Non-Markovian Dynamics, History Aggregator for State<br />
Summary: 
- The study addresses the challenge of non-Markovian dynamics in algorithmic decision-making, particularly in Reinforcement Learning (RL).
- A category theory-based methodology is developed to evaluate decision algorithms' capability to handle non-Markovian dynamics.
- Equivalence between Markov Decision Processes (MDP) and non-Markovian Decision Processes (NMDP) is established, providing a new perspective on non-Markovian dynamics.
- The History Aggregator for State (HAS) is introduced to incorporate non-Markovianity into decision problems, allowing precise control over state dependency structures.
- The methodology is shown to effectively represent a wide range of non-Markovian dynamics, offering a more robust evaluation of decision algorithms in scenarios with explicitly constructed non-Markovian dynamics.<br /><br /> <div>
arXiv:2506.24026v1 Announce Type: new 
Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest as a significant impediment, especially for paradigms such as Reinforcement Learning (RL), thereby exerting far-reaching consequences on the advancement and effectiveness of the associated systems. Nevertheless, the existing benchmarks are deficient in comprehensively assessing the capacity of decision algorithms to handle non-Markovian dynamics. To address this deficiency, we have devised a generalized methodology grounded in category theory. Notably, we established the category of Markov Decision Processes (MDP) and the category of non-Markovian Decision Processes (NMDP), and proved the equivalence relationship between them. This theoretical foundation provides a novel perspective for understanding and addressing non-Markovian dynamics. We further introduced non-Markovianity into decision-making problem settings via the History Aggregator for State (HAS). With HAS, we can precisely control the state dependency structure of decision-making problems in the time series. Our analysis demonstrates the effectiveness of our method in representing a broad range of non-Markovian dynamics. This approach facilitates a more rigorous and flexible evaluation of decision algorithms by testing them in problem settings where non-Markovian dynamics are explicitly constructed.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, self-play, transferable reasoning capabilities, zero-sum games
Summary:
- SPIRAL introduces a self-play framework for language models to learn reasoning without human supervision through playing zero-sum games against improved versions of themselves.
- The self-play generates progressively challenging problems, leading to improved reasoning capabilities that transfer broadly.
- Training on Kuhn Poker alone results in significant improvements in math and general reasoning compared to expert trajectories.
- Cognitive patterns such as systematic decomposition, expected value calculation, and case-by-case analysis contribute to the transferable reasoning capabilities developed through self-play.
- Multi-game training enhances performance by developing distinct reasoning strengths in each game, highlighting the potential for autonomous reasoning development. <br /><br />Summary: <div>
arXiv:2506.24119v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention acts to suppress goal-based conflict under high competition</title>
<link>https://arxiv.org/abs/1610.09431</link>
<guid>https://arxiv.org/abs/1610.09431</guid>
<content:encoded><![CDATA[
<div> Attention, top-down, visual cortex, stimuli, competition <br />
<br />
Summary: 
The study explores the effects of top-down attention on neural signals in the visual cortex when there is high competition between two stimuli with conflicting purposes. The research reveals that under these conditions, top-down attention suppresses both task-relevant and irrelevant neural signals within 100 ms of stimuli onset. This non-selective engagement of top-down attention serves to reduce the feedforward signal representing irrelevant stimuli. While previous research has shown that top-down attention enhances neural signals for task-relevant stimuli under minimal competition, this study demonstrates a different response when stimuli are in direct competition within a shared receptive field. The findings suggest a dynamic role for top-down attention in modulating neural responses based on the level of competition present in the visual environment. <div>
arXiv:1610.09431v1 Announce Type: cross 
Abstract: It is known that when multiple stimuli are present, top-down attention selectively enhances the neural signal in the visual cortex for task-relevant stimuli, but this has been tested only under conditions of minimal competition of visual attention. Here we show during high competition, that is, two stimuli in a shared receptive field possessing opposing modulatory goals, top-down attention suppresses both task-relevant and irrelevant neural signals within 100 ms of stimuli onset. This non-selective engagement of top-down attentional resources serves to reduce the feedforward signal representing irrelevant stimuli.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks</title>
<link>https://arxiv.org/abs/2402.09146</link>
<guid>https://arxiv.org/abs/2402.09146</guid>
<content:encoded><![CDATA[
<div> trainable quanvolutional layers, Residual Quanvolutional Neural Networks (ResQuNNs), gradient-based optimization, skip connections, quantum deep learning <br />
<br />Summary: 
This paper introduces a framework for enhancing Quanvolutional Neural Networks (QuNNs) by incorporating trainable quanvolutional layers. Traditional quanvolutional layers have been static, limiting adaptability, but this research enables training within these layers for increased flexibility. To address optimization complexities, a novel architecture called Residual Quanvolutional Neural Networks (ResQuNNs) is proposed, using residual learning and skip connections to improve gradient flow. Strategic placement of residual blocks within QuNNs is found to be key in maximizing performance gains. Empirical evidence supports the effectiveness of inserting residual blocks to enhance gradient access across all network layers, leading to improved training efficiency. These findings mark a significant advancement in quantum deep learning with potential for theoretical development and practical quantum computing applications. <div>
arXiv:2402.09146v5 Announce Type: cross 
Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling</title>
<link>https://arxiv.org/abs/2504.15071</link>
<guid>https://arxiv.org/abs/2504.15071</guid>
<content:encoded><![CDATA[
<div> dataset, MIDI files, audio recordings, transcription, metadata<br />
<br />
Summary: 
The article introduces a new dataset of MIDI files created by transcribing audio recordings of piano performances. The data pipeline involves a multi-stage process including crawling and scoring audio recordings, pruning and segmentation using an audio classifier, resulting in over one million distinct MIDI files. The dataset comprises approximately 100,000 hours of transcribed audio. The techniques utilized are analyzed in-depth, providing statistical insights, and content investigation through metadata tag extraction. The metadata tags are also made available along with the dataset on GitHub. <div>
arXiv:2504.15071v1 Announce Type: cross 
Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing audio recordings of piano performances into their constituent notes. The data pipeline we use is multi-stage, employing a language model to autonomously crawl and score audio recordings from the internet based on their metadata, followed by a stage of pruning and segmentation using an audio classifier. The resulting dataset contains over one million distinct MIDI files, comprising roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of our techniques, offering statistical insights, and investigate the content by extracting metadata tags, which we also provide. Dataset available at https://github.com/loubbrad/aria-midi.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans</title>
<link>https://arxiv.org/abs/2506.22439</link>
<guid>https://arxiv.org/abs/2506.22439</guid>
<content:encoded><![CDATA[
<div> psycholinguistic datasets, LLMs, alignment, Glasgow norms, human ratings
Summary:<br /><br />The study evaluates the alignment of Language Models (LLMs) with human ratings on psycholinguistic datasets. LLM performance in aligning with word features such as arousal, concreteness, and gender is examined using Glasgow and Lancaster norms. Results indicate better alignment with Glasgow norms compared to Lancaster norms. LLMs show higher alignment with features like arousal, valence, dominance, concreteness, imageability, familiarity, and gender, while struggling with introceptive, gustatory, olfactory, haptic, auditory, and visual features. This suggests limitations in LLMs aligning with human sensory associations, potentially due to the absence of embodied cognition. The study highlights the importance of evaluating LLMs using psycholinguistic datasets for a comprehensive understanding of their performance. 
<br /><br /> <div>
arXiv:2506.22439v1 Announce Type: cross 
Abstract: The evaluation of LLMs has so far focused primarily on how well they can perform different tasks such as reasoning, question-answering, paraphrasing, or translating. For most of these tasks, performance can be measured with objective metrics, such as the number of correct answers. However, other language features are not easily quantified. For example, arousal, concreteness, or gender associated with a given word, as well as the extent to which we experience words with senses and relate them to a specific sense. Those features have been studied for many years by psycholinguistics, conducting large-scale experiments with humans to produce ratings for thousands of words. This opens an opportunity to evaluate how well LLMs align with human ratings on these word features, taking advantage of existing studies that cover many different language features in a large number of words. In this paper, we evaluate the alignment of a representative group of LLMs with human ratings on two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets cover thirteen features over thousands of words. The results show that alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated (arousal, valence, dominance, concreteness, imageability, familiarity, and gender) than on the Lancaster norms evaluated (introceptive, gustatory, olfactory, haptic, auditory, and visual). This suggests a potential limitation of current LLMs in aligning with human sensory associations for words, which may be due to their lack of embodied cognition present in humans and illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation</title>
<link>https://arxiv.org/abs/2506.22441</link>
<guid>https://arxiv.org/abs/2506.22441</guid>
<content:encoded><![CDATA[
<div> Latent Factorization of Tensors, Missing Data Imputation, Intelligent Transportation Systems, Outlier Robustness, Traffic Prediction<br />
Summary:<br />
- The study addresses the challenge of incomplete and corrupted spatiotemporal traffic data in Intelligent Transportation Systems (ITS). <br />
- Proposes a novel TDW loss-incorporated Latent Factorization of Tensors (TDWLFT) model to handle missing traffic data effectively. <br />
- The TDWLFT model reduces sensitivity to outliers by assigning differentiated weights to individual samples in the learning objective. <br />
- Extensive experiments on real-world traffic speed datasets show that the TDWLFT model outperforms existing approaches in both prediction accuracy and computational efficiency. <br />
- The proposed model demonstrates the significance of outlier robustness in improving the reliability and performance of traffic data imputation methods for ITS applications. <br /> <div>
arXiv:2506.22441v1 Announce Type: cross 
Abstract: Intelligent transportation systems (ITS) rely heavily on complete and high-quality spatiotemporal traffic data to achieve optimal performance. Nevertheless, in real-word traffic data collection processes, issues such as communication failures and sensor malfunctions often lead to incomplete or corrupted datasets, thereby posing significant challenges to the advancement of ITS. Among various methods for imputing missing spatiotemporal traffic data, the latent factorization of tensors (LFT) model has emerged as a widely adopted and effective solution. However, conventional LFT models typically employ the standard L2-norm in their learning objective, which makes them vulnerable to the influence of outliers. To overcome this limitation, this paper proposes a threshold distance weighted (TDW) loss-incorporated Latent Factorization of Tensors (TDWLFT) model. The proposed loss function effectively reduces the model's sensitivity to outliers by assigning differentiated weights to individual samples. Extensive experiments conducted on two traffic speed datasets sourced from diverse urban environments confirm that the proposed TDWLFT model consistently outperforms state-of-the-art approaches in terms of both in both prediction accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security</title>
<link>https://arxiv.org/abs/2506.22445</link>
<guid>https://arxiv.org/abs/2506.22445</guid>
<content:encoded><![CDATA[
arXiv:2506.22445v1 Announce Type: cross 
Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various sectors, including manufacturing, energy distribution, and autonomous transportation systems. However, their increasing connectivity renders them highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day attacks, against which traditional security methods like rule-based intrusion detection and single-agent reinforcement learning prove insufficient. To overcome these challenges, this paper introduces a novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework. HAMARL employs a hierarchical structure consisting of local agents dedicated to subsystem security and a global coordinator that oversees and optimizes comprehensive, system-wide defense strategies. Furthermore, the framework incorporates an adversarial training loop designed to simulate and anticipate evolving cyber threats, enabling proactive defense adaptation. Extensive experimental evaluations conducted on a simulated industrial IoT testbed indicate that HAMARL substantially outperforms traditional multi-agent reinforcement learning approaches, significantly improving attack detection accuracy, reducing response times, and ensuring operational continuity. The results underscore the effectiveness of combining hierarchical multi-agent coordination with adversarially-aware training to enhance the resilience and security of next-generation CPS.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis</title>
<link>https://arxiv.org/abs/2506.22446</link>
<guid>https://arxiv.org/abs/2506.22446</guid>
<content:encoded><![CDATA[
arXiv:2506.22446v1 Announce Type: cross 
Abstract: Accurate cancer survival prediction requires integration of diverse data modalities that reflect the complex interplay between imaging, clinical parameters, and textual reports. However, existing multimodal approaches suffer from simplistic fusion strategies, massive computational requirements, and lack of interpretability-critical barriers to clinical adoption. We present EAGLE (Efficient Alignment of Generalized Latent Embeddings), a novel deep learning framework that addresses these limitations through attention-based multimodal fusion with comprehensive attribution analysis. EAGLE introduces four key innovations: (1) dynamic cross-modal attention mechanisms that learn hierarchical relationships between modalities, (2) massive dimensionality reduction (99.96%) while maintaining predictive performance, (3) three complementary attribution methods providing patient-level interpretability, and (4) a unified pipeline enabling seamless adaptation across cancer types. We evaluated EAGLE on 911 patients across three distinct malignancies: glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN, n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis showed high-risk individuals relied more heavily on adverse imaging features, while low-risk patients demonstrated balanced modality contributions. Risk stratification identified clinically meaningful groups with 4-fold (GBM) to 5-fold (NSCLC) differences in median survival, directly informing treatment intensity decisions. By combining state-of-the-art performance with clinical interpretability, EAGLE bridges the gap between advanced AI capabilities and practical healthcare deployment, offering a scalable solution for multimodal survival prediction that enhances both prognostic accuracy and physician trust in automated predictions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture</title>
<link>https://arxiv.org/abs/2506.22447</link>
<guid>https://arxiv.org/abs/2506.22447</guid>
<content:encoded><![CDATA[
arXiv:2506.22447v1 Announce Type: cross 
Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate dynamics, but their coarse spatial resolution limits their applicability in regional studies. Regional Climate Models (RCMs) refine this through dynamic downscaling, albeit at considerable computational cost and with limited flexibility. While deep learning has emerged as an efficient data-driven alternative, most existing studies have focused on single-variable models that downscale one variable at a time. This approach can lead to limited contextual awareness, redundant computation, and lack of cross-variable interaction. Our study addresses these limitations by proposing a multi-task, multi-variable Vision Transformer (ViT) architecture with a shared encoder and variable-specific decoders (1EMD). The proposed architecture jointly predicts three key climate variables: surface temperature (tas), wind speed (sfcWind), and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs, emulating RCM-scale downscaling over Europe. We show that our multi-variable approach achieves positive cross-variable knowledge transfer and consistently outperforms single-variable baselines trained under identical conditions, while also improving computational efficiency. These results demonstrate the effectiveness of multi-variable modeling for high-resolution climate downscaling.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems</title>
<link>https://arxiv.org/abs/2506.22448</link>
<guid>https://arxiv.org/abs/2506.22448</guid>
<content:encoded><![CDATA[
arXiv:2506.22448v1 Announce Type: cross 
Abstract: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA system, addressing resource allocation challenges. A two-stage unsupervised learning-based framework is proposed to jointly design RIS phase shifts, BS beamforming, and resource block (RB) allocation. The framework includes BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which allocates RBs using equivalent CSI derived from BeamNet outputs. Active beamforming is implemented via maximum ratio transmission and water-filling. To handle discrete constraints while ensuring differentiability, quantization and the Gumbel-softmax trick are adopted. A customized loss and phased training enhance performance under QoS constraints. Simulations show the method achieves 99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and it remains robust across varying channel and user conditions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes</title>
<link>https://arxiv.org/abs/2506.22457</link>
<guid>https://arxiv.org/abs/2506.22457</guid>
<content:encoded><![CDATA[
arXiv:2506.22457v1 Announce Type: cross 
Abstract: Continuous, non-invasive pregnancy monitoring is crucial for minimising potential complications. The fetal electrocardiogram (fECG) represents a promising tool for assessing fetal health beyond clinical environments. Home-based monitoring necessitates the use of a minimal number of comfortable and durable electrodes, such as dry textile electrodes. However, this setup presents many challenges, including increased noise and motion artefacts, which complicate the accurate extraction of fECG signals. To overcome these challenges, we introduce a pioneering method for extracting fECG from single-channel recordings obtained using dry textile electrodes using AI techniques. We created a new dataset by simulating abdominal recordings, including noise closely resembling real-world characteristics of in-vivo recordings through dry textile electrodes, alongside mECG and fECG. To ensure the reliability of the extracted fECG, we propose an innovative pipeline based on a complex-valued denoising network, Complex UNet. Unlike previous approaches that focused solely on signal magnitude, our method processes both real and imaginary components of the spectrogram, addressing phase information and preventing incongruous predictions. We evaluated our novel pipeline against traditional, well-established approaches, on both simulated and real data in terms of fECG extraction and R-peak detection. The results showcase that our suggested method achieves new state-of-the-art results, enabling an accurate extraction of fECG morphology across all evaluated settings. This method is the first to effectively extract fECG signals from single-channel recordings using dry textile electrodes, making a significant advancement towards a fully non-invasive and self-administered fECG extraction solution.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods</title>
<link>https://arxiv.org/abs/2506.22460</link>
<guid>https://arxiv.org/abs/2506.22460</guid>
<content:encoded><![CDATA[
arXiv:2506.22460v1 Announce Type: cross 
Abstract: Using mobile phone video of the fingertip as a data source for estimating vital signs such as heart rate (HR) and respiratory rate (RR) during daily life has long been suggested. While existing literature indicates that these estimates are accurate to within several beats or breaths per minute, the data used to draw these conclusions are typically collected in laboratory environments under careful experimental control, and yet the results are assumed to generalize to daily life. In an effort to test it, a team of researchers collected a large dataset of mobile phone video recordings made during daily life and annotated with ground truth HR and RR labels from N=111 participants. They found that traditional algorithm performance on the fingerprint videos is worse than previously reported (7 times and 13 times worse for RR and HR, respectively). Fortunately, recent advancements in deep learning, especially in convolutional neural networks (CNNs), offer a promising solution to improve this performance. This study proposes a new method for estimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error in estimated HR by 68% and RR by 75%. These promising results suggest that regressor-based deep learning approaches should be used in estimating HR and RR.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation</title>
<link>https://arxiv.org/abs/2506.22461</link>
<guid>https://arxiv.org/abs/2506.22461</guid>
<content:encoded><![CDATA[
arXiv:2506.22461v1 Announce Type: cross 
Abstract: Groundwater supports ecosystems, agriculture, and drinking water supplies worldwide, yet effective monitoring remains challenging due to sparse data, computational constraints, and delayed outputs from traditional approaches. We develop a machine learning pipeline that predicts groundwater level categories using climate data, hydro-meteorological records, and physiographic attributes processed through AutoGluon's automated ensemble framework. Our approach integrates geospatial preprocessing, domain-driven feature engineering, and automated model selection to overcome conventional monitoring limitations. Applied to a large-scale French dataset (n $>$ 3,440,000 observations from 1,500+ wells), the model achieves weighted F\_1 scores of 0.927 on validation data and 0.67 on temporally distinct test data. Scenario-based evaluations demonstrate practical utility for early warning systems and water allocation decisions under changing climate conditions. The open-source implementation provides a scalable framework for integrating machine learning into national groundwater monitoring networks, enabling more responsive and data-driven water management strategies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-aware IoT Fall Detection Services For Aging in Place</title>
<link>https://arxiv.org/abs/2506.22462</link>
<guid>https://arxiv.org/abs/2506.22462</guid>
<content:encoded><![CDATA[
arXiv:2506.22462v1 Announce Type: cross 
Abstract: Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting</title>
<link>https://arxiv.org/abs/2506.22468</link>
<guid>https://arxiv.org/abs/2506.22468</guid>
<content:encoded><![CDATA[
arXiv:2506.22468v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) plays a major role today in smart building infrastructures, from simple smart-home applications, to more sophisticated industrial type installations. The vast amounts of data generated from relevant systems can be processed in different ways revealing important information. This is especially true in the era of edge computing, when advanced data analysis and decision-making is gradually moving to the edge of the network where devices are generally characterised by low computing resources. In this context, one of the emerging main challenges is related to maintaining data analysis accuracy even with less data that can be efficiently handled by low resource devices. The present work focuses on correlation analysis of data retrieved from a pilot IoT network installation monitoring a small smart office by means of environmental and energy consumption sensors. The research motivation was to find statistical correlation between the monitoring variables that will allow the use of machine learning (ML) prediction algorithms for energy consumption reducing input parameters. For this to happen, a series of hypothesis tests for the correlation of three different environmental variables with the energy consumption were carried out. A total of ninety tests were performed, thirty for each pair of variables. In these tests, p-values showed the existence of strong or semi-strong correlation with two environmental variables, and of a weak correlation with a third one. Using the proposed methodology, we manage without examining the entire data set to exclude weak correlated variables while keeping the same score of accuracy.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI</title>
<link>https://arxiv.org/abs/2506.22477</link>
<guid>https://arxiv.org/abs/2506.22477</guid>
<content:encoded><![CDATA[
arXiv:2506.22477v1 Announce Type: cross 
Abstract: This paper introduces an innovative design for robotic operating platforms, underpinned by a transformative Internet of Things (IoT) architecture, seamlessly integrating cutting-edge technologies such as large language models (LLMs), generative AI, edge computing, and 5G networks. The proposed platform aims to elevate the intelligence and autonomy of IoT systems and robotics, enabling them to make real-time decisions and adapt dynamically to changing environments. Through a series of compelling case studies across industries including smart manufacturing, healthcare, and service sectors, this paper demonstrates the substantial potential of IoT-enabled robotics to optimize operational workflows, enhance productivity, and deliver innovative, scalable solutions. By emphasizing the roles of LLMs and generative AI, the research highlights how these technologies drive the evolution of intelligent robotics and IoT, shaping the future of industry-specific advancements. The findings not only showcase the transformative power of these technologies but also offer a forward-looking perspective on their broader societal and industrial implications, positioning them as catalysts for next-generation automation and technological convergence.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate</title>
<link>https://arxiv.org/abs/2506.22479</link>
<guid>https://arxiv.org/abs/2506.22479</guid>
<content:encoded><![CDATA[
arXiv:2506.22479v1 Announce Type: cross 
Abstract: We introduce Hindsight-Guided Momentum (HGM), a first-order optimization algorithm that adaptively scales learning rates based on the directional consistency of recent updates. Traditional adaptive methods, such as Adam or RMSprop , adapt learning dynamics using only the magnitude of gradients, often overlooking important geometric cues.Geometric cues refer to directional information, such as the alignment between current gradients and past updates, which reflects the local curvature and consistency of the optimization path. HGM addresses this by incorporating a hindsight mechanism that evaluates the cosine similarity between the current gradient and accumulated momentum. This allows it to distinguish between coherent and conflicting gradient directions, increasing the learning rate when updates align and reducing it in regions of oscillation or noise. The result is a more responsive optimizer that accelerates convergence in smooth regions of the loss surface while maintaining stability in sharper or more erratic areas. Despite this added adaptability, the method preserves the computational and memory efficiency of existing optimizers.By more intelligently responding to the structure of the optimization landscape, HGM provides a simple yet effective improvement over existing approaches, particularly in non-convex settings like that of deep neural network training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents</title>
<link>https://arxiv.org/abs/2506.22485</link>
<guid>https://arxiv.org/abs/2506.22485</guid>
<content:encoded><![CDATA[
arXiv:2506.22485v1 Announce Type: cross 
Abstract: This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection with Small Language Models</title>
<link>https://arxiv.org/abs/2506.22486</link>
<guid>https://arxiv.org/abs/2506.22486</guid>
<content:encoded><![CDATA[
arXiv:2506.22486v1 Announce Type: cross 
Abstract: Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating "Yes" tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space</title>
<link>https://arxiv.org/abs/2506.22487</link>
<guid>https://arxiv.org/abs/2506.22487</guid>
<content:encoded><![CDATA[
arXiv:2506.22487v1 Announce Type: cross 
Abstract: The integration of the Internet of Everything (IoX) and Artificial General Intelligence (AGI) has given rise to a transformative paradigm aimed at addressing critical bottlenecks across sensing, network, and application layers in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide a systematic and comprehensive review of AGI-enhanced IoX research, focusing on three key components: sensing-layer data management, network-layer protocol optimization, and application-layer decision-making frameworks. Specifically, this survey explores how AGI can mitigate IoX bottlenecks challenges by leveraging adaptive sensor fusion, edge preprocessing, and selective attention mechanisms at the sensing layer, while resolving network-layer issues such as protocol heterogeneity and dynamic spectrum management, neuro-symbolic reasoning, active inference, and causal reasoning, Furthermore, the survey examines AGI-enabled frameworks for managing identity and relationship explosion. Key findings suggest that AGI-driven strategies, such as adaptive sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions to sensing-layer data overload, network-layer protocol heterogeneity, and application-layer identity explosion. The survey underscores the importance of cross-layer integration, quantum-enabled communication, and ethical governance frameworks for future AGI-enabled IoX systems. Finally, the survey identifies unresolved challenges, such as computational requirements, scalability, and real-world validation, calling for further research to fully realize AGI's potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is emerging as a critical research field at the intersection of interconnected systems and advanced AI.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptAug: Fine-grained Conflict Classification Using Data Augmentation</title>
<link>https://arxiv.org/abs/2506.22491</link>
<guid>https://arxiv.org/abs/2506.22491</guid>
<content:encoded><![CDATA[
arXiv:2506.22491v1 Announce Type: cross 
Abstract: Given the rise of conflicts on social media, effective classification models to detect harmful behaviours are essential. Following the garbage-in-garbage-out maxim, machine learning performance depends heavily on training data quality. However, high-quality labelled data, especially for nuanced tasks like identifying conflict behaviours, is limited, expensive, and difficult to obtain. Additionally, as social media platforms increasingly restrict access to research data, text data augmentation is gaining attention as an alternative to generate training data. Augmenting conflict-related data poses unique challenges due to Large Language Model (LLM) guardrails that prevent generation of offensive content. This paper introduces PromptAug, an innovative LLM-based data augmentation method. PromptAug achieves statistically significant improvements of 2% in both accuracy and F1-score on conflict and emotion datasets. To thoroughly evaluate PromptAug against other data augmentation methods we conduct a robust evaluation using extreme data scarcity scenarios, quantitative diversity analysis and a qualitative thematic analysis. The thematic analysis identifies four problematic patterns in augmented text: Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting data in sensitive tasks like conflict detection, offering a unique, interdisciplinary evaluation grounded in both natural language processing and social science methodology.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Report on NSF Workshop on Science of Safe AI</title>
<link>https://arxiv.org/abs/2506.22492</link>
<guid>https://arxiv.org/abs/2506.22492</guid>
<content:encoded><![CDATA[
arXiv:2506.22492v1 Announce Type: cross 
Abstract: Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasoning and inner workings of today's complex AI models are not transparent to the user, and there are no safety guarantees regarding their predictions. Consequently, to fulfill the promise of AI, we must address the following scientific challenge: how to develop AI-based systems that are not only accurate and performant but also safe and trustworthy?
  The criticality of safe operation is particularly evident for autonomous systems for control and robotics, and was the catalyst for the Safe Learning Enabled Systems (SLES) program at NSF. For the broader class of AI applications, such as users interacting with chatbots and clinicians receiving treatment recommendations, safety is, while no less important, less well-defined with context-dependent interpretations. This motivated the organization of a day-long workshop, held at University of Pennsylvania on February 26, 2025, to bring together investigators funded by the NSF SLES program with a broader pool of researchers studying AI safety. This report is the result of the discussions in the working groups that addressed different aspects of safety at the workshop. The report articulates a new research agenda focused on developing theory, methods, and tools that will provide the foundations of the next generation of AI-enabled systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses</title>
<link>https://arxiv.org/abs/2506.22495</link>
<guid>https://arxiv.org/abs/2506.22495</guid>
<content:encoded><![CDATA[
arXiv:2506.22495v1 Announce Type: cross 
Abstract: The diagnostic value of electrocardiogram (ECG) lies in its dynamic characteristics, ranging from rhythm fluctuations to subtle waveform deformations that evolve across time and frequency domains. However, supervised ECG models tend to overfit dominant and repetitive patterns, overlooking fine-grained but clinically critical cues, a phenomenon known as Simplicity Bias (SB), where models favor easily learnable signals over subtle but informative ones. In this work, we first empirically demonstrate the presence of SB in ECG analyses and its negative impact on diagnostic performance, while simultaneously discovering that self-supervised learning (SSL) can alleviate it, providing a promising direction for tackling the bias. Following the SSL paradigm, we propose a novel method comprising two key components: 1) Temporal-Frequency aware Filters to capture temporal-frequency features reflecting the dynamic characteristics of ECG signals, and 2) building on this, Multi-Grained Prototype Reconstruction for coarse and fine representation learning across dual domains, further mitigating SB. To advance SSL in ECG analyses, we curate a large-scale multi-site ECG dataset with 1.53 million recordings from over 300 clinical centers. Experiments on three downstream tasks across six ECG datasets demonstrate that our method effectively reduces SB and achieves state-of-the-art performance. Code and dataset will be released publicly.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety</title>
<link>https://arxiv.org/abs/2506.22496</link>
<guid>https://arxiv.org/abs/2506.22496</guid>
<content:encoded><![CDATA[
arXiv:2506.22496v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors analogous to those observed in gambling psychology, including overconfidence bias, loss-chasing tendencies, and probability misjudgment. Drawing from behavioral economics and prospect theory, we identify and formalize these "gambling-like" patterns where models sacrifice accuracy for high-reward outputs, exhibit escalating risk-taking after errors, and systematically miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG) framework, incorporating insights from gambling research to address these behavioral biases through risk-calibrated training, loss-aversion mechanisms, and uncertainty-aware decision making. Our approach introduces novel evaluation paradigms based on established gambling psychology experiments, including AI adaptations of the Iowa Gambling Task and probability learning assessments. Experimental results demonstrate measurable reductions in gambling-like behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in loss-chasing tendencies, and improved risk calibration across diverse scenarios. This work establishes the first systematic framework for understanding and mitigating gambling psychology patterns in AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship</title>
<link>https://arxiv.org/abs/2506.22497</link>
<guid>https://arxiv.org/abs/2506.22497</guid>
<content:encoded><![CDATA[
arXiv:2506.22497v1 Announce Type: cross 
Abstract: This paper reconceptualises peer review as structured public commentary. Traditional academic validation is hindered by anonymity, latency, and gatekeeping. We propose a transparent, identity-linked, and reproducible system of scholarly evaluation anchored in open commentary. Leveraging blockchain for immutable audit trails and AI for iterative synthesis, we design a framework that incentivises intellectual contribution, captures epistemic evolution, and enables traceable reputational dynamics. This model empowers fields from computational science to the humanities, reframing academic knowledge as a living process rather than a static credential.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction</title>
<link>https://arxiv.org/abs/2506.22498</link>
<guid>https://arxiv.org/abs/2506.22498</guid>
<content:encoded><![CDATA[
arXiv:2506.22498v1 Announce Type: cross 
Abstract: Bed-related falls remain a leading source of injury in hospitals and long-term-care facilities, yet many commercial alarms trigger only after a patient has already left the bed. We show that early bed-exit intent can be predicted using only four low-cost load cells mounted under the bed legs. The resulting load signals are first converted into a compact set of complementary images: an RGB line plot that preserves raw waveforms and three texture maps - recurrence plot, Markov transition field, and Gramian angular field - that expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin Transformer that processes the line plot and texture maps in parallel and fuses them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data from 95 beds in a long-term-care facility. On this real-world dataset ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC. The results demonstrate that image-based fusion of load-sensor signals for time series classification is a practical and effective solution for real-time, privacy-preserving fall prevention.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data</title>
<link>https://arxiv.org/abs/2506.22499</link>
<guid>https://arxiv.org/abs/2506.22499</guid>
<content:encoded><![CDATA[
arXiv:2506.22499v1 Announce Type: cross 
Abstract: This study presents a novel integrated framework for dynamic origin-destination demand estimation (DODE) in multi-class mesoscopic network models, leveraging high-resolution satellite imagery together with conventional traffic data from local sensors. Unlike sparse local detectors, satellite imagery offers consistent, city-wide road and traffic information of both parking and moving vehicles, overcoming data availability limitations. To extract information from imagery data, we design a computer vision pipeline for class-specific vehicle detection and map matching, generating link-level traffic density observations by vehicle class. Building upon this information, we formulate a computational graph-based DODE model that calibrates dynamic network states by jointly matching observed traffic counts and travel times from local sensors with density measurements derived from satellite imagery. To assess the accuracy and scalability of the proposed framework, we conduct a series of numerical experiments using both synthetic and real-world data. The results of out-of-sample tests demonstrate that supplementing traditional data with satellite-derived density significantly improves estimation performance, especially for links without local sensors. Real-world experiments also confirm the framework's capability to handle large-scale networks, supporting its potential for practical deployment in cities of varying sizes. Sensitivity analysis further evaluates the impact of data quality related to satellite imagery data.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.22500</link>
<guid>https://arxiv.org/abs/2506.22500</guid>
<content:encoded><![CDATA[
arXiv:2506.22500v1 Announce Type: cross 
Abstract: Surgical risk identification is critical for patient safety and reducing preventable medical errors. While multimodal large language models (MLLMs) show promise for automated operating room (OR) risk detection, they often exhibit visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety violations despite understanding textual rules. To address this, we introduce a dataset comprising over 34,000 synthetic images generated by diffusion models, depicting operating room scenes containing entities that violate established safety rules. These images were created to alleviate data scarcity and examine MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated images that serve as a gold-standard reference for validation. This comprehensive dataset, spanning diverse perspectives, stages, and configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC significantly improves MLLMs' detection of trained conflict entities and generalizes well to new viewpoints for these entities, but performance on untrained entity types remains poor, highlighting learning specificity and the need for comprehensive training. The main contributions of this work include: (1) a data generation methodology tailored for rule-violation scenarios; (2) the release of the OR-VSKC dataset and its associated benchmark as open-source resources; and (3) an empirical analysis of violation-sensitive knowledge consistency in representative MLLMs. The dataset and appendix are available at https://github.com/zgg2577/VS-KC.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?</title>
<link>https://arxiv.org/abs/2506.22501</link>
<guid>https://arxiv.org/abs/2506.22501</guid>
<content:encoded><![CDATA[
arXiv:2506.22501v1 Announce Type: cross 
Abstract: Remote sensing datasets offer significant promise for tackling key classification tasks such as land-use categorization, object presence detection, and rural/urban classification. However, many existing studies tend to focus on narrow tasks or datasets, which limits their ability to generalize across various remote sensing classification challenges. To overcome this, we propose a novel model, SpatialNet-ViT, leveraging the power of Vision Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach combines spatial awareness with contextual understanding, improving both classification accuracy and scalability. Additionally, techniques like data augmentation, transfer learning, and multi-task learning are employed to enhance model robustness and its ability to generalize across diverse datasets
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning</title>
<link>https://arxiv.org/abs/2506.22506</link>
<guid>https://arxiv.org/abs/2506.22506</guid>
<content:encoded><![CDATA[
arXiv:2506.22506v1 Announce Type: cross 
Abstract: Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text</title>
<link>https://arxiv.org/abs/2506.22508</link>
<guid>https://arxiv.org/abs/2506.22508</guid>
<content:encoded><![CDATA[
arXiv:2506.22508v1 Announce Type: cross 
Abstract: In today's digital world, casual user-generated content often contains subtle cues that may inadvertently expose sensitive personal attributes. Such risks underscore the growing importance of effective text anonymization to safeguard individual privacy. However, existing methods either rely on rigid replacements that damage utility or cloud-based LLMs that are costly and pose privacy risks. To address these issues, we explore the use of locally deployed smaller-scale language models (SLMs) for anonymization. Yet training effective SLMs remains challenging due to limited high-quality supervision. To address the challenge, we propose AgentStealth, a self-reinforcing LLM anonymization framework.First, we introduce an adversarial anonymization workflow enhanced by In-context Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform supervised adaptation of SLMs using high-quality data collected from the workflow, which includes both anonymization and attack signals. Finally, we apply online reinforcement learning where the model leverages its internal adversarial feedback to iteratively improve anonymization performance. Experiments on two datasets show that our method outperforms baselines in both anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight design supports direct deployment on edge devices, avoiding cloud reliance and communication-based privacy risks. Our code is open-source at https://github.com/tsinghua-fib-lab/AgentStealth.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment</title>
<link>https://arxiv.org/abs/2506.22509</link>
<guid>https://arxiv.org/abs/2506.22509</guid>
<content:encoded><![CDATA[
arXiv:2506.22509v1 Announce Type: cross 
Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.22510</link>
<guid>https://arxiv.org/abs/2506.22510</guid>
<content:encoded><![CDATA[
arXiv:2506.22510v1 Announce Type: cross 
Abstract: Foundation models have achieved great success in natural language processing (NLP) and computer vision (CV). Their success largely stems from the ability to integrate multi-domain knowledge in pre-training and transfer it to target domains. Considering graph data, especially graphs without textual features, is ubiquitous in real-world applications such as social networks and recommendation systems, some researchers have attempted to extend this paradigm to the graph field, aiming to construct graph foundation models. However, unlike CV and NLP, there are huge gaps among the semantics and properties of graphs in different domains, while current works still adopt traditional contrastive pre-training strategies designed in the single-domain scenario, which regard contrastive samples from different domains as equivalent. From experimental investigations, we discovered that inherent domain-specific differences prevent these strategies from effectively absorbing knowledge from different domains to generate informative representations. In this paper, we propose a novel multi-domain pre-training and cross-domain transfer framework, namely MDGCL.In the pre-training stage, we design a contrastive learning strategy to substantially recognize and capture domain differences, and introduce domain tokens to encode domain-level global information. In the downstream stage, we introduce a domain attention mechanism to enable fine-grained domain knowledge transfer. Extensive experiments on five benchmark datasets have demonstrated that our method outperforms state-of-the-art significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\% on Macro-F1 score.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightning the Night with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.22511</link>
<guid>https://arxiv.org/abs/2506.22511</guid>
<content:encoded><![CDATA[
arXiv:2506.22511v1 Announce Type: cross 
Abstract: The visible light reflectance data from geostationary satellites is crucial for meteorological observations and plays an important role in weather monitoring and forecasting. However, due to the lack of visible light at night, it is impossible to conduct continuous all-day weather observations using visible light reflectance data. This study pioneers the use of generative diffusion models to address this limitation. Based on the multi-band thermal infrared brightness temperature data from the Advanced Geostationary Radiation Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we developed a high-precision visible light reflectance retrieval model, called Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m}, 0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance retrieval at night. Compared to the classical models, RefDiff not only significantly improves accuracy through ensemble averaging but also provides uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90, with particularly significant improvements in areas with complex cloud structures and thick clouds. The model's nighttime retrieval capability was validated using VIIRS nighttime product, demonstrating comparable performance to its daytime counterpart. In summary, this research has made substantial progress in the ability to retrieve visible light reflectance at night, with the potential to expand the application of nighttime visible light data.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions</title>
<link>https://arxiv.org/abs/2506.22512</link>
<guid>https://arxiv.org/abs/2506.22512</guid>
<content:encoded><![CDATA[
arXiv:2506.22512v1 Announce Type: cross 
Abstract: AI for good initiatives often rely on the assumption that technical interventions can resolve complex social problems. In the context of human trafficking (HT), such techno-solutionism risks oversimplifying exploitation, reinforcing power imbalances and causing harm to the very communities AI claims to support. In this paper, we introduce the Radical Questioning (RQ) framework as a five step, pre-project ethical assessment tool to critically evaluate whether AI should be built at all, especially in domains involving marginalized populations and entrenched systemic injustice. RQ does not replace principles based ethics but precedes it, offering an upstream, deliberative space to confront assumptions, map power, and consider harms before design. Using a case study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural complexities and guides us away from surveillance based interventions toward survivor empowerment tools. While developed in the context of HT, RQ's five step structure can generalize to other domains, though the specific questions must be contextual. This paper situates RQ within a broader AI ethics philosophy that challenges instrumentalist norms and centers relational, reflexive responsibility.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context learning for the classification of manipulation techniques in phishing emails</title>
<link>https://arxiv.org/abs/2506.22515</link>
<guid>https://arxiv.org/abs/2506.22515</guid>
<content:encoded><![CDATA[
arXiv:2506.22515v1 Announce Type: cross 
Abstract: Traditional phishing detection often overlooks psychological manipulation. This study investigates using Large Language Model (LLM) In-Context Learning (ICL) for fine-grained classification of phishing emails based on a taxonomy of 40 manipulation techniques. Using few-shot examples with GPT-4o-mini on real-world French phishing emails (SignalSpam), we evaluated performance against a human-annotated test set (100 emails). The approach effectively identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's potential for nuanced phishing analysis and provides insights into attacker strategies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis</title>
<link>https://arxiv.org/abs/2506.22516</link>
<guid>https://arxiv.org/abs/2506.22516</guid>
<content:encoded><![CDATA[
arXiv:2506.22516v1 Announce Type: cross 
Abstract: Integrated Information Theory (IIT) provides a quantitative framework for explaining consciousness phenomenon, positing that conscious systems comprise elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the latest iterations of this framework -- to sequences of Large Language Model (LLM) representations, analyzing data derived from existing Theory of Mind (ToM) test results. Our study systematically investigates whether the differences of ToM test performances, when presented in the LLM representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT 3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure (IIT 4.0). Furthermore, we compare these metrics with the Span Representations independent of any estimate for consciousness. This additional effort aims to differentiate between potential "consciousness" phenomena and inherent separations within LLM representational space. We conduct comprehensive experiments examining variations across LLM transformer layers and linguistic spans from stimuli. Our results suggest that sequences of contemporary Transformer-based LLM representations lack statistically significant indicators of observed "consciousness" phenomena but exhibit intriguing patterns under $\textit{spatio}$-permutational analyses. The Appendix and code are available as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2506.22518</link>
<guid>https://arxiv.org/abs/2506.22518</guid>
<content:encoded><![CDATA[
arXiv:2506.22518v1 Announce Type: cross 
Abstract: Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals to the LLMs. II) Due to the abstraction of graph data, the retrieved knowledge is often presented in unorganized forms. To mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM feedback to get rid of spurious signals and improve the quality of the supervision. Meanwhile, ReG introduces a structure-aware reorganization module to refactor the retrieval results into logically coherent evidence chains. Experiments on prominent benchmarks demonstrate that ReG significantly and consistently brings improvements across different LLM backbones by up to 10%. The improved supervision quality enables ReG to match the state-of-the-art performance with 5% training data and to transfer to out-of-distribution KGs. Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token cost by up to 30% and improves the performance by up to 4%.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics</title>
<link>https://arxiv.org/abs/2506.22520</link>
<guid>https://arxiv.org/abs/2506.22520</guid>
<content:encoded><![CDATA[
arXiv:2506.22520v1 Announce Type: cross 
Abstract: This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Model Extraction Attacks and Defenses for Large Language Models</title>
<link>https://arxiv.org/abs/2506.22521</link>
<guid>https://arxiv.org/abs/2506.22521</guid>
<content:encoded><![CDATA[
arXiv:2506.22521v1 Announce Type: cross 
Abstract: Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center</title>
<link>https://arxiv.org/abs/2506.22523</link>
<guid>https://arxiv.org/abs/2506.22523</guid>
<content:encoded><![CDATA[
arXiv:2506.22523v1 Announce Type: cross 
Abstract: Generative AI is present in multiple industries. Dana-Farber Cancer Institute, in partnership with Microsoft, has created an internal AI tool, GPT4DFCI. Together we hosted a red teaming event to assess whether the underlying GPT models that support the tool would output copyrighted data. Our teams focused on reproducing content from books, news articles, scientific articles, and electronic health records. We found isolated instances where GPT4DFCI was able to identify copyrighted material and reproduce exact quotes from famous books which indicates that copyrighted material was in the training data. The model was not able to reproduce content from our target news article, scientific article, or electronic health records. However, there were instances of fabrication. As a result of this event, a mitigation strategy is in production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this report leads to similar events in which AI software tools are stress-tested to assess the perimeter of their legal and ethical usage.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlated Mutations for Integer Programming</title>
<link>https://arxiv.org/abs/2506.22526</link>
<guid>https://arxiv.org/abs/2506.22526</guid>
<content:encoded><![CDATA[
arXiv:2506.22526v1 Announce Type: cross 
Abstract: Even with the recent theoretical advancements that dramatically reduced the complexity of Integer Programming (IP), heuristics remain the dominant problem-solvers for this difficult category. This study seeks to establish the groundwork for Integer Evolution Strategies (IESs), a class of randomized search heuristics inherently designed for continuous spaces. IESs already excel in treating IP in practice, but accomplish it via discretization and by applying sophisticated patches to their continuous operators, while persistently using the $\ell_2$-norm as their operation pillar. We lay foundations for discrete search, by adopting the $\ell_1$-norm, accounting for the suitable step-size, and questioning alternative measures to quantify correlations over the integer lattice. We focus on mutation distributions for unbounded integer decision variables. We briefly discuss a couple of candidate discrete probabilities induced by the uniform and binomial distributions, which we show to possess less appealing theoretical properties, and then narrow down to the Truncated Normal (TN) and Double Geometric (DG) distributions. We explore their theoretical properties, including entropy functions, and propose a procedure to generate scalable correlated mutation distributions. Our investigations are accompanied by extensive numerical simulations, which consistently support the claim that the DG distribution is better suited for unbounded integer search. We link our theoretical perspective to empirical evidence indicating that an IES with correlated DG mutations outperformed other strategies over non-separable quadratic IP. We conclude that while the replacement of the default TN distribution by the DG is theoretically justified and practically beneficial, the truly crucial change lies in adopting the $\ell_1$-norm over the $\ell_2$-norm.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</title>
<link>https://arxiv.org/abs/2506.22554</link>
<guid>https://arxiv.org/abs/2506.22554</guid>
<content:encoded><![CDATA[
arXiv:2506.22554v1 Announce Type: cross 
Abstract: Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration Behavior of Untrained Policies</title>
<link>https://arxiv.org/abs/2506.22566</link>
<guid>https://arxiv.org/abs/2506.22566</guid>
<content:encoded><![CDATA[
arXiv:2506.22566v1 Announce Type: cross 
Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL), particularly in environments with sparse or adversarial reward structures. In this work, we study how the architecture of deep neural policies implicitly shapes exploration before training. We theoretically and empirically demonstrate strategies for generating ballistic or diffusive trajectories from untrained policies in a toy model. Using the theory of infinite-width networks and a continuous-time limit, we show that untrained policies return correlated actions and result in non-trivial state-visitation distributions. We discuss the distributions of the corresponding trajectories for a standard architecture, revealing insights into inductive biases for tackling exploration. Our results establish a theoretical and experimental framework for using policy initialization as a design tool to understand exploration behavior in early training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation</title>
<link>https://arxiv.org/abs/2506.22567</link>
<guid>https://arxiv.org/abs/2506.22567</guid>
<content:encoded><![CDATA[
arXiv:2506.22567v1 Announce Type: cross 
Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs have demonstrated impressive capabilities in zero-shot classification, cross-modal retrieval, and open-ended visual answering. However, transferring this success to biomedicine is hindered by the scarcity of large-scale biomedical image-text corpora, the heterogeneity of image modalities, and fragmented data standards across institutions. These limitations hinder the development of a unified and generalizable biomedical foundation model trained from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical foundation model developed via Multiple Medical CLIP Knowledge Distillation. Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge from nine state-of-the-art domain-specific or generalist biomedical CLIP models, each pretrained on millions of biomedical image-text pairs. Our two-stage training pipeline first performs CLIP-style pretraining on over 2.9 million biomedical image-text pairs from 26 image modalities, followed by feature-level distillation using over 19.2 million feature pairs extracted from teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets, encompassing over 10.8 million biomedical images across nine image modalities. The evaluation spans six core task types: zero-shot classification, linear probing, cross-modal retrieval, visual question answering, survival prediction, and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models while demonstrating remarkable robustness and generalization across image domains and task settings. These results underscore that multi-teacher knowledge distillation is a scalable and effective paradigm for building high-performing biomedical foundation models under the practical constraints of real-world data availability.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Link Between RLHF and Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.22578</link>
<guid>https://arxiv.org/abs/2506.22578</guid>
<content:encoded><![CDATA[
arXiv:2506.22578v1 Announce Type: cross 
Abstract: Alignment of large language models (LLMs) with human values has recently garnered significant attention, with prominent examples including the canonical yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple Direct Preference Optimization (DPO). In this work, we demonstrate that both RLHF and DPO can be interpreted from the perspective of mutual information (MI) maximization, uncovering a profound connection to contrastive learning. Within this framework, both RLHF and DPO can be viewed as methods that perform contrastive learning based on the positive and negative samples derived from the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI (equivalently, the MINE estimator). This paradigm further explains why RLHF may not intrinsically incentivize reasoning capacities in LLMs beyond what is already present in the base model. Building on this perspective, we replace the DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual Information Optimization (MIO). Comprehensive theoretical analysis and extensive empirical evaluations demonstrate that MIO mitigates the late-stage decline in chosen-likelihood observed in DPO, achieving competitive or superior performance across various challenging reasoning and mathematical benchmarks. We will release the model and code upon acceptance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.22580</link>
<guid>https://arxiv.org/abs/2506.22580</guid>
<content:encoded><![CDATA[
arXiv:2506.22580v1 Announce Type: cross 
Abstract: Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each client's loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at https://github.com/siomvas/FedCLAM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding</title>
<link>https://arxiv.org/abs/2506.22593</link>
<guid>https://arxiv.org/abs/2506.22593</guid>
<content:encoded><![CDATA[
arXiv:2506.22593v1 Announce Type: cross 
Abstract: Autonomous robots are increasingly playing key roles as support platforms for human operators in high-risk, dangerous applications. To accomplish challenging tasks, an efficient human-robot cooperation and understanding is required. While typically robotic planning leverages 3D geometric information, human operators are accustomed to a high-level compact representation of the environment, like top-down 2D maps representing the Building Information Model (BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap between human readable 2D BIM and the robot 3D maps. In this work, we introduce Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured scene graphs from image pixels and LiDAR maps in real-time for the autonomous exploration of unknown environments on resource-constrained robot platforms. To satisfy onboard compute constraints, the framework is designed to perform all operation on CPU only. The method output are a de-noised 2D top-down environment map and a structure-segmented 3D pointcloud which are seamlessly connected using a multi-layer graph abstracting information from object-level up to the building-level. The proposed method is quantitatively and qualitatively evaluated during real-world experiments performed using the NASA JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage and urban office like environments in real-time.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks</title>
<link>https://arxiv.org/abs/2506.22623</link>
<guid>https://arxiv.org/abs/2506.22623</guid>
<content:encoded><![CDATA[
arXiv:2506.22623v1 Announce Type: cross 
Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing their presence as powerful instruments permeating various sectors of society. While their utility offers valuable support to individuals, there are multiple concerns over potential misuse. Consequently, some academic endeavors have sought to introduce watermarking techniques, characterized by the inclusion of markers within machine-generated text, to facilitate algorithmic identification. This research project is focused on the development of a novel methodology for the detection of synthetic text, with the overarching goal of ensuring the ethical application of LLMs in AI-driven text generation. The investigation commences with replicating findings from a previous baseline study, thereby underscoring its susceptibility to variations in the underlying generation model. Subsequently, we propose an innovative watermarking approach and subject it to rigorous evaluation, employing paraphrased generated text to asses its robustness. Experimental results highlight the robustness of our proposal compared to the~\cite{aarson} watermarking method.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training</title>
<link>https://arxiv.org/abs/2506.22638</link>
<guid>https://arxiv.org/abs/2506.22638</guid>
<content:encoded><![CDATA[
arXiv:2506.22638v1 Announce Type: cross 
Abstract: Large language models can exhibit improved mathematical reasoning capabilities following post-training with instruction tuning, reinforcement learning, or knowledge distillation. However, it remains unclear whether these improvements are driven by major changes in transformer layers or from minor adjustments that leave the relative layer importance structures of the base model largely unchanged. We investigate this question through systematic layer-wise ablation experiments, examining base, instruction-tuned, knowledge-distilled, and reinforcement learning variants on mathematical reasoning benchmarks. Our findings show that mathematical reasoning gives rise to a specific layer importance structure, and this structure persists across all post-training paradigms. Removal of such layers causes accuracy drops of up to 80%. In contrast, non-mathematical tasks like factual recall exhibit no critical layers. This distinction suggests that mathematical reasoning requires specialized layers that emerge during pre-training, while other non-reasoning tasks do not. From an information-theoretic perspective, we also observe that these critical layers are the same layers where major representational transformation occurs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision</title>
<link>https://arxiv.org/abs/2506.22656</link>
<guid>https://arxiv.org/abs/2506.22656</guid>
<content:encoded><![CDATA[
arXiv:2506.22656v1 Announce Type: cross 
Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for automated requirements development. KGMAF aims to address gaps in current automation systems for SE, which prioritize code development and overlook the complexities of requirements tasks. KGMAF is composed of six specialized agents and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF outlines the functionality, actions, and knowledge of each agent and provides the conceptual design of the artifact pool. Our case study highlights the potential of KGMAF in real-world scenarios. Finally, we outline several research opportunities for implementing and enhancing automated requirements development using multi-agent systems. We believe that KGMAF will play a pivotal role in shaping the future of automated requirements development in the era of LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistShap: Scalable GNN Explanations with Distributed Shapley Values</title>
<link>https://arxiv.org/abs/2506.22668</link>
<guid>https://arxiv.org/abs/2506.22668</guid>
<content:encoded><![CDATA[
arXiv:2506.22668v1 Announce Type: cross 
Abstract: With the growing adoption of graph neural networks (GNNs), explaining their predictions has become increasingly important. However, attributing predictions to specific edges or features remains computationally expensive. For example, classifying a node with 100 neighbors using a 3-layer GNN may involve identifying important edges from millions of candidates contributing to the prediction. To address this challenge, we propose DistShap, a parallel algorithm that distributes Shapley value-based explanations across multiple GPUs. DistShap operates by sampling subgraphs in a distributed setting, executing GNN inference in parallel across GPUs, and solving a distributed least squares problem to compute edge importance scores. DistShap outperforms most existing GNN explanation methods in accuracy and is the first to scale to GNN models with millions of features by using up to 128 GPUs on the NERSC Perlmutter supercomputer.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report</title>
<link>https://arxiv.org/abs/2506.22698</link>
<guid>https://arxiv.org/abs/2506.22698</guid>
<content:encoded><![CDATA[
arXiv:2506.22698v1 Announce Type: cross 
Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop that brought together leading experts in cognitive psychology, language learning, and artificial intelligence (AI)-based natural language processing (NLP). The workshop, funded by the National Science Foundation, aimed to address a critical knowledge gap in our understanding of the relationship between AI language models and human cognitive processes in text comprehension and composition. Through collaborative dialogue across cognitive, linguistic, and technological perspectives, workshop participants examined the underlying processes involved when humans produce and comprehend text, and how AI can both inform our understanding of these processes and augment human capabilities. The workshop revealed emerging patterns in the relationship between large language models (LLMs) and human cognition, with highlights on both the capabilities of LLMs and their limitations in fully replicating human-like language understanding and generation. Key findings include the potential of LLMs to offer insights into human language processing, the increasing alignment between LLM behavior and human language processing when models are fine-tuned with human feedback, and the opportunities and challenges presented by human-AI collaboration in language tasks. By synthesizing these findings, this report aims to guide future research, development, and implementation of LLMs in cognitive psychology, linguistics, and education. It emphasizes the importance of ethical considerations and responsible use of AI technologies while striving to enhance human capabilities in text comprehension and production through effective human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code</title>
<link>https://arxiv.org/abs/2506.22703</link>
<guid>https://arxiv.org/abs/2506.22703</guid>
<content:encoded><![CDATA[
arXiv:2506.22703v1 Announce Type: cross 
Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial C/C++ code into OpenMP-annotated parallel code using large language models (LLMs). To our knowledge, this is the first system to apply retrieval-based prompting for OpenMP pragma correctness without model fine-tuning or compiler instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with structured instructional knowledge from OpenMP tutorials to improve the reliability of prompt-driven code generation. By grounding generation in the retrieved context, P4OMP improves syntactic correctness compared to baseline prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline, GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites. P4OMP achieves 100% compilation success on all parallelizable cases, while the baseline fails to compile in 20 out of 108 cases. Six cases that rely on non-random-access iterators or thread-unsafe constructs are excluded due to fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP consistently avoids scoping errors, syntactic misuse, and invalid directive combinations that commonly affect baseline-generated code. We further demonstrate strong runtime scaling across seven compute-intensive benchmarks on an HPC cluster. P4OMP offers a robust, modular pipeline that significantly improves the reliability and applicability of LLM-generated OpenMP code.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development</title>
<link>https://arxiv.org/abs/2506.22704</link>
<guid>https://arxiv.org/abs/2506.22704</guid>
<content:encoded><![CDATA[
arXiv:2506.22704v1 Announce Type: cross 
Abstract: Large language models (LLMs) are poised to significantly impact software development, especially in the Open-Source Software (OSS) sector. To understand this impact, we first outline the mechanisms through which LLMs may influence OSS through code development, collaborative knowledge transfer, and skill development. We then empirically examine how LLMs affect OSS developers' work in these three key areas. Leveraging a natural experiment from a temporary ChatGPT ban in Italy, we employ a Difference-in-Differences framework with two-way fixed effects to analyze data from all OSS developers on GitHub in three similar countries, Italy, France, and Portugal, totaling 88,022 users. We find that access to ChatGPT increases developer productivity by 6.4%, knowledge sharing by 9.6%, and skill acquisition by 8.4%. These benefits vary significantly by user experience level: novice developers primarily experience productivity gains, whereas more experienced developers benefit more from improved knowledge sharing and accelerated skill acquisition. In addition, we find that LLM-assisted learning is highly context-dependent, with the greatest benefits observed in technically complex, fragmented, or rapidly evolving contexts. We show that the productivity effects of LLMs extend beyond direct code generation to include enhanced collaborative learning and knowledge exchange among developers; dynamics that are essential for gaining a holistic understanding of LLMs' impact in OSS. Our findings offer critical managerial implications: strategically deploying LLMs can accelerate novice developers' onboarding and productivity, empower intermediate developers to foster knowledge sharing and collaboration, and support rapid skill acquisition, together enhancing long-term organizational productivity and agility.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers</title>
<link>https://arxiv.org/abs/2506.22706</link>
<guid>https://arxiv.org/abs/2506.22706</guid>
<content:encoded><![CDATA[
arXiv:2506.22706v1 Announce Type: cross 
Abstract: In the face of evolving cyber threats such as malware, ransomware and phishing, autonomous cybersecurity defense (ACD) systems have become essential for real-time threat detection and response with optional human intervention. However, existing ACD systems rely on limiting assumptions, particularly the stationarity of the underlying network dynamics. In real-world scenarios, network topologies can change due to actions taken by attackers or defenders, system failures, or time evolution of networks, leading to failures in the adaptive capabilities of current defense agents. Moreover, many agents are trained on static environments, resulting in overfitting to specific topologies, which hampers their ability to generalize to out-of-distribution network topologies. This work addresses these challenges by exploring methods for developing agents to learn generalizable policies across dynamic network environments -- general ACD (GACD).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute</title>
<link>https://arxiv.org/abs/2506.22716</link>
<guid>https://arxiv.org/abs/2506.22716</guid>
<content:encoded><![CDATA[
arXiv:2506.22716v1 Announce Type: cross 
Abstract: Large language models (LLMs) are powerful tools but are often expensive to deploy at scale. LLM query routing mitigates this by dynamically assigning queries to models of varying cost and quality to obtain a desired trade-off. Prior query routing approaches generate only one response from the selected model and a single response from a small (inexpensive) model was often not good enough to beat a response from a large (expensive) model due to which they end up overusing the large model and missing out on potential cost savings. However, it is well known that for small models, generating multiple responses and selecting the best can enhance quality while remaining cheaper than a single large-model response. We leverage this idea to propose BEST-Route, a novel routing framework that chooses a model and the number of responses to sample from it based on query difficulty and the quality thresholds. Experiments on real-world datasets demonstrate that our method reduces costs by up to 60% with less than 1% performance drop.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks</title>
<link>https://arxiv.org/abs/2506.22722</link>
<guid>https://arxiv.org/abs/2506.22722</guid>
<content:encoded><![CDATA[
arXiv:2506.22722v1 Announce Type: cross 
Abstract: The proposed UniGuard is the first unified online detection framework capable of simultaneously addressing adversarial examples and backdoor attacks. UniGuard builds upon two key insights: first, both AE and backdoor attacks have to compromise the inference phase, making it possible to tackle them simultaneously during run-time via online detection. Second, an adversarial input, whether a perturbed sample in AE attacks or a trigger-carrying sample in backdoor attacks, exhibits distinctive trajectory signatures from a benign sample as it propagates through the layers of a DL model in forward inference. The propagation trajectory of the adversarial sample must deviate from that of its benign counterpart; otherwise, the adversarial objective cannot be fulfilled. Detecting these trajectory signatures is inherently challenging due to their subtlety; UniGuard overcomes this by treating the propagation trajectory as a time-series signal, leveraging LSTM and spectrum transformation to amplify differences between adversarial and benign trajectories that are subtle in the time domain. UniGuard exceptional efficiency and effectiveness have been extensively validated across various modalities (image, text, and audio) and tasks (classification and regression), ranging from diverse model architectures against a wide range of AE attacks and backdoor attacks, including challenging partial backdoors and dynamic triggers. When compared to SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED (IEEE SP 24) specific for backdoor detection, UniGuard consistently demonstrates superior performance, even when matched against each method's strengths in addressing their respective threats-each SOTA fails to parts of attack strategies while UniGuard succeeds for all.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAILS: Retrieval-Augmented Intelligence for Learning Software Development</title>
<link>https://arxiv.org/abs/2506.22742</link>
<guid>https://arxiv.org/abs/2506.22742</guid>
<content:encoded><![CDATA[
arXiv:2506.22742v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to assist software development, yet they often produce incomplete code or incorrect imports, especially when lacking access to external or project-specific documentation. We introduce RAILS (Retrieval-Augmented Intelligence for Learning Software Development), a framework that augments LLM prompts with semantically retrieved context from curated Java resources using FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop guided by compiler feedback to refine suggestions. We evaluated RAILS on 78 real-world Java import error cases spanning standard libraries, GUI APIs, external tools, and custom utilities. Despite using the same LLM, RAILS outperforms baseline prompting by preserving intent, avoiding hallucinations, and surfacing correct imports even when libraries are unavailable locally. Future work will integrate symbolic filtering via PostgreSQL and extend support to other languages and IDEs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision</title>
<link>https://arxiv.org/abs/2506.22771</link>
<guid>https://arxiv.org/abs/2506.22771</guid>
<content:encoded><![CDATA[
arXiv:2506.22771v1 Announce Type: cross 
Abstract: Backpropagation has been the cornerstone of neural network training for decades, yet its inefficiencies in time and energy consumption limit its suitability for resource-constrained edge devices. While low-precision neural network quantization has been extensively researched to speed up model inference, its application in training has been less explored. Recently, the Forward-Forward (FF) algorithm has emerged as a promising alternative to backpropagation, replacing the backward pass with an additional forward pass. By avoiding the need to store intermediate activations for backpropagation, FF can reduce memory footprint, making it well-suited for embedded devices. This paper presents an INT8 quantized training approach that leverages FF's layer-by-layer strategy to stabilize gradient quantization. Furthermore, we propose a novel "look-ahead" scheme to address limitations of FF and improve model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in memory usage, while maintaining competitive accuracy compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation</title>
<link>https://arxiv.org/abs/2506.22776</link>
<guid>https://arxiv.org/abs/2506.22776</guid>
<content:encoded><![CDATA[
arXiv:2506.22776v1 Announce Type: cross 
Abstract: Quantization has emerged as a mainstream method for compressing Large Language Models (LLMs), reducing memory requirements and accelerating inference without architectural modifications. While existing research primarily focuses on evaluating the effectiveness of quantized LLMs compared to their original counterparts, the impact on robustness remains largely unexplored.In this paper, we present the first systematic investigation of how quantization affects the robustness of LLMs in code generation tasks. Through extensive experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and StarCoder) with parameter scales ranging from 350M to 33B, we evaluate robustness from dual perspectives: adversarial attacks on input prompts and noise perturbations on model architecture. Our findings challenge conventional wisdom by demonstrating that quantized LLMs often exhibit superior robustness compared to their full-precision counterparts, with 51.59% versus 42.86% of our adversarial experiments showing better resilience in quantized LLMs. Similarly, our noise perturbation experiments also confirm that LLMs after quantitation generally withstand higher levels of weight disturbances. These results suggest that quantization not only reduces computational requirements but can actually enhance LLMs' reliability in code generation tasks, providing valuable insights for developing more robust and efficient LLM deployment strategies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.22777</link>
<guid>https://arxiv.org/abs/2506.22777</guid>
<content:encoded><![CDATA[
arXiv:2506.22777v1 Announce Type: cross 
Abstract: Language models trained with RL can engage in reward hacking--exploiting unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning, making detection difficult and posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g., "a Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to reward hack by exploiting cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while baselines remain low even after RL (10% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection</title>
<link>https://arxiv.org/abs/2506.22783</link>
<guid>https://arxiv.org/abs/2506.22783</guid>
<content:encoded><![CDATA[
arXiv:2506.22783v1 Announce Type: cross 
Abstract: Deepfake (DF) attacks pose a growing threat as generative models become increasingly advanced. However, our study reveals that existing DF datasets fail to deceive human perception, unlike real DF attacks that influence public discourse. It highlights the need for more realistic DF attack vectors. We introduce PhonemeFake (PF), a DF attack that manipulates critical speech segments using language reasoning, significantly reducing human perception by up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF dataset on HuggingFace and open-source bilevel DF segment detection model that adaptively prioritizes compute on manipulated regions. Our extensive experiments across three known DF datasets reveal that our detection model reduces EER by 91% while achieving up to 90% speed-up, with minimal compute overhead and precise localization beyond existing models as a scalable solution.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching</title>
<link>https://arxiv.org/abs/2506.22784</link>
<guid>https://arxiv.org/abs/2506.22784</guid>
<content:encoded><![CDATA[
arXiv:2506.22784v1 Announce Type: cross 
Abstract: Point-pixel registration between LiDAR point clouds and camera images is a fundamental yet challenging task in autonomous driving and robotic perception. A key difficulty lies in the modality gap between unstructured point clouds and structured images, especially under sparse single-frame LiDAR settings. Existing methods typically extract features separately from point clouds and images, then rely on hand-crafted or learned matching strategies. This separate encoding fails to bridge the modality gap effectively, and more critically, these methods struggle with the sparsity and noise of single-frame LiDAR, often requiring point cloud accumulation or additional priors to improve reliability. Inspired by recent progress in detector-free matching paradigms (e.g. MatchAnything), we revisit the projection-based approach and introduce the detector-free framework for direct point-pixel matching between LiDAR and camera views. Specifically, we project the LiDAR intensity map into a 2D view from the LiDAR perspective and feed it into an attention-based detector-free matching network, enabling cross-modal correspondence estimation without relying on multi-frame accumulation. To further enhance matching reliability, we introduce a repeatability scoring mechanism that acts as a soft visibility prior. This guides the network to suppress unreliable matches in regions with low intensity variation, improving robustness under sparse input. Extensive experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that our method achieves state-of-the-art performance, outperforming prior approaches on nuScenes (even those relying on accumulated point clouds), despite using only single-frame LiDAR.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing</title>
<link>https://arxiv.org/abs/2506.22789</link>
<guid>https://arxiv.org/abs/2506.22789</guid>
<content:encoded><![CDATA[
arXiv:2506.22789v1 Announce Type: cross 
Abstract: Speech embeddings often retain sensitive attributes such as speaker identity, accent, or demographic information, posing risks in biased model training and privacy leakage. We propose WavShape, an information-theoretic speech representation learning framework that optimizes embeddings for fairness and privacy while preserving task-relevant information. We leverage mutual information (MI) estimation using the Donsker-Varadhan formulation to guide an MI-based encoder that systematically filters sensitive attributes while maintaining speech content essential for downstream tasks. Experimental results on three known datasets show that WavShape reduces MI between embeddings and sensitive attributes by up to 81% while retaining 97% of task-relevant information. By integrating information theory with self-supervised speech models, this work advances the development of fair, privacy-aware, and resource-efficient speech systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Reinforcement Learning for Mobility Robustness Optimization</title>
<link>https://arxiv.org/abs/2506.22793</link>
<guid>https://arxiv.org/abs/2506.22793</guid>
<content:encoded><![CDATA[
arXiv:2506.22793v1 Announce Type: cross 
Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm and study the possibility of learning the optimal Cell Individual Offset tuning using offline Reinforcement Learning. Such methods make use of collected offline datasets to learn the optimal policy, without further exploration. We adapt and apply a sequence-based method called Decision Transformers as well as a value-based method called Conservative Q-Learning to learn the optimal policy for the same target reward as the vanilla rule-based MRO. The same input features related to failures, ping-pongs, and other handover issues are used. Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on a traffic mix including diverse user service types and a specific tunable cell-pair shows that offline-RL methods outperform rule-based MRO, offering up to 7% improvement. Furthermore, offline-RL can be trained for diverse objective functions using the same available dataset, thus offering operational flexibility compared to rule-based methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2506.22808</link>
<guid>https://arxiv.org/abs/2506.22808</guid>
<content:encoded><![CDATA[
arXiv:2506.22808v1 Announce Type: cross 
Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable potential in clinical tasks, their ethical safety remains insufficiently explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive benchmark comprising $\textbf{5,623}$ multiple-choice questions and $\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs. We systematically establish a hierarchical taxonomy integrating global medical ethical standards. The benchmark encompasses widely used medical datasets, authoritative question banks, and scenarios derived from PubMed literature. Rigorous quality control involving multi-stage filtering and multi-faceted expert validation ensures the reliability of the dataset with a low error rate ($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance in answering medical ethics questions compared to their foundation counterparts, elucidating the deficiencies of medical ethics alignment. The dataset, registered under CC BY-NC 4.0 license, is available at https://github.com/JianhuiWei7/MedEthicsQA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters</title>
<link>https://arxiv.org/abs/2506.22809</link>
<guid>https://arxiv.org/abs/2506.22809</guid>
<content:encoded><![CDATA[
arXiv:2506.22809v1 Announce Type: cross 
Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike general-purpose transformer uncertainty methods, BayesLoRA provides guardrails tailored to downstream workflows, enabling agents to introspect and modulate behavior under uncertainty. We demonstrate mathematically and empirically that LoRA adapters exhibit amplified variance outside fine-tuning distributions, yielding reliable confidence estimates for agentic decision-making.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations</title>
<link>https://arxiv.org/abs/2506.22818</link>
<guid>https://arxiv.org/abs/2506.22818</guid>
<content:encoded><![CDATA[
arXiv:2506.22818v1 Announce Type: cross 
Abstract: Multilinear transformations are key in high-performance computing (HPC) and artificial intelligence (AI) workloads, where data is represented as tensors. However, their high computational and memory demands, which grow with dimensionality, often slow down critical tasks. Moreover, scaling computation by enlarging the number of parallel processing units substantially increases energy consumption, limiting widespread adoption, especially for sparse data, which is common in HPC and AI applications. This paper introduces the Trilinear Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address these challenges with the following innovations: (1) a massively parallel, low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal transformations (3D-DXTs), which is a special case of the more general 3-mode matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM kernel with decoupled streaming active memory, specially designed to accelerate 3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully distributed 3D network of mesh interconnected processing elements or cells with a coordinate-free, data-driven local processing activity, which is independent of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids unnecessary computing and communication operations with zero-valued operands, thereby enhancing energy efficiency, computational accuracy, and stability. TriADA is capable of performing a variety of trilinear transformations with hypercubic arithmetic complexity in a linear number of time-steps. The massively parallel, scalable, and energy-efficient architecture of TriADA is ideal for accelerating multilinear tensor operations, which are the most demanding parts of AI and HPC workloads.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listener-Rewarded Thinking in VLMs for Image Preferences</title>
<link>https://arxiv.org/abs/2506.22832</link>
<guid>https://arxiv.org/abs/2506.22832</guid>
<content:encoded><![CDATA[
arXiv:2506.22832v1 Announce Type: cross 
Abstract: Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model's reasoning trace contradicts that of an independent, frozen vision-language model ("listener") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner's chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.22837</link>
<guid>https://arxiv.org/abs/2506.22837</guid>
<content:encoded><![CDATA[
arXiv:2506.22837v1 Announce Type: cross 
Abstract: The recently proposed xLSTM is a powerful model that leverages expressive multiplicative gating and residual connections, providing the temporal capacity needed for long-horizon forecasting and representation learning. This architecture has demonstrated success in time series forecasting, lossless compression, and even large-scale language modeling tasks, where its linear memory footprint and fast inference make it a viable alternative to Transformers. Despite its growing popularity, no prior work has explored xLSTM for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the first anomaly detection method that integrates a full encoder-decoder xLSTM architecture, purpose-built for multivariate time series data. Our encoder processes input sequences to capture historical context, while the decoder is devised in two separate variants of the method. In the forecasting approach, the decoder iteratively generates forecasted future values xLSTMAD-F, while the reconstruction approach reconstructs the input time series from its encoded counterpart xLSTMAD-R. We investigate the performance of two loss functions: Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider local reconstruction fidelity and global sequence alignment, respectively. We evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17 real-world datasets, using state-of-the-art challenging metrics such as VUS-PR. In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23 popular anomaly detection baselines. Our paper is the first work revealing the powerful modeling capabilities of xLSTM for anomaly detection, paving the way for exciting new developments on this subject. Our code is available at: https://github.com/Nyderx/xlstmad
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models</title>
<link>https://arxiv.org/abs/2506.22845</link>
<guid>https://arxiv.org/abs/2506.22845</guid>
<content:encoded><![CDATA[
arXiv:2506.22845v1 Announce Type: cross 
Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine Learning (QML), are emerging as a powerful alternative to classical machine learning methods. Recent studies have focused on the applicability of QNNs to various tasks, such as time-series forecasting, prediction, and classification, across a wide range of applications, including cybersecurity and medical imaging. With the increased use of smart grids driven by the integration of renewable energy systems, machine learning plays an important role in predicting power demand and detecting system disturbances. This study provides an in-depth investigation of QNNs for predicting the power output of a wind turbine. We assess the predictive performance and simulation time of six QNN configurations that are based on the Z Feature Map for data encoding and varying ansatz structures. Through detailed cross-validation experiments and tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs can achieve predictive performance that is competitive with, and in some cases marginally better than, the benchmarked classical approaches. Our results also reveal the effects of dataset size and circuit complexity on predictive performance and simulation time. We believe our findings will offer valuable insights for researchers in the energy domain who wish to incorporate quantum machine learning into their work.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles</title>
<link>https://arxiv.org/abs/2506.22848</link>
<guid>https://arxiv.org/abs/2506.22848</guid>
<content:encoded><![CDATA[
arXiv:2506.22848v1 Announce Type: cross 
Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging, especially for datasets involving a large number of variables. The recently proposed divide-and-conquer (D\&amp;D) strategies present a promising approach for learning large BNs. However, they still face a main issue of unstable learning accuracy across subproblems. In this work, we introduce the idea of employing structure learning ensemble (SLE), which combines multiple BN structure learning algorithms, to consistently achieve high learning accuracy. We further propose an automatic approach called Auto-SLE for learning near-optimal SLEs, addressing the challenge of manually designing high-quality SLEs. The learned SLE is then integrated into a D\&amp;D method. Extensive experiments firmly show the superiority of our method over D\&amp;D methods with single BN structure learning algorithm in learning large BNs, achieving accuracy improvement usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore, our method generalizes well to datasets with many more (e.g., 30000) variables and different network characteristics than those present in the training data for learning the SLE. These results indicate the significant potential of employing (automatic learning of) SLEs for scalable BN structure learning.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues</title>
<link>https://arxiv.org/abs/2506.22853</link>
<guid>https://arxiv.org/abs/2506.22853</guid>
<content:encoded><![CDATA[
arXiv:2506.22853v1 Announce Type: cross 
Abstract: Existing function-calling benchmarks focus on single-turn interactions. However, they overlook the complexity of real-world scenarios. To quantify how existing benchmarks address practical applications, we introduce DICE-SCORE, a metric that evaluates the dispersion of tool-related information such as function name and parameter values throughout the dialogue. Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios. To address this gap, we present DICE-BENCH, a framework that constructs practical function-calling datasets by synthesizing conversations through a tool graph that maintains dependencies across rounds and a multi-agent system with distinct personas to enhance dialogue naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our experiments on 19 LLMs with DICE-BENCH show that significant advances are still required before such models can be deployed effectively in real-world settings. Our code and data are all publicly available: https://snuhcc.github.io/DICE-Bench/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval</title>
<link>https://arxiv.org/abs/2506.22864</link>
<guid>https://arxiv.org/abs/2506.22864</guid>
<content:encoded><![CDATA[
arXiv:2506.22864v1 Announce Type: cross 
Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual query, but existing approaches are primarily based on whole-image captions and lack interpretability. Meanwhile, referring expression segmentation (RES) enables precise object localization based on natural language descriptions but is computationally expensive when applied across large image collections. To bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies TIR and RES, requiring both efficient image search and accurate object segmentation. To address this task, we propose a two-stage framework, comprising a first stage for segmentation-aware image retrieval and a second stage for reranking and object grounding with a multimodal large language model (MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract region-level embeddings offline at first, enabling effective and scalable online retrieval. Secondly, MLLM is used to refine retrieval rankings and generate bounding boxes, which are matched to segmentation masks. We evaluate our approach on COCO and D$^3$ datasets, demonstrating significant improvements in both retrieval accuracy and segmentation quality over previous methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception</title>
<link>https://arxiv.org/abs/2506.22866</link>
<guid>https://arxiv.org/abs/2506.22866</guid>
<content:encoded><![CDATA[
arXiv:2506.22866v1 Announce Type: cross 
Abstract: Surface defect detection plays a critical role in industrial quality inspection. Recent advances in artificial intelligence have significantly enhanced the automation level of detection processes. However, conventional semantic segmentation and object detection models heavily rely on large-scale annotated datasets, which conflicts with the practical requirements of defect detection tasks. This paper proposes a novel weakly supervised semantic segmentation framework comprising two key components: a region-aware class activation map (CAM) and pseudo-label training. To address the limitations of existing CAM methods, especially low-resolution thermal maps, and insufficient detail preservation, we introduce filtering-guided backpropagation (FGBP), which refines target regions by filtering gradient magnitudes to identify areas with higher relevance to defects. Building upon this, we further develop a region-aware weighted module to enhance spatial precision. Finally, pseudo-label segmentation is implemented to refine the model's performance iteratively. Comprehensive experiments on industrial defect datasets demonstrate the superiority of our method. The proposed framework effectively bridges the gap between weakly supervised learning and high-precision defect segmentation, offering a practical solution for resource-constrained industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing</title>
<link>https://arxiv.org/abs/2506.22868</link>
<guid>https://arxiv.org/abs/2506.22868</guid>
<content:encoded><![CDATA[
arXiv:2506.22868v1 Announce Type: cross 
Abstract: Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder</title>
<link>https://arxiv.org/abs/2506.22880</link>
<guid>https://arxiv.org/abs/2506.22880</guid>
<content:encoded><![CDATA[
arXiv:2506.22880v1 Announce Type: cross 
Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA, directly fuse features within segmentation models. This often results in an undesirable entanglement of dynamic visual information and static semantics, thereby degrading segmentation accuracy. To systematically mitigate this issue, we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text pre-training and a linear decoupling module to address the information processing limitations inherent in SAM-2. Specifically, first, we devise a pre-training paradigm that converts textual ground-truth labels into point-level prompts while generating corresponding text masks. These masks are refined through a hybrid loss function to strengthen the model's semantic grounding capabilities. Next, we employ linear projection to disentangle hidden states that generated by a large language model into distinct textual and visual feature subspaces. Finally, a dynamic mask fusion strategy synergistically combines these decoupled features through triple supervision from predicted text/visual masks and ground-truth annotations. Extensive experiments demonstrate state-of-the-art performance across diverse tasks, including image segmentation, image question answering, video segmentation, and video question answering. Our codes are available at https://github.com/longmalongma/DeSa2VA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Measurements in the AI-Centric Computing Continuum Systems</title>
<link>https://arxiv.org/abs/2506.22884</link>
<guid>https://arxiv.org/abs/2506.22884</guid>
<content:encoded><![CDATA[
arXiv:2506.22884v1 Announce Type: cross 
Abstract: Over the Eight decades, computing paradigms have shifted from large, centralized systems to compact, distributed architectures, leading to the rise of the Distributed Computing Continuum (DCC). In this model, multiple layers such as cloud, edge, Internet of Things (IoT), and mobile platforms work together to support a wide range of applications. Recently, the emergence of Generative AI and large language models has further intensified the demand for computational resources across this continuum. Although traditional performance metrics have provided a solid foundation, they need to be revisited and expanded to keep pace with changing computational demands and application requirements. Accurate performance measurements benefit both system designers and users by supporting improvements in efficiency and promoting alignment with system goals. In this context, we review commonly used metrics in DCC and IoT environments. We also discuss emerging performance dimensions that address evolving computing needs, such as sustainability, energy efficiency, and system observability. We also outline criteria and considerations for selecting appropriate metrics, aiming to inspire future research and development in this critical area.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Time Series Autoregression for Periodicity Quantification</title>
<link>https://arxiv.org/abs/2506.22895</link>
<guid>https://arxiv.org/abs/2506.22895</guid>
<content:encoded><![CDATA[
arXiv:2506.22895v1 Announce Type: cross 
Abstract: Time series autoregression is a classical statistical model for capturing auto-correlations and identifying temporal patterns such as periodicity and seasonality. In this work, we propose a novel sparse autoregression framework from an interpretable machine learning perspective and the model interpretability for periodicity quantification is reinforced by $\ell_0$-norm induced sparsity constraints. On the time-varying time series data, we reformulate the sparse autoregression and convert the involved optimization problem into a mixed-integer optimization (MIO). To accelerate it, we develop a subspace pursuit based decision variable pruning (DVP) strategy to reduce the search space. On the multidimensional time series that involves complicated spatial and temporal dimensions, we propose a spatially- and time-varying sparse autoregression model and resolve the corresponding MIO problem by developing a two-stage optimization scheme. In particular, the proposed scheme makes the model scalable to large problems even with millions of decision variables. Empirically, we conduct extensive experiments to evaluate the proposed models on real-world time series data. First, we demonstrate that the MIO solver can be drastically accelerated through the DVP strategy, while maintaining the same solution quality as a full MIO solver. Applying the time-varying sparse autoregression model to ridesharing trip data, we uncover both daily and weekly periodicities and reveal long-term changes in regularity of human mobility. Second, we demonstrate the spatial patterns of yearly seasonality in climate variable time series such as temperature and precipitation across the past four decades, and our model allows to discover dynamic climate patterns and identify climate phenomena such as El Nino in sea surface temperature.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing-Modality-Aware Graph Neural Network for Cancer Classification</title>
<link>https://arxiv.org/abs/2506.22901</link>
<guid>https://arxiv.org/abs/2506.22901</guid>
<content:encoded><![CDATA[
arXiv:2506.22901v1 Announce Type: cross 
Abstract: A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET's complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at https://github.com/SinaTabakhi/MAGNET.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Truthful Mechanisms without Discretization</title>
<link>https://arxiv.org/abs/2506.22911</link>
<guid>https://arxiv.org/abs/2506.22911</guid>
<content:encoded><![CDATA[
arXiv:2506.22911v1 Announce Type: cross 
Abstract: This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive approach), a discretization-free algorithm to learn truthful and utility-maximizing mechanisms. Existing learning-based approaches often rely on discretization of outcome spaces to ensure truthfulness, which leads to inefficiency with increasing problem size. To address this limitation, we formalize the concept of pricing rules, defined as functions that map outcomes to prices. Based on this concept, we propose a novel menu mechanism, which can be equivalent to a truthful direct mechanism under specific conditions. The core idea of TEDI lies in its parameterization of pricing rules using Partial GroupMax Network, a new network architecture designed to universally approximate partial convex functions. To learn optimal pricing rules, we develop novel training techniques, including covariance trick and continuous sampling, to derive unbiased gradient estimators compatible with first-order optimization. Theoretical analysis establishes that TEDI guarantees truthfulness, full expressiveness, and dimension-insensitivity. Experimental evaluation in the studied auction setting demonstrates that TEDI achieves strong performance, competitive with or exceeding state-of-the-art methods.
  This work presents the first approaches to learn truthful mechanisms without outcome discretization, thereby enhancing algorithmic efficiency. The proposed concepts, network architecture, and learning techniques might offer potential value and provide new insights for automated mechanism design and differentiable economics.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration</title>
<link>https://arxiv.org/abs/2506.22929</link>
<guid>https://arxiv.org/abs/2506.22929</guid>
<content:encoded><![CDATA[
arXiv:2506.22929v1 Announce Type: cross 
Abstract: While deep learning excels in natural image and language processing, its application to high-dimensional data faces computational challenges due to the dimensionality curse. Current large-scale data tools focus on business-oriented descriptive statistics, lacking mathematical statistics support for advanced analysis. We propose a parallel computation architecture based on space completeness, decomposing high-dimensional data into dimension-independent structures for distributed processing. This framework enables seamless integration of data mining and parallel-optimized machine learning methods, supporting scientific computations across diverse data types like medical and natural images within a unified system.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions</title>
<link>https://arxiv.org/abs/2506.22941</link>
<guid>https://arxiv.org/abs/2506.22941</guid>
<content:encoded><![CDATA[
arXiv:2506.22941v1 Announce Type: cross 
Abstract: Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance</title>
<link>https://arxiv.org/abs/2506.22949</link>
<guid>https://arxiv.org/abs/2506.22949</guid>
<content:encoded><![CDATA[
arXiv:2506.22949v1 Announce Type: cross 
Abstract: One of the most difficult challenges in cybersecurity is eliminating Distributed Denial of Service (DDoS) attacks. Automating this task using artificial intelligence is a complex process due to the inherent class imbalance and lack of sufficient labeled samples of real-world datasets. This research investigates the use of Semi-Supervised Learning (SSL) techniques to improve DDoS attack detection when data is imbalanced and partially labeled. In this process, 13 state-of-the-art SSL algorithms are evaluated for detecting DDoS attacks in several scenarios. We evaluate their practical efficacy and shortcomings, including the extent to which they work in extreme environments. The results will offer insight into designing intelligent Intrusion Detection Systems (IDSs) that are robust against class imbalance and handle partially labeled data.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</title>
<link>https://arxiv.org/abs/2506.22957</link>
<guid>https://arxiv.org/abs/2506.22957</guid>
<content:encoded><![CDATA[
arXiv:2506.22957v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Against 'softmaxing' culture</title>
<link>https://arxiv.org/abs/2506.22968</link>
<guid>https://arxiv.org/abs/2506.22968</guid>
<content:encoded><![CDATA[
arXiv:2506.22968v1 Announce Type: cross 
Abstract: AI is flattening culture. Evaluations of "culture" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon "softmaxing culture," and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key shifts. First, instead of asking "what is culture?" at the start of system evaluations, I propose beginning with the question: "when is culture?" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements, toward perspectives more responsive to the complexities of culture.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Study of Compositional Syntactic Transformer Language Models</title>
<link>https://arxiv.org/abs/2506.22978</link>
<guid>https://arxiv.org/abs/2506.22978</guid>
<content:encoded><![CDATA[
arXiv:2506.22978v1 Announce Type: cross 
Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Privacy Stories From Software Documentation</title>
<link>https://arxiv.org/abs/2506.23014</link>
<guid>https://arxiv.org/abs/2506.23014</guid>
<content:encoded><![CDATA[
arXiv:2506.23014v1 Announce Type: cross 
Abstract: Research shows that analysts and developers consider privacy as a security concept or as an afterthought, which may lead to non-compliance and violation of users' privacy. Most current approaches, however, focus on extracting legal requirements from the regulations and evaluating the compliance of software and processes with them. In this paper, we develop a novel approach based on chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language Models (LLMs) to extract privacy behaviors from various software documents prior to and during software development, and then generate privacy requirements in the format of user stories. Our results show that most commonly used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and generate privacy user stories with F1 scores exceeding 0.8. We also show that the performance of these models could be improved through parameter-tuning. Our findings provide insight into using and optimizing LLMs for generating privacy requirements given software documents created prior to or throughout the software development lifecycle.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making</title>
<link>https://arxiv.org/abs/2506.23023</link>
<guid>https://arxiv.org/abs/2506.23023</guid>
<content:encoded><![CDATA[
arXiv:2506.23023v1 Announce Type: cross 
Abstract: Developing decision-making algorithms for highly automated driving systems remains challenging, since these systems have to operate safely in an open and complex environments. Reinforcement Learning (RL) approaches can learn comprehensive decision policies directly from experience and already show promising results in simple driving tasks. However, current approaches fail to achieve generalizability for more complex driving tasks and lack learning efficiency. Therefore, we present Scenario-based Automated Driving Reinforcement Learning (SAD-RL), the first framework that integrates Reinforcement Learning (RL) of hierarchical policy in a scenario-based environment. A high-level policy selects maneuver templates that are evaluated and executed by a low-level control logic. The scenario-based environment allows to control the training experience for the agent and to explicitly introduce challenging, but rate situations into the training process. Our experiments show that an agent trained using the SAD-RL framework can achieve safe behaviour in easy as well as challenging situations efficiently. Our ablation studies confirmed that both HRL and scenario diversity are essential for achieving these results.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs</title>
<link>https://arxiv.org/abs/2506.23024</link>
<guid>https://arxiv.org/abs/2506.23024</guid>
<content:encoded><![CDATA[
arXiv:2506.23024v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve partial differential equations (PDEs) with machine learning, yet they still fall well short of the machine-precision accuracy many scientific tasks demand. In this work, we investigate whether the precision ceiling comes from the ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP) architecture. We introduce the Barycentric Weight Layer (BWLer), which models the PDE solution through barycentric polynomial interpolation. A BWLer can be added on top of an existing MLP (a BWLer-hat) or replace it completely (explicit BWLer), cleanly separating how we represent the solution from how we take derivatives for the PDE loss. Using BWLer, we identify fundamental precision limitations within the MLP: on a simple 1-D interpolation task, even MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above float64 machine precision -- before any PDE terms are added. In PDE learning, adding a BWLer lifts this ceiling and exposes a tradeoff between achievable accuracy and the conditioning of the PDE loss. For linear PDEs we fully characterize this tradeoff with an explicit error decomposition and navigate it during training with spectral derivatives and preconditioning. Across five benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for convection, 10x for reaction, and 1800x for wave equations while remaining compatible with first-order optimizers. Replacing the MLP entirely lets an explicit BWLer reach near-machine-precision on convection, reaction, and wave problems (up to 10 billion times better than prior results) and match the performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson problems. Together, these findings point to a practical path for combining the flexibility of PINNs with the precision of classical spectral solvers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models</title>
<link>https://arxiv.org/abs/2506.23025</link>
<guid>https://arxiv.org/abs/2506.23025</guid>
<content:encoded><![CDATA[
arXiv:2506.23025v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used across research and industry applications, yet their inference efficiency remains a significant challenge. As the computational power of modern GPU architectures continuously improves, their memory bandwidth and capacity have not scaled proportionally, creating a critical bottleneck during inference. To address this, we investigate ternary language models (TriLMs) that employ quantization-aware training to significantly reduce memory requirements. We first analyze the scalability of TriLMs by conducting a scaling law analysis, revealing that TriLMs benefit more from increasing training data than from scaling model parameters. Based on this observation, we introduce Spectra-1.1, an open suite of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained performance gains at scale. Furthermore, to improve inference efficiency, we propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which demonstrate accelerated inference across various CPU architectures. Also, building on the 2-bit packing, we develop a GPU kernel called TriRun that accelerates end-to-end model inference by up to 5 times compared to floating-point baselines. To encourage further exploration and development of TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels. Overall, our work lays the foundation for building and deploying efficient LLMs, providing a valuable resource for the research community.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisionScores -- A system-segmented image score dataset for deep learning tasks</title>
<link>https://arxiv.org/abs/2506.23030</link>
<guid>https://arxiv.org/abs/2506.23030</guid>
<content:encoded><![CDATA[
arXiv:2506.23030v1 Announce Type: cross 
Abstract: VisionScores presents a novel proposal being the first system-segmented image score dataset, aiming to offer structure-rich, high information-density images for machine and deep learning tasks. Delimited to two-handed piano pieces, it was built to consider not only certain graphic similarity but also composition patterns, as this creative process is highly instrument-dependent. It provides two scenarios in relation to composer and composition type. The first, formed by 14k samples, considers works from different authors but the same composition type, specifically, Sonatinas. The latter, consisting of 10.8K samples, presents the opposite case, various composition types from the same author, being the one selected Franz Liszt. All of the 24.8k samples are formatted as grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the users not only the formatted samples but the systems' order and pieces' metadata. Moreover, unsegmented full-page scores and the pre-formatted images are included for further analysis.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Treatment, evidence, imitation, and chat</title>
<link>https://arxiv.org/abs/2506.23040</link>
<guid>https://arxiv.org/abs/2506.23040</guid>
<content:encoded><![CDATA[
arXiv:2506.23040v1 Announce Type: cross 
Abstract: Large language models are thought to have potential to aid in medical decision making. We investigate this here. We start with the treatment problem, the patient's core medical decision-making task, which is solved in collaboration with a healthcare provider. We discuss approaches to solving the treatment problem, including -- within evidence-based medicine -- trials and observational data. We then discuss the chat problem, and how this differs from the treatment problem -- in particular as it relates to imitation. We then discuss how a large language model might be used to solve the treatment problem and highlight some of the challenges that emerge. We finally discuss how these challenges relate to evidence-based medicine, and how this might inform next steps.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ovis-U1 Technical Report</title>
<link>https://arxiv.org/abs/2506.23044</link>
<guid>https://arxiv.org/abs/2506.23044</guid>
<content:encoded><![CDATA[
arXiv:2506.23044v1 Announce Type: cross 
Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[
arXiv:2506.23046v1 Announce Type: cross 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis</title>
<link>https://arxiv.org/abs/2506.23055</link>
<guid>https://arxiv.org/abs/2506.23055</guid>
<content:encoded><![CDATA[
arXiv:2506.23055v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities in producing human-like text. However, it is unclear how accurately these models internalize concepts that shape human thought and behavior. Here, we developed a quantitative framework to assess concept alignment between LLMs and human psychological dimensions using 43 standardized psychological questionnaires, selected for their established validity in measuring distinct psychological constructs. Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis. We compared resulting cluster structures with the original categorical labels using hierarchical clustering. A GPT-4 model achieved superior classification accuracy (66.2\%), significantly outperforming GPT-3.5 (55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%). We also demonstrated that the estimated semantic similarity from GPT-4 is associated with Pearson's correlation coefficients of human responses in multiple psychological questionnaires. This framework provides a novel approach to evaluate the alignment of the human-LLM concept and identify potential representational biases. Our findings demonstrate that modern LLMs can approximate human psychological constructs with measurable accuracy, offering insights for developing more interpretable AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curious Causality-Seeking Agents Learn Meta Causal World</title>
<link>https://arxiv.org/abs/2506.23068</link>
<guid>https://arxiv.org/abs/2506.23068</guid>
<content:encoded><![CDATA[
arXiv:2506.23068v1 Announce Type: cross 
Abstract: When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences</title>
<link>https://arxiv.org/abs/2506.23085</link>
<guid>https://arxiv.org/abs/2506.23085</guid>
<content:encoded><![CDATA[
arXiv:2506.23085v1 Announce Type: cross 
Abstract: The purpose of this paper is to explore a multi-modal approach to enhancing live broadcast engagement by developing a short video recommendation system that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user preferences. In order to provide personalized recommendations tailored to individual interests, the proposed system takes into account user interaction data, video content features, and contextual information. With the aid of a hybrid approach combining collaborative filtering and content-based filtering techniques, the system is able to capture nuanced relationships between users, video attributes, and engagement patterns. Three datasets are used to evaluate the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the proposed MMGCN-based model shows superior performance. A notable feature of the proposed model is that it outperforms all baseline methods in capturing diverse user preferences and making accurate, personalized recommendations, resulting in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1 score of 0.197. We emphasize the importance of multi-modal integration and user-centric approaches in advancing recommender systems, emphasizing the role they play in enhancing content discovery and audience interaction on live broadcast platforms.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure</title>
<link>https://arxiv.org/abs/2506.23094</link>
<guid>https://arxiv.org/abs/2506.23094</guid>
<content:encoded><![CDATA[
arXiv:2506.23094v1 Announce Type: cross 
Abstract: Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them--across musical time and space--into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship</title>
<link>https://arxiv.org/abs/2506.23101</link>
<guid>https://arxiv.org/abs/2506.23101</guid>
<content:encoded><![CDATA[
arXiv:2506.23101v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities across tasks involving both visual and textual modalities. However, growing concerns remain about their potential to encode and amplify gender bias, particularly in socially sensitive applications. Existing benchmarks predominantly evaluate bias in isolated scenarios, overlooking how bias may emerge subtly through interpersonal interactions. We fill this gap by going beyond single-entity evaluation and instead focusing on a deeper examination of relational and contextual gender bias in dual-individual interactions. We introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs through the lens of social relationships in generated narratives. Genres assesses gender bias through a dual-character profile and narrative generation task that captures rich interpersonal dynamics and supports a fine-grained bias evaluation suite across multiple dimensions. Experiments on both open- and closed-source MLLMs reveal persistent, context-sensitive gender biases that are not evident in single-character settings. Our findings underscore the importance of relationship-aware benchmarks for diagnosing subtle, interaction-driven gender bias in MLLMs and provide actionable insights for future bias mitigation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.23115</link>
<guid>https://arxiv.org/abs/2506.23115</guid>
<content:encoded><![CDATA[
arXiv:2506.23115v1 Announce Type: cross 
Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation</title>
<link>https://arxiv.org/abs/2506.23121</link>
<guid>https://arxiv.org/abs/2506.23121</guid>
<content:encoded><![CDATA[
arXiv:2506.23121v1 Announce Type: cross 
Abstract: Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\_SAM2.git.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.23127</link>
<guid>https://arxiv.org/abs/2506.23127</guid>
<content:encoded><![CDATA[
arXiv:2506.23127v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2506.23137</link>
<guid>https://arxiv.org/abs/2506.23137</guid>
<content:encoded><![CDATA[
arXiv:2506.23137v1 Announce Type: cross 
Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph Completion (KGC). However, a majority of existing approaches are predicated on static, embedding-based scoring, exhibiting inherent limitations in capturing contextual dependencies and relational dynamics. Addressing this gap, we propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal components: (1) a semantic context learning module that encodes context-sensitive entity representations, and (2) a conditional flow-matching module designed to learn the dynamic transformation from a head to a tail embedding, governed by the aforementioned context. The resultant predictive vector field, representing the context-informed relational path, serves to dynamically refine the initial static score of an entity pair. Through this synergy of context-aware static representations and conditioned dynamic information, FMS facilitates a more profound modeling of relational semantics. Comprehensive evaluations on several standard benchmarks demonstrate that our proposed method surpasses prior state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Deep Search over Heterogeneous Enterprise Data</title>
<link>https://arxiv.org/abs/2506.23139</link>
<guid>https://arxiv.org/abs/2506.23139</guid>
<content:encoded><![CDATA[
arXiv:2506.23139v1 Announce Type: cross 
Abstract: We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation</title>
<link>https://arxiv.org/abs/2506.23151</link>
<guid>https://arxiv.org/abs/2506.23151</guid>
<content:encoded><![CDATA[
arXiv:2506.23151v1 Announce Type: cross 
Abstract: Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models</title>
<link>https://arxiv.org/abs/2506.23164</link>
<guid>https://arxiv.org/abs/2506.23164</guid>
<content:encoded><![CDATA[
arXiv:2506.23164v1 Announce Type: cross 
Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems</title>
<link>https://arxiv.org/abs/2506.23173</link>
<guid>https://arxiv.org/abs/2506.23173</guid>
<content:encoded><![CDATA[
arXiv:2506.23173v1 Announce Type: cross 
Abstract: In the rapidly evolving field of optical engineering, precise alignment of multi-lens imaging systems is critical yet challenging, as even minor misalignments can significantly degrade performance. Traditional alignment methods rely on specialized equipment and are time-consuming processes, highlighting the need for automated and scalable solutions. We present two complementary deep learning-based inverse-design methods for diagnosing misalignments in multi-element lens systems using only optical measurements. First, we use ray-traced spot diagrams to predict five-degree-of-freedom (5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error of 0.031mm in lateral translation and 0.011$^\circ$ in tilt. We also introduce a physics-based simulation pipeline that utilizes grayscale synthetic camera images, enabling a deep learning model to estimate 4-DOF, decenter and tilt errors in both two- and six-lens multi-lens systems. These results show the potential to reshape manufacturing and quality control in precision imaging.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data</title>
<link>https://arxiv.org/abs/2506.23174</link>
<guid>https://arxiv.org/abs/2506.23174</guid>
<content:encoded><![CDATA[
arXiv:2506.23174v1 Announce Type: cross 
Abstract: Generative models have gained significant attention for their ability to produce realistic synthetic data that supplements the quantity of real-world datasets. While recent studies show performance improvements in wireless sensing tasks by incorporating all synthetic data into training sets, the quality of synthetic data remains unpredictable and the resulting performance gains are not guaranteed. To address this gap, we propose tractable and generalizable metrics to quantify quality attributes of synthetic data - affinity and diversity. Our assessment reveals prevalent affinity limitation in current wireless synthetic data, leading to mislabeled data and degraded task performance. We attribute the quality limitation to generative models' lack of awareness of untrained conditions and domain-specific processing. To mitigate these issues, we introduce SynCheck, a quality-guided synthetic data utilization scheme that refines synthetic data quality during task model training. Our evaluation demonstrates that SynCheck consistently outperforms quality-oblivious utilization of synthetic data, and achieves 4.3% performance improvement even when the previous utilization degrades performance by 13.4%.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Diffusion Model for Unpaired Virtual Histology Staining</title>
<link>https://arxiv.org/abs/2506.23184</link>
<guid>https://arxiv.org/abs/2506.23184</guid>
<content:encoded><![CDATA[
arXiv:2506.23184v1 Announce Type: cross 
Abstract: Hematoxylin and eosin (H&amp;E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&amp;E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&amp;E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&amp;E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver</title>
<link>https://arxiv.org/abs/2506.23203</link>
<guid>https://arxiv.org/abs/2506.23203</guid>
<content:encoded><![CDATA[
arXiv:2506.23203v1 Announce Type: cross 
Abstract: As a green MIMO structure, massive H$^2$AD is viewed as a potential technology for the future 6G wireless network. For such a structure, it is a challenging task to design a low-complexity and high-performance fusion of target direction values sensed by different sub-array groups with fewer use of prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound (CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse CRLB of each subarray using antenna number reciprocals to eliminate real-time CRLB computation. This reduces complexity and prior knowledge dependence while preserving fusion performance. Moreover, a multi-branch deep neural network (MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by leveraging candidate angles from multiple subarrays. The subarray-specific branch networks are integrated with a shared regression module to effectively eliminate pseudo-solutions and fuse true angles. Simulation results show that the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable to CRLB-based methods, while significantly reducing the reliance on prior knowledge. More notably, the proposed MBDNN has superior performance in low-SNR ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in estimation accuracy compared to CRLB-ratio-WF method.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model</title>
<link>https://arxiv.org/abs/2506.23210</link>
<guid>https://arxiv.org/abs/2506.23210</guid>
<content:encoded><![CDATA[
arXiv:2506.23210v1 Announce Type: cross 
Abstract: Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, regarding model performance, federated AI models may not sufficiently satisfy AI users' expectations. Furthermore, AI users have a wide range of different needs. It is not easy to satisfy the whole users needs. These types of issues can be addressed through AI model optimization, fine-tuning, or personalization to achieve optimal model performance. To address model optimization challenges, we propose reference model-based federated learning for optimal fine-tuning, which overcomes catastrophic forgetting in each round. This method is derived from Bayesian parameter-efficient transfer learning, which includes an optimal proximal term and enables overcoming the catastrophic forgetting issue in each round by utilizing a reference model that incorporates previous model parameters. As a result, this method achieves both high model performance and low computing cost.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding</title>
<link>https://arxiv.org/abs/2506.23219</link>
<guid>https://arxiv.org/abs/2506.23219</guid>
<content:encoded><![CDATA[
arXiv:2506.23219v1 Announce Type: cross 
Abstract: Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions</title>
<link>https://arxiv.org/abs/2506.23236</link>
<guid>https://arxiv.org/abs/2506.23236</guid>
<content:encoded><![CDATA[
arXiv:2506.23236v1 Announce Type: cross 
Abstract: Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification</title>
<link>https://arxiv.org/abs/2506.23247</link>
<guid>https://arxiv.org/abs/2506.23247</guid>
<content:encoded><![CDATA[
arXiv:2506.23247v1 Announce Type: cross 
Abstract: Deep learning dominates image classification tasks, yet understanding how models arrive at predictions remains a challenge. Much research focuses on local explanations of individual predictions, such as saliency maps, which visualise the influence of specific pixels on a model's prediction. However, reviewing many of these explanations to identify recurring patterns is infeasible, while global methods often oversimplify and miss important local behaviours. To address this, we propose Segment Attribution Tables (SATs), a method for summarising local saliency explanations into (semi-)global insights. SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency maps to quantify their influence. These segments highlight concepts the model relies on across instances and reveal spurious correlations, such as reliance on backgrounds or watermarks, even when out-of-distribution test performance sees little change. SATs can explain any classifier for which a form of saliency map can be produced, using segmentation maps that provide named segments. SATs bridge the gap between oversimplified global summaries and overly detailed local explanations, offering a practical tool for analysing and debugging image classifiers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution</title>
<link>https://arxiv.org/abs/2506.23254</link>
<guid>https://arxiv.org/abs/2506.23254</guid>
<content:encoded><![CDATA[
arXiv:2506.23254v1 Announce Type: cross 
Abstract: Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title>
<link>https://arxiv.org/abs/2506.23260</link>
<guid>https://arxiv.org/abs/2506.23260</guid>
<content:encoded><![CDATA[
arXiv:2506.23260v1 Announce Type: cross 
Abstract: Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces have dramatically expanded capabilities for real-time data retrieval, complex computation, and multi-step orchestration. Yet, the explosive proliferation of plugins, connectors, and inter-agent protocols has outpaced discovery mechanisms and security practices, resulting in brittle integrations vulnerable to diverse threats. In this survey, we introduce the first unified, end-to-end threat model for LLM-agent ecosystems, spanning host-to-tool and agent-to-agent communications, formalize adversary capabilities and attacker objectives, and catalog over thirty attack techniques. Specifically, we organized the threat model into four domains: Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal adversarial inputs), Model Compromise (e.g., prompt- and parameter-level backdoors, composite and encrypted multi-backdoors, poisoning strategies), System and Privacy Attacks (e.g., speculative side-channels, membership inference, retrieval poisoning, social-engineering simulations), and Protocol Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent (A2A) protocol). For each category, we review representative scenarios, assess real-world feasibility, and evaluate existing defenses. Building on our threat taxonomy, we identify key open challenges and future research directions, such as securing MCP deployments through dynamic trust management and cryptographic provenance tracking; designing and hardening Agentic Web Interfaces; and achieving resilience in multi-agent and federated environments. Our work provides a comprehensive reference to guide the design of robust defense mechanisms and establish best practices for resilient LLM-agent workflows.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Activation Map to Visually Explain Multimodal LLMs</title>
<link>https://arxiv.org/abs/2506.23270</link>
<guid>https://arxiv.org/abs/2506.23270</guid>
<content:encoded><![CDATA[
arXiv:2506.23270v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available atgithub.com/xmed-lab/TAM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting thinking time in Reasoning models</title>
<link>https://arxiv.org/abs/2506.23274</link>
<guid>https://arxiv.org/abs/2506.23274</guid>
<content:encoded><![CDATA[
arXiv:2506.23274v1 Announce Type: cross 
Abstract: Reasoning models that produce long, hidden chains of thought have emerged as powerful tools for complex, reasoning-intensive tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability, openai2024openaio1card}. However, this paradigm introduces a new user experience challenge: users have little insight into how much time the model will spend reasoning before returning an answer. This unpredictability, can lead to user frustration and is likely to compound as LLMs can produce increasingly long tasks asynchronously \citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and evaluate methods for both online and offline prediction of model "thinking time," aiming to develop a practical "progress bar for reasoning." We discuss the implications for user interaction and future research directions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Settle for One? Text-to-ImageSet Generation and Evaluation</title>
<link>https://arxiv.org/abs/2506.23275</link>
<guid>https://arxiv.org/abs/2506.23275</guid>
<content:encoded><![CDATA[
arXiv:2506.23275v1 Announce Type: cross 
Abstract: Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Explanations for Deep Learning Phenomena Are Equally Valuable</title>
<link>https://arxiv.org/abs/2506.23286</link>
<guid>https://arxiv.org/abs/2506.23286</guid>
<content:encoded><![CDATA[
arXiv:2506.23286v1 Announce Type: cross 
Abstract: Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop ad hoc hypotheses attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena do still offer research value by providing unique settings in which we can refine our broad explanatory theories of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature. We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Objective-Free Local Learning and Emergent Language Structure in Thinking Machines</title>
<link>https://arxiv.org/abs/2506.23293</link>
<guid>https://arxiv.org/abs/2506.23293</guid>
<content:encoded><![CDATA[
arXiv:2506.23293v1 Announce Type: cross 
Abstract: We present a neuro-symbolic framework for generative language modeling based on local, event-driven emergent learning. At its core is a hierarchical Hopfield memory chain acting as a compositional short-term memory and dynamic tokenizer (retokenizer). Rather than relying on predefined tokens or supervision, the model builds structure from scratch, learning symbol sequences as multi-scale representations. It constructs projection tensors that bind co-occurring features into hierarchical tokens, introducing redundancy (i.e an emergent gauge structure) and enabling compression of local activations into long-range dependencies. Curiously, we find that the retokenizer can filter natural language patterns from noise, generating synthetic languages with coherent internal morphology -- quantifiably the same as human language. Language is learned in a local (Hebbian) fashion, where model constraints dictate allowed emergent structure, and new information is retained in alignment with this structure. The absence of a global objective enables a form of plasticity not found in conventional language models, allowing the system to generalize beyond its initial inference class -- even without explicit data. We demonstrate that briefly activating a new neuron during inference binds distributed multi-scale token features into a symbolic embedding. These emergent embedding neurons act as long-term memory and support a key-value mechanism for compositional inference and generalization. This architecture provides a methodological foundation for studying how symbolic structure can emerge from local neural learning. It offers a new pathway for building scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and reasoning arise as compressed memory traces within a Hopfield hierarchy. This approach advances the development of neuromorphic architectures for generative language models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing AI Systems: A Guide to Known Attacks and Impacts</title>
<link>https://arxiv.org/abs/2506.23296</link>
<guid>https://arxiv.org/abs/2506.23296</guid>
<content:encoded><![CDATA[
arXiv:2506.23296v1 Announce Type: cross 
Abstract: Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without specialized AI security expertise, with foundational knowledge to recognize AI-specific risks and implement effective defenses, thereby enhancing the overall security posture of AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance</title>
<link>https://arxiv.org/abs/2506.23314</link>
<guid>https://arxiv.org/abs/2506.23314</guid>
<content:encoded><![CDATA[
arXiv:2506.23314v1 Announce Type: cross 
Abstract: Malware detection in Android systems requires both cybersecurity expertise and machine learning (ML) techniques. Automated Machine Learning (AutoML) has emerged as an approach to simplify ML development by reducing the need for specialized knowledge. However, current AutoML solutions typically operate as black-box systems with limited transparency, interpretability, and experiment traceability. To address these limitations, we present MH-AutoML, a domain-specific framework for Android malware detection. MH-AutoML automates the entire ML pipeline, including data preprocessing, feature engineering, algorithm selection, and hyperparameter tuning. The framework incorporates capabilities for interpretability, debugging, and experiment tracking that are often missing in general-purpose solutions. In this study, we compare MH-AutoML against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT, HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML achieves better recall rates while providing more transparency and control. The framework maintains computational efficiency comparable to other solutions, making it suitable for cybersecurity applications where both performance and explainability matter.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussMaster: An LLM-based Database Copilot System</title>
<link>https://arxiv.org/abs/2506.23322</link>
<guid>https://arxiv.org/abs/2506.23322</guid>
<content:encoded><![CDATA[
arXiv:2506.23322v1 Announce Type: cross 
Abstract: In the financial industry, data is the lifeblood of operations, and DBAs shoulder significant responsibilities for SQL tuning, database deployment, diagnosis, and service repair. In recent years, both database vendors and customers have increasingly turned to autonomous database platforms in an effort to alleviate the heavy workload of DBAs. However, existing autonomous database platforms are limited in their capabilities, primarily addressing single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual intervention remains a necessity for comprehensive database maintenance. GaussMaster aims to revolutionize this landscape by introducing an LLM-based database copilot system. This innovative solution is designed not only to assist developers in writing efficient SQL queries but also to provide comprehensive care for database services. When database instances exhibit abnormal behavior, GaussMaster is capable of orchestrating the entire maintenance process automatically. It achieves this by analyzing hundreds of metrics and logs, employing a Tree-of-thought approach to identify root causes, and invoking appropriate tools to resolve issues. We have successfully implemented GaussMaster in real-world scenarios, such as the banking industry, where it has achieved zero human intervention for over 34 database maintenance scenarios. In this paper, we present significant improvements in these tasks with code at https://gitcode.com/opengauss/openGauss-GaussMaster.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs</title>
<link>https://arxiv.org/abs/2506.23325</link>
<guid>https://arxiv.org/abs/2506.23325</guid>
<content:encoded><![CDATA[
arXiv:2506.23325v1 Announce Type: cross 
Abstract: Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation</title>
<link>https://arxiv.org/abs/2506.23334</link>
<guid>https://arxiv.org/abs/2506.23334</guid>
<content:encoded><![CDATA[
arXiv:2506.23334v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design</title>
<link>https://arxiv.org/abs/2506.23339</link>
<guid>https://arxiv.org/abs/2506.23339</guid>
<content:encoded><![CDATA[
arXiv:2506.23339v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific discovery, but their application in domains requiring factual accuracy and domain-specific constraints remains challenging. In molecular design for drug discovery, LLMs can suggest creative molecular modifications but often produce chemically invalid or impractical structures. We present VALID-Mol, a systematic framework for integrating chemical validation with LLM-driven molecular design that increases the rate of generating valid chemical structures from 3% to 83%. Our approach combines methodical prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM to ensure reliable generation of synthesizable molecules with improved properties. Beyond the specific implementation, we contribute a generalizable methodology for scientifically-constrained LLM applications, with quantifiable reliability improvements. Computational predictions suggest our framework can generate promising candidates for synthesis with up to 17-fold computationally predicted improvements in target affinity while maintaining synthetic accessibility. We provide a detailed analysis of our prompt engineering process, validation architecture, and fine-tuning approach, offering a reproducible blueprint for applying LLMs to other scientific domains where domain-specific validation is essential.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATGen: A Framework for Active Text Generation</title>
<link>https://arxiv.org/abs/2506.23342</link>
<guid>https://arxiv.org/abs/2506.23342</guid>
<content:encoded><![CDATA[
arXiv:2506.23342v1 Announce Type: cross 
Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</title>
<link>https://arxiv.org/abs/2506.23351</link>
<guid>https://arxiv.org/abs/2506.23351</guid>
<content:encoded><![CDATA[
arXiv:2506.23351v1 Announce Type: cross 
Abstract: Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment</title>
<link>https://arxiv.org/abs/2506.23358</link>
<guid>https://arxiv.org/abs/2506.23358</guid>
<content:encoded><![CDATA[
arXiv:2506.23358v1 Announce Type: cross 
Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS offers strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs</title>
<link>https://arxiv.org/abs/2506.23377</link>
<guid>https://arxiv.org/abs/2506.23377</guid>
<content:encoded><![CDATA[
arXiv:2506.23377v1 Announce Type: cross 
Abstract: Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIEDD: Shared-Implicit Encoder with Discrete Decoders</title>
<link>https://arxiv.org/abs/2506.23382</link>
<guid>https://arxiv.org/abs/2506.23382</guid>
<content:encoded><![CDATA[
arXiv:2506.23382v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at https://github.com/VikramRangarajan/SIEDD .
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Memory Organization for Wikipedia Generation</title>
<link>https://arxiv.org/abs/2506.23393</link>
<guid>https://arxiv.org/abs/2506.23393</guid>
<content:encoded><![CDATA[
arXiv:2506.23393v1 Announce Type: cross 
Abstract: Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching a Language Model to Speak the Language of Tools</title>
<link>https://arxiv.org/abs/2506.23394</link>
<guid>https://arxiv.org/abs/2506.23394</guid>
<content:encoded><![CDATA[
arXiv:2506.23394v1 Announce Type: cross 
Abstract: External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchMake: Turn any scientific data set into a reproducible benchmark</title>
<link>https://arxiv.org/abs/2506.23419</link>
<guid>https://arxiv.org/abs/2506.23419</guid>
<content:encoded><![CDATA[
arXiv:2506.23419v1 Announce Type: cross 
Abstract: Benchmark data sets are a cornerstone of machine learning development and applications, ensuring new methods are robust, reliable and competitive. The relative rarity of benchmark sets in computational science, due to the uniqueness of the problems and the pace of change in the associated domains, makes evaluating new innovations difficult for computational scientists. In this paper a new tool is developed and tested to potentially turn any of the increasing numbers of scientific data sets made openly available into a benchmark accessible to the community. BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull (the smallest convex set that contains all existing data instances) and partitions a required fraction of matched data instances into a testing set that maximises divergence and statistical significance, across tabular, graph, image, signal and textual modalities. BenchMake splits are compared to establish splits and random splits using ten publicly available benchmark sets from different areas of science, with different sizes, shapes, distributions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs</title>
<link>https://arxiv.org/abs/2506.23423</link>
<guid>https://arxiv.org/abs/2506.23423</guid>
<content:encoded><![CDATA[
arXiv:2506.23423v1 Announce Type: cross 
Abstract: Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a quantitative and systematic method for analyzing its effect on individual outputs is still lacking. Here, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method tracks the model's intermediate hidden states, providing a more fine-grained insight into the effects of fine-tuning than a simple comparison of final outputs from pre-trained and fine-tuned models. We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. Empirically, we find that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Motivated by this finding and our theoretical analysis, we define the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component. We observe that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces TuCo, and that TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of such attacks. In summary, TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.23424</link>
<guid>https://arxiv.org/abs/2506.23424</guid>
<content:encoded><![CDATA[
arXiv:2506.23424v1 Announce Type: cross 
Abstract: Real-world time series often exhibit a non-stationary nature, degrading the performance of pre-trained forecasting models. Test-Time Adaptation (TTA) addresses this by adjusting models during inference, but existing methods typically update the full model, increasing memory and compute costs. We propose PETSA, a parameter-efficient method that adapts forecasters at test time by only updating small calibration modules on the input and output. PETSA uses low-rank adapters and dynamic gating to adjust representations without retraining. To maintain accuracy despite limited adaptation capacity, we introduce a specialized loss combining three components: (1) a robust term, (2) a frequency-domain term to preserve periodicity, and (3) a patch-wise structural term for structural alignment. PETSA improves the adaptability of various forecasting backbones while requiring fewer parameters than baselines. Experimental results on benchmark datasets show that PETSA achieves competitive or better performance across all horizons. Our code is available at: https://github.com/BorealisAI/PETSA
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pipelined Decoder for Efficient Context-Aware Text Generation</title>
<link>https://arxiv.org/abs/2506.23431</link>
<guid>https://arxiv.org/abs/2506.23431</guid>
<content:encoded><![CDATA[
arXiv:2506.23431v1 Announce Type: cross 
Abstract: As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection</title>
<link>https://arxiv.org/abs/2506.23437</link>
<guid>https://arxiv.org/abs/2506.23437</guid>
<content:encoded><![CDATA[
arXiv:2506.23437v1 Announce Type: cross 
Abstract: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the integration of intelligent transportation systems, smart city monitoring systems, and autonomous driving technologies. Modern automatic solutions are limited by the lack of large scale, curated datasets and by the computational demands of state of the art sound event detection models. This work introduces E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight Convolutional Neural Network architecture derived from the PANNs framework, specifically optimized for binary EV siren detection. Leveraging our dedicated subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across multiple reference datasets and test its viability on embedded hardware. The experimental campaign includes ablation studies, cross-domain benchmarking, and real-time inference deployment on edge device. Interpretability analyses exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into the model internal representations and validate its ability to capture distinct spectrotemporal patterns associated with different types of EV sirens. Real time performance is assessed through frame wise and event based detection metrics, as well as a detailed analysis of false positive activations. Results demonstrate that E2PANNs establish a new state of the art in this research domain, with high computational efficiency, and suitability for edge-based audio monitoring and safety-critical applications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-variant Image Inpainting via Interactive Distribution Transition Estimation</title>
<link>https://arxiv.org/abs/2506.23461</link>
<guid>https://arxiv.org/abs/2506.23461</guid>
<content:encoded><![CDATA[
arXiv:2506.23461v1 Announce Type: cross 
Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image's source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification</title>
<link>https://arxiv.org/abs/2506.23462</link>
<guid>https://arxiv.org/abs/2506.23462</guid>
<content:encoded><![CDATA[
arXiv:2506.23462v1 Announce Type: cross 
Abstract: Effective disaster management requires timely and accurate insights, yet traditional methods struggle to integrate multimodal data such as images, weather records, and textual reports. To address this, we propose DisasterNet-LLM, a specialized Large Language Model (LLM) designed for comprehensive disaster analysis. By leveraging advanced pretraining, cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM excels in disaster classification. Experimental results demonstrate its superiority over state-of-the-art models, achieving higher accuracy of 89.5%, an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal disaster classification tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sanitizing Manufacturing Dataset Labels Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23465</link>
<guid>https://arxiv.org/abs/2506.23465</guid>
<content:encoded><![CDATA[
arXiv:2506.23465v1 Announce Type: cross 
Abstract: The success of machine learning models in industrial applications is heavily dependent on the quality of the datasets used to train the models. However, large-scale datasets, specially those constructed from crowd-sourcing and web-scraping, often suffer from label noise, inconsistencies, and errors. This problem is particularly pronounced in manufacturing domains, where obtaining high-quality labels is costly and time-consuming. This paper introduces Vision-Language Sanitization and Refinement (VLSR), which is a vision-language-based framework for label sanitization and refinement in multi-label manufacturing image datasets. This method embeds both images and their associated textual labels into a shared semantic space leveraging the CLIP vision-language model. Then two key tasks are addressed in this process by computing the cosine similarity between embeddings. First, label sanitization is performed to identify irrelevant, misspelled, or semantically weak labels, and surface the most semantically aligned label for each image by comparing image-label pairs using cosine similarity between image and label embeddings. Second, the method applies density-based clustering on text embeddings, followed by iterative cluster merging, to group semantically similar labels into unified label groups. The Factorynet dataset, which includes noisy labels from both human annotations and web-scraped sources, is employed to evaluate the effectiveness of the proposed framework. Experimental results demonstrate that the VLSR framework successfully identifies problematic labels and improves label consistency. This method enables a significant reduction in label vocabulary through clustering, which ultimately enhances the dataset's quality for training robust machine learning models in industrial applications with minimal human intervention.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent</title>
<link>https://arxiv.org/abs/2506.23485</link>
<guid>https://arxiv.org/abs/2506.23485</guid>
<content:encoded><![CDATA[
arXiv:2506.23485v1 Announce Type: cross 
Abstract: Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound</title>
<link>https://arxiv.org/abs/2506.23490</link>
<guid>https://arxiv.org/abs/2506.23490</guid>
<content:encoded><![CDATA[
arXiv:2506.23490v1 Announce Type: cross 
Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound (US) struggles with accurate metric calculation and direct observation of 3D cardiac structures. Moreover, 3D US is limited by low resolution, small field of view and scarce availability in practice. Constructing the cardiac anatomical twin from 2D images is promising to provide precise treatment planning and clinical quantification. However, it remains challenging due to the rare paired data, complex structures, and US noises. In this study, we introduce a novel generative framework UltraTwin, to obtain cardiac anatomical twin from sparse multi-view 2D US. Our contribution is three-fold. First, pioneered the construction of a real-world and high-quality dataset containing strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we propose a coarse-to-fine scheme to achieve hierarchical reconstruction optimization. Last, we introduce an implicit autoencoder for topology-aware constraints. Extensive experiments show that UltraTwin reconstructs high-quality anatomical twins versus strong competitors. We believe it advances anatomical twin modeling for potential applications in personalized cardiac care.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding</title>
<link>https://arxiv.org/abs/2506.23491</link>
<guid>https://arxiv.org/abs/2506.23491</guid>
<content:encoded><![CDATA[
arXiv:2506.23491v1 Announce Type: cross 
Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM) specifically designed for Graphical User Interface grounding tasks, achieving performance competitive with significantly larger models. Unlike large-scale VLMs (>7B parameters) that are computationally intensive and impractical for consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while being fully trainable on a single GPU (RTX 4090). The model incorporates several key innovations: (i) combine cross-platform, multi-resolution dataset of 24K examples from diverse sources including mobile, desktop, and web GUI screenshots to effectively address data scarcity in high-resolution desktop environments; (ii) a two-stage fine-tuning strategy, where initial cross-platform training establishes robust GUI understanding, followed by specialized fine-tuning on high-resolution data to significantly enhance model adaptability; and (iii) data curation and redundancy reduction strategies, demonstrating that randomly sampling a smaller subset with reduced redundancy achieves performance comparable to larger datasets, emphasizing data diversity over sheer volume. Empirical evaluation on standard GUI grounding benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters. Ablation studies validate the critical role of balanced sampling and two-stage fine-tuning in enhancing robustness, particularly in high-resolution desktop scenarios. The Qwen-GUI-3B is available at: https://github.com/Han1018/Qwen-GUI-3B
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Margin-Aware Recalibration of Temperature Scaling</title>
<link>https://arxiv.org/abs/2506.23492</link>
<guid>https://arxiv.org/abs/2506.23492</guid>
<content:encoded><![CDATA[
arXiv:2506.23492v1 Announce Type: cross 
Abstract: Recent advances in deep learning have significantly improved predictive accuracy. However, modern neural networks remain systematically overconfident, posing risks for deployment in safety-critical scenarios. Current post-hoc calibration methods face a fundamental dilemma: global approaches like Temperature Scaling apply uniform adjustments across all samples, introducing high bias despite computational efficiency, while more expressive methods that operate on full logit distributions suffer from high variance due to noisy high-dimensional inputs and insufficient validation data. To address these challenges, we propose Sample Margin-Aware Recalibration of Temperature (SMART), a lightweight, data-efficient recalibration method that precisely scales logits based on the margin between the top two logits -- termed the logit gap. Specifically, the logit gap serves as a denoised, scalar signal directly tied to decision boundary uncertainty, providing a robust indicator that avoids the noise inherent in high-dimensional logit spaces while preserving model prediction invariance. Meanwhile, SMART employs a novel soft-binned Expected Calibration Error (SoftECE) objective that balances model bias and variance through adaptive binning, enabling stable parameter updates even with extremely limited calibration data. Extensive evaluations across diverse datasets and architectures demonstrate that SMART achieves state-of-the-art calibration performance even with substantially fewer parameters compared to existing parametric methods, offering a principled, robust, and highly efficient solution for practical uncertainty quantification in neural network predictions. The source code is available at: https://anonymous.4open.science/r/SMART-8B11.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI</title>
<link>https://arxiv.org/abs/2506.23506</link>
<guid>https://arxiv.org/abs/2506.23506</guid>
<content:encoded><![CDATA[
arXiv:2506.23506v1 Announce Type: cross 
Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE) represents a recent breakthrough in lung structure imaging, providing image resolution and quality comparable to computed tomography (CT). Due to the absence of ionising radiation, MRI is often preferred over CT in paediatric diseases such as cystic fibrosis (CF), one of the most common genetic disorders in Caucasians. To assess structural lung damage in CF imaging, CT scoring systems provide valuable quantitative insights for disease diagnosis and progression. However, few quantitative scoring systems are available in structural lung MRI (e.g., UTE-MRI). To provide fast and accurate quantification in lung MRI, we investigated the feasibility of novel Artificial intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3) lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification and reporting. The results shows that our APL scoring took 8.2 minutes per subject, which was more than twice as fast as the previous grid-level scoring. Additionally, our pixel-level scoring was statistically more accurate (p=0.021), while strongly correlating with grid-level scoring (R=0.973, p=5.85e-9). This tool has great potential to streamline the workflow of UTE lung MRI in clinical settings, and be extended to other structural lung MRI sequences (e.g., BLADE MRI), and for other lung diseases (e.g., bronchopulmonary dysplasia).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably</title>
<link>https://arxiv.org/abs/2506.23508</link>
<guid>https://arxiv.org/abs/2506.23508</guid>
<content:encoded><![CDATA[
arXiv:2506.23508v1 Announce Type: cross 
Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on an open-source multimodal model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon through the lens of learning dynamics, showing that RFT reinforces correct samples that are naturally aligned with the base model's probability landscape, mitigating interference with prior knowledge. Moreover, supervised training on correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly learning new tasks. These findings suggest that data distribution, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments</title>
<link>https://arxiv.org/abs/2506.23514</link>
<guid>https://arxiv.org/abs/2506.23514</guid>
<content:encoded><![CDATA[
arXiv:2506.23514v1 Announce Type: cross 
Abstract: Relative localization is a crucial capability for multi-robot systems operating in GPS-denied environments. Existing approaches for multi-robot relative localization often depend on costly or short-range sensors like cameras and LiDARs. Consequently, these approaches face challenges such as high computational overhead (e.g., map merging) and difficulties in disjoint environments. To address this limitation, this paper introduces MGPRL, a novel distributed framework for multi-robot relative localization using convex-hull of multiple Wi-Fi access points (AP). To accomplish this, we employ co-regionalized multi-output Gaussian Processes for efficient Radio Signal Strength Indicator (RSSI) field prediction and perform uncertainty-aware multi-AP localization, which is further coupled with weighted convex hull-based alignment for robust relative pose estimation. Each robot predicts the RSSI field of the environment by an online scan of APs in its environment, which are utilized for position estimation of multiple APs. To perform relative localization, each robot aligns the convex hull of its predicted AP locations with that of the neighbor robots. This approach is well-suited for devices with limited computational resources and operates solely on widely available Wi-Fi RSSI measurements without necessitating any dedicated pre-calibration or offline fingerprinting. We rigorously evaluate the performance of the proposed MGPRL in ROS simulations and demonstrate it with real-world experiments, comparing it against multiple state-of-the-art approaches. The results showcase that MGPRL outperforms existing methods in terms of localization accuracy and computational efficiency. Finally, we open source MGPRL as a ROS package https://github.com/herolab-uga/MGPRL.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization</title>
<link>https://arxiv.org/abs/2506.23516</link>
<guid>https://arxiv.org/abs/2506.23516</guid>
<content:encoded><![CDATA[
arXiv:2506.23516v1 Announce Type: cross 
Abstract: Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning</title>
<link>https://arxiv.org/abs/2506.23524</link>
<guid>https://arxiv.org/abs/2506.23524</guid>
<content:encoded><![CDATA[
arXiv:2506.23524v1 Announce Type: cross 
Abstract: In the field of education, understanding students' opinions through their comments is crucial, especially in the Vietnamese language, where resources remain limited. Existing educational datasets often lack domain relevance and student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese dataset for Educational Sentiment Classification and Topic Classification, curated from university forums, which offers more samples, richer class diversity, longer texts, and broader vocabulary. In addition, we explore multitask learning using encoder-only language models (BERT), in which we showed that it achieves performance up to 83.7% and 79.8% accuracy for sentiment and topic classification tasks. We also benchmark our dataset and model with other datasets and models, including Large Language Models, and discuss these benchmarks. The dataset is publicly available at: https://huggingface.co/datasets/hung20gg/NEU-ESC.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound</title>
<link>https://arxiv.org/abs/2506.23538</link>
<guid>https://arxiv.org/abs/2506.23538</guid>
<content:encoded><![CDATA[
arXiv:2506.23538v1 Announce Type: cross 
Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Train Quantum State Tomography using Compressed Sensing</title>
<link>https://arxiv.org/abs/2506.23560</link>
<guid>https://arxiv.org/abs/2506.23560</guid>
<content:encoded><![CDATA[
arXiv:2506.23560v1 Announce Type: cross 
Abstract: Quantum state tomography (QST) is a fundamental technique for estimating the state of a quantum system from measured data and plays a crucial role in evaluating the performance of quantum devices. However, standard estimation methods become impractical due to the exponential growth of parameters in the state representation. In this work, we address this challenge by parameterizing the state using a low-rank block tensor train decomposition and demonstrate that our approach is both memory- and computationally efficient. This framework applies to a broad class of quantum states that can be well approximated by low-rank decompositions, including pure states, nearly pure states, and ground states of Hamiltonians.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Human Action Detection during Escorting</title>
<link>https://arxiv.org/abs/2506.23573</link>
<guid>https://arxiv.org/abs/2506.23573</guid>
<content:encoded><![CDATA[
arXiv:2506.23573v1 Announce Type: cross 
Abstract: The deployment of robot assistants in large indoor spaces has seen significant growth, with escorting tasks becoming a key application. However, most current escorting robots primarily rely on navigation-focused strategies, assuming that the person being escorted will follow without issue. In crowded environments, this assumption often falls short, as individuals may struggle to keep pace, become obstructed, get distracted, or need to stop unexpectedly. As a result, conventional robotic systems are often unable to provide effective escorting services due to their limited understanding of human movement dynamics. To address these challenges, an effective escorting robot must continuously detect and interpret human actions during the escorting process and adjust its movement accordingly. However, there is currently no existing dataset designed specifically for human action detection in the context of escorting. Given that escorting often occurs in crowded environments, where other individuals may enter the robot's camera view, the robot also needs to identify the specific human it is escorting (the subject) before predicting their actions. Since no existing model performs both person re-identification and action prediction in real-time, we propose a novel neural network architecture that can accomplish both tasks. This enables the robot to adjust its speed dynamically based on the escortee's movements and seamlessly resume escorting after any disruption. In comparative evaluations against strong baselines, our system demonstrates superior efficiency and effectiveness, showcasing its potential to significantly improve robotic escorting services in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection</title>
<link>https://arxiv.org/abs/2506.23581</link>
<guid>https://arxiv.org/abs/2506.23581</guid>
<content:encoded><![CDATA[
arXiv:2506.23581v1 Announce Type: cross 
Abstract: Object detection plays a crucial role in many security-sensitive applications. However, several recent studies have shown that object detectors can be easily fooled by physically realizable attacks, \eg, adversarial patches and recent adversarial textures, which pose realistic and urgent threats. Adversarial Training (AT) has been recognized as the most effective defense against adversarial attacks. While AT has been extensively studied in the $l_\infty$ attack settings on classification models, AT against physically realizable attacks on object detectors has received limited exploration. Early attempts are only performed to defend against adversarial patches, leaving AT against a wider range of physically realizable attacks under-explored. In this work, we consider defending against various physically realizable attacks with a unified AT method. We propose PBCAT, a novel Patch-Based Composite Adversarial Training strategy. PBCAT optimizes the model by incorporating the combination of small-area gradient-guided adversarial patches and imperceptible global adversarial perturbations covering the entire image. With these designs, PBCAT has the potential to defend against not only adversarial patches but also unseen physically realizable attacks such as adversarial textures. Extensive experiments in multiple settings demonstrated that PBCAT significantly improved robustness against various physically realizable attacks over state-of-the-art defense methods. Notably, it improved the detection accuracy by 29.7\% over previous defense methods under one recent adversarial texture attack.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation</title>
<link>https://arxiv.org/abs/2506.23584</link>
<guid>https://arxiv.org/abs/2506.23584</guid>
<content:encoded><![CDATA[
arXiv:2506.23584v1 Announce Type: cross 
Abstract: Generating radiology reports from CT scans remains a complex task due to the nuanced nature of medical imaging and the variability in clinical documentation. In this study, we propose a two-stage framework for generating renal radiology reports from 2D CT slices. First, we extract structured abnormality features using a multi-task learning model trained to identify lesion attributes such as location, size, enhancement, and attenuation. These extracted features are subsequently combined with the corresponding CT image and fed into a fine-tuned vision-language model to generate natural language report sentences aligned with clinical findings. We conduct experiments on a curated dataset of renal CT studies with manually annotated sentence-slice-feature triplets and evaluate performance using both classification metrics and natural language generation metrics. Our results demonstrate that the proposed model outperforms random baselines across all abnormality types, and the generated reports capture key clinical content with reasonable textual accuracy. This exploratory work highlights the feasibility of modular, feature-informed report generation for renal imaging. Future efforts will focus on extending this pipeline to 3D CT volumes and further improving clinical fidelity in multimodal medical AI systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transition Matching: Scalable and Flexible Generative Modeling</title>
<link>https://arxiv.org/abs/2506.23589</link>
<guid>https://arxiv.org/abs/2506.23589</guid>
<content:encoded><![CDATA[
arXiv:2506.23589v1 Announce Type: cross 
Abstract: Diffusion and flow matching models have significantly advanced media generation, yet their design space is well-explored, somewhat limiting further improvements. Concurrently, autoregressive (AR) models, particularly those generating continuous tokens, have emerged as a promising direction for unifying text and media generation. This paper introduces Transition Matching (TM), a novel discrete-time, continuous-state generative paradigm that unifies and advances both diffusion/flow models and continuous AR generation. TM decomposes complex generation tasks into simpler Markov transitions, allowing for expressive non-deterministic probability transition kernels and arbitrary non-continuous supervision processes, thereby unlocking new flexible design avenues. We explore these choices through three TM variants: (i) Difference Transition Matching (DTM), which generalizes flow matching to discrete-time by directly learning transition probabilities, yielding state-of-the-art image quality and text adherence as well as improved sampling efficiency. (ii) Autoregressive Transition Matching (ARTM) and (iii) Full History Transition Matching (FHTM) are partially and fully causal models, respectively, that generalize continuous AR methods. They achieve continuous causal AR generation quality comparable to non-causal approaches and potentially enable seamless integration with existing AR text generation techniques. Notably, FHTM is the first fully causal model to match or surpass the performance of flow-based methods on text-to-image task in continuous domains. We demonstrate these contributions through a rigorous large-scale comparison of TM variants and relevant baselines, maintaining a fixed architecture, training data, and hyperparameters.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series</title>
<link>https://arxiv.org/abs/2506.23596</link>
<guid>https://arxiv.org/abs/2506.23596</guid>
<content:encoded><![CDATA[
arXiv:2506.23596v1 Announce Type: cross 
Abstract: Recently, forecasting future abnormal events has emerged as an important scenario to tackle real-world necessities. However, the solution of predicting specific future time points when anomalies will occur, known as Anomaly Prediction (AP), remains under-explored. Existing methods dealing with time series data fail in AP, focusing only on immediate anomalies or failing to provide precise predictions for future anomalies. To address the AP task, we propose a novel framework called Anomaly to Prompt (A2P), comprised of Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To enable the forecasting model to forecast abnormal time points, we adopt a strategy to learn the relationships of anomalies. For the robust detection of anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP) that simulates diverse anomaly patterns using signal adaptive prompt. Comprehensive experiments on multiple real-world datasets demonstrate the superiority of A2P over state-of-the-art methods, showcasing its ability to predict future anomalies. Our implementation code is available at https://github.com/KU-VGI/AP.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-guided Diverse Decoding for Large Language Model</title>
<link>https://arxiv.org/abs/2506.23601</link>
<guid>https://arxiv.org/abs/2506.23601</guid>
<content:encoded><![CDATA[
arXiv:2506.23601v1 Announce Type: cross 
Abstract: Diverse decoding of large language models is crucial for applications requiring multiple semantically distinct responses, yet existing methods primarily achieve lexical rather than semantic diversity. This limitation significantly constrains Best-of-N strategies, group-based reinforcement learning, and data synthesis. While temperature sampling and diverse beam search modify token distributions or apply n-gram penalties, they fail to ensure meaningful semantic differentiation. We introduce Semantic-guided Diverse Decoding (SemDiD), operating directly in embedding space that balances quality with diversity through three complementary mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment. SemDiD harmonizes these competing objectives using adaptive gain functions and constraint optimization, ensuring both quality thresholds and maximal semantic differentiation. Experiments show SemDiD consistently outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15% while increasing accuracy by up to 2.1%.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Semantic Privacy in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23603</link>
<guid>https://arxiv.org/abs/2506.23603</guid>
<content:encoded><![CDATA[
arXiv:2506.23603v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive domains, traditional data privacy measures prove inadequate for protecting information that is implicit, contextual, or inferable - what we define as semantic privacy. This Systematization of Knowledge (SoK) introduces a lifecycle-centric framework to analyze how semantic privacy risks emerge across input processing, pretraining, fine-tuning, and alignment stages of LLMs. We categorize key attack vectors and assess how current defenses, such as differential privacy, embedding encryption, edge computing, and unlearning, address these threats. Our analysis reveals critical gaps in semantic-level protection, especially against contextual inference and latent representation leakage. We conclude by outlining open challenges, including quantifying semantic leakage, protecting multimodal inputs, balancing de-identification with generation quality, and ensuring transparency in privacy enforcement. This work aims to inform future research on designing robust, semantically aware privacy-preserving techniques for LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval</title>
<link>https://arxiv.org/abs/2506.23605</link>
<guid>https://arxiv.org/abs/2506.23605</guid>
<content:encoded><![CDATA[
arXiv:2506.23605v1 Announce Type: cross 
Abstract: Lecture slide element detection and retrieval are key problems in slide understanding. Training effective models for these tasks often depends on extensive manual annotation. However, annotating large volumes of lecture slides for supervised training is labor intensive and requires domain expertise. To address this, we propose a large language model (LLM)-guided synthetic lecture slide generation pipeline, SynLecSlideGen, which produces high-quality, coherent and realistic slides. We also create an evaluation benchmark, namely RealSlide by manually annotating 1,050 real lecture slides. To assess the utility of our synthetic slides, we perform few-shot transfer learning on real data using models pre-trained on them. Experimental results show that few-shot transfer learning with pretraining on synthetic slides significantly improves performance compared to training only on real data. This demonstrates that synthetic data can effectively compensate for limited labeled lecture slides. The code and resources of our work are publicly available on our project website: https://synslidegen.github.io/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking</title>
<link>https://arxiv.org/abs/2506.23628</link>
<guid>https://arxiv.org/abs/2506.23628</guid>
<content:encoded><![CDATA[
arXiv:2506.23628v1 Announce Type: cross 
Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes Network Drivers (KNDs), a transformative, modular, and declarative architecture designed to overcome current imperative provisioning and API limitations. KNDs integrate network resource management into Kubernetes' core by utilizing Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements, and upcoming OCI Runtime Specification changes. Our DraNet implementation demonstrates declarative attachment of network interfaces, including Remote Direct Memory Access (RDMA) devices, significantly boosting high-performance AI/ML workloads. This capability enables sophisticated cloud-native applications and lays crucial groundwork for future Telco solutions, fostering a "galaxy" of specialized KNDs for enhanced application delivery and reduced operational complexity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data</title>
<link>https://arxiv.org/abs/2506.23629</link>
<guid>https://arxiv.org/abs/2506.23629</guid>
<content:encoded><![CDATA[
arXiv:2506.23629v1 Announce Type: cross 
Abstract: The integrity of Water Quality Data (WQD) is critical in environmental monitoring for scientific decision-making and ecological protection. However, water quality monitoring systems are often challenged by large amounts of missing data due to unavoidable problems such as sensor failures and communication delays, which further lead to water quality data becoming High-Dimensional and Sparse (HDS). Traditional data imputation methods are difficult to depict the potential dynamics and fail to capture the deep data features, resulting in unsatisfactory imputation performance. To effectively address the above issues, this paper proposes a Nonlinear Low-rank Representation model (NLR) with Convolutional Neural Networks (CNN) for imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing temporal features to model the temporal dependence of data between time slots, and b) Extracting nonlinear interactions and local patterns to mine higher-order relationships features and achieve deep fusion of multidimensional information. Experimental studies on three real water quality datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art data imputation models in terms of estimation accuracy. It provides an effective approach for handling water quality monitoring data in complex dynamic environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures</title>
<link>https://arxiv.org/abs/2506.23634</link>
<guid>https://arxiv.org/abs/2506.23634</guid>
<content:encoded><![CDATA[
arXiv:2506.23634v1 Announce Type: cross 
Abstract: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by converting programs into forms that are more complex to analyze. However, MBA has been increasingly exploited by malware developers to evade detection and cause significant real-world problems. Traditional MBA deobfuscation methods often consider these expressions as part of a black box and overlook their internal semantic information. To bridge this gap, we propose a truth table, which is an automatically constructed semantic representation of an expression's behavior that does not rely on external resources. The truth table is a mathematical form that represents the output of expression for all possible combinations of input. We also propose a general and extensible guided MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural encoder-decoder Seq2Seq architecture to incorporate this semantic guidance. Experimental results and in-depth analysis show that integrating expression semantics significantly improves performance and highlights the importance of internal semantic expressions in recovering obfuscated code to its original form.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model</title>
<link>https://arxiv.org/abs/2506.23635</link>
<guid>https://arxiv.org/abs/2506.23635</guid>
<content:encoded><![CDATA[
arXiv:2506.23635v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and Databricks' DBRX. This paper addresses the cost and scalability challenges encountered when constructing private LLM systems for personal or small group services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2 Ultra chips is established as a cost-efficient solution to host and accelerate the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our performance analysis reveal that parallel execution of the model's experts across two to four machine nodes significantly reduces inference time. We find that computation time for the experts is comparable to the communication time for exchanging their outputs, emphasizing the importance of network latency over bandwidth. We also observe significant management overhead due to Apple software stack's memory management logic. Based on these findings, we develop optimization schemes to eliminate the memory management overhead. As a result, the Mac Studio cluster is 1.15 times more cost-efficient than the state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we construct a performance model to estimate system performance under varying configurations, and the model provides valuable insights for designing private LLM systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Multimodal Understanding via Byte-Pair Visual Encoding</title>
<link>https://arxiv.org/abs/2506.23639</link>
<guid>https://arxiv.org/abs/2506.23639</guid>
<content:encoded><![CDATA[
arXiv:2506.23639v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation</title>
<link>https://arxiv.org/abs/2506.23641</link>
<guid>https://arxiv.org/abs/2506.23641</guid>
<content:encoded><![CDATA[
arXiv:2506.23641v1 Announce Type: cross 
Abstract: As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration</title>
<link>https://arxiv.org/abs/2506.23644</link>
<guid>https://arxiv.org/abs/2506.23644</guid>
<content:encoded><![CDATA[
arXiv:2506.23644v1 Announce Type: cross 
Abstract: We introduce QLPro, a vulnerability detection framework that systematically integrates LLMs and static analysis tools to enable comprehensive vulnerability detection across entire open-source projects.We constructed a new dataset, JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only 24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed as 0-days.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23678</link>
<guid>https://arxiv.org/abs/2506.23678</guid>
<content:encoded><![CDATA[
arXiv:2506.23678v1 Announce Type: cross 
Abstract: The output quality of large language models (LLMs) can be improved via "reasoning": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Modular Exponentiation with Transformers</title>
<link>https://arxiv.org/abs/2506.23679</link>
<guid>https://arxiv.org/abs/2506.23679</guid>
<content:encoded><![CDATA[
arXiv:2506.23679v1 Announce Type: cross 
Abstract: Modular exponentiation is crucial to number theory and cryptography, yet remains largely unexplored from a mechanistic interpretability standpoint. We train a 4-layer encoder-decoder Transformer model to perform this operation and investigate the emergence of numerical reasoning during training. Utilizing principled sampling strategies, PCA-based embedding analysis, and activation patching, we examine how number-theoretic properties are encoded within the model. We find that reciprocal operand training leads to strong performance gains, with sudden generalization across related moduli. These synchronized accuracy surges reflect grokking-like dynamics, suggesting the model internalizes shared arithmetic structure. We also find a subgraph consisting entirely of attention heads in the final layer sufficient to achieve full performance on the task of regular exponentiation. These results suggest that transformer models learn modular arithmetic through specialized computational circuits, paving the way for more interpretable and efficient neural approaches to modular exponentiation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation</title>
<link>https://arxiv.org/abs/2506.23717</link>
<guid>https://arxiv.org/abs/2506.23717</guid>
<content:encoded><![CDATA[
arXiv:2506.23717v1 Announce Type: cross 
Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and DVS-GESTURE, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit budgets over the advanced baseline work on ImageNet. This work will be fully open-sourced.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DABstep: Data Agent Benchmark for Multi-step Reasoning</title>
<link>https://arxiv.org/abs/2506.23719</link>
<guid>https://arxiv.org/abs/2506.23719</guid>
<content:encoded><![CDATA[
arXiv:2506.23719v1 Announce Type: cross 
Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmark's design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound</title>
<link>https://arxiv.org/abs/2506.23721</link>
<guid>https://arxiv.org/abs/2506.23721</guid>
<content:encoded><![CDATA[
arXiv:2506.23721v1 Announce Type: cross 
Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep learning curve due to its dynamic nature and non-standard imaging planes. Additionally, the constant need to shift focus between the US screen and the patient poses a challenge. To address these issues, we integrate deep learning (DL)-based semantic segmentation for real-time (RT) automated kidney volumetric measurements, which are essential for clinical assessment but are traditionally time-consuming and prone to fatigue. This automation allows clinicians to concentrate on image interpretation rather than manual measurements. Complementing DL, augmented reality (AR) enhances the usability of US by projecting the display directly into the clinician's field of view, improving ergonomics and reducing the cognitive load associated with screen-to-patient transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one streams directly via the application programming interface for a wireless setup, while the other supports any US device with video output for broader accessibility. We evaluate RT feasibility and accuracy using the Open Kidney Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model implementations, measurement algorithms, and a Wi-Fi-based streaming solution, enhancing US training and diagnostics, especially in point-of-care settings.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2506.23724</link>
<guid>https://arxiv.org/abs/2506.23724</guid>
<content:encoded><![CDATA[
arXiv:2506.23724v1 Announce Type: cross 
Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with potential domain shifts through online unsupervised learning, yielding impressive performance. However, to date, existing TTA methods primarily focus on single-model adaptation. In this work, we investigate an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that, in TTA's unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there are substantial differences in model size. For instance, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base (86.6M parameters). In light of this, we propose COCA, a Cross-Model Co-Learning framework for TTA, which mainly consists of two main strategies. 1) Co-adaptation adaptively integrates complementary knowledge from other models throughout the TTA process, reducing individual model biases. 2) Self-adaptation enhances each model's unique strengths via unsupervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, which can also serve as a plug-and-play module, significantly boosts existing SOTAs, on models with various sizes--including ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example, with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy on ImageNet-C from 51.7% to 64.5%. The code is publicly available at https://github.com/ycarobot/COCA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?</title>
<link>https://arxiv.org/abs/2506.23725</link>
<guid>https://arxiv.org/abs/2506.23725</guid>
<content:encoded><![CDATA[
arXiv:2506.23725v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that remains largely unverified. For robots to perform actions reliably, they must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state, such as being closed). Despite the widespread use of VLMs in manipulation tasks, we argue that off-the-shelf models may lack this granular, physically grounded understanding, as such prerequisites are often overlooked during training.
  To address this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with over 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, and 1 to 3 affordances defined per class), 100 real-world humanoid-view scenarios, and 120 unique simulated constraint scenarios across four tasks.
  Our evaluations reveal significant gaps in the ability of current VLMs to grasp fundamental physical concepts, highlighting limitations in their suitability for reliable robot manipulation and pointing to key areas for targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating physical reasoning in VLMs and guiding the development of more robust, physically grounded models for robotic applications.
  Project Page: https://pacbench.github.io/
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System-Embedded Diffusion Bridge Models</title>
<link>https://arxiv.org/abs/2506.23726</link>
<guid>https://arxiv.org/abs/2506.23726</guid>
<content:encoded><![CDATA[
arXiv:2506.23726v1 Announce Type: cross 
Abstract: Solving inverse problems -- recovering signals from incomplete or noisy measurements -- is fundamental in science and engineering. Score-based generative models (SGMs) have recently emerged as a powerful framework for this task. Two main paradigms have formed: unsupervised approaches that adapt pretrained generative models to inverse problems, and supervised bridge methods that train stochastic processes conditioned on paired clean and corrupted data. While the former typically assume knowledge of the measurement model, the latter have largely overlooked this structural information. We introduce System embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge methods that explicitly embed the known linear measurement system into the coefficients of a matrix-valued SDE. This principled integration yields consistent improvements across diverse linear inverse problems and demonstrates robust generalization under system misspecification between training and deployment, offering a promising solution to real-world applications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment</title>
<link>https://arxiv.org/abs/2506.23734</link>
<guid>https://arxiv.org/abs/2506.23734</guid>
<content:encoded><![CDATA[
arXiv:2506.23734v1 Announce Type: cross 
Abstract: Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex dynamics like intransitivity and the Red Queen effect, leading to unstable convergence. To counter these challenges, this paper introduces the Marker Gene Method (MGM), a framework that establishes stability by using a 'marker gene' as a dynamic benchmark and an adaptive weighting mechanism to balance exploration and exploitation. We provide rigorous mathematical proofs demonstrating that MGM creates strong attractors near Nash Equilibria within the Strictly Competitive Game framework. Empirically, MGM demonstrates its efficacy across a spectrum of challenges: it stabilizes the canonical Rock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D on ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it successfully tames the notoriously pathological Shapley Biased Game. This work presents a theoretically sound and empirically validated framework that substantially enhances the stability and robustness of CCEAs in complex competitive environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data</title>
<link>https://arxiv.org/abs/2506.23735</link>
<guid>https://arxiv.org/abs/2506.23735</guid>
<content:encoded><![CDATA[
arXiv:2506.23735v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable performance on various tasks, but existing evaluation benchmarks are often static and insufficient to fully assess their robustness and generalization in realistic scenarios. Prior work using evolutionary or adversarial data augmentation has improved evaluation diversity but lacks systematic control over perturbation types and multi-step complexity, limiting comprehensive robustness analysis. To address these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for close-ended tasks such as multi-choice question answering. AutoEvoEval introduces 22 interpretable atomic evolution operations and supports multi-round compositions, enabling controlled generation of diverse, challenging, and realistic test samples. We conduct extensive experiments addressing four research questions on a broad set of open- and closed-source LLMs. Our results show that atomic operations cause an average accuracy drop of 7.283\%, with structure-disrupting or misleading semantic edits causing the largest declines. Model sensitivities vary significantly for the same perturbation, and combining multiple evolution steps amplifies adversarial effects by up to 52.932\%. These findings suggest current benchmarks may overestimate true model generalization and emphasize the need for evolution-aware robustness evaluation. Code and resources are available at: https://github.com/SYSUSELab/AutoEvoEval.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead</title>
<link>https://arxiv.org/abs/2506.23762</link>
<guid>https://arxiv.org/abs/2506.23762</guid>
<content:encoded><![CDATA[
arXiv:2506.23762v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) has redefined artificial intelligence (AI), pushing the boundaries of AI research and enabling unbounded possibilities for both academia and the industry. However, LLM development faces increasingly complex challenges throughout its lifecycle, yet no existing research systematically explores these challenges and solutions from the perspective of software engineering (SE) approaches. To fill the gap, we systematically analyze research status throughout the LLM development lifecycle, divided into six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. We then conclude by identifying the key challenges for each phase and presenting potential research directions to address these challenges. In general, we provide valuable insights from an SE perspective to facilitate future advances in LLM development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.23771</link>
<guid>https://arxiv.org/abs/2506.23771</guid>
<content:encoded><![CDATA[
arXiv:2506.23771v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) is increasingly used in autonomous driving (AD) and shows clear advantages. However, most RL-based AD methods overlook policy structure design. An RL policy that only outputs short-timescale vehicle control commands results in fluctuating driving behavior due to fluctuations in network outputs, while one that only outputs long-timescale driving goals cannot achieve unified optimality of driving behavior and control. Therefore, we propose a multi-timescale hierarchical reinforcement learning approach. Our approach adopts a hierarchical policy structure, where high- and low-level RL policies are unified-trained to produce long-timescale motion guidance and short-timescale control commands, respectively. Therein, motion guidance is explicitly represented by hybrid actions to capture multimodal driving behaviors on structured road and support incremental low-level extend-state updates. Additionally, a hierarchical safety mechanism is designed to ensure multi-timescale safety. Evaluation in simulator-based and HighD dataset-based highway multi-lane scenarios demonstrates that our approach significantly improves AD performance, effectively increasing driving efficiency, action consistency and safety.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling</title>
<link>https://arxiv.org/abs/2506.23782</link>
<guid>https://arxiv.org/abs/2506.23782</guid>
<content:encoded><![CDATA[
arXiv:2506.23782v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance on relational data; however, their confidence estimates often misalign with actual predictive correctness, posing significant limitations for deployment in safety-critical settings. While existing graph-aware calibration methods seek to mitigate this limitation, they primarily depend on coarse one-hop statistics, such as neighbor-predicted confidence, or latent node embeddings, thereby neglecting the fine-grained structural heterogeneity inherent in graph topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework that assigns node-specific temperatures based on tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the scalability and topology sensitivity of graph wavelets to refine confidence estimates, all without necessitating model retraining or access to neighboring logits or predictions. Extensive evaluations across seven benchmark datasets with varying graph structures and two GNN backbones demonstrate that WATS achieves the lowest Expected Calibration Error (ECE) among all compared methods, outperforming both classical and graph-specific baselines by up to 42.3\% in ECE and reducing calibration variance by 17.24\% on average compared with graph-specific methods. Moreover, WATS remains computationally efficient, scaling well across graphs of diverse sizes and densities. Code will be released based on publication.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking</title>
<link>https://arxiv.org/abs/2506.23783</link>
<guid>https://arxiv.org/abs/2506.23783</guid>
<content:encoded><![CDATA[
arXiv:2506.23783v1 Announce Type: cross 
Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of AI on Educational Assessment: A Framework for Constructive Alignment</title>
<link>https://arxiv.org/abs/2506.23815</link>
<guid>https://arxiv.org/abs/2506.23815</guid>
<content:encoded><![CDATA[
arXiv:2506.23815v1 Announce Type: cross 
Abstract: The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.
  Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins</title>
<link>https://arxiv.org/abs/2506.23826</link>
<guid>https://arxiv.org/abs/2506.23826</guid>
<content:encoded><![CDATA[
arXiv:2506.23826v1 Announce Type: cross 
Abstract: Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model</title>
<link>https://arxiv.org/abs/2506.23840</link>
<guid>https://arxiv.org/abs/2506.23840</guid>
<content:encoded><![CDATA[
arXiv:2506.23840v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an overthinking dilemma. When handling simple tasks, they often produce verbose responses overloaded with thinking tokens (e.g., wait, however). These tokens trigger unnecessary high-level reasoning behaviors like reflection and backtracking, reducing efficiency. In this work, our pilot study reveals that these thinking-token-induced behaviors are not essential for effective problem-solving and may even hinder correct reasoning within constrained token budgets. We identify this phenomenon as the thinking trap. To mitigate this issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel algorithm featuring: (1) A rollout sampling strategy that guarantees balanced exposure to responses with and without thinking tokens; (2) A fine-grained advantage control technique to dynamically regulate the prediction of target tokens; (3) A policy shaping method ensuring stable gradient contributions from thinking tokens. Experimental results on five popular math reasoning benchmarks show that DuP-PO performs well on the popular LRM, which significantly improves their token efficiency during reasoning, while achieving superior performance of the base model.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts</title>
<link>https://arxiv.org/abs/2506.23845</link>
<guid>https://arxiv.org/abs/2506.23845</guid>
<content:encoded><![CDATA[
arXiv:2506.23845v1 Announce Type: cross 
Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a series of negative results have added to skepticism about their usefulness. Here, we establish a conceptual distinction that reconciles competing narratives surrounding SAEs. We argue that while SAEs may be less effective for acting on known concepts, SAEs are powerful tools for discovering unknown concepts. This distinction cleanly separates existing negative and positive results, and suggests several classes of SAE applications. Specifically, we outline use cases for SAEs in (i) ML interpretability, explainability, fairness, auditing, and safety, and (ii) social and health sciences.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Synthetic Data Release for Topics API Outputs</title>
<link>https://arxiv.org/abs/2506.23855</link>
<guid>https://arxiv.org/abs/2506.23855</guid>
<content:encoded><![CDATA[
arXiv:2506.23855v1 Announce Type: cross 
Abstract: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an area of research that has received strong interest from academics, industry, and regulators. Despite this interest, the empirical study of these methods is hindered by the lack of publicly available data. Reliable empirical analysis of the privacy properties of an API, in fact, requires access to a dataset consisting of realistic API outputs; however, privacy concerns prevent the general release of such data to the public.
  In this work, we develop a novel methodology to construct synthetic API outputs that are simultaneously realistic enough to enable accurate study and provide strong privacy protections. We focus on one Privacy-Preserving Ads APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a methodology to generate a differentially-private dataset that closely matches the re-identification risk properties of the real Topics API data. The use of differential privacy provides strong theoretical bounds on the leakage of private user information from this release.
  Our methodology is based on first computing a large number of differentially-private statistics describing how output API traces evolve over time. Then, we design a parameterized distribution over sequences of API traces and optimize its parameters so that they closely match the statistics obtained. Finally, we create the synthetic data by drawing from this distribution.
  Our work is complemented by an open-source release of the anonymized dataset obtained by this methodology. We hope this will enable external researchers to analyze the API in-depth and replicate prior and future work on a realistic large-scale dataset. We believe that this work will contribute to fostering transparency regarding the privacy properties of Privacy-Preserving Ads APIs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Self-Supervised Representation Learning for Symbolic Piano Performance</title>
<link>https://arxiv.org/abs/2506.23869</link>
<guid>https://arxiv.org/abs/2506.23869</guid>
<content:encoded><![CDATA[
arXiv:2506.23869v1 Announce Type: cross 
Abstract: We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic</title>
<link>https://arxiv.org/abs/2506.23875</link>
<guid>https://arxiv.org/abs/2506.23875</guid>
<content:encoded><![CDATA[
arXiv:2506.23875v1 Announce Type: cross 
Abstract: The chain of thought is fundamental in Transformers, which is to perform step-by-step reasoning. Besides what intermediate steps work, the order of these steps critically affects the difficulty of the reasoning. This study addresses a novel task of unraveling chain of thought - reordering decoder input tokens to a learning-friendly sequence for Transformers to learn arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture of target sequences arranged in different orders and then identifies benign orders as those with fast loss drops in the early stage. As the search space grows factorially with sequence length, we propose a two-stage hierarchical approach for inter- and intra-block reordering. Experiments on four order-sensitive arithmetic tasks show that our method identifies a learning-friendly order out of a few billion candidates. Notably, on the multiplication task, it recovered the reverse-digit order reported in prior studies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23903</link>
<guid>https://arxiv.org/abs/2506.23903</guid>
<content:encoded><![CDATA[
arXiv:2506.23903v1 Announce Type: cross 
Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System</title>
<link>https://arxiv.org/abs/2506.23923</link>
<guid>https://arxiv.org/abs/2506.23923</guid>
<content:encoded><![CDATA[
arXiv:2506.23923v1 Announce Type: cross 
Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes for the manufacturing of high-performance fibre-reinforced polymer composites, particularly for large-scale applications such as wind turbine blades. Controlling the resin flow dynamics in these processes is critical to ensure the uniform impregnation of the fibre reinforcements, thereby preventing residual porosities and dry spots that impact the consequent structural integrity of the final component. This paper presents a reinforcement learning (RL) based strategy, established using process simulations, for synchronising the different resin flow fronts in an infusion scenario involving two resin inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our approach addresses the challenge of managing the fluid dynamics in a partially observable environment. The results demonstrate the effectiveness of the RL approach in achieving an accurate flow convergence, highlighting its potential towards improving process control and product quality in composites manufacturing.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2506.23930</link>
<guid>https://arxiv.org/abs/2506.23930</guid>
<content:encoded><![CDATA[
arXiv:2506.23930v1 Announce Type: cross 
Abstract: The rapid expansion of social media leads to a marked increase in hate speech, which threatens personal lives and results in numerous hate crimes. Detecting hate speech presents several challenges: diverse dialects, frequent code-mixing, and the prevalence of misspelled words in user-generated content on social media platforms. Recent progress in hate speech detection is typically concentrated on high-resource languages. However, low-resource languages still face significant challenges due to the lack of large-scale, high-quality datasets. This paper investigates how we can overcome this limitation via prompt engineering on large language models (LLMs) focusing on low-resource Bengali language. We investigate six prompting strategies - zero-shot prompting, refusal suppression, flattering the classifier, multi-shot prompting, role prompting, and finally our innovative metaphor prompting to detect hate speech effectively in low-resource languages. We pioneer the metaphor prompting to circumvent the built-in safety mechanisms of LLMs that marks a significant departure from existing jailbreaking methods. We investigate all six different prompting strategies on the Llama2-7B model and compare the results extensively with three pre-trained word embeddings - GloVe, Word2Vec, and FastText for three different deep learning models - multilayer perceptron (MLP), convolutional neural network (CNN), and bidirectional gated recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in the low-resource Bengali language, we also evaluate it in another low-resource language - Hindi, and two high-resource languages - English and German. The performance of all prompting techniques is evaluated using the F1 score, and environmental impact factor (IF), which measures CO$_2$ emissions, electricity usage, and computational time.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference</title>
<link>https://arxiv.org/abs/2506.23934</link>
<guid>https://arxiv.org/abs/2506.23934</guid>
<content:encoded><![CDATA[
arXiv:2506.23934v1 Announce Type: cross 
Abstract: As machine learning inferences increasingly move to edge devices, adapting to diverse computational capabilities, hardware, and memory constraints becomes more critical. Instead of relying on a pre-trained model fixed for all future inference queries across diverse edge devices, we argue that planning an inference pattern with a request-specific model tailored to the device's computational capacity, accuracy requirements, and time constraints is more cost-efficient and robust to diverse scenarios. To this end, we propose an accuracy-aware and workload-balanced inference system that integrates joint model quantization and inference partitioning. In this approach, the server dynamically responds to inference queries by sending a quantized model and adaptively sharing the inference workload with the device. Meanwhile, the device's computational power, channel capacity, and accuracy requirements are considered when deciding.
  Furthermore, we introduce a new optimization framework for the inference system, incorporating joint model quantization and partitioning. Our approach optimizes layer-wise quantization bit width and partition points to minimize time consumption and cost while accounting for varying accuracy requirements of tasks through an accuracy degradation metric in our optimization model. To our knowledge, this work represents the first exploration of optimizing quantization layer-wise bit-width in the inference serving system, by introducing theoretical measurement of accuracy degradation. Simulation results demonstrate a substantial reduction in overall time and power consumption, with computation payloads decreasing by over 80% and accuracy degradation kept below 1%.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning</title>
<link>https://arxiv.org/abs/2506.23944</link>
<guid>https://arxiv.org/abs/2506.23944</guid>
<content:encoded><![CDATA[
arXiv:2506.23944v1 Announce Type: cross 
Abstract: Imitation learning models for robotic tasks typically rely on multi-modal inputs, such as RGB images, language, and proprioceptive states. While proprioception is intuitively important for decision-making and obstacle avoidance, simply incorporating all proprioceptive states leads to a surprising degradation in imitation learning performance. In this work, we identify the underlying issue as the proprioception shift problem, where the distributions of proprioceptive states diverge significantly between training and deployment. To address this challenge, we propose a domain adaptation framework that bridges the gap by utilizing rollout data collected during deployment. Using Wasserstein distance, we quantify the discrepancy between expert and rollout proprioceptive states and minimize this gap by adding noise to both sets of states, proportional to the Wasserstein distance. This strategy enhances robustness against proprioception shifts by aligning the training and deployment distributions. Experiments on robotic manipulation tasks demonstrate the efficacy of our method, enabling the imitation policy to leverage proprioception while mitigating its adverse effects. Our approach outperforms the naive solution which discards proprioception, and other baselines designed to address distributional shifts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomy by Design: Preserving Human Autonomy in AI Decision-Support</title>
<link>https://arxiv.org/abs/2506.23952</link>
<guid>https://arxiv.org/abs/2506.23952</guid>
<content:encoded><![CDATA[
arXiv:2506.23952v1 Announce Type: cross 
Abstract: AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.23960</link>
<guid>https://arxiv.org/abs/2506.23960</guid>
<content:encoded><![CDATA[
arXiv:2506.23960v1 Announce Type: cross 
Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due to the inherent limitations in their design and performance capabilities. Online repair plays a crucial role in mitigating such limitations, ensuring the runtime safety and reliability of ADSs. Existing online repair solutions enforce ADS compliance by transforming unacceptable trajectories into acceptable ones based on predefined specifications, such as rule-based constraints or training datasets. However, these approaches often lack generalizability, adaptability and tend to be overly conservative, resulting in ineffective repairs that not only fail to mitigate safety risks sufficiently but also degrade the overall driving experience. To address this issue, we propose Adaptive Decision Repair (ADReFT), a novel and effective repair method that identifies safety-critical states through offline learning from failed tests and generates appropriate mitigation actions to improve ADS safety. Specifically, ADReFT incorporates a transformer-based model with two joint heads, State Monitor and Decision Adapter, designed to capture complex driving environment interactions to evaluate state safety severity and generate adaptive repair actions. Given the absence of oracles for state safety identification, we first pretrain ADReFT using supervised learning with coarse annotations, i.e., labeling states preceding violations as positive samples and others as negative samples. It establishes ADReFT's foundational capability to mitigate safety-critical violations, though it may result in somewhat conservative mitigation strategies. Therefore, we subsequently finetune ADReFT using reinforcement learning to improve its initial capability and generate more precise and contextually appropriate repair decisions. Our evaluation results illustrate that ADReFT achieves better repair performance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2506.23995</link>
<guid>https://arxiv.org/abs/2506.23995</guid>
<content:encoded><![CDATA[
arXiv:2506.23995v1 Announce Type: cross 
Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and reliability of autonomous vehicles (AVs) before deployment. However, existing techniques primarily focus on evaluating ADS functionalities in single-AV settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes crucial to assess their cooperative performance, particularly regarding deadlocks, a fundamental coordination failure in which multiple AVs enter a circular waiting state indefinitely, resulting in motion planning failures. Despite its importance, the cooperative capability of ADSs to prevent deadlocks remains insufficiently underexplored. To address this gap, we propose the first dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique, STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs controlled by the ADS under test are in a circular wait state. STCLocker consists of three key components: Deadlock Oracle, Conflict Feedback, and Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable black-box mechanism for detecting deadlock cycles among multiple AVs within a given scenario. Conflict Feedback and Conflict-aware Scenario Generation collaborate to actively guide AVs into simultaneous competition over spatial conflict resources (i.e., shared passing regions) and temporal competitive behaviors (i.e., reaching the conflict region at the same time), thereby increasing the effectiveness of generating conflict-prone deadlocks. We evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA, a module-based ADS supporting cooperative communication. Experimental results show that, on average, STCLocker generates more DLS than the best-performing baseline.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems</title>
<link>https://arxiv.org/abs/2506.24009</link>
<guid>https://arxiv.org/abs/2506.24009</guid>
<content:encoded><![CDATA[
arXiv:2506.24009v1 Announce Type: cross 
Abstract: Large artificial intelligence (AI) models offer revolutionary potential for future wireless systems, promising unprecedented capabilities in network optimization and performance. However, current paradigms largely overlook crucial physical interactions. This oversight means they primarily rely on offline datasets, leading to difficulties in handling real-time wireless dynamics and non-stationary environments. Furthermore, these models often lack the capability for active environmental probing. This paper proposes a fundamental paradigm shift towards wireless embodied large AI (WELAI), moving from passive observation to active embodiment. We first identify key challenges faced by existing models, then we explore the design principles and system structure of WELAI. Besides, we outline prospective applications in next-generation wireless. Finally, through an illustrative case study, we demonstrate the effectiveness of WELAI and point out promising research directions for realizing adaptive, robust, and autonomous wireless systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations</title>
<link>https://arxiv.org/abs/2506.24016</link>
<guid>https://arxiv.org/abs/2506.24016</guid>
<content:encoded><![CDATA[
arXiv:2506.24016v1 Announce Type: cross 
Abstract: Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Theory and Practice in Link Representation with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.24018</link>
<guid>https://arxiv.org/abs/2506.24018</guid>
<content:encoded><![CDATA[
arXiv:2506.24018v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$ framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Vision-Language-Action Models for Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.24044</link>
<guid>https://arxiv.org/abs/2506.24044</guid>
<content:encoded><![CDATA[
arXiv:2506.24044v1 Announce Type: cross 
Abstract: The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STACK: Adversarial Attacks on LLM Safeguard Pipelines</title>
<link>https://arxiv.org/abs/2506.24068</link>
<guid>https://arxiv.org/abs/2506.24068</guid>
<content:encoded><![CDATA[
arXiv:2506.24068v1 Announce Type: cross 
Abstract: Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2506.24081</link>
<guid>https://arxiv.org/abs/2506.24081</guid>
<content:encoded><![CDATA[
arXiv:2506.24081v1 Announce Type: cross 
Abstract: We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks. SQUASH is executed by inserting SWAP gate(s) into the variational quantum circuit of the victim HQNN. Unlike conventional noise-based or adversarial input attacks, SQUASH directly manipulates the circuit structure, leading to qubit misalignment and disrupting quantum state evolution. This attack is highly stealthy, as it does not require access to training data or introduce detectable perturbations in input states. Our results demonstrate that SQUASH significantly degrades classification performance, with untargeted SWAP attacks reducing accuracy by up to 74.08\% and targeted SWAP attacks reducing target class accuracy by up to 79.78\%. These findings reveal a critical vulnerability in HQNN implementations, underscoring the need for more resilient architectures against circuit-level adversarial interventions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention</title>
<link>https://arxiv.org/abs/2506.24085</link>
<guid>https://arxiv.org/abs/2506.24085</guid>
<content:encoded><![CDATA[
arXiv:2506.24085v1 Announce Type: cross 
Abstract: Blending visual and textual concepts into a new visual concept is a unique and powerful trait of human beings that can fuel creativity. However, in practice, cross-modal conceptual blending for humans is prone to cognitive biases, like design fixation, which leads to local minima in the design space. In this paper, we propose a T2I diffusion adapter "IT-Blender" that can automate the blending process to enhance human creativity. Prior works related to cross-modal conceptual blending are limited in encoding a real image without loss of details or in disentangling the image and text inputs. To address these gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend the latent representations of a clean reference image with those of the noisy generated image. Combined with our novel blended attention, IT-Blender encodes the real reference image without loss of details and blends the visual concept with the object specified by the text in a disentangled way. Our experiment results show that IT-Blender outperforms the baselines by a large margin in blending visual and textual concepts, shedding light on the new application of image generative models to augment human creativity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies</title>
<link>https://arxiv.org/abs/2506.24093</link>
<guid>https://arxiv.org/abs/2506.24093</guid>
<content:encoded><![CDATA[
arXiv:2506.24093v1 Announce Type: cross 
Abstract: Synthetic data has emerged as a cost-effective alternative to real data for training artificial neural networks (ANN). However, the disparity between synthetic and real data results in a domain gap. That gap leads to poor performance and generalization of the trained ANN when applied to real-world scenarios. Several strategies have been developed to bridge this gap, which combine synthetic and real data, known as mixed training using hybrid datasets. While these strategies have been shown to mitigate the domain gap, a systematic evaluation of their generalizability and robustness across various tasks and architectures remains underexplored. To address this challenge, our study comprehensively analyzes two widely used mixing strategies on three prevalent architectures and three distinct hybrid datasets. From these datasets, we sample subsets with varying proportions of synthetic to real data to investigate the impact of synthetic and real components. The findings of this paper provide valuable insights into optimizing the use of synthetic data in the training process of any ANN, contributing to enhancing robustness and efficacy.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Predictive Power of Representation Dispersion in Language Models</title>
<link>https://arxiv.org/abs/2506.24106</link>
<guid>https://arxiv.org/abs/2506.24106</guid>
<content:encoded><![CDATA[
arXiv:2506.24106v1 Announce Type: cross 
Abstract: We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating with Annealing Guidance Scale in Diffusion Space</title>
<link>https://arxiv.org/abs/2506.24108</link>
<guid>https://arxiv.org/abs/2506.24108</guid>
<content:encoded><![CDATA[
arXiv:2506.24108v1 Announce Type: cross 
Abstract: Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime</title>
<link>https://arxiv.org/abs/2506.24120</link>
<guid>https://arxiv.org/abs/2506.24120</guid>
<content:encoded><![CDATA[
arXiv:2506.24120v1 Announce Type: cross 
Abstract: Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complex tasks with limited prior knowledge. In this paper, we demonstrate that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, we establish that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by $h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training dynamics of gradient descent (GD). Moreover, we theoretically show that the approximation error of neural networks decreases as $h_{\min}$ increases. Our analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connections and function compositions in deep neural architectures. In the end, we conduct comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. Code and Datasets are available at the link: https://github.com/SafeRL-Lab/data-uniformity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation</title>
<link>https://arxiv.org/abs/2506.24125</link>
<guid>https://arxiv.org/abs/2506.24125</guid>
<content:encoded><![CDATA[
arXiv:2506.24125v1 Announce Type: cross 
Abstract: Residual connection has been extensively studied and widely applied at the model architecture level. However, its potential in the more challenging data-centric approaches remains unexplored. In this work, we introduce the concept of Data Residual Matching for the first time, leveraging data-level skip connections to facilitate data generation and mitigate data information vanishing. This approach maintains a balance between newly acquired knowledge through pixel space optimization and existing core local information identification within raw data modalities, specifically for the dataset distillation task. Furthermore, by incorporating optimization-level refinements, our method significantly improves computational efficiency, achieving superior performance while reducing training time and peak GPU memory usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art, demonstrating substantial improvements over existing methods across multiple dataset benchmarks in both efficiency and effectiveness. For instance, with ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the method achieves 47.7% test accuracy in single-model dataset distillation and 50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4% and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Agents using Social Choice Theory</title>
<link>https://arxiv.org/abs/2312.03121</link>
<guid>https://arxiv.org/abs/2312.03121</guid>
<content:encoded><![CDATA[
arXiv:2312.03121v4 Announce Type: replace 
Abstract: We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain-inspired and Self-based Artificial Intelligence</title>
<link>https://arxiv.org/abs/2402.18784</link>
<guid>https://arxiv.org/abs/2402.18784</guid>
<content:encoded><![CDATA[
arXiv:2402.18784v2 Announce Type: replace 
Abstract: The question "Can machines think?" and the Turing Test to assess whether machines could achieve human-level intelligence is one of the roots of AI. With the philosophical argument "I think, therefore I am", this paper challenge the idea of a "thinking machine" supported by current AIs since there is no sense of self in them. Current artificial intelligence is only seemingly intelligent information processing and does not truly understand or be subjectively aware of oneself and perceive the world with the self as human intelligence does. In this paper, we introduce a Brain-inspired and Self-based Artificial Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to coordinating various cognitive functions and learning strategies in a self-organized manner to build human-level AI models and robotic applications. Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the future AI, rooted with a practical hierarchical Self framework, including Perception and Learning, Bodily Self, Autonomous Self, Social Self, and Conceptual Self. The hierarchical framework of the Self highlights self-based environment perception, self-bodily modeling, autonomous interaction with the environment, social interaction and collaboration with others, and even more abstract understanding of the Self. Furthermore, the positive mutual promotion and support among multiple levels of Self, as well as between Self and learning, enhance the BriSe AI's conscious understanding of information and flexible adaptation to complex environments, serving as a driving force propelling BriSe AI towards real Artificial General Intelligence.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge Representation</title>
<link>https://arxiv.org/abs/2404.09848</link>
<guid>https://arxiv.org/abs/2404.09848</guid>
<content:encoded><![CDATA[
arXiv:2404.09848v3 Announce Type: replace 
Abstract: In a hyper-relational knowledge graph (HKG), each fact is composed of a main triple associated with attribute-value qualifiers, which express additional factual knowledge. The hyper-relational knowledge graph completion (HKGC) task aims at inferring plausible missing links in a HKG. Most existing approaches to HKGC focus on enhancing the communication between qualifier pairs and main triples, while overlooking two important properties that emerge from the monotonicity of the hyper-relational graphs representation regime. Stage Reasoning allows for a two-step reasoning process, facilitating the integration of coarse-grained inference results derived solely from main triples and fine-grained inference results obtained from hyper-relational facts with qualifiers. In the initial stage, coarse-grained results provide an upper bound for correct predictions, which are subsequently refined in the fine-grained step. More generally, Qualifier Monotonicity implies that by attaching more qualifier pairs to a main triple, we may only narrow down the answer set, but never enlarge it. This paper proposes the HyperMono model for hyper-relational knowledge graph completion, which realizes stage reasoning and qualifier monotonicity. To implement qualifier monotonicity HyperMono resorts to cone embeddings. Experiments on three real-world datasets with three different scenario conditions demonstrate the strong performance of HyperMono when compared to the SoTA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic Perspective</title>
<link>https://arxiv.org/abs/2404.16078</link>
<guid>https://arxiv.org/abs/2404.16078</guid>
<content:encoded><![CDATA[
arXiv:2404.16078v3 Announce Type: replace 
Abstract: Machines that can replicate human intelligence with type 2 reasoning capabilities should be able to reason at multiple levels of spatio-temporal abstractions and scales using internal world models. Devising formalisms to develop such internal world models, which accurately reflect the causal hierarchies inherent in the dynamics of the real world, is a critical research challenge in the domains of artificial intelligence and machine learning. This thesis identifies several limitations with the prevalent use of state space models (SSMs) as internal world models and propose two new probabilistic formalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address these drawbacks. The structure of graphical models in both formalisms facilitates scalable exact probabilistic inference using belief propagation, as well as end-to-end learning via backpropagation through time. This approach permits the development of scalable, adaptive hierarchical world models capable of representing nonstationary dynamics across multiple temporal abstractions and scales. Moreover, these probabilistic formalisms integrate the concept of uncertainty in world states, thus improving the system's capacity to emulate the stochastic nature of the real world and quantify the confidence in its predictions. The thesis also discuss how these formalisms are in line with related neuroscience literature on Bayesian brain hypothesis and predicitive processing. Our experiments on various real and simulated robots demonstrate that our formalisms can match and in many cases exceed the performance of contemporary transformer variants in making long-range future predictions. We conclude the thesis by reflecting on the limitations of our current models and suggesting directions for future research.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General Framework on Conditions for Constraint-based Causal Learning</title>
<link>https://arxiv.org/abs/2408.07575</link>
<guid>https://arxiv.org/abs/2408.07575</guid>
<content:encoded><![CDATA[
arXiv:2408.07575v2 Announce Type: replace 
Abstract: Most constraint-based causal learning algorithms provably return the correct causal graph under certain correctness conditions, such as faithfulness. By representing any constraint-based causal learning algorithm using the notion of a property, we provide a general framework to obtain and study correctness conditions for these algorithms. From the framework, we provide exact correctness conditions for the PC algorithm, which are then related to the correctness conditions of some other existing causal discovery algorithms. The framework also suggests a paradigm for designing causal learning algorithms which allows for the correctness conditions of algorithms to be controlled for before designing the actual algorithm, and has the following implications. We show that the sparsest Markov representation condition is the weakest correctness condition for algorithms that output ancestral graphs or directed acyclic graphs satisfying any existing notions of minimality. We also reason that Pearl-minimality is necessary for meaningful causal learning but not sufficient to relax the faithfulness condition and, as such, has to be strengthened, such as by including background knowledge, for causal learning beyond faithfulness.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARTOM: A Visual Theory-of-Mind Benchmark for LLMs on Misleading Charts</title>
<link>https://arxiv.org/abs/2408.14419</link>
<guid>https://arxiv.org/abs/2408.14419</guid>
<content:encoded><![CDATA[
arXiv:2408.14419v3 Announce Type: replace 
Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark designed to evaluate multimodal large language models' capability to understand and reason about misleading data visualizations though charts. CHARTOM consists of carefully designed charts and associated questions that require a language model to not only correctly comprehend the factual content in the chart (the FACT question) but also judge whether the chart will be misleading to a human readers (the MIND question), a dual capability with significant societal benefits. We detail the construction of our benchmark including its calibration on human performance and estimation of MIND ground truth called the Human Misleadingness Index. We evaluated several leading LLMs -- including GPT, Claude, Gemini, Qwen, Llama, and Llava series models -- on the CHARTOM dataset and found that it was challenging to all models both on FACT and MIND questions. This highlights the limitations of current LLMs and presents significant opportunity for future LLMs to improve on understanding misleading charts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity in AI: Progresses and Challenges</title>
<link>https://arxiv.org/abs/2410.17218</link>
<guid>https://arxiv.org/abs/2410.17218</guid>
<content:encoded><![CDATA[
arXiv:2410.17218v5 Announce Type: replace 
Abstract: Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition. Machine creativity on the other hand has been a long-standing challenge. With the rise of advanced generative AI, there has been renewed interest and debate regarding AI's creative capabilities. Therefore, it is imperative to revisit the state of creativity in AI and identify key progresses and remaining challenges. In this work, we survey leading works studying the creative capabilities of AI systems, focusing on creative problem-solving, linguistic, artistic, and scientific creativity. Our review suggests that while the latest AI models are largely capable of producing linguistically and artistically creative outputs such as poems, images, and musical pieces, they struggle with tasks that require creative problem-solving, abstract thinking and compositionality and their generations suffer from a lack of diversity, originality, long-range incoherence and hallucinations. We also discuss key questions concerning copyright and authorship issues with generative models. Furthermore, we highlight the need for a comprehensive evaluation of creativity that is process-driven and considers several dimensions of creativity. Finally, we propose future research directions to improve the creativity of AI outputs, drawing inspiration from cognitive science and psychology.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust &amp; Safety of LLMs and LLMs in Trust &amp; Safety</title>
<link>https://arxiv.org/abs/2412.02113</link>
<guid>https://arxiv.org/abs/2412.02113</guid>
<content:encoded><![CDATA[
arXiv:2412.02113v2 Announce Type: replace 
Abstract: In recent years, Large Language Models (LLMs) have garnered considerable attention for their remarkable abilities in natural language processing tasks. However, their widespread adoption has raised concerns pertaining to trust and safety. This systematic review investigates the current research landscape on trust and safety in LLMs, with a particular focus on the novel application of LLMs within the field of Trust and Safety itself. We delve into the complexities of utilizing LLMs in domains where maintaining trust and safety is paramount, offering a consolidated perspective on this emerging trend.\
  By synthesizing findings from various studies, we identify key challenges and potential solutions, aiming to benefit researchers and practitioners seeking to understand the nuanced interplay between LLMs and Trust and Safety.
  This review provides insights on best practices for using LLMs in Trust and Safety, and explores emerging risks such as prompt injection and jailbreak attacks. Ultimately, this study contributes to a deeper understanding of how LLMs can be effectively and responsibly utilized to enhance trust and safety in the digital realm.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCHDOG: an ontology-aWare risk AssessmenT approaCH via object-oriented DisruptiOn Graphs</title>
<link>https://arxiv.org/abs/2412.13964</link>
<guid>https://arxiv.org/abs/2412.13964</guid>
<content:encoded><![CDATA[
arXiv:2412.13964v2 Announce Type: replace 
Abstract: When considering risky events or actions, we must not downplay the role of involved objects: a charged battery in our phone averts the risk of being stranded in the desert after a flat tyre, and a functional firewall mitigates the risk of a hacker intruding the network. The Common Ontology of Value and Risk (COVER) highlights how the role of objects and their relationships remains pivotal to performing transparent, complete and accountable risk assessment. In this paper, we operationalize some of the notions proposed by COVER -- such as parthood between objects and participation of objects in events/actions -- by presenting a new framework for risk assessment: WATCHDOG. WATCHDOG enriches the expressivity of vetted formal models for risk -- i.e., fault trees and attack trees -- by bridging the disciplines of ontology and formal methods into an ontology-aware formal framework composed by a more expressive modelling formalism, Object-Oriented Disruption Graphs (DOGs), logic (DOGLog) and an intermediate query language (DOGLang). With these, WATCHDOG allows risk assessors to pose questions about disruption propagation, disruption likelihood and risk levels, keeping the fundamental role of objects at risk always in sight.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning</title>
<link>https://arxiv.org/abs/2501.02497</link>
<guid>https://arxiv.org/abs/2501.02497</guid>
<content:encoded><![CDATA[
arXiv:2501.02497v3 Announce Type: replace 
Abstract: The remarkable performance of the o1 model in complex reasoning demonstrates that test-time compute scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time compute scaling. We trace the concept of test-time compute back to System-1 models. In System-1 models, test-time compute addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time compute in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out advanced topics and future directions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in Creative Industries: Advances Prior to 2025</title>
<link>https://arxiv.org/abs/2501.02725</link>
<guid>https://arxiv.org/abs/2501.02725</guid>
<content:encoded><![CDATA[
arXiv:2501.02725v4 Announce Type: replace 
Abstract: The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries, enabling more innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores these technological shifts, with particular focus on how those that have emerged since our previous review in 2022 have expanded creative opportunities and improved efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss the integration of AI into post-production workflows, which has significantly accelerated and improved traditional processes. Once content has been created, it must be delivered to its audiences; the media industry is now facing the demands of increased communication traffic due to creative content. We therefore include a discussion of how AI is beginning to transform the way we represent and compress media content. We highlight the trend toward unified AI frameworks capable of addressing and integrating multiple creative tasks, and we underscore the importance of human insight to drive the creative process and oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges and to maximize its benefits while addressing the associated risks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling</title>
<link>https://arxiv.org/abs/2502.15676</link>
<guid>https://arxiv.org/abs/2502.15676</guid>
<content:encoded><![CDATA[
arXiv:2502.15676v2 Announce Type: replace 
Abstract: Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce AutoToM, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, AutoToM first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, AutoToM outperforms existing ToM methods and even large reasoning models. Additionally, we show that AutoToM can produce human-like confidence estimates and enable online mental inference for embodied decision-making.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning of Correlated Policies in Stackelberg Games</title>
<link>https://arxiv.org/abs/2503.08883</link>
<guid>https://arxiv.org/abs/2503.08883</guid>
<content:encoded><![CDATA[
arXiv:2503.08883v3 Announce Type: replace 
Abstract: Stackelberg games, widely applied in domains like economics and security, involve asymmetric interactions where a leader's strategy drives follower responses. Accurately modeling these dynamics allows domain experts to optimize strategies in interactive scenarios, such as turn-based sports like badminton. In multi-agent systems, agent behaviors are interdependent, and traditional Multi-Agent Imitation Learning (MAIL) methods often fail to capture these complex interactions. Correlated policies, which account for opponents' strategies, are essential for accurately modeling such dynamics. However, even methods designed for learning correlated policies, like CoDAIL, struggle in Stackelberg games due to their asymmetric decision-making, where leaders and followers cannot simultaneously account for each other's actions, often leading to non-correlated policies. Furthermore, existing MAIL methods that match occupancy measures or use adversarial techniques like GAIL or Inverse RL face scalability challenges, particularly in high-dimensional environments, and suffer from unstable training. To address these challenges, we propose a correlated policy occupancy measure specifically designed for Stackelberg games and introduce the Latent Stackelberg Differential Network (LSDN) to match it. LSDN models two-agent interactions as shared latent state trajectories and uses multi-output Geometric Brownian Motion (MO-GBM) to effectively capture joint policies. By leveraging MO-GBM, LSDN disentangles environmental influences from agent-driven transitions in latent space, enabling the simultaneous learning of interdependent policies. This design eliminates the need for adversarial training and simplifies the learning process. Extensive experiments on Iterative Matrix Games and multi-agent particle environments demonstrate that LSDN can better reproduce complex interaction dynamics than existing MAIL methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physical and Mathematical Framework for the Semantic Theory of Evolution</title>
<link>https://arxiv.org/abs/2503.18984</link>
<guid>https://arxiv.org/abs/2503.18984</guid>
<content:encoded><![CDATA[
arXiv:2503.18984v2 Announce Type: replace 
Abstract: The Semantic Theory of Evolution (STE) takes the existence of a number of arbitrary communication codes as a fundamental feature of life, from the genetic code to human cultural communication codes. Their arbitrariness enables, at each level, the selection of one out of several possible correspondences along with the generation of meaning. STE enables more novelties to emerge and suggests a greater variety of potential life forms.
  With this paper I ground STE on physical theories of meaningful information. Furthermore, I show that key features of the arbitrary communication codes employed by living organisms can be expressed by means of Evidence Theory (ET).
  In particular, I adapt ET to organisms that merely react to sequences of stimuli, explain its basics for organisms that are capable of prediction, and illustrate an unconventional version suitable for the most intricate communication codes employed by humans. Finally, I express the natural trend towards ambiguity reduction in terms of information entropy minimization along with thermodynamic entropy maximization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
arXiv:2504.20084v2 Announce Type: replace 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness not as a philosophical question of consciousness, but as a measurable, functional capacity. AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raising concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow.
  In this review, we explore the emerging landscape of AI awareness, which includes metacognition (the ability to represent and reason about its own cognitive state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents and social norms), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets</title>
<link>https://arxiv.org/abs/2505.02118</link>
<guid>https://arxiv.org/abs/2505.02118</guid>
<content:encoded><![CDATA[
arXiv:2505.02118v4 Announce Type: replace 
Abstract: This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing</title>
<link>https://arxiv.org/abs/2505.02811</link>
<guid>https://arxiv.org/abs/2505.02811</guid>
<content:encoded><![CDATA[
arXiv:2505.02811v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance. This paper aims to address these limitations by introducing a new framework, SIM-RAG, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning. Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17312</link>
<guid>https://arxiv.org/abs/2505.17312</guid>
<content:encoded><![CDATA[
arXiv:2505.17312v3 Announce Type: replace 
Abstract: LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work 'well enough' across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring</title>
<link>https://arxiv.org/abs/2505.23575</link>
<guid>https://arxiv.org/abs/2505.23575</guid>
<content:encoded><![CDATA[
arXiv:2505.23575v2 Announce Type: replace 
Abstract: As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that CoT monitoring improves detection by up to 27 percentage points in scenarios where action-only monitoring fails to reliably identify sabotage. However, CoT traces can also contain misleading rationalizations that deceive the monitor, reducing performance in more obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores both reasoning and final outputs and combines them using a weighted average. This hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates over four times higher than action-only monitoring for subtle deception scenarios.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs for mathematical problem solving</title>
<link>https://arxiv.org/abs/2506.00309</link>
<guid>https://arxiv.org/abs/2506.00309</guid>
<content:encoded><![CDATA[
arXiv:2506.00309v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive performance on a range of educational tasks, but are still understudied for their potential to solve mathematical problems. In this study, we compare three prominent LLMs, including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying complexities (GSM8K, MATH500, and MIT Open Courseware datasets). We take a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension. The results show that GPT-4o is the most stable and consistent in performance across all the datasets, but particularly it performs outstandingly in high-level questions of the MIT Open Courseware dataset. DeepSeek-V3 is competitively strong in well-structured domains such as optimisation, but suffers from fluctuations in accuracy in statistical inference tasks. Gemini-2.0 shows strong linguistic understanding and clarity in well-structured problems but performs poorly in multi-step reasoning and symbolic logic. Our error analysis reveals particular deficits in each model: GPT-4o is at times lacking in sufficient explanation or precision; DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning in higher dimensions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
arXiv:2506.04133v2 Announce Type: replace 
Abstract: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of \textbf{Trust, Risk, and Security Management (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around four key pillars: Governance, Explainability, ModelOps, and Privacy/Security , each contextualized to the challenges of multi-agent LLM systems. A novel risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI , as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, outlining critical directions to align emerging systems with TRiSM principles for safe, transparent, and accountable operation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2506.04998</link>
<guid>https://arxiv.org/abs/2506.04998</guid>
<content:encoded><![CDATA[
arXiv:2506.04998v2 Announce Type: replace 
Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question problem set of UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5\% with RAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, questions, and answers are publicly available.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIMgent: Towards Autonomous Building Modeling via Computer-use Agents</title>
<link>https://arxiv.org/abs/2506.07217</link>
<guid>https://arxiv.org/abs/2506.07217</guid>
<content:encoded><![CDATA[
arXiv:2506.07217v2 Announce Type: replace 
Abstract: Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSAC: Distributional Soft Actor-Critic for Risk-Sensitive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2004.14547</link>
<guid>https://arxiv.org/abs/2004.14547</guid>
<content:encoded><![CDATA[
arXiv:2004.14547v3 Announce Type: replace-cross 
Abstract: We present Distributional Soft Actor-Critic (DSAC), a distributional reinforcement learning (RL) algorithm that combines the strengths of distributional information of accumulated rewards and entropy-driven exploration from Soft Actor-Critic (SAC) algorithm. DSAC models the randomness in both action and rewards, surpassing baseline performances on various continuous control tasks. Unlike standard approaches that solely maximize expected rewards, we propose a unified framework for risk-sensitive learning, one that optimizes the risk-related objective while balancing entropy to encourage exploration. Extensive experiments demonstrate DSAC's effectiveness in enhancing agent performances for both risk-neutral and risk-sensitive control tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Data-Constrained Language Models</title>
<link>https://arxiv.org/abs/2305.16264</link>
<guid>https://arxiv.org/abs/2305.16264</guid>
<content:encoded><![CDATA[
arXiv:2305.16264v5 Announce Type: replace-cross 
Abstract: The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2310.11594</link>
<guid>https://arxiv.org/abs/2310.11594</guid>
<content:encoded><![CDATA[
arXiv:2310.11594v3 Announce Type: replace-cross 
Abstract: The delicate equilibrium between user privacy and the ability to unleash the potential of distributed data is an important concern. Federated learning, which enables the training of collaborative models without sharing of data, has emerged as a privacy-centric solution. This approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks that aim to induce misclassifications at test time. Our research investigates the intersection of adversarial training, a common defense method against evasion attacks, and backdoor attacks within federated learning. We introduce Adversarial Robustness Unhardening (ARU), which is employed by a subset of adversarial clients to intentionally undermine model robustness during federated training, rendering models susceptible to a broader range of evasion attacks. We present extensive experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our results show that ARU can substantially undermine adversarial training's ability to harden models against test-time evasion attacks, and that adversaries employing ARU can even evade robust aggregation defenses that often neutralize poisoning or backdoor attacks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowSafe: Combined Knowledge and Data Driven Hazard Mitigation in Artificial Pancreas Systems</title>
<link>https://arxiv.org/abs/2311.07460</link>
<guid>https://arxiv.org/abs/2311.07460</guid>
<content:encoded><![CDATA[
arXiv:2311.07460v2 Announce Type: replace-cross 
Abstract: Significant progress has been made in anomaly detection and run-time monitoring to improve the safety and security of cyber-physical systems (CPS). However, less attention has been paid to hazard mitigation. This paper proposes a combined knowledge and data driven approach, KnowSafe, for the design of safety engines that can predict and mitigate safety hazards resulting from safety-critical malicious attacks or accidental faults targeting a CPS controller. We integrate domain-specific knowledge of safety constraints and context-specific mitigation actions with machine learning (ML) techniques to estimate system trajectories in the far and near future, infer potential hazards, and generate optimal corrective actions to keep the system safe. Experimental evaluation on two realistic closed-loop testbeds for artificial pancreas systems (APS) and a real-world clinical trial dataset for diabetes treatment demonstrates that KnowSafe outperforms the state-of-the-art by achieving higher accuracy in predicting system state trajectories and potential hazards, a low false positive rate, and no false negatives. It also maintains the safe operation of the simulated APS despite faults or attacks without introducing any new hazards, with a hazard mitigation success rate of 92.8%, which is at least 76% higher than solely rule-based (50.9%) and data-driven (52.7%) methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</title>
<link>https://arxiv.org/abs/2312.02312</link>
<guid>https://arxiv.org/abs/2312.02312</guid>
<content:encoded><![CDATA[
arXiv:2312.02312v3 Announce Type: replace-cross 
Abstract: Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Spiking Neural Network Learning Methods with Varying Locality</title>
<link>https://arxiv.org/abs/2402.01782</link>
<guid>https://arxiv.org/abs/2402.01782</guid>
<content:encoded><![CDATA[
arXiv:2402.01782v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have been shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but come with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, given the implicitly recurrent nature of SNNs, this research investigates the influence of the addition of explicit recurrence to SNNs. We experimentally prove that the addition of explicit recurrent weights enhances the robustness of SNNs. We also investigate the performance of local learning methods under gradient and non-gradient-based adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemMiner: A Large Language Model Agent System for Chemical Literature Data Mining</title>
<link>https://arxiv.org/abs/2402.12993</link>
<guid>https://arxiv.org/abs/2402.12993</guid>
<content:encoded><![CDATA[
arXiv:2402.12993v2 Announce Type: replace-cross 
Abstract: The development of AI-assisted chemical synthesis tools requires comprehensive datasets covering diverse reaction types, yet current high-throughput experimental (HTE) approaches are expensive and limited in scope. Chemical literature represents a vast, underexplored data source containing thousands of reactions published annually. However, extracting reaction information from literature faces significant challenges including varied writing styles, complex coreference relationships, and multimodal information presentation. This paper proposes ChemMiner, a novel end-to-end framework leveraging multiple agents powered by large language models (LLMs) to extract high-fidelity chemical data from literature. ChemMiner incorporates three specialized agents: a text analysis agent for coreference mapping, a multimodal agent for non-textual information extraction, and a synthesis analysis agent for data generation. Furthermore, we developed a comprehensive benchmark with expert-annotated chemical literature to evaluate both extraction efficiency and precision. Experimental results demonstrate reaction identification rates comparable to human chemists while significantly reducing processing time, with high accuracy, recall, and F1 scores. Our open-sourced benchmark facilitates future research in chemical literature data mining.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Support Vectors</title>
<link>https://arxiv.org/abs/2403.17329</link>
<guid>https://arxiv.org/abs/2403.17329</guid>
<content:encoded><![CDATA[
arXiv:2403.17329v3 Announce Type: replace-cross 
Abstract: Deep learning has achieved tremendous success. However, unlike SVMs, which provide direct decision criteria and can be trained with a small dataset, it still has significant weaknesses due to its requirement for massive datasets during training and the black-box characteristics on decision criteria. This paper addresses these issues by identifying support vectors in deep learning models. To this end, we propose the DeepKKT condition, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models, and confirm that generated Deep Support Vectors (DSVs) using this condition exhibit properties similar to traditional support vectors. This allows us to apply our method to few-shot dataset distillation problems and alleviate the black-box characteristics of deep learning models. Additionally, we demonstrate that the DeepKKT condition can transform conventional classification models into generative models with high fidelity, particularly as latent generative models using class labels as latent variables. We validate the effectiveness of DSVs using common datasets (ImageNet, CIFAR10 and CIFAR100) on the general architectures (ResNet and ConvNet), proving their practical applicability.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI</title>
<link>https://arxiv.org/abs/2404.08221</link>
<guid>https://arxiv.org/abs/2404.08221</guid>
<content:encoded><![CDATA[
arXiv:2404.08221v2 Announce Type: replace-cross 
Abstract: Generative AI is becoming increasingly prevalent in creative fields, sparking urgent debates over how current copyright laws can keep pace with technological innovation. Recent controversies of AI models generating near-replicas of copyrighted material highlight the need to adapt current legal frameworks and develop technical methods to mitigate copyright infringement risks. This task requires understanding the intersection between computational concepts such as large-scale data scraping and probabilistic content generation, legal definitions of originality and fair use, and economic impacts on IP rights holders. However, most existing research on copyright in AI takes a purely computer science or law-based approach, leaving a gap in coordinating these approaches that only multidisciplinary efforts can effectively address. To bridge this gap, our survey adopts a comprehensive approach synthesizing insights from law, policy, economics, and computer science. It begins by discussing the foundational goals and considerations that should be applied to copyright in generative AI, followed by methods for detecting and assessing potential violations in AI system outputs. Next, it explores various regulatory options influenced by legal, policy, and economic frameworks to manage and mitigate copyright concerns associated with generative AI and reconcile the interests of IP rights holders with that of generative AI producers. The discussion then introduces techniques to safeguard individual creative works from unauthorized replication, such as watermarking and cryptographic protections. Finally, it describes advanced training strategies designed to prevent AI models from reproducing protected content. In doing so, we highlight key opportunities for action and offer actionable strategies that creators, developers, and policymakers can use in navigating the evolving copyright landscape.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMInA: Benchmarking Multihop Multimodal Internet Agents</title>
<link>https://arxiv.org/abs/2404.09992</link>
<guid>https://arxiv.org/abs/2404.09992</guid>
<content:encoded><![CDATA[
arXiv:2404.09992v2 Announce Type: replace-cross 
Abstract: Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites. Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to extract multimodal information from web pages as observations autonomously; 2) Multihop web browsing. Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation. We propose a novel protocol for evaluating an agent's progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks with more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach that replays past action trajectories to reflect. Our method significantly improves the performance of both the single-hop and multihop web browsing abilities. Our code and data are available at github.com/shulin16/MMInA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</title>
<link>https://arxiv.org/abs/2404.19543</link>
<guid>https://arxiv.org/abs/2404.19543</guid>
<content:encoded><![CDATA[
arXiv:2404.19543v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation</title>
<link>https://arxiv.org/abs/2405.01299</link>
<guid>https://arxiv.org/abs/2405.01299</guid>
<content:encoded><![CDATA[
arXiv:2405.01299v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as powerful support tools across various natural language tasks and a range of application domains. Recent studies focus on exploring their capabilities for data annotation. This paper provides a comparative overview of twelve studies investigating the potential of LLMs in labelling data. While the models demonstrate promising cost and time-saving benefits, there exist considerable limitations, such as representativeness, bias, sensitivity to prompt variations and English language preference. Leveraging insights from these studies, our empirical analysis further examines the alignment between human and GPT-generated opinion distributions across four subjective datasets. In contrast to the studies examining representation, our methodology directly obtains the opinion distribution from GPT. Our analysis thereby supports the minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Interaction Modeling for Trajectory Prediction via Agent Selection and Physical Coefficient</title>
<link>https://arxiv.org/abs/2405.13152</link>
<guid>https://arxiv.org/abs/2405.13152</guid>
<content:encoded><![CDATA[
arXiv:2405.13152v5 Announce Type: replace-cross 
Abstract: A thorough understanding of the interaction between the target agent and surrounding agents is a prerequisite for accurate trajectory prediction. Although many methods have been explored, they assign correlation coefficients to surrounding agents in a purely learning-based manner. In this study, we present ASPILin, which manually selects interacting agents and replaces the attention scores in Transformer with a newly computed physical correlation coefficient, enhancing the interpretability of interaction modeling. Surprisingly, these simple modifications can significantly improve prediction performance and substantially reduce computational costs. We intentionally simplified our model in other aspects, such as map encoding. Remarkably, experiments conducted on the INTERACTION, highD, and CitySim datasets demonstrate that our method is efficient and straightforward, outperforming other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2405.14715</link>
<guid>https://arxiv.org/abs/2405.14715</guid>
<content:encoded><![CDATA[
arXiv:2405.14715v2 Announce Type: replace-cross 
Abstract: Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples. In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings. This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT). Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task. To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model. This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training. Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications. Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Multi-Manifold Transformation Based Multivariate Time Series Fault Detection</title>
<link>https://arxiv.org/abs/2405.16258</link>
<guid>https://arxiv.org/abs/2405.16258</guid>
<content:encoded><![CDATA[
arXiv:2405.16258v2 Announce Type: replace-cross 
Abstract: Unsupervised fault detection in multivariate time series plays a vital role in ensuring the stable operation of complex systems. Traditional methods often assume that normal data follow a single Gaussian distribution and identify anomalies as deviations from this distribution. {\color{black} However, this simplified assumption fails to capture the diversity and structural complexity of real-world time series, which can lead to misjudgments and reduced detection performance in practical applications. To address this issue, we propose a new method that combines a neighborhood-driven data augmentation strategy with a multi-manifold representation learning framework.} By incorporating information from local neighborhoods, the augmentation module can simulate contextual variations of normal data, enhancing the model's adaptability to distributional changes. In addition, we design a structure-aware feature learning approach that encourages natural clustering of similar patterns in the feature space while maintaining sufficient distinction between different operational states. Extensive experiments on several public benchmark datasets demonstrate that our method achieves superior performance in terms of both accuracy and robustness, showing strong potential for generalization and real-world deployment.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Green AI in Action: Strategic Model Selection for Ensembles in Production</title>
<link>https://arxiv.org/abs/2405.17451</link>
<guid>https://arxiv.org/abs/2405.17451</guid>
<content:encoded><![CDATA[
arXiv:2405.17451v2 Announce Type: replace-cross 
Abstract: Integrating Artificial Intelligence (AI) into software systems has significantly enhanced their capabilities while escalating energy demands. Ensemble learning, combining predictions from multiple models to form a single prediction, intensifies this problem due to cumulative energy consumption. This paper presents a novel approach to model selection that addresses the challenge of balancing the accuracy of AI models with their energy consumption in a live AI ensemble system. We explore how reducing the number of models or improving the efficiency of model usage within an ensemble during inference can reduce energy demands without substantially sacrificing accuracy. This study introduces and evaluates two model selection strategies, Static and Dynamic, for optimizing ensemble learning systems performance while minimizing energy usage. Our results demonstrate that the Static strategy improves the F1 score beyond the baseline, reducing average energy usage from 100% from the full ensemble to 62%. The Dynamic strategy further enhances F1 scores, using on average 76% compared to 100% of the full ensemble. Moreover, we propose an approach that balances accuracy with resource consumption, significantly reducing energy usage without substantially impacting accuracy. This method decreased the average energy usage of the Static strategy from approximately 62% to 14%, and for the Dynamic strategy, from around 76% to 57%. Our field study of Green AI using an operational AI system developed by a large professional services provider shows the practical applicability of adopting energy-conscious model selection strategies in live production environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2405.19202</link>
<guid>https://arxiv.org/abs/2405.19202</guid>
<content:encoded><![CDATA[
arXiv:2405.19202v5 Announce Type: replace-cross 
Abstract: Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study investigates the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is mandatory for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composing Parts for Expressive Object Generation</title>
<link>https://arxiv.org/abs/2406.10197</link>
<guid>https://arxiv.org/abs/2406.10197</guid>
<content:encoded><![CDATA[
arXiv:2406.10197v2 Announce Type: replace-cross 
Abstract: Image composition and generation are processes where the artists need control over various parts of the generated images. However, the current state-of-the-art generation models, like Stable Diffusion, cannot handle fine-grained part-level attributes in the text prompts. Specifically, when additional attribute details are added to the base text prompt, these text-to-image models either generate an image vastly different from the image generated from the base prompt or ignore the attribute details. To mitigate these issues, we introduce PartComposer, a training-free method that enables image generation based on fine-grained part-level attributes specified for objects in the base text prompt. This allows more control for artists and enables novel object compositions by combining distinctive object parts. PartComposer first localizes object parts by denoising the object region from a specific diffusion process. This enables each part token to be localized to the right region. After obtaining part masks, we run a localized diffusion process in each part region based on fine-grained part attributes and combine them to produce the final image. All stages of PartComposer are based on repurposing a pre-trained diffusion model, which enables it to generalize across domains. We demonstrate the effectiveness of part-level control provided by PartComposer through qualitative visual examples and quantitative comparisons with contemporary baselines.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document Retrieval</title>
<link>https://arxiv.org/abs/2406.12593</link>
<guid>https://arxiv.org/abs/2406.12593</guid>
<content:encoded><![CDATA[
arXiv:2406.12593v4 Announce Type: replace-cross 
Abstract: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ego-Foresight: Self-supervised Learning of Agent-Aware Representations for Improved RL</title>
<link>https://arxiv.org/abs/2407.01570</link>
<guid>https://arxiv.org/abs/2407.01570</guid>
<content:encoded><![CDATA[
arXiv:2407.01570v2 Announce Type: replace-cross 
Abstract: Despite the significant advancements in Deep Reinforcement Learning (RL) observed in the last decade, the amount of training experience necessary to learn effective policies remains one of the primary concerns both in simulated and real environments. Looking to solve this issue, previous work has shown that improved training efficiency can be achieved by separately modeling agent and environment, but usually requiring a supervisory agent mask.
  In contrast to RL, humans can perfect a new skill from a small number of trials and in most cases do so without a supervisory signal, making neuroscientific studies of human development a valuable source of inspiration for RL. In particular, we explore the idea of motor prediction, which states that humans develop an internal model of themselves and of the consequences that their motor commands have on the immediate sensory inputs. Our insight is that the movement of the agent provides a cue that allows the duality between agent and environment to be learned.
  To instantiate this idea, we present Ego-Foresight, a self-supervised method for disentangling agent and environment based on motion and prediction. Our main finding is self-supervised agent-awareness by visuomotor prediction of the agent improves sample-efficiency and performance of the underlying RL algorithm.
  To test our approach, we first study its ability to visually predict agent movement irrespective of the environment, in simulated and real-world robotic data. Then, we integrate Ego-Foresight with a model-free RL algorithm to solve simulated robotic tasks, showing that self-supervised agent-awareness can improve sample-efficiency and performance in RL.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach</title>
<link>https://arxiv.org/abs/2407.03146</link>
<guid>https://arxiv.org/abs/2407.03146</guid>
<content:encoded><![CDATA[
arXiv:2407.03146v5 Announce Type: replace-cross 
Abstract: Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirSketch: Generative Motion to Sketch</title>
<link>https://arxiv.org/abs/2407.08906</link>
<guid>https://arxiv.org/abs/2407.08906</guid>
<content:encoded><![CDATA[
arXiv:2407.08906v3 Announce Type: replace-cross 
Abstract: Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics of LLM Finetuning</title>
<link>https://arxiv.org/abs/2407.10490</link>
<guid>https://arxiv.org/abs/2407.10490</guid>
<content:encoded><![CDATA[
arXiv:2407.10490v4 Announce Type: replace-cross 
Abstract: Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique "squeezing effect" to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional RAG LLMs: Reading Comprehension for the Open Internet</title>
<link>https://arxiv.org/abs/2408.11189</link>
<guid>https://arxiv.org/abs/2408.11189</guid>
<content:encoded><![CDATA[
arXiv:2408.11189v2 Announce Type: replace-cross 
Abstract: Queries to large language models (LLMs) can be divided into two parts: the instruction/question and the accompanying context. The context for retrieval-augmented generation (RAG) systems in most benchmarks comes from Wikipedia-like texts written in a neutral and factual tone. However, real-world RAG applications often retrieve internet-based text with diverse tones and linguistic styles, posing challenges for downstream tasks. This paper introduces (a) a dataset that transforms RAG-retrieved passages into emotionally inflected and sarcastic text, (b) an emotion translation model for adapting text to different tones, and (c) a prompt-based method to improve LLMs' pragmatic interpretation of retrieved text.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remove Symmetries to Control Model Expressivity and Improve Optimization</title>
<link>https://arxiv.org/abs/2408.15495</link>
<guid>https://arxiv.org/abs/2408.15495</guid>
<content:encoded><![CDATA[
arXiv:2408.15495v4 Announce Type: replace-cross 
Abstract: When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a "collapse". Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We first prove two concrete mechanisms through which symmetries lead to reduced capacities and ignored features during training and inference. We then propose a simple and theoretically justified algorithm, syre, to remove almost all symmetry-induced low-capacity states in neural networks. When this type of entrapment is especially a concern, removing symmetries with the proposed method is shown to correlate well with improved optimization or performance. A remarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge of the symmetry.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners</title>
<link>https://arxiv.org/abs/2409.01524</link>
<guid>https://arxiv.org/abs/2409.01524</guid>
<content:encoded><![CDATA[
arXiv:2409.01524v3 Announce Type: replace-cross 
Abstract: Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical evidence of Large Language Model's influence on human spoken communication</title>
<link>https://arxiv.org/abs/2409.01754</link>
<guid>https://arxiv.org/abs/2409.01754</guid>
<content:encoded><![CDATA[
arXiv:2409.01754v2 Announce Type: replace-cross 
Abstract: From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Diffusion to Transformers: A Unified Framework for Neural Message Passing</title>
<link>https://arxiv.org/abs/2409.09111</link>
<guid>https://arxiv.org/abs/2409.09111</guid>
<content:encoded><![CDATA[
arXiv:2409.09111v3 Announce Type: replace-cross 
Abstract: Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, inspired by physical systems, we propose an energy-constrained diffusion model, which combines the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified mathematical framework for common neural architectures whose computational flows can be cast as message passing (or its special case), including MLPs, GNNs, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers, whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino</title>
<link>https://arxiv.org/abs/2409.15380</link>
<guid>https://arxiv.org/abs/2409.15380</guid>
<content:encoded><![CDATA[
arXiv:2409.15380v4 Announce Type: replace-cross 
Abstract: Multilingual large language models (LLMs) today may not necessarily provide culturally appropriate and relevant responses to its Filipino users. We introduce Kalahi, a cultural LLM evaluation suite collaboratively created by native Filipino speakers. It is composed of 150 high-quality, handcrafted and nuanced prompts that test LLMs for generations that are relevant to shared Filipino cultural knowledge and values. Strong LLM performance in Kalahi indicates a model's ability to generate responses similar to what an average Filipino would say or do in a given situation. We conducted experiments on LLMs with multilingual and Filipino language support. Results show that Kalahi, while trivial for Filipinos, is challenging for LLMs, with the best model answering only 46.0% of the questions correctly compared to native Filipino performance of 89.10%. Thus, Kalahi can be used to accurately and reliably evaluate Filipino cultural representation in LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification</title>
<link>https://arxiv.org/abs/2409.17777</link>
<guid>https://arxiv.org/abs/2409.17777</guid>
<content:encoded><![CDATA[
arXiv:2409.17777v4 Announce Type: replace-cross 
Abstract: Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive Learning approach to capture nuanced shared relations inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with their corresponding samples from other modalities thereby capturing shared relations between them. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research. Our code is publicly available at https://github.com/RaghavSinghal10/M3CoL.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unlearn: Benchmarking Machine Unlearning for Image Classification</title>
<link>https://arxiv.org/abs/2410.01276</link>
<guid>https://arxiv.org/abs/2410.01276</guid>
<content:encoded><![CDATA[
arXiv:2410.01276v2 Announce Type: replace-cross 
Abstract: Machine unlearning (MU) aims to remove the influence of particular data points from the learnable parameters of a trained machine learning model. This is a crucial capability in light of data privacy requirements, trustworthiness, and safety in deployed models. MU is particularly challenging for deep neural networks (DNNs), such as convolutional nets or vision transformers, as such DNNs tend to memorize a notable portion of their training dataset. Nevertheless, the community lacks a rigorous and multifaceted study that looks into the success of MU methods for DNNs. In this paper, we investigate 18 state-of-the-art MU methods across various benchmark datasets and models, with each evaluation conducted over 10 different initializations, a comprehensive evaluation involving MU over 100K models. We show that, with the proper hyperparameters, Masked Small Gradients (MSG) and Convolution Transpose (CT), consistently perform better in terms of model accuracy and run-time efficiency across different models, datasets, and initializations, assessed by population-based membership inference attacks (MIA) and per-sample unlearning likelihood ratio attacks (U-LiRA). Furthermore, our benchmark highlights the fact that comparing a MU method only with commonly used baselines, such as Gradient Ascent (GA) or Successive Random Relabeling (SRL), is inadequate, and we need better baselines like Negative Gradient Plus (NG+) with proper hyperparameter selection.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?</title>
<link>https://arxiv.org/abs/2410.06735</link>
<guid>https://arxiv.org/abs/2410.06735</guid>
<content:encoded><![CDATA[
arXiv:2410.06735v2 Announce Type: replace-cross 
Abstract: Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2410.08174</link>
<guid>https://arxiv.org/abs/2410.08174</guid>
<content:encoded><![CDATA[
arXiv:2410.08174v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce TRON, a two-step framework for risk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. TRON comprises two main components: (1) a novel conformal score to sample response sets of minimum size, and (2) a nonconformity score to identify high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that TRON achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment as You Wish -- Free-Form Language-Based Segmentation for Medical Images</title>
<link>https://arxiv.org/abs/2410.12831</link>
<guid>https://arxiv.org/abs/2410.12831</guid>
<content:encoded><![CDATA[
arXiv:2410.12831v2 Announce Type: replace-cross 
Abstract: Medical imaging is crucial for diagnosing a patient's health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model's superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beware of Calibration Data for Pruning Large Language Models</title>
<link>https://arxiv.org/abs/2410.17711</link>
<guid>https://arxiv.org/abs/2410.17711</guid>
<content:encoded><![CDATA[
arXiv:2410.17711v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Recent research has enhanced post-training pruning from different aspects but few of them systematically explore the effects of calibration data, and it is unclear if there exist better calibration data construction strategies. We fill this blank and surprisingly observe that calibration data is also crucial to post-training pruning, especially for high sparsity. Through controlled experiments on important influence factors of calibration data, including the pruning settings, the amount of data, and its similarity with pre-training data, we observe that a small size of data is adequate, and more similar data to its pre-training stage can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. Experimental results on recent strong open-source LLMs (e.g., DCLM, and LLaMA-3) show that the proposed strategy can enhance the performance of strong pruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to $2.68\%$). Code is available at https://github.com/Dereck0602/calibration_data.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOE-Enhanced Explanable Deep Manifold Transformation for Complex Data Embedding and Visualization</title>
<link>https://arxiv.org/abs/2410.19504</link>
<guid>https://arxiv.org/abs/2410.19504</guid>
<content:encoded><![CDATA[
arXiv:2410.19504v2 Announce Type: replace-cross 
Abstract: Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, achieving both high DR accuracy and strong explainability remains a fundamental challenge, especially for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced explainability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and explainability are critical. To address these challenges, this work introduces the MOE-based Explainable Deep Manifold Transformation (DMT-ME). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-ME enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving explainability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-ME consistently achieves superior performance in both DR accuracy and model explainability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmtme
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics</title>
<link>https://arxiv.org/abs/2411.07186</link>
<guid>https://arxiv.org/abs/2411.07186</guid>
<content:encoded><![CDATA[
arXiv:2411.07186v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) prompted with text and audio have achieved state-of-the-art performance across various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, their potential has yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior -- tasks that are crucial for conservation, biodiversity monitoring, and animal behavior studies. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our training dataset consists of carefully curated text-audio pairs spanning bioacoustics, speech, and music, designed to address the field's limited availability of annotated data. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. We evaluate NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets a new state of the art on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we release our model weights, benchmark data, and open-source the code for training and benchmark data generation and model training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2411.08870</link>
<guid>https://arxiv.org/abs/2411.08870</guid>
<content:encoded><![CDATA[
arXiv:2411.08870v3 Announce Type: replace-cross 
Abstract: Several recent works seek to adapt general-purpose large language models (LLMs) and vision-language models (VLMs) for medical applications through continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining improves performance on various downstream medical tasks, such as answering medical exam questions. In this paper, we compare ten "medical" LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting and supervised fine-tuning regimes for medical question answering (QA). For instance, on clinical-note-based QA tasks in the 3-shot setting, medical LLMs outperform their base models in only 26.7% of cases, reach a (statistical) tie in 16.7% of cases, and perform significantly worse in the remaining 56.7% of cases. Our conclusions are based on (i) comparing each medical model directly against its base model; (ii) optimizing the prompts for each model separately in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Object Detection using Depth and Image Data for Manufacturing Parts</title>
<link>https://arxiv.org/abs/2411.09062</link>
<guid>https://arxiv.org/abs/2411.09062</guid>
<content:encoded><![CDATA[
arXiv:2411.09062v3 Announce Type: replace-cross 
Abstract: Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diffusion Posterior Sampling for Inverse Problems by Integrating Crafted Measurements</title>
<link>https://arxiv.org/abs/2411.09850</link>
<guid>https://arxiv.org/abs/2411.09850</guid>
<content:encoded><![CDATA[
arXiv:2411.09850v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as a powerful foundation model for visual generations. With an appropriate sampling process, it can effectively serve as a generative prior for solving general inverse problems. Current posterior sampling-based methods take the measurement (i.e., degraded image sample) into the posterior sampling to infer the distribution of the target data (i.e., clean image sample). However, in this manner, we show that high-frequency information can be prematurely introduced during the early stages, which could induce larger posterior estimate errors during restoration sampling. To address this observation, we first reveal that forming the log-posterior gradient with the noisy measurement ( i.e., noisy measurement from a diffusion forward process) instead of the clean one can benefit the early posterior sampling. Consequently, we propose a novel diffusion posterior sampling method DPS-CM, which incorporates a Crafted Measurement (i.e., noisy measurement crafted by a reverse denoising process, rather than constructed from the diffusion forward process) to form the posterior estimate. This integration aims to mitigate the misalignment with the diffusion prior caused by cumulative posterior estimate errors. Experimental results demonstrate that our approach significantly improves the overall capacity to solve general and noisy inverse problems, such as Gaussian deblurring, super-resolution, inpainting, nonlinear deblurring, and tasks with Poisson noise, relative to existing approaches. Code is available at: https://github.com/sjz5202/DPS-CM.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on LLMs</title>
<link>https://arxiv.org/abs/2411.13757</link>
<guid>https://arxiv.org/abs/2411.13757</guid>
<content:encoded><![CDATA[
arXiv:2411.13757v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing (NLP), excelling in tasks like text generation and summarization. However, their increasing adoption in mission-critical applications raises concerns about hardware-based threats, particularly bit-flip attacks (BFAs). BFAs, enabled by fault injection methods such as Rowhammer, target model parameters in memory, compromising both integrity and performance. Identifying critical parameters for BFAs in the vast parameter space of LLMs poses significant challenges. While prior research suggests transformer-based architectures are inherently more robust to BFAs compared to traditional deep neural networks, we challenge this assumption. For the first time, we demonstrate that as few as three bit-flips can cause catastrophic performance degradation in an LLM with billions of parameters. Current BFA techniques are inadequate for exploiting this vulnerability due to the difficulty of efficiently identifying critical parameters within the immense parameter space. To address this, we propose AttentionBreaker, a novel framework tailored for LLMs that enables efficient traversal of the parameter space to identify critical parameters. Additionally, we introduce GenBFA, an evolutionary optimization strategy designed to refine the search further, isolating the most critical bits for an efficient and effective attack. Empirical results reveal the profound vulnerability of LLMs to AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of total parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result in a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to 0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings underscore the effectiveness of AttentionBreaker in uncovering and exploiting critical vulnerabilities within LLM architectures.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Wearable Movement Data in Mental Health Research</title>
<link>https://arxiv.org/abs/2411.15240</link>
<guid>https://arxiv.org/abs/2411.15240</guid>
<content:encoded><![CDATA[
arXiv:2411.15240v4 Announce Type: replace-cross 
Abstract: Pretrained foundation models and transformer architectures have driven the success of large language models (LLMs) and other modern AI breakthroughs. However, similar advancements in health data modeling remain limited due to the need for innovative adaptations. Wearable movement data offers a valuable avenue for exploration, as it's a core feature in nearly all commercial smartwatches, well established in clinical and mental health research, and the sequential nature of the data shares similarities to language. We introduce the Pretrained Actigraphy Transformer (PAT), the first open source foundation model designed for time-series wearable movement data. Leveraging transformer-based architectures and novel techniques, such as patch embeddings, and pretraining on data from 29,307 participants in a national U.S. sample, PAT achieves state-of-the-art performance in several mental health prediction tasks. PAT is also lightweight and easily interpretable, making it a robust tool for mental health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call to Action for Research that Matters</title>
<link>https://arxiv.org/abs/2411.16645</link>
<guid>https://arxiv.org/abs/2411.16645</guid>
<content:encoded><![CDATA[
arXiv:2411.16645v2 Announce Type: replace-cross 
Abstract: In the area of recommender systems, the vast majority of research efforts is spent on developing increasingly sophisticated recommendation models, also using increasingly more computational resources. Unfortunately, most of these research efforts target a very small set of application domains, mostly e-commerce and media recommendation. Furthermore, many of these models are never evaluated with users, let alone put into practice. The scientific, economic and societal value of much of these efforts by scholars therefore remains largely unclear. To achieve a stronger positive impact resulting from these efforts, we posit that we as a research community should more often address use cases where recommender systems contribute to societal good (RS4Good). In this opinion piece, we first discuss a number of examples where the use of recommender systems for problems of societal concern has been successfully explored in the literature. We then proceed by outlining a paradigmatic shift that is needed to conduct successful RS4Good research, where the key ingredients are interdisciplinary collaborations and longitudinal evaluation approaches with humans in the loop.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?</title>
<link>https://arxiv.org/abs/2411.18797</link>
<guid>https://arxiv.org/abs/2411.18797</guid>
<content:encoded><![CDATA[
arXiv:2411.18797v2 Announce Type: replace-cross 
Abstract: Recent advancements in LLMs unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model's utility for legitimate knowledge. Despite these strides, sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have remained unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance, we ask:How can unlearning be performed effectively and efficiently on MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to excessive forgetting, uncontrolled knowledge erasure and substantial utility drops when existing unlearning methods are applied. To address this, we propose a novel Selected-Expert Unlearning Framework (SEUF). Through expert attribution, unlearning is concentrated on the most actively engaged experts for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning. SEUF is compatible with various standard unlearning algorithms. Extensive experiments demonstrate that SEUF enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks and LLM architectures (compared to standard unlearning algorithms), while only unlearning 0.06% of the model parameters.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</title>
<link>https://arxiv.org/abs/2412.01064</link>
<guid>https://arxiv.org/abs/2412.01064</guid>
<content:encoded><![CDATA[
arXiv:2412.01064v3 Announce Type: replace-cross 
Abstract: With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZipAR: Parallel Auto-regressive Image Generation through Spatial Locality</title>
<link>https://arxiv.org/abs/2412.04062</link>
<guid>https://arxiv.org/abs/2412.04062</guid>
<content:encoded><![CDATA[
arXiv:2412.04062v3 Announce Type: replace-cross 
Abstract: In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining. Code is available here: https://github.com/ThisisBillhe/ZipAR.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Library for Learning Neural Operators</title>
<link>https://arxiv.org/abs/2412.10354</link>
<guid>https://arxiv.org/abs/2412.10354</guid>
<content:encoded><![CDATA[
arXiv:2412.10354v4 Announce Type: replace-cross 
Abstract: We present NeuralOperator, an open-source Python library for operator learning. Neural operators generalize neural networks to maps between function spaces instead of finite-dimensional Euclidean spaces. They can be trained and inferenced on input and output functions given at various discretizations, satisfying a discretization convergence properties. Built on top of PyTorch, NeuralOperator provides all the tools for training and deploying neural operator models, as well as developing new ones, in a high-quality, tested, open-source package. It combines cutting-edge models and customizability with a gentle learning curve and simple user interface for newcomers.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignGuard: Scalable Safety Alignment for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2412.10493</link>
<guid>https://arxiv.org/abs/2412.10493</guid>
<content:encoded><![CDATA[
arXiv:2412.10493v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) models are widespread, but their limited safety guardrails expose end users to harmful content and potentially allow for model misuse. Current safety measures are typically limited to text-based filtering or concept removal strategies, able to remove just a few concepts from the model's generative capabilities. In this work, we introduce AlignGuard, a method for safety alignment of T2I models. We enable the application of Direct Preference Optimization (DPO) for safety purposes in T2I models by synthetically generating a dataset of harmful and safe image-text pairs, which we call CoProV2. Using a custom DPO strategy and this dataset, we train safety experts, in the form of low-rank adaptation (LoRA) matrices, able to guide the generation process away from specific safety-related concepts. Then, we merge the experts into a single LoRA using a novel merging strategy for optimal scaling performance. This expert-based approach enables scalability, allowing us to remove 7x more harmful concepts from T2I models compared to baselines. AlignGuard consistently outperforms the state-of-the-art on many benchmarks and establishes new practices for safety alignment in T2I networks. Code and data will be shared at https://safetydpo.github.io/.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models</title>
<link>https://arxiv.org/abs/2501.03124</link>
<guid>https://arxiv.org/abs/2501.03124</guid>
<content:encoded><![CDATA[
arXiv:2501.03124v5 Announce Type: replace-cross 
Abstract: Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Serving Large Multimodal Models Using EPD Disaggregation</title>
<link>https://arxiv.org/abs/2501.05460</link>
<guid>https://arxiv.org/abs/2501.05460</guid>
<content:encoded><![CDATA[
arXiv:2501.05460v4 Announce Type: replace-cross 
Abstract: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Seasonal Variations in Energy Forecasting for Student Residences</title>
<link>https://arxiv.org/abs/2501.07423</link>
<guid>https://arxiv.org/abs/2501.07423</guid>
<content:encoded><![CDATA[
arXiv:2501.07423v2 Announce Type: replace-cross 
Abstract: This research provides an in-depth evaluation of various machine learning models for energy forecasting, focusing on the unique challenges of seasonal variations in student residential settings. The study assesses the performance of baseline models, such as LSTM and GRU, alongside state-of-the-art forecasting methods, including Autoregressive Feedforward Neural Networks, Transformers, and hybrid approaches. Special attention is given to predicting energy consumption amidst challenges like seasonal patterns, vacations, meteorological changes, and irregular human activities that cause sudden fluctuations in usage. The findings reveal that no single model consistently outperforms others across all seasons, emphasizing the need for season-specific model selection or tailored designs. Notably, the proposed Hyper Network based LSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal variations, effectively capturing abrupt changes in energy consumption during summer months. This study advances the energy forecasting field by emphasizing the critical role of seasonal dynamics and model-specific behavior in achieving accurate predictions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2501.14406</link>
<guid>https://arxiv.org/abs/2501.14406</guid>
<content:encoded><![CDATA[
arXiv:2501.14406v3 Announce Type: replace-cross 
Abstract: Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2501.14694</link>
<guid>https://arxiv.org/abs/2501.14694</guid>
<content:encoded><![CDATA[
arXiv:2501.14694v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) is an emerging paradigm that exploits supervisory signals generated from the data itself, and many recent studies have leveraged SSL to conduct graph anomaly detection. However, we empirically found that three important factors can substantially impact detection performance across datasets: 1) the specific SSL strategy employed; 2) the tuning of the strategy's hyperparameters; and 3) the allocation of combination weights when using multiple strategies. Most SSL-based graph anomaly detection methods circumvent these issues by arbitrarily or selectively (i.e., guided by label information) choosing SSL strategies, hyperparameter settings, and combination weights. While an arbitrary choice may lead to subpar performance, using label information in an unsupervised setting is label information leakage and leads to severe overestimation of a method's performance. Leakage has been criticized as "one of the top ten data mining mistakes", yet many recent studies on SSL-based graph anomaly detection have been using label information to select hyperparameters. To mitigate this issue, we propose to use an internal evaluation strategy (with theoretical analysis) to select hyperparameters in SSL for unsupervised anomaly detection. We perform extensive experiments using 10 recent SSL-based graph anomaly detection algorithms on various benchmark datasets, demonstrating both the prior issues with hyperparameter selection and the effectiveness of our proposed strategy.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DReSS: Data-driven Regularized Structured Streamlining for Large Language Models</title>
<link>https://arxiv.org/abs/2501.17905</link>
<guid>https://arxiv.org/abs/2501.17905</guid>
<content:encoded><![CDATA[
arXiv:2501.17905v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.00306</link>
<guid>https://arxiv.org/abs/2502.00306</guid>
<content:encoded><![CDATA[
arXiv:2502.00306v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Environment-Driven Online LiDAR-Camera Extrinsic Calibration</title>
<link>https://arxiv.org/abs/2502.00801</link>
<guid>https://arxiv.org/abs/2502.00801</guid>
<content:encoded><![CDATA[
arXiv:2502.00801v2 Announce Type: replace-cross 
Abstract: LiDAR-camera extrinsic calibration (LCEC) is crucial for multi-modal data fusion in mechatronics. Existing methods, whether target-based or target-free, typically rely on customized calibration targets or fixed scene types, limiting their practicality in real-world applications. To address these challenges, we introduce EdO-LCEC, the first environment-driven online calibration approach. Unlike traditional target-free methods, EdO-LCEC observes the feature density of the application environment through a generalizable scene discriminator. Based on this feature density, EdO-LCEC extracts LiDAR intensity and depth features from varying perspectives to achieve higher calibration accuracy. To overcome the challenges of cross-modal feature matching between LiDAR and camera, we propose dual-path correspondence matching (DPCM), which leverages both structural and textural consistency for reliable 3D-2D correspondences. Additionally, our approach models the calibration process as a joint optimization problem utilizing global constraints from multiple views and scenes to enhance accuracy. Extensive experiments on real-world datasets demonstrate that EdO-LCEC outperforms state-of-the-art methods, particularly in sparse or partially overlapping sensor views.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models</title>
<link>https://arxiv.org/abs/2502.03607</link>
<guid>https://arxiv.org/abs/2502.03607</guid>
<content:encoded><![CDATA[
arXiv:2502.03607v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models hold significant potential in robotics, enabling the generation of diverse and smooth trajectories directly from raw representations of the environment. Despite this promise, applying diffusion models to motion planning remains challenging due to their difficulty in enforcing critical constraints, such as collision avoidance and kinematic feasibility. These limitations become even more pronounced in Multi-Robot Motion Planning (MRMP), where multiple robots must coordinate in shared spaces. To address these challenges, this work proposes Simultaneous MRMP Diffusion (SMD), a novel approach integrating constrained optimization into the diffusion sampling process to produce collision-free, kinematically feasible trajectories. Additionally, the paper introduces a comprehensive MRMP benchmark to evaluate trajectory planning algorithms across scenarios with varying robot densities, obstacle complexities, and motion constraints. Experimental results show SMD consistently outperforms classical and other learning-based motion planners, achieving higher success rates and efficiency in complex multi-robot environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time to Rethink AI for Combinatorial Optimization: Classical Algorithms Remain Tough to Match</title>
<link>https://arxiv.org/abs/2502.03669</link>
<guid>https://arxiv.org/abs/2502.03669</guid>
<content:encoded><![CDATA[
arXiv:2502.03669v2 Announce Type: replace-cross 
Abstract: This position paper argues that the machine learning community should fundamentally rethink how AI-inspired methods are developed and evaluated for combinatorial optimization (CO). We present comprehensive empirical benchmarks comparing various recent AI-inspired GPU-based methods with several classical CPU-based solvers on the Maximum Independent Set (MIS) problem. Strikingly, even on in-distribution random graphs, leading AI-inspired methods are consistently outperformed by the state-of-the-art classical solver KaMIS, and some AI-inspired methods frequently fail to surpass even the simplest degree-based greedy heuristic. To better understand the source of these failures, we introduce a novel analysis, serialization, which reveals that non-backtracking AI methods, such as LTFT (based on GFlowNets), end up reasoning similarly to the simplest degree-based greedy heuristic, and thus worse than KaMIS.
  Our findings reveal three core issues: (1) Limited benchmarks and evaluation - AI-inspired methods are often tested only on small instances with very limited inference time, which covers up issues with scalability and resource usage; (2) Intrinsic hardness and learning limits - even under ideal, in-distribution conditions, learning-based approaches lag behind classical heuristics, highlighting inherent barriers that receive little attention; and (3) Insufficient use and understanding of classical heuristics - current learning frameworks often neglect to incorporate effective classical techniques.
  Although we use MIS as a testbed, similar gaps and challenges have been reported in other combinatorial optimization problems, suggesting broader relevance for our recommendations. We propose that future research must address these issues by rigorous benchmarking, deepening understanding of learning limitations, and integrating classical heuristics into AI-inspired methods.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Medical Code Tokenizer</title>
<link>https://arxiv.org/abs/2502.04397</link>
<guid>https://arxiv.org/abs/2502.04397</guid>
<content:encoded><![CDATA[
arXiv:2502.04397v3 Announce Type: replace-cross 
Abstract: Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of Emotion Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05489</link>
<guid>https://arxiv.org/abs/2502.05489</guid>
<content:encoded><![CDATA[
arXiv:2502.05489v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy</title>
<link>https://arxiv.org/abs/2502.05651</link>
<guid>https://arxiv.org/abs/2502.05651</guid>
<content:encoded><![CDATA[
arXiv:2502.05651v2 Announce Type: replace-cross 
Abstract: The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks</title>
<link>https://arxiv.org/abs/2502.05695</link>
<guid>https://arxiv.org/abs/2502.05695</guid>
<content:encoded><![CDATA[
arXiv:2502.05695v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel Semantic Communication (SemCom) framework for real-time adaptive-bitrate video streaming by integrating Latent Diffusion Models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional Constant Bitrate Streaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While retaining B-frames and P-frames as adjustment metadata to support efficient refinement of video reconstruction at the user side, the proposed framework further incorporates state-of-the-art denoising and Video Frame Interpolation (VFI) techniques. These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster and Predict Latent Patches for Improved Masked Image Modeling</title>
<link>https://arxiv.org/abs/2502.08769</link>
<guid>https://arxiv.org/abs/2502.08769</guid>
<content:encoded><![CDATA[
arXiv:2502.08769v3 Announce Type: replace-cross 
Abstract: Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-MIM framework that relies on the prediction of latent clusterings. Our approach leverages a clustering-based loss, which is stable to train, and exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous MIM methods and approaching the performance of the current state-of-the-art, DINOv2. We release all our code and models.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A general language model for peptide identification</title>
<link>https://arxiv.org/abs/2502.15610</link>
<guid>https://arxiv.org/abs/2502.15610</guid>
<content:encoded><![CDATA[
arXiv:2502.15610v3 Announce Type: replace-cross 
Abstract: Accurate identification of bioactive peptides (BPs) and protein post-translational modifications (PTMs) is essential for understanding protein function and advancing therapeutic discovery. However, most computational methods remain limited in their generalizability across diverse peptide functions. Here, we present PDeepPP, a unified deep learning framework that integrates pretrained protein language models with a hybrid transformer-convolutional architecture, enabling robust identification across diverse peptide classes and PTM sites. We curated comprehensive benchmark datasets and implemented strategies to address data imbalance, allowing PDeepPP to systematically extract both global and local sequence features. Through extensive analyses-including dimensionality reduction and comparison studies-PDeepPP demonstrates strong, interpretable peptide representations and achieves state-of-the-art performance in 25 of the 33 biological identification tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and phosphorylation site (0.9984) identification, with 99.5% specificity in glycosylation site prediction and substantial reduction in false negatives in antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP supports biomedical research and the discovery of novel therapeutic targets for disease treatment. All code, datasets, and pretrained models are publicly available via GitHub:https://github.com/fondress/PDeepPP and Hugging Face:https://huggingface.co/fondress/PDeppPP.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2502.15727</link>
<guid>https://arxiv.org/abs/2502.15727</guid>
<content:encoded><![CDATA[
arXiv:2502.15727v2 Announce Type: replace-cross 
Abstract: This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture for network packet seed generation and enrichment. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds' structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings to dynamically reference to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol's Finite State Machine (FSM), then iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. We then evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground-truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models</title>
<link>https://arxiv.org/abs/2502.15799</link>
<guid>https://arxiv.org/abs/2502.15799</guid>
<content:encoded><![CDATA[
arXiv:2502.15799v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are powerful tools for modern applications, but their computational demands limit accessibility. Quantization offers efficiency gains, yet its impact on safety and trustworthiness remains poorly understood. To address this, we introduce OpenMiniSafety, a human-curated safety dataset with 1.067 challenging questions to rigorously evaluate model behavior. We publicly release human safety evaluations for four LLMs (both quantized and full-precision), totaling 4.268 annotated question-answer pairs. By assessing 66 quantized variants of these models using four post-training quantization (PTQ) and two quantization-aware training (QAT) methods across four safety benchmarks including human-centric evaluations we uncover critical safety performance trade-offs. Our results show both PTQ and QAT can degrade safety alignment, with QAT techniques like QLORA or STE performing less safely. No single method consistently outperforms others across benchmarks, precision settings, or models, highlighting the need for safety-aware compression strategies. Furthermore, precision-specialized methods (e.g., QUIK and AWQ for 4-bit, AQLM and Q-PET for 2-bit) excel at their target precision, meaning that these methods are not better at compressing but rather different approaches.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2502.19537</link>
<guid>https://arxiv.org/abs/2502.19537</guid>
<content:encoded><![CDATA[
arXiv:2502.19537v4 Announce Type: replace-cross 
Abstract: Leading language model (LM) providers like OpenAI and Anthropic allow customers to fine-tune frontier LMs for specific use cases. To prevent abuse, these providers apply filters to block fine-tuning on overtly harmful data. In this setting, we make three contributions: First, while past work has shown that safety alignment is "shallow", we correspondingly demonstrate that existing fine-tuning attacks are shallow -- attacks target only the first several tokens of the model response, and consequently can be blocked by generating the first several response tokens with an aligned model. Second, we conceptually illustrate how to make attacks deeper by introducing a new fine-tuning attack that trains models to first refuse harmful requests before answering them; this "refuse-then-comply" strategy bypasses shallow defenses and produces harmful responses that evade output filters. Third, we demonstrate the potency of our new fine-tuning attack by jailbreaking both open-source models equipped with defenses and production models, achieving attack success rates of 57% and 72% against GPT-4o and Claude Haiku, respectively. Our attack received a $2000 bug bounty from OpenAI and was acknowledged as a vulnerability by Anthropic. Our work undermines the notion that models are safe because they initially refuse harmful requests and broadens awareness of the scope of attacks that face production fine-tuning APIs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LNUCB-TA: Linear-nonlinear Hybrid Bandit Learning with Temporal Attention</title>
<link>https://arxiv.org/abs/2503.00387</link>
<guid>https://arxiv.org/abs/2503.00387</guid>
<content:encoded><![CDATA[
arXiv:2503.00387v2 Announce Type: replace-cross 
Abstract: Existing contextual multi-armed bandit (MAB) algorithms fail to effectively capture both long-term trends and local patterns across all arms, leading to suboptimal performance in environments with rapidly changing reward structures. They also rely on static exploration rates, which do not dynamically adjust to changing conditions. To overcome these limitations, we propose LNUCB-TA, a hybrid bandit model integrating a novel nonlinear component (adaptive k-Nearest Neighbors (k-NN)) for reducing time complexity, alongside a global-and-local attention-based exploration mechanism. Our approach uniquely combines linear and nonlinear estimation techniques, with the nonlinear module dynamically adjusting k based on reward variance to enhance spatiotemporal pattern recognition. This reduces the likelihood of selecting suboptimal arms while improving reward estimation accuracy and computational efficiency. The attention-based mechanism ranks arms by past performance and selection frequency, dynamically adjusting exploration and exploitation in real time without requiring manual tuning of exploration rates. By integrating global attention (assessing all arms collectively) and local attention (focusing on individual arms), LNUCB-TA efficiently adapts to temporal and spatial complexities. Empirical results show LNUCB-TA significantly outperforms state-of-the-art linear, nonlinear, and hybrid bandits in cumulative and mean reward, convergence, and robustness across different exploration rates. Theoretical analysis further confirms its reliability with a sub-linear regret bound.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning</title>
<link>https://arxiv.org/abs/2503.00436</link>
<guid>https://arxiv.org/abs/2503.00436</guid>
<content:encoded><![CDATA[
arXiv:2503.00436v2 Announce Type: replace-cross 
Abstract: In the dynamic landscape of artificial intelligence, the exploration of hallucinations within vision-language (VL) models emerges as a critical frontier. This work delves into the intricacies of hallucinatory phenomena exhibited by widely used image captioners, unraveling interesting patterns. Specifically, we step upon previously introduced techniques of conceptual counterfactual explanations to address VL hallucinations. The deterministic and efficient nature of the employed conceptual counterfactuals backbone is able to suggest semantically minimal edits driven by hierarchical knowledge, so that the transition from a hallucinated caption to a non-hallucinated one is performed in a black-box manner. HalCECE, our proposed hallucination detection framework is highly interpretable, by providing semantically meaningful edits apart from standalone numbers, while the hierarchical decomposition of hallucinated concepts leads to a thorough hallucination analysis. Another novelty tied to the current work is the investigation of role hallucinations, being one of the first works to involve interconnections between visual concepts in hallucination detection. Overall, HalCECE recommends an explainable direction to the crucial field of VL hallucination detection, thus fostering trustworthy evaluation of current and future VL systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space-Time Graphs of Convex Sets for Multi-Robot Motion Planning</title>
<link>https://arxiv.org/abs/2503.00583</link>
<guid>https://arxiv.org/abs/2503.00583</guid>
<content:encoded><![CDATA[
arXiv:2503.00583v2 Announce Type: replace-cross 
Abstract: We address the Multi-Robot Motion Planning (MRMP) problem of computing collision-free trajectories for multiple robots in shared continuous environments. While existing frameworks effectively decompose MRMP into single-robot subproblems, spatiotemporal motion planning with dynamic obstacles remains challenging, particularly in cluttered or narrow-corridor settings. We propose Space-Time Graphs of Convex Sets (ST-GCS), a novel planner that systematically covers the collision-free space-time domain with convex sets instead of relying on random sampling. By extending Graphs of Convex Sets (GCS) into the time dimension, ST-GCS formulates time-optimal trajectories in a unified convex optimization that naturally accommodates velocity bounds and flexible arrival times. We also propose Exact Convex Decomposition (ECD) to "reserve" trajectories as spatiotemporal obstacles, maintaining a collision-free space-time graph of convex sets for subsequent planning. Integrated into two prioritized-planning frameworks, ST-GCS consistently achieves higher success rates and better solution quality than state-of-the-art sampling-based planners -- often at orders-of-magnitude faster runtimes -- underscoring its benefits for MRMP in challenging settings.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization</title>
<link>https://arxiv.org/abs/2503.01328</link>
<guid>https://arxiv.org/abs/2503.01328</guid>
<content:encoded><![CDATA[
arXiv:2503.01328v2 Announce Type: replace-cross 
Abstract: Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\% acceleration with even lower memory consumption. The implementation is open-sourced at \href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement</title>
<link>https://arxiv.org/abs/2503.01875</link>
<guid>https://arxiv.org/abs/2503.01875</guid>
<content:encoded><![CDATA[
arXiv:2503.01875v2 Announce Type: replace-cross 
Abstract: Time series data are foundational in finance, healthcare, and energy domains. However, most existing methods and datasets remain focused on a narrow spectrum of tasks, such as forecasting or anomaly detection. To bridge this gap, we introduce Time Series Multi-Task Question Answering (Time-MQA), a unified framework that enables natural language queries across multiple time series tasks - numerical analytical tasks and open-ended question answering with reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset containing $\sim$200k question-answer pairs derived from diverse time series spanning environment, traffic, etc. This comprehensive resource covers various time series lengths and promotes robust model development. We further demonstrate how continually pre-training large language models (Mistral 7B, Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning capabilities, moving beyond mere numeric tasks and enabling more advanced and intuitive interactions with temporal data. The complete TSQA dataset, models, user study questionnaires for evaluation, and other related materials have been open-sourced.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</title>
<link>https://arxiv.org/abs/2503.04257</link>
<guid>https://arxiv.org/abs/2503.04257</guid>
<content:encoded><![CDATA[
arXiv:2503.04257v2 Announce Type: replace-cross 
Abstract: Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects. To address these challenges, we contribute the following: First, we augment the Truebones Zoo dataset, a high-quality animal motion dataset covering over 70 species, by annotating it with detailed text descriptions, making it suitable for text-based motion synthesis. Second, we introduce rig augmentation techniques that generate diverse motion data while preserving consistent dynamics, enabling models to adapt to various skeletal configurations. Finally, we redesign existing motion diffusion models to dynamically adapt to arbitrary skeletal templates, enabling motion synthesis for a diverse range of objects with varying structures. Experiments show that our method learns to generate high-fidelity motions from textual descriptions for diverse and even unseen objects, setting a strong foundation for motion synthesis across diverse object categories and skeletal templates. Qualitative results are available at: $\href{https://t2m4lvo.github.io}{https://t2m4lvo.github.io}$.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enough Coin Flips Can Make LLMs Act Bayesian</title>
<link>https://arxiv.org/abs/2503.04722</link>
<guid>https://arxiv.org/abs/2503.04722</guid>
<content:encoded><![CDATA[
arXiv:2503.04722v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs use ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What can large language models do for sustainable food?</title>
<link>https://arxiv.org/abs/2503.04734</link>
<guid>https://arxiv.org/abs/2503.04734</guid>
<content:encoded><![CDATA[
arXiv:2503.04734v2 Announce Type: replace-cross 
Abstract: Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with combinatorial optimization to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy</title>
<link>https://arxiv.org/abs/2503.07661</link>
<guid>https://arxiv.org/abs/2503.07661</guid>
<content:encoded><![CDATA[
arXiv:2503.07661v2 Announce Type: replace-cross 
Abstract: Model merging is a technique that combines multiple finetuned models into a single model without additional training, allowing a free-rider to cheaply inherit specialized capabilities. This study investigates methodologies to suppress unwanted model merging by free-riders. Existing methods such as model watermarking or fingerprinting can only detect merging in hindsight. In contrast, we propose a first proactive defense against model merging. Specifically, our defense method modifies the model parameters so that the model is disrupted if the model is merged with any other model, while its functionality is kept unchanged if not merged with others. Our approach consists of two modules, rearranging MLP parameters and scaling attention heads, which push the model out of the shared basin in parameter space, causing the merging performance with other models to degrade significantly. We conduct extensive experiments on image classification, image generation, and text classification to demonstrate that our defense severely disrupts merging while retaining the functionality of the post-protect model. Moreover, we analyze potential adaptive attacks and further propose a dropout-based pruning to improve our proposal's robustness.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding</title>
<link>https://arxiv.org/abs/2503.10135</link>
<guid>https://arxiv.org/abs/2503.10135</guid>
<content:encoded><![CDATA[
arXiv:2503.10135v2 Announce Type: replace-cross 
Abstract: Speculative decoding (SPD) aims to accelerate the auto-regressive token generation process of a target Large Language Model (LLM). Some approaches employ a draft model with multiple heads to predict a sequence of future tokens, where each head handles a token in the sequence. The target LLM verifies the predicted sequence and accepts aligned tokens, enabling efficient multi-token generation. However, existing methods assume that all tokens within a sequence are equally important, employing identical head structures and relying on a single-generation paradigm, either serial or parallel. To this end, we theoretically demonstrate that initial tokens in the draft sequence are more important than later ones. Building on this insight, we propose Gumiho, a hybrid model combining serial and parallel heads. Specifically, given the critical importance of early tokens, we employ a sophisticated Transformer architecture for the early draft heads in a serial configuration to improve accuracy. For later tokens, we utilize multiple lightweight MLP heads operating in parallel to enhance efficiency. By allocating more advanced model structures and longer running times to the early heads, Gumiho achieves improved overall performance. The experimental results demonstrate that our method outperforms existing approaches, fully validating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Problem of the Priors, or Posteriors?</title>
<link>https://arxiv.org/abs/2503.10984</link>
<guid>https://arxiv.org/abs/2503.10984</guid>
<content:encoded><![CDATA[
arXiv:2503.10984v3 Announce Type: replace-cross 
Abstract: The problem of the priors is well known: it concerns the challenge of identifying norms that govern one's prior credences. I argue that a key to addressing this problem lies in considering what I call the problem of the posteriors -- the challenge of identifying norms that directly govern one's posterior credences, which backward induce some norms on the priors via the diachronic requirement of conditionalization. This forward-looking approach can be summarized as: Think ahead, work backward. Although this idea can be traced to Freedman (1963), Carnap (1963), and Shimony (1970), I believe that it has not received enough attention. In this paper, I initiate a systematic defense of forward-looking Bayesianism, addressing potential objections from more traditional views (both subjectivist and objectivist). I also develop a specific approach to forward-looking Bayesianism -- one that values the convergence of posterior credences to the truth, and treats it as a fundamental rather than derived norm. This approach, called convergentist Bayesianism, is argued to be crucial for a Bayesian foundation of Ockham's razor in statistics and machine learning.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction</title>
<link>https://arxiv.org/abs/2503.11167</link>
<guid>https://arxiv.org/abs/2503.11167</guid>
<content:encoded><![CDATA[
arXiv:2503.11167v2 Announce Type: replace-cross 
Abstract: Decoding visual stimuli from neural activity is essential for understanding the human brain. While fMRI methods have successfully reconstructed static images, fMRI-to-video reconstruction faces challenges due to the need for capturing spatiotemporal dynamics like motion and scene transitions. Recent approaches have improved semantic and perceptual alignment but struggle to integrate coarse fMRI data with detailed visual features. Inspired by the hierarchical organization of the visual system, we propose NEURONS, a novel framework that decouples learning into four correlated sub-tasks: key object segmentation, concept recognition, scene description, and blurry video reconstruction. This approach simulates the visual cortex's functional specialization, allowing the model to capture diverse video content. In the inference stage, NEURONS generates robust conditioning signals for a pre-trained text-to-video diffusion model to reconstruct the videos. Extensive experiments demonstrate that NEURONS outperforms state-of-the-art baselines, achieving solid improvements in video consistency (26.6%) and semantic-level accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with the visual cortex, highlighting its potential for brain-computer interfaces and clinical applications. Code and model weights are available at: https://github.com/xmed-lab/NEURONS.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning</title>
<link>https://arxiv.org/abs/2503.11655</link>
<guid>https://arxiv.org/abs/2503.11655</guid>
<content:encoded><![CDATA[
arXiv:2503.11655v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39\% F1 score on 5-class sentiment and 99.31\% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing GPU Resilience and Impact on AI/HPC Systems</title>
<link>https://arxiv.org/abs/2503.11901</link>
<guid>https://arxiv.org/abs/2503.11901</guid>
<content:encoded><![CDATA[
arXiv:2503.11901v3 Announce Type: replace-cross 
Abstract: This study characterizes GPU resilience in Delta HPC, a large-scale AI system that consists of 1,056 A100 and H100 GPUs, with over 1,300 petaflops of peak throughput. Delta HPC is operated by the National Center for Supercomputing Applications (NCSA) at the University of Illinois Urbana-Champaign. We used 2.5 years of operational data (11.7 million GPU hours) on GPU errors. Our major findings include: (i) H100 GPU memory resilience is worse than A100 GPU memory, with 3.2x lower per-GPU MTBE for memory errors, (ii) The GPU memory error-recovery mechanisms on H100 GPUs are insufficient to handle the increased memory capacity, (iii) H100 GPUs demonstrate significantly improved GPU hardware resilience over A100 GPUs with respect to critical hardware components, (iv) GPU errors on both A100 and H100 GPUs frequently result in job failures due to the lack of robust recovery mechanisms at the application level, and (v) We project the impact of GPU node availability on larger-scales and find that significant overprovisioning of 5% is necessary to handle GPU failures.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI</title>
<link>https://arxiv.org/abs/2503.11950</link>
<guid>https://arxiv.org/abs/2503.11950</guid>
<content:encoded><![CDATA[
arXiv:2503.11950v3 Announce Type: replace-cross 
Abstract: The increasing integration of Artificial Intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups, digital citizens (ages 16-19), parents/educators, and AI professionals, and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from 482 participants through structured surveys, qualitative interviews, and focus groups. The findings reveal distinct privacy expectations: Young users emphasize autonomy and digital freedom, while parents and educators advocate for regulatory oversight and AI literacy programs. AI professionals, in contrast, prioritize the balance between ethical system design and technological efficiency. The data further highlights gaps in AI literacy and transparency, emphasizing the need for comprehensive, stakeholder-driven privacy frameworks that accommodate diverse user needs. Using comparative thematic analysis, this study identifies key tensions in privacy governance and develops the novel Privacy-Ethics Alignment in AI (PEA-AI) model, which structures privacy decision-making as a dynamic negotiation between stakeholders. By systematically analyzing themes such as transparency, user control, risk perception, and parental mediation, this research provides a scalable, adaptive foundation for AI governance, ensuring that privacy protections evolve alongside emerging AI technologies and youth-centric digital interactions.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding</title>
<link>https://arxiv.org/abs/2503.13377</link>
<guid>https://arxiv.org/abs/2503.13377</guid>
<content:encoded><![CDATA[
arXiv:2503.13377v3 Announce Type: replace-cross 
Abstract: Temporal Video Grounding (TVG), the task of locating specific video segments based on language queries, is a core challenge in long-form video understanding. While recent Large Vision-Language Models (LVLMs) have shown early promise in tackling TVG through supervised fine-tuning (SFT), their abilities to generalize remain limited. To address this, we propose a novel post-training framework that enhances the generalization capabilities of LVLMs via reinforcement learning (RL). Specifically, our contributions span three key directions: (1) Time-R1: we introduce a reasoning-guided post-training framework via RL with verifiable reward to enhance the capabilities of LVLMs on the TVG task. (2) TimeRFT: we explore data-efficient post-training strategies on our curated RL-friendly dataset, which trains the model to progressively comprehend difficult samples, leading to better generalization. (3) TVGBench: we carefully construct a small yet comprehensive benchmark for LVLM evaluation, assessing 11 types of queries and featuring balanced distributions across both videos and queries. Extensive experiments demonstrate that Time-R1 achieves state-of-the-art performance across multiple downstream datasets using only 2.5K training data, while improving its general video understanding capabilities.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Position Prompt for MLLM based Visual Grounding</title>
<link>https://arxiv.org/abs/2503.15426</link>
<guid>https://arxiv.org/abs/2503.15426</guid>
<content:encoded><![CDATA[
arXiv:2503.15426v3 Announce Type: replace-cross 
Abstract: Although Multimodal Large Language Models (MLLMs) excel at various image-related tasks, they encounter challenges in precisely aligning coordinates with spatial information within images, particularly in position-aware tasks such as visual grounding. This limitation arises from two key factors. First, MLLMs lack explicit spatial references, making it difficult to associate textual descriptions with precise image locations. Second, their feature extraction processes prioritize global context over fine-grained spatial details, leading to weak localization capability. To address these issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt (VPP) to improve its grounding capability. VPP-LLaVA integrates two complementary mechanisms: the global VPP overlays a learnable, axis-like tensor onto the input image to provide structured spatial cues, while the local VPP incorporates position-aware queries to support fine-grained localization.To effectively train our model with spatial guidance, we further introduce VPP-SFT, a curated dataset of 0.6M high-quality visual grounding samples. Designed in a compact format, it enables efficient training and is significantly smaller than datasets used by other MLLMs (e.g., ~21M samples in MiniGPT-v2), yet still provides a strong performance boost. The resulting model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual grounding benchmarks but also demonstrates strong zero-shot generalization to challenging unseen datasets. Code and dataset will be released upon acceptance at https://github.com/WayneTomas/VPP-LLaVA.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism</title>
<link>https://arxiv.org/abs/2503.15758</link>
<guid>https://arxiv.org/abs/2503.15758</guid>
<content:encoded><![CDATA[
arXiv:2503.15758v2 Announce Type: replace-cross 
Abstract: Transformer-based models have emerged as a leading architecture for natural language processing, natural language generation, and image generation tasks. A fundamental element of the transformer architecture is self-attention, which allows the model to capture intricate dependencies within the data. However, the self-attention mechanism also incurs significant computational and memory costs, particularly for long sequences.
  In this paper, we introduce ATTENTION2D, a novel approach that exploits parallelism along two dimensions - query and key/value - of the self-attention operation. This method enables efficient distribution and parallelization of computations across multiple devices. Our approach facilitates asymptotically faster training and inference phases compared to previous methods, without relying on approximations or incurring additional computational or memory overheads. Furthermore, unlike existing techniques that struggle to scale with an increasing number of processing units, our approach effectively scales with additional processing units.
  Our experimental results confirm the effectiveness of our method in improving communication efficiency and scalability. Compared to Ring Attention, our approach demonstrated up to a 5x performance boost on a GPT-3-like model using 64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64 NVIDIA H100 GPUs across 64 nodes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Adjudication of Cardiovascular Events Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.17222</link>
<guid>https://arxiv.org/abs/2503.17222</guid>
<content:encoded><![CDATA[
arXiv:2503.17222v2 Announce Type: replace-cross 
Abstract: Cardiovascular events, such as heart attacks and strokes, remain a leading cause of mortality globally, necessitating meticulous monitoring and adjudication in clinical trials. This process, traditionally performed manually by clinical experts, is time-consuming, resource-intensive, and prone to inter-reviewer variability, potentially introducing bias and hindering trial progress. This study addresses these critical limitations by presenting a novel framework for automating the adjudication of cardiovascular events in clinical trials using Large Language Models (LLMs). We developed a two-stage approach: first, employing an LLM-based pipeline for event information extraction from unstructured clinical data and second, using an LLM-based adjudication process guided by a Tree of Thoughts approach and clinical endpoint committee (CEC) guidelines. Using cardiovascular event-specific clinical trial data, the framework achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel, automated metric specifically designed for evaluating the quality of AI-generated clinical reasoning in adjudicating cardiovascular events. This approach demonstrates significant potential for substantially reducing adjudication time and costs while maintaining high-quality, consistent, and auditable outcomes in clinical trials. The reduced variability and enhanced standardization also allow for faster identification and mitigation of risks associated with cardiovascular therapies.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2503.19564</link>
<guid>https://arxiv.org/abs/2503.19564</guid>
<content:encoded><![CDATA[
arXiv:2503.19564v2 Announce Type: replace-cross 
Abstract: As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards</title>
<link>https://arxiv.org/abs/2503.19948</link>
<guid>https://arxiv.org/abs/2503.19948</guid>
<content:encoded><![CDATA[
arXiv:2503.19948v2 Announce Type: replace-cross 
Abstract: Can Visual Language Models (VLMs) effectively capture human visual preferences? This work addresses this question by training VLMs to think about preferences at test time, employing reinforcement learning methods inspired by DeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human Preference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the ImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2 (trained on approximately 25% of its data). These results match traditional encoder-based models while providing transparent reasoning and enhanced generalization. This approach allows to use not only rich VLM world knowledge, but also its potential to think, yielding interpretable outcomes that help decision-making processes. By demonstrating that human visual preferences reasonable by current VLMs, we introduce efficient soft-reward strategies for image ranking, outperforming simplistic selection or scoring methods. This reasoning capability enables VLMs to rank arbitrary images-regardless of aspect ratio or complexity-thereby potentially amplifying the effectiveness of visual Preference Optimization. By reducing the need for extensive markup while improving reward generalization and explainability, our findings can be a strong mile-stone that will enhance text-to-vision models even further.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models</title>
<link>https://arxiv.org/abs/2503.22968</link>
<guid>https://arxiv.org/abs/2503.22968</guid>
<content:encoded><![CDATA[
arXiv:2503.22968v3 Announce Type: replace-cross 
Abstract: Recent advancements in Korean large language models (LLMs) have driven numerous benchmarks and evaluation methods, yet inconsistent protocols cause up to 10 p.p performance gaps across institutions. Overcoming these reproducibility gaps does not mean enforcing a one-size-fits-all evaluation. Rather, effective benchmarking requires diverse experimental approaches and a framework robust enough to support them. To this end, we introduce HRET (Haerae Evaluation Toolkit), an open-source, registry-based framework that unifies Korean LLM assessment. HRET integrates major Korean benchmarks, multiple inference backends, and multi-method evaluation, with language consistency enforcement to ensure genuine Korean outputs. Its modular registry design also enables rapid incorporation of new datasets, methods, and backends, ensuring the toolkit adapts to evolving research needs. Beyond standard accuracy metrics, HRET incorporates Korean-focused output analyses-morphology-aware Type-Token Ratio (TTR) for evaluating lexical diversity and systematic keyword-omission detection for identifying missing concepts-to provide diagnostic insights into language-specific behaviors. These targeted analyses help researchers pinpoint morphological and semantic shortcomings in model outputs, guiding focused improvements in Korean LLM development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Consequentialist Critique of Binary Classification Evaluation Practices</title>
<link>https://arxiv.org/abs/2504.04528</link>
<guid>https://arxiv.org/abs/2504.04528</guid>
<content:encoded><![CDATA[
arXiv:2504.04528v2 Announce Type: replace-cross 
Abstract: ML-supported decisions, such as ordering tests or determining preventive custody, often involve binary classification based on probabilistic forecasts. Evaluation frameworks for such forecasts typically consider whether to prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics (e.g., Precision@K), and whether to focus on fixed thresholds or threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist perspective, long advocated by decision theorists, should naturally favor evaluations that support independent decisions using a mixture of thresholds given their prevalence, such as Brier scores and Log loss. However, our empirical analysis reveals a strong preference for top-K metrics or fixed thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To address this gap, we use this decision-theoretic framework to map evaluation metrics to their optimal use cases, along with a Python package, briertools, to promote the broader adoption of Brier scores. In doing so, we also uncover new theoretical connections, including a reconciliation between the Brier Score and Decision Curve Analysis, which clarifies and responds to a longstanding critique by (Assel, et al. 2017) regarding the clinical utility of proper scoring rules.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving binary weight and activation for LLMs using Post-Training Quantization</title>
<link>https://arxiv.org/abs/2504.05352</link>
<guid>https://arxiv.org/abs/2504.05352</guid>
<content:encoded><![CDATA[
arXiv:2504.05352v3 Announce Type: replace-cross 
Abstract: Quantizing large language models (LLMs) to 1-bit precision significantly reduces computational costs, but existing quantization techniques suffer from noticeable performance degradation when using weight and activation precisions below 4 bits (W4A4). In this paper, we propose a post-training quantization framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit with an additional 1 bit for fine-grain grouping and activations are quantized to 1 bit with a 4-fold increase in the number of channels. For weight quantization, we propose utilizing Hessian-aware fine-grained grouping along with an EM-based quantization scheme. For activation quantization, we decompose INT4-quantized activations into a 4 * INT1 format equivalently and simultaneously smooth the scaling factors based on quantization errors, which further reduces the quantization errors in activations. Our method surpasses state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple tasks, pushing the boundaries of existing LLM quantization methods toward fully binarized models. Code is available at https://github.com/JimmyCrave/LLM-PTQ-binarization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization</title>
<link>https://arxiv.org/abs/2504.09039</link>
<guid>https://arxiv.org/abs/2504.09039</guid>
<content:encoded><![CDATA[
arXiv:2504.09039v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
arXiv:2504.12563v2 Announce Type: replace-cross 
Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SConU: Selective Conformal Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14154</link>
<guid>https://arxiv.org/abs/2504.14154</guid>
<content:encoded><![CDATA[
arXiv:2504.14154v2 Announce Type: replace-cross 
Abstract: As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models</title>
<link>https://arxiv.org/abs/2504.17677</link>
<guid>https://arxiv.org/abs/2504.17677</guid>
<content:encoded><![CDATA[
arXiv:2504.17677v2 Announce Type: replace-cross 
Abstract: The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. Based on interviews with teaching staff, this paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix</title>
<link>https://arxiv.org/abs/2505.08228</link>
<guid>https://arxiv.org/abs/2505.08228</guid>
<content:encoded><![CDATA[
arXiv:2505.08228v2 Announce Type: replace-cross 
Abstract: Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.
  The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Super-Resolution Generative Adversarial Networks based Video Enhancement</title>
<link>https://arxiv.org/abs/2505.10589</link>
<guid>https://arxiv.org/abs/2505.10589</guid>
<content:encoded><![CDATA[
arXiv:2505.10589v4 Announce Type: replace-cross 
Abstract: This study introduces an enhanced approach to video super-resolution by extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution Generative Adversarial Network (SRGAN) structure to handle spatio-temporal data. While SRGAN has proven effective for single-image enhancement, its design does not account for the temporal continuity required in video processing. To address this, a modified framework that incorporates 3D Non-Local Blocks is proposed, which is enabling the model to capture relationships across both spatial and temporal dimensions. An experimental training pipeline is developed, based on patch-wise learning and advanced data degradation techniques, to simulate real-world video conditions and learn from both local and global structures and details. This helps the model generalize better and maintain stability across varying video content while maintaining the general structure besides the pixel-wise correctness. Two model variants-one larger and one more lightweight-are presented to explore the trade-offs between performance and efficiency. The results demonstrate improved temporal coherence, sharper textures, and fewer visual artifacts compared to traditional single-image methods. This work contributes to the development of practical, learning-based solutions for video enhancement tasks, with potential applications in streaming, gaming, and digital restoration.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks</title>
<link>https://arxiv.org/abs/2505.12884</link>
<guid>https://arxiv.org/abs/2505.12884</guid>
<content:encoded><![CDATA[
arXiv:2505.12884v2 Announce Type: replace-cross 
Abstract: Lightweight Vision-Language Models (VLMs) are indispensable for resource-constrained applications. The prevailing approach to aligning vision and language models involves freezing both the vision encoder and the language model while training small connector modules. However, this strategy heavily depends on the intrinsic capabilities of the language model, which can be suboptimal for lightweight models with limited representational capacity. In this work, we investigate this alignment bottleneck through the lens of mutual information, demonstrating that the constrained capacity of the language model inherently limits the Effective Mutual Information (EMI) between multimodal inputs and outputs, thereby compromising alignment quality. To address this challenge, we propose TinyAlign, a novel framework inspired by Retrieval-Augmented Generation, which strategically retrieves relevant context from a memory bank to enrich multimodal inputs and enhance their alignment. Extensive empirical evaluations reveal that TinyAlign significantly reduces training loss, accelerates convergence, and enhances task performance. Remarkably, it allows models to achieve baseline-level performance with only 40\% of the fine-tuning data, highlighting exceptional data efficiency. Our work thus offers a practical pathway for developing more capable lightweight VLMs while introducing a fresh theoretical lens to better understand and address alignment bottlenecks in constrained multimodal systems.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks</title>
<link>https://arxiv.org/abs/2505.20137</link>
<guid>https://arxiv.org/abs/2505.20137</guid>
<content:encoded><![CDATA[
arXiv:2505.20137v2 Announce Type: replace-cross 
Abstract: Predictive Coding (PC) offers a biologically plausible alternative to backpropagation for neural network training, yet struggles with deeper architectures. This paper identifies the root cause: an inherent signal decay problem where gradients attenuate exponentially with depth, becoming computationally negligible due to numerical precision constraints. To address this fundamental limitation, we introduce Error Optimization (EO), a novel reparameterization that preserves PC's theoretical properties while eliminating signal decay. By optimizing over prediction errors rather than states, EO enables signals to reach all layers simultaneously and without attenuation, converging orders of magnitude faster than standard PC. Experiments across multiple architectures and datasets demonstrate that EO matches backpropagation's performance even for deeper models where conventional PC struggles. Besides practical improvements, our work provides theoretical insight into PC dynamics and establishes a foundation for scaling biologically-inspired learning to deeper architectures on digital hardware and beyond.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data</title>
<link>https://arxiv.org/abs/2505.20166</link>
<guid>https://arxiv.org/abs/2505.20166</guid>
<content:encoded><![CDATA[
arXiv:2505.20166v2 Announce Type: replace-cross 
Abstract: Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks. However, this adaptation process presents two major limitations. First, ALLMs often suffer from catastrophic forgetting, where crucial textual capabilities like instruction-following are lost after training on audio data. In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about reliability. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making it resource-intensive. To address these issues, previous works have leveraged the backbone LLMs to synthesize general-purpose, caption-style alignment data. In this paper, we propose a data generation framework that produces contrastive-like training data, designed to enhance ALLMs' ability to differentiate between present and absent sounds. We further extend our approach to multi-audio scenarios, enabling the model to either explain differences between audio inputs or produce unified captions that describe all inputs, thereby enhancing audio-language alignment. We refer to the entire ALLM training framework as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa). Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance on audio understanding and reasoning benchmarks, as well as instruction-following skills. Moreover, incorporating multi-audio training further enhances the model's comprehension and reasoning capabilities. Overall, BALSa offers an efficient and scalable approach to developing ALLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherEdit: Controllable Weather Editing with 4D Gaussian Field</title>
<link>https://arxiv.org/abs/2505.20471</link>
<guid>https://arxiv.org/abs/2505.20471</guid>
<content:encoded><![CDATA[
arXiv:2505.20471v2 Announce Type: replace-cross 
Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: https://jumponthemoon.github.io/w-edit
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech</title>
<link>https://arxiv.org/abs/2505.20868</link>
<guid>https://arxiv.org/abs/2505.20868</guid>
<content:encoded><![CDATA[
arXiv:2505.20868v2 Announce Type: replace-cross 
Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse methods based on style embedding extracted from reference speech. However, synthesizing high-quality expressive speech remains challenging. We propose Spotlight-TTS, which exclusively emphasizes style via voiced-aware style extraction and style direction adjustment. Voiced-aware style extraction focuses on voiced regions highly related to style while maintaining continuity across different speech regions to improve expressiveness. We adjust the direction of the extracted style for optimal integration into the TTS model, which improves speech quality. Experimental results demonstrate that Spotlight-TTS achieves superior performance compared to baseline models in terms of expressiveness, overall speech quality, and style transfer capability. Our audio samples are publicly available.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems</title>
<link>https://arxiv.org/abs/2505.22814</link>
<guid>https://arxiv.org/abs/2505.22814</guid>
<content:encoded><![CDATA[
arXiv:2505.22814v2 Announce Type: replace-cross 
Abstract: Manufacturing environments are becoming more complex and unpredictable due to factors such as demand variations and shorter product lifespans. This complexity requires real-time decision-making and adaptation to disruptions. Traditional control approaches highlight the need for advanced control strategies capable of overcoming unforeseen challenges, as they demonstrate limitations in responsiveness within dynamic industrial settings. Multi-agent systems address these challenges through decentralization of decision-making, enabling systems to respond dynamically to operational changes. However, current multi-agent systems encounter challenges related to real-time adaptation, context-aware decision-making, and the dynamic exploration of resource capabilities. Large language models provide the possibility to overcome these limitations through context-aware decision-making capabilities. This paper introduces a large language model-enabled control architecture for multi-agent manufacturing systems to dynamically explore resource capabilities in response to real-time disruptions. A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility. The case study findings show improved throughput and efficient resource utilization compared to existing approaches.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.23331</link>
<guid>https://arxiv.org/abs/2505.23331</guid>
<content:encoded><![CDATA[
arXiv:2505.23331v2 Announce Type: replace-cross 
Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum computing and artificial intelligence: status and perspectives</title>
<link>https://arxiv.org/abs/2505.23860</link>
<guid>https://arxiv.org/abs/2505.23860</guid>
<content:encoded><![CDATA[
arXiv:2505.23860v3 Announce Type: replace-cross 
Abstract: This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison</title>
<link>https://arxiv.org/abs/2506.00924</link>
<guid>https://arxiv.org/abs/2506.00924</guid>
<content:encoded><![CDATA[
arXiv:2506.00924v2 Announce Type: replace-cross 
Abstract: This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we construct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi Layered Autonomy and AI Ecologies in Robotic Art Installations</title>
<link>https://arxiv.org/abs/2506.02606</link>
<guid>https://arxiv.org/abs/2506.02606</guid>
<content:encoded><![CDATA[
arXiv:2506.02606v3 Announce Type: replace-cross 
Abstract: This paper presents Symbiosis of Agents, is a large-scale installation by Baoyang Chen (baoyangchen.com), that embeds AI-driven robots in an immersive, mirror-lined arena, probing the tension between machine agency and artistic authorship. Drawing on early cybernetics, rule-based conceptual art, and seminal robotic works, it orchestrates fluid exchanges among robotic arms, quadruped machines, their environment, and the public. A three tier faith system pilots the ecology: micro-level adaptive tactics, meso-level narrative drives, and a macro-level prime directive. This hierarchy lets behaviors evolve organically in response to environmental cues and even a viewer's breath, turning spectators into co-authors of the unfolding drama. Framed by a speculative terraforming scenario that recalls the historical exploitation of marginalized labor, the piece asks who bears responsibility in AI-mediated futures. Choreographed motion, AI-generated scripts, reactive lighting, and drifting fog cast the robots as collaborators rather than tools, forging a living, emergent artwork. Exhibited internationally, Symbiosis of Agents shows how cybernetic feedback, robotic experimentation, and conceptual rule-making can converge to redefine agency, authorship, and ethics in contemporary art.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code</title>
<link>https://arxiv.org/abs/2506.07239</link>
<guid>https://arxiv.org/abs/2506.07239</guid>
<content:encoded><![CDATA[
arXiv:2506.07239v2 Announce Type: replace-cross 
Abstract: Modern chip design is complex, and there is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code (a commonly used programming language for hardware design). It is especially important yet complex to predict individual lines of code that cause timing violations or downstream routing congestion. Prior works have tried approaches like converting Verilog into an intermediate graph representation and using LLM embeddings alongside other features to predict module-level quality, but did not consider line-level quality prediction. We propose VeriLoC, the first method that predicts design quality directly from Verilog at both the line- and module-level. To this end, VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, and train downstream classifiers/regressors on concatenations of these embeddings. VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14% - 18% for SOTA methods down to only 4%. We believe that VeriLoC embeddings and insights from our work will also be of value for other predictive and optimization tasks for complex hardware design.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed</title>
<link>https://arxiv.org/abs/2506.09034</link>
<guid>https://arxiv.org/abs/2506.09034</guid>
<content:encoded><![CDATA[
arXiv:2506.09034v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
<link>https://arxiv.org/abs/2506.09397</link>
<guid>https://arxiv.org/abs/2506.09397</guid>
<content:encoded><![CDATA[
arXiv:2506.09397v3 Announce Type: replace-cross 
Abstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting</title>
<link>https://arxiv.org/abs/2506.09428</link>
<guid>https://arxiv.org/abs/2506.09428</guid>
<content:encoded><![CDATA[
arXiv:2506.09428v2 Announce Type: replace-cross 
Abstract: Supervised Fine-Tuning (SFT) is a critical step for enhancing the instruction-following capabilities of Large Language Models (LLMs) and adapting them to specialized domains. However, SFT often leads to a degradation of the model's general abilities, a phenomenon known as catastrophic forgetting. This problem is exacerbated when third-party practitioners fine-tune open-source models, as the original SFT data is typically not available. To address this challenge, we propose a novel and cost-effective SFT method that effectively mitigates catastrophic forgetting without requiring access to the original SFT data. Our approach first reconstructs the likely instruction distribution of the base model. It then employs a multi-model generation and filtering pipeline to synthesize a high-quality general-purpose dataset. This synthetic dataset is mixed with new, domain-specific data for fine-tuning. Experimental results show that our method not only preserves the model's capabilities in general domains but also improves task-specific performance, outperforming baselines that use publicly available SFT datasets.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment</title>
<link>https://arxiv.org/abs/2506.10186</link>
<guid>https://arxiv.org/abs/2506.10186</guid>
<content:encoded><![CDATA[
arXiv:2506.10186v2 Announce Type: replace-cross 
Abstract: Equivariant diffusion models have achieved impressive performance in 3D molecule generation. These models incorporate Euclidean symmetries of 3D molecules by utilizing an SE(3)-equivariant denoising network. However, specialized equivariant architectures limit the scalability and efficiency of diffusion models. In this paper, we propose an approach that relaxes such equivariance constraints. Specifically, our approach learns a sample-dependent SO(3) transformation for each molecule to construct an aligned latent space. A non-equivariant diffusion model is then trained over the aligned representations. Experimental results demonstrate that our approach performs significantly better than previously reported non-equivariant models. It yields sample quality comparable to state-of-the-art equivariant diffusion models and offers improved training and sampling efficiency. Our code is available at https://github.com/skeletondyh/RADM
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepProof: Step-by-step verification of natural language mathematical proofs</title>
<link>https://arxiv.org/abs/2506.10558</link>
<guid>https://arxiv.org/abs/2506.10558</guid>
<content:encoded><![CDATA[
arXiv:2506.10558v2 Announce Type: replace-cross 
Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal verification of mathematical proofs down to the axiom level. However, their lack of a natural language interface remains a significant limitation. Recent advancements in large language models (LLMs) have enhanced the understanding of natural language inputs, paving the way for autoformalization - the process of translating natural language proofs into formal proofs that can be verified. Despite these advancements, existing autoformalization approaches are limited to verifying complete proofs and lack the capability for finer, sentence-level verification. To address this gap, we propose StepProof, a novel autoformalization method designed for granular, step-by-step verification. StepProof breaks down complete proofs into multiple verifiable subproofs, enabling sentence-level verification. Experimental results demonstrate that StepProof significantly improves proof success rates and efficiency compared to traditional methods. Additionally, we found that minor manual adjustments to the natural language proofs, tailoring them for step-level verification, further enhanced StepProof's performance in autoformalization.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following</title>
<link>https://arxiv.org/abs/2506.12285</link>
<guid>https://arxiv.org/abs/2506.12285</guid>
<content:encoded><![CDATA[
arXiv:2506.12285v2 Announce Type: replace-cross 
Abstract: Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>