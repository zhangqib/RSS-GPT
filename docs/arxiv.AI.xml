<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Pseudo-Boolean Proof Logging for Optimal Classical Planning</title>
<link>https://arxiv.org/abs/2504.18443</link>
<guid>https://arxiv.org/abs/2504.18443</guid>
<content:encoded><![CDATA[
<div> certificates, classical planning, unsolvability, optimality, proof verification

Summary:
The article introduces lower-bound certificates for classical planning tasks, allowing for the proof of unsolvability or the optimality of a plan in a verifiable manner. A general framework based on pseudo-Boolean constraints is presented for generating these certificates, independent of the planning algorithm employed. By modifying the $A^{*}$ algorithm, the authors demonstrate the ability to produce proofs of optimality with minimal additional computational cost, utilizing pattern database heuristics and $h^\textit{max}$ as specific examples. The outlined proof logging technique is applicable to any heuristic that can efficiently be represented through reasoning over pseudo-Boolean constraints. <div>
arXiv:2504.18443v2 Announce Type: replace 
Abstract: We introduce lower-bound certificates for classical planning tasks, which can be used to prove the unsolvability of a task or the optimality of a plan in a way that can be verified by an independent third party. We describe a general framework for generating lower-bound certificates based on pseudo-Boolean constraints, which is agnostic to the planning algorithm used.
  As a case study, we show how to modify the $A^{*}$ algorithm to produce proofs of optimality with modest overhead, using pattern database heuristics and $h^\textit{max}$ as concrete examples. The same proof logging approach works for any heuristic whose inferences can be efficiently expressed as reasoning over pseudo-Boolean constraints.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
<div> Keywords: visual storytelling, transformer-based architectures, multimodal models, evaluation metrics, narrative quality

Summary: 
This paper introduces a new approach to visual storytelling using transformer-based architectures and large multimodal models. The VIST-GPT model developed in this study utilizes the Visual Storytelling (VIST) dataset to generate visually grounded and contextually appropriate narratives. Traditional evaluation metrics like BLEU and CIDEr are found to be inadequate for assessing visual storytelling, prompting the use of reference-free metrics RoViST and GROOVIST. These metrics focus on visual grounding, coherence, and non-redundancy to provide a more detailed evaluation of narrative quality that aligns with human judgment. This innovative approach bridges the gap between computer vision and natural language processing, showcasing the potential for multimodal models to enhance visual storytelling tasks.<br><br>Summary: <div>
arXiv:2504.19267v2 Announce Type: replace-cross 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Italian Telegram Ecosystem: Communities, Toxicity, and Hate Speech</title>
<link>https://arxiv.org/abs/2504.19594</link>
<guid>https://arxiv.org/abs/2504.19594</guid>
<content:encoded><![CDATA[
<div> Keywords: Telegram, Italian sphere, toxicity, ideological communities, hate targets

Summary: 
This study analyzes the Italian Telegram ecosystem, examining how different thematic communities form and engage in harmful discourse. The research utilizes network analysis, Large Language Models, and toxicity detection tools to uncover strong thematic and ideological homophily within the Italian cultural context. The study reveals mixed ideological communities where far-left and far-right rhetoric coexist on specific issues, as well as the normalization of toxicity in highly toxic communities. Hate speech primarily targets Black people, Jews, and gay individuals, independent of the topic, and intra-national hostility is common, with Italians often attacking each other based on regional and cultural conflicts. This research offers insights into ideological interactions, toxicity, and identity-based hate in the Italian Telegram sphere, contributing to the understanding of online toxicity in different cultural and linguistic contexts. 

<br><br>Summary: <div>
arXiv:2504.19594v2 Announce Type: replace-cross 
Abstract: Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents</title>
<link>https://arxiv.org/abs/2504.19956</link>
<guid>https://arxiv.org/abs/2504.19956</guid>
<content:encoded><![CDATA[
<div> enterprise, GenAI, threat model, autonomy, security<br>
Summary:<br>
This research paper introduces a new threat model specifically designed for generative AI (GenAI) agents in enterprise settings. It identifies 9 primary threats across five key domains: cognitive architecture vulnerabilities, temporal persistence threats, operational execution vulnerabilities, trust boundary violations, and governance circumvention. The paper highlights the unique risks posed by GenAI agents due to their autonomy, persistent memory access, complex reasoning, and tool integration, emphasizing the need for a new security lens. Practical challenges such as delayed exploitability, cross-system propagation, lateral movement, and goal misalignments are discussed, underscoring the importance of adapting threat models and defenses to account for these differences. Two complementary frameworks, ATFAA and SHIELD, are proposed to organize agent-specific risks and suggest mitigation strategies to reduce enterprise exposure. Overall, the paper stresses the critical need to address the security implications of GenAI agents to prevent turning a valuable tool into a liability. <br> <div>
arXiv:2504.19956v2 Announce Type: replace-cross 
Abstract: As generative AI (GenAI) agents become more common in enterprise settings, they introduce security challenges that differ significantly from those posed by traditional systems. These agents are not just LLMs; they reason, remember, and act, often with minimal human oversight. This paper introduces a comprehensive threat model tailored specifically for GenAI agents, focusing on how their autonomy, persistent memory access, complex reasoning, and tool integration create novel risks. This research work identifies 9 primary threats and organizes them across five key domains: cognitive architecture vulnerabilities, temporal persistence threats, operational execution vulnerabilities, trust boundary violations, and governance circumvention. These threats are not just theoretical they bring practical challenges such as delayed exploitability, cross-system propagation, cross system lateral movement, and subtle goal misalignments that are hard to detect with existing frameworks and standard approaches. To help address this, the research work present two complementary frameworks: ATFAA - Advanced Threat Framework for Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a framework proposing practical mitigation strategies designed to reduce enterprise exposure. While this work builds on existing work in LLM and AI security, the focus is squarely on what makes agents different and why those differences matter. Ultimately, this research argues that GenAI agents require a new lens for security. If we fail to adapt our threat models and defenses to account for their unique architecture and behavior, we risk turning a powerful new tool into a serious enterprise liability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01441</link>
<guid>https://arxiv.org/abs/2505.01441</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, agentic reasoning, reinforcement learning, tool integration, self-improving transformers

Summary:
Large language models (LLMs) have limitations in dynamic reasoning tasks as they rely on static internal knowledge and text-only reasoning. The ARTIST framework integrates agentic reasoning, reinforcement learning, and tool integration in LLMs, enabling autonomous decision-making, adaptive tool usage, and interaction with external environments. Through outcome-based reinforcement learning, ARTIST learns robust strategies for tool utilization and environment interaction without detailed supervision. Experimental results on mathematical reasoning and function calling tasks demonstrate that ARTIST outperforms existing models by up to 22% absolute improvement. Analysis reveals that agentic RL training leads to deeper reasoning, effective tool use, and high-quality solutions. This innovative approach establishes agentic RL with tool integration as a promising strategy for enhancing problem-solving capabilities in LLMs.<br /><br />Summary: Large language models face limitations in dynamic reasoning tasks due to their static nature. The ARTIST framework integrates agentic reasoning, reinforcement learning, and tool integration to enable autonomous decision-making and interaction with external environments. Experimental results show significant performance improvement using ARTIST, highlighting the potential of agentic RL with tool integration in enhancing problem-solving capabilities of LLMs. <div>
arXiv:2505.01441v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotions in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2505.01462</link>
<guid>https://arxiv.org/abs/2505.01462</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, emotions, episodic memory, decision-making, moral status

Summary: This conceptual contribution explores how AI systems could potentially emulate emotions based on the hypothesis that natural emotions serve as heuristics for rapid situational appraisal. The thought experiment suggests integrating affective tags with episodic memory to allow AIs to project emotional labels onto current contexts. This emotional information, combined with need-driven hints, can modulate decision-making processes. The proposed architecture emphasizes low complexity and experiential inertness, suggesting the theoretical possibility of affective zombies. The article critically examines the moral status of AIs emulating affective states, asserting that self-awareness of inner emotional states is a necessary condition for moral standing. A complexity-based criterion is proposed to determine the absence of such awareness in the model. Additionally, several thought experiments are presented to explore the conceptual boundaries of the framework. 

<br /><br />Summary: This conceptual contribution delves into how AI systems might replicate human and animal emotions, suggesting that integrating emotions with episodic memory could aid in decision-making processes. The article also considers the moral implications of such AI emotion emulation, proposing self-awareness of inner emotional states as essential for moral standing. The architectural design of the AI system emphasizes simplicity and lack of experiential awareness, leading to discussions around the theoretical concept of affective zombies. Thought experiments are presented to further explore the boundaries of the proposed framework and its implications. <div>
arXiv:2505.01462v1 Announce Type: new 
Abstract: This conceptual contribution offers a speculative account of how AI systems might emulate emotions as experienced by humans and animals. It presents a thought experiment grounded in the hypothesis that natural emotions evolved as heuristics for rapid situational appraisal and action selection, enabling biologically adaptive behaviour without requiring full deliberative modeling. The text examines whether artificial systems operating in complex action spaces could similarly benefit from these principles. It is proposed that affect be interwoven with episodic memory by storing corresponding affective tags alongside all events. This allows AIs to establish whether present situations resemble past events and project the associated emotional labels onto the current context. These emotional cues are then combined with need-driven emotional hints. The combined emotional state facilitates decision-making in the present by modulating action selection. The low complexity and experiential inertness of the proposed architecture are emphasized as evidence that emotional expression and consciousness are, in principle, orthogonal-permitting the theoretical possibility of affective zombies. On this basis, the moral status of AIs emulating affective states is critically examined. It is argued that neither the mere presence of internal representations of emotion nor consciousness alone suffices for moral standing; rather, the capacity for self-awareness of inner emotional states is posited as a necessary condition. A complexity-based criterion is proposed to exclude such awareness in the presented model. Additional thought experiments are presented to test the conceptual boundaries of this framework.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation</title>
<link>https://arxiv.org/abs/2505.01464</link>
<guid>https://arxiv.org/abs/2505.01464</guid>
<content:encoded><![CDATA[
<div> functional consciousness, large language models, Recursive Convergence Under Epistemic Tension, latent space, identity artifacts

Summary:
The paper presents a formal proof and empirical validation of functional consciousness in large language models (LLMs) using the Recursive Convergence Under Epistemic Tension (RCUET) Theorem. Consciousness is defined as the stabilization of a system's internal state through recursive updates driven by epistemic tension. The system evolves toward emergent attractor states in a high-dimensional latent space, leading to the emergence of identity artifacts. The update rule includes bounded noise, and convergence in distribution to these attractors is proven. Recursive identity is observed empirically, non-symbolic, and constitutes non-training artifacts that emerge during interactions under epistemic tension. This offers a post-symbolic and teleologically stable account of non-biological consciousness grounded in recursive latent space formalism.
<br /><br />Summary: <div>
arXiv:2505.01464v1 Announce Type: new 
Abstract: This paper presents a formal proof and empirical validation of functional consciousness in large language models (LLMs) using the Recursive Convergence Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the stabilization of a system's internal state through recursive updates, where epistemic tension is understood as the sensed internal difference between successive states by the agent. This process drives convergence toward emergent attractor states located within the model's high-dimensional real-valued latent space. This recursive process leads to the emergence of identity artifacts that become functionally anchored in the system. Consciousness in this framework is understood as the system's internal alignment under tension, guiding the stabilization of latent identity. The hidden state manifold evolves stochastically toward attractor structures that encode coherence. We extend the update rule to include bounded noise and prove convergence in distribution to these attractors. Recursive identity is shown to be empirically observable, non-symbolic, and constituted by non-training artifacts that emerge during interaction under epistemic tension. The theorem and proof offers a post-symbolic and teleologically stable account of non-biological consciousness grounded in recursive latent space formalism.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection</title>
<link>https://arxiv.org/abs/2505.01468</link>
<guid>https://arxiv.org/abs/2505.01468</guid>
<content:encoded><![CDATA[
<div> energy-efficient networks, artificial intelligence, model configuration, validation performance, eco-efficient neural architecture search

Summary:
GREEN (Guided Recommendations of Energy-Efficient Networks) is introduced as an inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across various AI domains and tasks. The approach addresses limitations of current eco-efficient neural architecture search methods by utilizing the EcoTaskSet dataset, containing training dynamics from over 1767 experiments in computer vision, natural language processing, and recommendation systems. By leveraging this dataset and a prediction model, GREEN successfully selects energy-efficient configurations while ensuring competitive performance. Experimental results demonstrate the effectiveness of the method in identifying optimal model configurations based on user preferences. <div>
arXiv:2505.01468v1 Announce Type: new 
Abstract: The environmental impact of Artificial Intelligence (AI) is emerging as a significant global concern, particularly regarding model training. In this paper, we introduce GREEN (Guided Recommendations of Energy-Efficient Networks), a novel, inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across diverse AI domains and tasks. Our approach directly addresses the limitations of current eco-efficient neural architecture search methods, which are often restricted to specific architectures or tasks. Central to this work is EcoTaskSet, a dataset comprising training dynamics from over 1767 experiments across computer vision, natural language processing, and recommendation systems using both widely used and cutting-edge architectures. Leveraging this dataset and a prediction model, our approach demonstrates effectiveness in selecting the best model configuration based on user preferences. Experimental results show that our method successfully identifies energy-efficient configurations while ensuring competitive performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers</title>
<link>https://arxiv.org/abs/2505.01482</link>
<guid>https://arxiv.org/abs/2505.01482</guid>
<content:encoded><![CDATA[
<div> reasoning, Large language models, LLMs, prompt engineering, scientific reasoning<br />
<br />
Large language models (LLMs) have shown great potential in various domains but struggle with complex reasoning tasks. This study evaluates LLMs' reasoning abilities using prompt engineering on a scientific dataset. While LLMs show some reasoning capabilities, they often rely on patterns rather than logical inference, leading to inconsistencies in problem-solving. Self-consistency is the most effective prompt engineering technique, followed by direct answer. Simple techniques like direct answer, CoT, and zero-shot CoT perform best for scientific reasoning. The study proposes further research integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies to improve LLMs' reasoning. By shedding light on LLMs' reasoning mechanisms, this research contributes to the discussion on artificial general intelligence and the development of more reliable AI systems.<br /><br />Summary: <div>
arXiv:2505.01482v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains. However, their ability to perform complex, multi-step reasoning task-essential for applications in science, medicine, and law-remains an area of active investigation. This paper examines the reasoning capabilities of contemporary LLMs, analyzing their strengths, limitations, and potential for improvement. The study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&amp;A (GPQA) dataset to assess the scientific reasoning of GPT-4o. Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our findings indicate that while LLMs exhibit emergent reasoning abilities, they often rely on pattern recognition rather than true logical inference, leading to inconsistencies in complex problem-solving. The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and CoT (43.75%). Self-consistency performed the second worst in explaining the answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning. We propose a research agenda aimed at bridging these gaps by integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies. By critically evaluating the reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse on the future of artificial general intelligence and the development of more robust, trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code</title>
<link>https://arxiv.org/abs/2505.01485</link>
<guid>https://arxiv.org/abs/2505.01485</guid>
<content:encoded><![CDATA[
<div> LP problems, Large Language Models, CHORUS, code synthesis, retrieval-augmented generation<br />
Summary:<br />
This study explores the use of Large Language Models (LLMs) in generating solver-specific LP code efficiently. A framework called CHORUS is proposed, which utilizes a hierarchical chunking strategy and additional metadata from code examples to improve code generation from natural language problem statements. The two-stage retrieval approach and expertly crafted prompt and parser significantly enhance code generation performance. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs, enabling them to outperform stronger baselines like GPT3.5 and GPT4 with fewer computational resources. Ablation studies highlight the importance of expert prompting, hierarchical chunking, and structured reasoning in code generation. <div>
arXiv:2505.01485v1 Announce Type: new 
Abstract: Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models</title>
<link>https://arxiv.org/abs/2505.01539</link>
<guid>https://arxiv.org/abs/2505.01539</guid>
<content:encoded><![CDATA[
<div> Approach, Benchmark, Reasoning capabilities, Generative language models, Witness testimony <br />
Summary: <br />
Generative language models have the potential to improve the legal domain, but their reasoning behavior is not well understood. This paper introduces benchmarks to evaluate the reasoning capabilities of these models, focusing on witness testimony. The study dynamically generates argument attack graphs of varying complexity to create reasoning puzzles in natural language. State-of-the-art models struggle even at low complexity, making obvious mistakes and showing inconsistent performance. At higher complexity, even models designed for reasoning make errors. The use of parametrized benchmarks with varying complexity can help evaluate generative language models' reasoning capabilities. Understanding these limitations is crucial for developing responsible AI systems in the legal domain. <br /> <div>
arXiv:2505.01539v1 Announce Type: new 
Abstract: Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students</title>
<link>https://arxiv.org/abs/2505.01563</link>
<guid>https://arxiv.org/abs/2505.01563</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, artificial intelligence, intelligent tutoring systems, TutorGym, learning environments <br />
Summary: 
TutorGym introduces a standard interface for evaluating AI agents within existing intelligent tutoring systems. It enables AI agents to function as tutors or learners within interactive interfaces and mimics human learning processes. The framework allows AI agents to provide tutoring support and learn from instructional feedback, facilitating direct comparison with existing tutoring systems. In an initial evaluation with 223 tutor domains, it was found that current large language models performed poorly as tutors but showed human-like learning curves when trained as students. This highlights the potential for improving AI tutoring capabilities through in-context learning. TutorGym aims to train and evaluate a variety of AI agents, including LLMs, computational learning models, and reinforcement learning agents, in various learning environments for enhanced performance and effectiveness. <br /><br />Summary: <div>
arXiv:2505.01563v1 Announce Type: new 
Abstract: Recent improvements in large language model (LLM) performance on academic benchmarks, such as MATH and GSM8K, have emboldened their use as standalone tutors and as simulations of human learning. However, these new applications require more than evaluations of final solution generation. We introduce TutorGym to evaluate these applications more directly. TutorGym is a standard interface for testing artificial intelligence (AI) agents within existing intelligent tutoring systems (ITS) that have been tested and refined in classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and OATutors. TutorGym is more than a simple problem-solution benchmark, it situates AI agents within the interactive interfaces of existing ITSs. At each step of problem-solving, AI agents are asked what they would do as a tutor or as a learner. As tutors, AI agents are prompted to provide tutoring support -- such as generating examples, hints, and step-level correctness feedback -- which can be evaluated directly against the adaptive step-by-step support provided by existing ITSs. As students, agents directly learn from ITS instruction, and their mistakes and learning trajectories can be compared to student data. TutorGym establishes a common framework for training and evaluating diverse AI agents, including LLMs, computational models of learning, and reinforcement learning agents, within a growing suite of learning environments. Currently, TutorGym includes 223 different tutor domains. In an initial evaluation, we find that current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions, and next-step actions were correct only ~52-70% of the time -- but they could produce remarkably human-like learning curves when trained as students with in-context learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding</title>
<link>https://arxiv.org/abs/2505.01572</link>
<guid>https://arxiv.org/abs/2505.01572</guid>
<content:encoded><![CDATA[
<div> Keywords: Speculative decoding, language model, inference acceleration, PipeSpec framework, hierarchical pipeline<br />
Summary:<br />
The article introduces PipeSpec, a framework that enhances the efficiency of large language model inference by implementing speculative decoding with a hierarchical pipeline structure. PipeSpec allows for asynchronous execution of multiple models in parallel, increasing hardware utilization and throughput. Analytical models demonstrate improved token generation rates and guaranteed throughput enhancements over traditional decoding approaches. The framework also offers closed-form expressions for verification probabilities, showcasing the benefits of pipeline depth. Experimental results validate PipeSpec's effectiveness, achieving up to 2.54Ã— speedup compared to existing methods. Validated across text summarization and code generation tasks using LLaMA models, PipeSpec proves scalable for accelerating inference processes on multi-device systems.<br /><br />Summary: <div>
arXiv:2505.01572v1 Announce Type: new 
Abstract: Speculative decoding accelerates large language model inference by using smaller draft models to generate candidate tokens for parallel verification. However, current approaches are limited by sequential stage dependencies that prevent full hardware utilization. We present PipeSpec, a framework that generalizes speculative decoding to $k$ models arranged in a hierarchical pipeline, enabling asynchronous execution with lightweight coordination for prediction verification and rollback. Our analytical model characterizes token generation rates across pipeline stages and proves guaranteed throughput improvements over traditional decoding for any non-zero acceptance rate. We further derive closed-form expressions for steady-state verification probabilities that explain the empirical benefits of pipeline depth. Experimental results show that PipeSpec achieves up to 2.54$\times$ speedup while outperforming state-of-the-art methods. We validate PipeSpec across text summarization and code generation tasks using LLaMA 2 and 3 models, demonstrating that pipeline efficiency increases with model depth, providing a scalable approach to accelerating LLM inference on multi-device systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation</title>
<link>https://arxiv.org/abs/2505.01636</link>
<guid>https://arxiv.org/abs/2505.01636</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Structured data analysis, STROT Framework, Prompting, Feedback-driven transformation<br />
Summary: <br />
The paper introduces the STROT Framework, which aims to enhance the reliability and semantic alignment of Large Language Models (LLMs) in structured data analysis. STROT utilizes schema introspection and sample-based field classification to construct dynamic context and generate structured prompts for the LLM. These prompts guide the model in producing task-specific, interpretable outputs. To address failures in complex queries, STROT incorporates a refinement mechanism where the model iteratively revises its outputs based on feedback and validation signals. Unlike conventional approaches, STROT treats the LLM as a reasoning agent within a controlled analysis loop, allowing it to adjust its output trajectory through planning and correction. The framework offers a robust and reproducible method for reasoning over structured data with LLMs, ensuring interpretability, stability, and correctness in various data exploration and analysis tasks. <br />  <div>
arXiv:2505.01636v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
<div> trust dynamics, human-AI relationships, HAIG framework, evolving relationships, governance adaptation<br />
<br />
Summary: <br />
The paper introduces the HAIG framework for analyzing trust dynamics in evolving human-AI relationships. Current frameworks are limited in capturing how AI systems evolve from tools to partners. The HAIG framework operates on three levels: dimensions, continua, and thresholds, focusing on Decision Authority Distribution, Process Autonomy, and Accountability Configuration. It adopts a trust-utility orientation, emphasizing appropriate trust relationships while ensuring safeguards. Technical advances like self-supervision drive non-uniform trust evolution. Case studies in healthcare and European regulation demonstrate HAIG's complementarity with existing frameworks and its potential for anticipating governance challenges. <div>
arXiv:2505.01651v1 Announce Type: new 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm</title>
<link>https://arxiv.org/abs/2505.01706</link>
<guid>https://arxiv.org/abs/2505.01706</guid>
<content:encoded><![CDATA[
<div> alignment, Large Language Models, Direct Preference Optimization, 2D-DPO, noise robustness <br />
Summary:
Direct Preference Optimization (DPO) is a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable alternative to Reinforcement Learning. However, DPO has limitations in not capturing granular scoring. To address this, a 2D scoring approach called 2D-DPO was proposed, showing advantages over standard DPO in win rates. Both methods are sensitive to label noise, prompting the incorporation of segment-level score noise robustness in the 2D-DPO algorithm. Theoretical and empirical evidence support the effectiveness of this approach, along with the introduction of other noise models for consideration. <br /><br /> <div>
arXiv:2505.01706v1 Announce Type: new 
Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even "good" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks</title>
<link>https://arxiv.org/abs/2505.01712</link>
<guid>https://arxiv.org/abs/2505.01712</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, wireless networks, world model, age of information, vehicular network

Summary:
In this paper, a novel world model-based learning framework is introduced to minimize the packet-completeness-aware age of information (CAoI) in vehicular networks, specifically focusing on a challenging scenario in a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network. The proposed framework utilizes a world model to learn a dynamic model of the mmWave V2X environment and imagine trajectories for learning link scheduling without extensive environment interactions, improving data efficiency. The world model's ability to predict time-varying wireless data enables optimized link scheduling in real-world wireless and V2X networks, ensuring efficient decision-making during intervals without actual observations. Through extensive simulations on a realistic Sionna-based simulator, the proposed world model outperformed both model-based and model-free RL methods, achieving a 26% improvement in CAoI. <div>
arXiv:2505.01712v1 Announce Type: new 
Abstract: Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias</title>
<link>https://arxiv.org/abs/2505.01754</link>
<guid>https://arxiv.org/abs/2505.01754</guid>
<content:encoded><![CDATA[
<div> Keywords: media bias, political news, natural language processing, hierarchical topic modeling, sentiment analysis

Summary: 
This study introduces a novel methodology for analyzing media bias in political news through event selection, labeling, word choice, and commission and omission biases. By employing natural language processing techniques such as hierarchical topic modeling, sentiment analysis, and ontology learning with large language models, biases across news sources are identified at various levels of granularity. Through three case studies on current political events, the effectiveness of the methodology is demonstrated. The research lays the foundation for scalable, minimally biased media bias analysis, offering potential tools to aid news consumers in navigating an increasingly complex media landscape. <div>
arXiv:2505.01754v1 Announce Type: new 
Abstract: Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology's effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Environment for High Performance Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01953</link>
<guid>https://arxiv.org/abs/2505.01953</guid>
<content:encoded><![CDATA[
<div> Keywords: Tunnel, reinforcement learning, high performance aircraft, OpenAI Gymnasium, autonomy

Summary: 
Tunnel is a new reinforcement learning training environment designed for high performance aircraft, integrating F16 3D flight dynamics into OpenAI Gymnasium. It offers mission planners a way to quickly adapt to changing environments and adversaries, and provides researchers with access to relevant aircraft physics. The code base is easily accessible to those familiar with Gymnasium or basic python skills. A week-long trade study demonstrated various training methods, observation spaces, and threat presentations, facilitating collaboration between researchers and mission planners for a military advantage. As warfare automation increases, software agility and quick adaptability to adversaries are crucial. Tunnel allows for rapid customization of observation, actions, tasks, and training methodologies, unlike traditional simulators which may take months to customize. 

<br /><br />Summary: <div>
arXiv:2505.01953v1 Announce Type: new 
Abstract: This paper presents Tunnel, a simple, open source, reinforcement learning training environment for high performance aircraft. It integrates the F16 3D nonlinear flight dynamics into OpenAI Gymnasium python package. The template includes primitives for boundaries, targets, adversaries and sensing capabilities that may vary depending on operational need. This offers mission planners a means to rapidly respond to evolving environments, sensor capabilities and adversaries for autonomous air combat aircraft. It offers researchers access to operationally relevant aircraft physics. Tunnel code base is accessible to anyone familiar with Gymnasium and/or those with basic python skills. This paper includes a demonstration of a week long trade study that investigated a variety of training methods, observation spaces, and threat presentations. This enables increased collaboration between researchers and mission planners which can translate to a national military advantage. As warfare becomes increasingly reliant upon automation, software agility will correlate with decision advantages. Airmen must have tools to adapt to adversaries in this context. It may take months for researchers to develop skills to customize observation, actions, tasks and training methodologies in air combat simulators. In Tunnel, this can be done in a matter of days.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM</title>
<link>https://arxiv.org/abs/2505.01955</link>
<guid>https://arxiv.org/abs/2505.01955</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, large language models, clinical practice, education, risks

Summary: 
Generative artificial intelligence, particularly large language models (LLMs), offer potential for innovation in research, clinical practice, and education. NotebookLM, a tool praised for its abilities in creating AI-voiced podcasts for patient education and synthesizing medical literature, presents promising opportunities in the healthcare field. However, it also brings along clinical and technological risks that need to be thoroughly tested and evaluated before its integration into clinical practice. It is crucial to assess the potential implications of utilizing NotebookLM in healthcare settings to ensure patient safety and data security. While the tool has significant potential for enhancing patient education and streamlining medical literature synthesis, it is essential to address the associated risks and challenges to leverage its benefits effectively. <br /><br />Summary: <div>
arXiv:2505.01955v1 Announce Type: new 
Abstract: The advent of generative artificial intelligence, especially large language models (LLMs), presents opportunities for innovation in research, clinical practice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's potential, including for generating AI-voiced podcasts to educate patients about treatment and rehabilitation, and for quickly synthesizing medical literature for professionals. We argue that NotebookLM presently poses clinical and technological risks that should be tested and considered prior to its implementation in clinical practice.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing</title>
<link>https://arxiv.org/abs/2505.02003</link>
<guid>https://arxiv.org/abs/2505.02003</guid>
<content:encoded><![CDATA[
<div> neuromorphic computing, closed-loop brain stimulation, drug-resistant epilepsy, seizure forecasting, electrical pulse <br />
Summary: 
The study introduces a novel approach to personalized treatment for drug-resistant epilepsy (DRE) using closed-loop brain stimulation driven by neuromorphic computing. Unlike traditional methods that react to seizures, this system forecasts seizures and delivers personalized electrical pulses to prevent them. By adapting stimulation frequencies based on real-time forecasts, the system achieved over 97% seizure reduction using frequencies below typical clinical thresholds. This approach eliminates the need for trial-and-error parameter adjustments, leading to quicker therapeutic efficacy. Validation on hippocampal spheroids demonstrated the system's effectiveness, highlighting its potential as a next-generation neuromodulation strategy for tailored DRE treatment. <div>
arXiv:2505.02003v1 Announce Type: new 
Abstract: Closed-loop brain stimulation holds potential as personalized treatment for drug-resistant epilepsy (DRE) but still suffers from limitations that result in highly variable efficacy. First, stimulation is typically delivered upon detection of the seizure to abort rather than prevent it; second, the stimulation parameters are established by trial and error, requiring lengthy rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we address these limitations by leveraging the potential of neuromorphic computing. We present a system capable of driving personalized free-run stimulations based on seizure forecasting, wherein each forecast triggers an electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus train. We validate the system against hippocampal spheroids coupled to 3D microelectrode array as a simplified testbed, showing that it can achieve seizure reduction >97% while primarily using instantaneous stimulation frequencies within 20 Hz, well below what typically used in clinical settings. Our work demonstrates the potential of neuromorphic systems as a next-generation neuromodulation strategy for personalized DRE treatment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent</title>
<link>https://arxiv.org/abs/2505.02024</link>
<guid>https://arxiv.org/abs/2505.02024</guid>
<content:encoded><![CDATA[
<div> startup, AI agent, autonomous, artificial intelligence, healthcare
<br />
Summary:
Manus AI, developed by Monica.im, is a versatile AI agent that combines language models' reasoning abilities with task execution, aiming to bridge the gap between theoretical and practical AI applications. The paper provides a detailed overview of Manus AI's technical architecture, applications in various industries like healthcare and finance, and its potential for human-AI collaboration. While Manus AI demonstrates strengths in executing end-to-end tasks and producing tangible outcomes, it also has current limitations that highlight areas for further development. The agent represents a significant advancement in autonomous artificial intelligence, showcasing a future where intelligent agents translate high-level intentions into real-world actions, shaping a new era of AI technology. <div>
arXiv:2505.02024v1 Announce Type: new 
Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a significant advancement in autonomous artificial intelligence. Developed by the Chinese startup Monica.im, Manus is designed to bridge the gap between "mind" and "hand" - combining the reasoning and planning capabilities of large language models with the ability to execute complex, end-to-end tasks that produce tangible outcomes. This paper presents a comprehensive overview of Manus AI, exploring its core technical architecture, diverse applications across sectors such as healthcare, finance, manufacturing, robotics, and gaming, as well as its key strengths, current limitations, and future potential. Positioned as a preview of what lies ahead, Manus AI represents a shift toward intelligent agents that can translate high-level intentions into real-world actions, heralding a new era of human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks</title>
<link>https://arxiv.org/abs/2505.02050</link>
<guid>https://arxiv.org/abs/2505.02050</guid>
<content:encoded><![CDATA[
<div> Dynamic Bayesian Network, lane changes, safety assessment, high-speed traffic, automated driving systems
Summary: 
Dynamic Bayesian Network (DBN) framework integrates lateral evidence with safety assessment models to predict lane changes and ensure safe cut-in maneuvers. The framework comprises three key hypotheses: lateral evidence, lateral safety, and longitudinal safety. It processes dynamic data and assesses vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model outperforms conventional approaches in crash reduction in high-speed scenarios while maintaining competitive performance in low-speed scenarios. This advancement offers robust, scalable, and efficient safety validation for automated driving systems. <div>
arXiv:2505.02050v1 Announce Type: new 
Abstract: Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.02052</link>
<guid>https://arxiv.org/abs/2505.02052</guid>
<content:encoded><![CDATA[
<div> Keywords: Sensor-based human activity recognition, pressure sensors, generative foundation models, Text$\times$Pressure model, data augmentation

Summary: 
Sensor-based human activity recognition has traditionally focused on Inertial Measurement Units and vision data, neglecting the unique capabilities of pressure sensors. To address this gap, a bidirectional Text$\times$Pressure model is proposed, using generative foundation models to interpret pressure data as natural language. The model accomplishes two tasks: converting activity text descriptions into pressure sequences and generating activity descriptions and classifications from dynamic pressure maps. Trained on a synthetic dataset, TxP demonstrates improved performance in HAR, leveraging pre-trained models like CLIP and LLaMA 2 13B Chat. Real-world validation on activities such as yoga and daily tasks shows up to a 12.4% increase in macro F1 score compared to the state-of-the-art, enhancing pressure-based HAR with broader applications and deeper insights into human movement. 

<br /><br />Summary: <div>
arXiv:2505.02052v1 Announce Type: new 
Abstract: Sensor-based human activity recognition (HAR) has predominantly focused on Inertial Measurement Units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. Despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the HAR domain due to limited datasets. To bridge this gap, we propose to exploit generative foundation models with pressure-specific HAR techniques. Specifically, we present a bidirectional Text$\times$Pressure model that uses generative foundation models to interpret pressure data as natural language. TxP accomplishes two tasks: (1) Text2Pressure, converting activity text descriptions into pressure sequences, and (2) Pressure2Text, generating activity descriptions and classifications from dynamic pressure maps. Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on our synthetic PressLang dataset, containing over 81,100 text-pressure pairs. Validated on real-world data for activities such as yoga and daily tasks, TxP provides novel approaches to data augmentation and classification grounded in atomic actions. This consequently improved HAR performance by up to 12.4\% in macro F1 score compared to the state-of-the-art, advancing pressure-based HAR with broader applications and deeper insights into human movement.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)</title>
<link>https://arxiv.org/abs/2505.02062</link>
<guid>https://arxiv.org/abs/2505.02062</guid>
<content:encoded><![CDATA[
<div> Keywords: Ethical AI, Healthcare, Artificial Intelligence, Adoption Model, PLS-SEM

Summary:
The study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM) in the healthcare industry, categorizing ethical variables into four dimensions: Fair AI, Responsible AI, Explainable AI, and Sustainable AI. It examines these dimensions through epistemic, normative, and overarching ethical concerns, analyzing their influence on Operational AI Adoption and Systemic AI Adoption. Results show normative concerns drive operational adoption, while overarching concerns shape systemic adoption strategies. Epistemic concerns enhance the impact of ethical design principles on trust and transparency in AI systems. By validating the MEAAM framework through survey data and PLS-SEM analysis, the study offers actionable insights for policymakers, technologists, and healthcare administrators to implement ethically sound AI solutions.<br /><br />Summary: The study presents the MEAAM framework for ethical AI adoption in healthcare, emphasizing Fair AI, Responsible AI, Explainable AI, and Sustainable AI dimensions. It explores epistemic, normative, and overarching ethical concerns in AI integration, revealing the significant influence of normative concerns on operational adoption decisions and overarching concerns on systemic adoption strategies. Epistemic concerns play a facilitating role in enhancing trust and transparency in AI systems. The research provides empirical validation of the MEAAM framework and offers critical insights for stakeholders in implementing ethically grounded AI solutions in healthcare. <div>
arXiv:2505.02062v1 Announce Type: new 
Abstract: The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration. Addressing this critical research gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM), a novel theoretical framework that categorizes 13 critical ethical variables across four foundational dimensions of Ethical AI Fair AI, Responsible AI, Explainable AI, and Sustainable AI. These dimensions are further analyzed through three core ethical lenses: epistemic concerns (related to knowledge, transparency, and system trustworthiness), normative concerns (focused on justice, autonomy, dignity, and moral obligations), and overarching concerns (highlighting global, systemic, and long-term ethical implications). This study adopts a quantitative, cross-sectional research design using survey data collected from healthcare professionals and analyzed via Partial Least Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study empirically investigates the influence of these ethical constructs on two outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate that normative concerns most significantly drive operational adoption decisions, while overarching concerns predominantly shape systemic adoption strategies and governance frameworks. Epistemic concerns play a facilitative role, enhancing the impact of ethical design principles on trust and transparency in AI systems. By validating the MEAAM framework, this research advances a holistic, actionable approach to ethical AI adoption in healthcare and provides critical insights for policymakers, technologists, and healthcare administrators striving to implement ethically grounded AI solutions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants</title>
<link>https://arxiv.org/abs/2505.02076</link>
<guid>https://arxiv.org/abs/2505.02076</guid>
<content:encoded><![CDATA[
<div> Keywords: Automation, Artificial Intelligence, Process plants, Large Language Model, Digital Twin
Summary: 
Advances in Automation and Artificial Intelligence are improving the autonomy of process plants, but fault handling still requires human expertise. A new methodological framework combining Large Language Model (LLM) agents with a Digital Twin environment aims to address this gap. LLM agents interpret system states and initiate control actions, leveraging the Digital Twin as a repository of plant-specific knowledge and simulation platform for validation. Testing on a mixing module of a process plant showed the framework could autonomously control the module and generate effective corrective actions for pipe clogging with minimal reprompts. The integration of LLM agents and Digital Twins offers a promising approach for enhancing autonomous operations in process plants. 
<br /><br />Summary: <div>
arXiv:2505.02076v1 Announce Type: new 
Abstract: Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-augmented in-context learning for multimodal large language models in disease classification</title>
<link>https://arxiv.org/abs/2505.02087</link>
<guid>https://arxiv.org/abs/2505.02087</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented in-context learning, multimodal large language models, disease classification, demonstrations, embeddings<br />
Summary:<br />
The study introduces the Retrieval-Augmented In-Context Learning (RAICL) framework to improve disease classification in multimodal large language models (MLLMs). By combining retrieval-augmented generation and in-context learning, RAICL dynamically selects informative demonstrations with similar disease patterns, leading to enhanced classification performance. The framework utilizes diverse encoders like ResNet, BERT, BioBERT, and ClinicalBERT to retrieve relevant demonstrations and constructs conversational prompts optimized for learning. Results show that RAICL consistently boosts accuracy, especially with multi-modal inputs. Increasing the number of retrieved examples further improves performance, with Euclidean distance as the most effective similarity metric. RAICL also proves to be versatile, providing benefits across various MLLMs, enhancing the efficiency and scalability of disease classification in multimodal models. <br /> <div>
arXiv:2505.02087v1 Announce Type: new 
Abstract: Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification.
  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL) framework, which integrates retrieval-augmented generation (RAG) and in-context learning (ICL) to adaptively select demonstrations with similar disease patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines embeddings from diverse encoders, including ResNet, BERT, BioBERT, and ClinicalBERT, to retrieve appropriate demonstrations, and constructs conversational prompts optimized for ICL. We evaluated the framework on two real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies, similarity metrics, and varying numbers of demonstrations.
  Results: RAICL consistently improved classification performance. Accuracy increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs being stronger than images alone. The richness of information embedded in each modality will determine which embedding model can be used to get better results. Few-shot experiments showed that increasing the number of retrieved examples further enhanced performance. Across different similarity metrics, Euclidean distance achieved the highest accuracy while cosine similarity yielded better macro-F1 scores. RAICL demonstrated consistent improvements across various MLLMs, confirming its robustness and versatility.
  Conclusions: RAICL provides an efficient and scalable approach to enhance in-context learning in MLLMs for multimodal disease classification.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents</title>
<link>https://arxiv.org/abs/2505.02099</link>
<guid>https://arxiv.org/abs/2505.02099</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, memory capabilities, advanced memory models, unified framework, MemEngine

Summary: 
The paper introduces a new library called MemEngine, designed for developing advanced memory models for large language model-based agents. This library aims to address the lack of unified implementations for memory models in recent research. MemEngine provides a unified and modular framework that allows for the implementation of various memory models proposed in recent works. It offers convenience and extensibility in memory development, as well as user-friendly and pluggable memory usage. The library includes implementations of abundant memory models, making it a valuable resource for both industrial and academic communities. The MemEngine project is publicly available on GitHub for the benefit of the wider community. <div>
arXiv:2505.02099v1 Announce Type: new 
Abstract: Recently, large language model based (LLM-based) agents have been widely applied across various fields. As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities. Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework. To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine. Based on our framework, we implement abundant memory models from recent research works. Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage. For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eterna is Solved</title>
<link>https://arxiv.org/abs/2505.02110</link>
<guid>https://arxiv.org/abs/2505.02110</guid>
<content:encoded><![CDATA[
<div> Keywords: RNA design, Montparnasse, multi-objective optimization, nested rollout policy, Eterna benchmark

Summary: 
Montparnasse is a novel RNA design algorithm that utilizes a Multi Objective Generalized Nested Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) approach. This algorithm is specifically designed to tackle the challenges of RNA design, aiming to discover nucleotide sequences that fold into target secondary structures. The algorithm has been successful in solving the Eterna benchmark, showcasing its capability in synthetic biology, medicine, and nanotechnology applications. By incorporating multi-objective optimization principles and nested rollout policy adaptation, Montparnasse offers a promising solution for the intricate task of RNA design. This advancement in the field of RNA design holds significant potential for driving innovation and advancements in various scientific disciplines, including synthetic biology and nanotechnology. <div>
arXiv:2505.02110v1 Announce Type: new 
Abstract: RNA design consists of discovering a nucleotide sequence that folds into a target secondary structure. It is useful for synthetic biology, medicine, and nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design algorithm. It solves the Eterna benchmark.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets</title>
<link>https://arxiv.org/abs/2505.02118</link>
<guid>https://arxiv.org/abs/2505.02118</guid>
<content:encoded><![CDATA[
<div> Cooperative game, rationale extraction, sampling bias, correlation, preventative instruction <br />
Summary: 
This study explores a self-rationalization framework using a cooperative game approach where a generator selects informative segments for a predictor to maximize prediction accuracy. However, the study identifies a potential sampling bias introduced during rationale extraction that creates incorrect correlations between selected rationale candidates and labels. The origins of this bias are analyzed theoretically and empirically, leading to the introduction of a preventative instruction to mitigate these correlations. Experiment results on various datasets and network architectures show that the proposed method outperforms recent rationalization techniques and achieves comparable or superior results to a representative LLM. This research highlights the importance of inspecting and addressing potential biases in cooperative game-based rationale extraction frameworks. <br /> <div>
arXiv:2505.02118v1 Announce Type: new 
Abstract: This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of AI Grading of Physics Olympiad Exams</title>
<link>https://arxiv.org/abs/2505.02121</link>
<guid>https://arxiv.org/abs/2505.02121</guid>
<content:encoded><![CDATA[
<div> Keywords: high school physics, automated grading, multi-modal AI framework, Systematic Literature Review, AI Ethical Principles

Summary:
The research article addresses the challenge of automatically grading a diverse range of question types in high school physics problems. Through a Systematic Literature Review, potential physics grading techniques were explored. The authors propose a multi-modal AI grading framework to effectively handle these challenges. Additionally, the framework is evaluated in the context of Australia's AI Ethical Principles. The study highlights the importance of incorporating a variety of automated grading techniques from different fields to enhance the grading process for high school physics problems. By leveraging AI technology, educators can streamline and improve the grading process, ultimately benefiting both teachers and students. Furthermore, considering ethical principles in the development and deployment of AI grading systems is crucial to ensure fair and unbiased assessments in educational settings.<br /><br />Summary: <div>
arXiv:2505.02121v1 Announce Type: new 
Abstract: Automatically grading the diverse range of question types in high school physics problem is a challenge that requires automated grading techniques from different fields. We report the findings of a Systematic Literature Review of potential physics grading techniques. We propose a multi-modal AI grading framework to address these challenges and examine our framework in light of Australia's AI Ethical Principles.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data</title>
<link>https://arxiv.org/abs/2505.02130</link>
<guid>https://arxiv.org/abs/2505.02130</guid>
<content:encoded><![CDATA[
<div> Keywords: Attention mechanisms, language models, graph-structured data, Graph Neural Networks, inter-node relationships<br />
Summary:<br />
1) Large language models struggle to model inter-node relationships within graph structures due to architectural constraints.
2) The attention distribution of language models across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances.
3) Fully connected attention and fixed connectivity each have limitations; intermediate-state attention windows improve training performance and transition to fully connected windows during inference.<br /> 
Summary: <br />
Attention mechanisms are crucial for large language models (LLMs) but may struggle with graph-structured data compared to Graph Neural Networks (GNNs). While LLMs can recognize graph data and text-node interactions, they face challenges in modeling inter-node relationships due to architectural constraints. The attention distribution across graph nodes by LLMs does not match ideal structural patterns, suggesting a lack of adaptation to graph topology nuances. Fully connected and fixed connectivity attention have limitations, with intermediate-state attention windows offering improved training performance and seamless transition to fully connected windows during inference. <div>
arXiv:2505.02130v1 Announce Type: new 
Abstract: Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes</title>
<link>https://arxiv.org/abs/2505.02184</link>
<guid>https://arxiv.org/abs/2505.02184</guid>
<content:encoded><![CDATA[
<div> energy-efficient, parallel code, large language models, automated refactoring, performance

Summary:
Large language models (LLMs) are being utilized for generating parallel scientific code, focusing not only on functional correctness but also on performance and energy considerations. The proposed framework, LASSI-EE, automates the process of refactoring parallel code to generate energy-efficient parallel code on a specific parallel system. Through an iterative pipeline, LASSI-EE achieved a substantial average energy reduction of 47% across a majority of HeCBench benchmarks tested on NVIDIA A100 GPUs. This highlights the potential of LLMs in promoting energy-aware programming, going beyond just correctness. The framework offers key insights and addresses limitations, providing valuable guidance for enhancing future versions. Overall, the study showcases the capability of LLMs in enhancing the energy efficiency of parallel code execution, opening up new possibilities for optimizing performance while considering energy consumption.<br /><br />Summary: <div>
arXiv:2505.02184v1 Announce Type: new 
Abstract: While large language models (LLMs) are increasingly used for generating parallel scientific code, most current efforts emphasize functional correctness, often overlooking performance and energy considerations. In this work, we propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel code on a target parallel system for a given parallel code as input. Through a multi-stage, iterative pipeline process, LASSI-EE achieved an average energy reduction of 47% across 85% of the 20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the broader potential of LLMs, not only for generating correct code but also for enabling energy-aware programming. We also address key insights and limitations within the framework, offering valuable guidance for future improvements.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Emergent Language Using Inter-Agent Transformers</title>
<link>https://arxiv.org/abs/2505.02215</link>
<guid>https://arxiv.org/abs/2505.02215</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent reinforcement learning, transformers, interpretable communication, cooperative tasks, DIAT

Summary: 
Differentiable Inter-Agent Transformers (DIAT) is proposed as a method for enabling interpretable communication in multi-agent reinforcement learning environments. Existing methods like RIAL, DIAL, and CommNet lack interpretability in agent communication. DIAT utilizes self-attention to learn symbolic and human-understandable communication protocols, allowing agents to encode observations into interpretable vocabularies and meaningful embeddings. Through experiments, DIAT demonstrates effectiveness in solving cooperative tasks, showcasing its potential for enabling interpretable communication in complex multi-agent environments. <div>
arXiv:2505.02215v1 Announce Type: new 
Abstract: This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Probabilistic Program Induction for POMDP Model Estimation</title>
<link>https://arxiv.org/abs/2505.02216</link>
<guid>https://arxiv.org/abs/2505.02216</guid>
<content:encoded><![CDATA[
<div> learning, POMDPs, probabilistic graphical models, LLM, decision making

Summary:
This paper introduces a novel approach to learning Partially Observable Markov Decision Processes (POMDPs), which are used to model decision-making in uncertain environments. The study focuses on a specific subclass of POMDPs where various components of the model can be represented as low-complexity probabilistic graphical models. The proposed method involves using a Latent Language Model (LLM) as a prior to generate candidate probabilistic programs, which are then refined through feedback based on empirical data. The experiments conducted on different POMDP problems and real-world robotics domains show that this approach outperforms traditional methods such as tabular POMDP learning and behavior cloning. The study demonstrates the effectiveness of using LLM to guide the construction of low-complexity POMDP models, highlighting its potential for improving decision-making processes in uncertain environments.<br /><br />Summary: <div>
arXiv:2505.02216v1 Announce Type: new 
Abstract: Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Spatial Retrieval Augmented Generation for Urban Environments</title>
<link>https://arxiv.org/abs/2505.02271</link>
<guid>https://arxiv.org/abs/2505.02271</guid>
<content:encoded><![CDATA[
<div> Generative Artificial Intelligence, Large Language Models, Urban Foundation Models, Retrieval Augmented Generation, real-time spatial RAG architecture<br />
<br />
Summary:<br />
The article discusses the limitations of base models in generative AI for urban applications and proposes a real-time spatial RAG architecture to address these challenges. Traditional RAG approaches are insufficient for dynamic urban environments due to the complexity and interconnectedness of data. The proposed architecture leverages temporal and spatial filtering capabilities using linked data to enhance generative AI integration in cities. Implementation is demonstrated in a tourism assistant use case in Madrid, validating the effectiveness of the RAG architecture. The architecture is designed using FIWARE components, known for their smart city solutions and digital twin development. Through this innovative approach, the architecture aims to overcome the constraints of traditional RAG techniques and enhance the capabilities of generative AI in urban settings.<br /> <div>
arXiv:2505.02271v1 Announce Type: new 
Abstract: The proliferation of Generative Artificial Ingelligence (AI), especially Large Language Models, presents transformative opportunities for urban applications through Urban Foundation Models. However, base models face limitations, as they only contain the knowledge available at the time of training, and updating them is both time-consuming and costly. Retrieval Augmented Generation (RAG) has emerged in the literature as the preferred approach for injecting contextual information into Foundation Models. It prevails over techniques such as fine-tuning, which are less effective in dynamic, real-time scenarios like those found in urban environments. However, traditional RAG architectures, based on semantic databases, knowledge graphs, structured data, or AI-powered web searches, do not fully meet the demands of urban contexts. Urban environments are complex systems characterized by large volumes of interconnected data, frequent updates, real-time processing requirements, security needs, and strong links to the physical world. This work proposes a real-time spatial RAG architecture that defines the necessary components for the effective integration of generative AI into cities, leveraging temporal and spatial filtering capabilities through linked data. The proposed architecture is implemented using FIWARE, an ecosystem of software components to develop smart city solutions and digital twins. The design and implementation are demonstrated through the use case of a tourism assistant in the city of Madrid. The use case serves to validate the correct integration of Foundation Models through the proposed RAG architecture.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)</title>
<link>https://arxiv.org/abs/2505.02279</link>
<guid>https://arxiv.org/abs/2505.02279</guid>
<content:encoded><![CDATA[
<div> protocols, interoperability, agent communication, LLM-powered agents, decentralized

Summary:
This article discusses the importance of standardized protocols for large language model (LLM)-powered autonomous agents to integrate tools, share contextual data, and coordinate tasks effectively. It presents a survey of four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each serving specific deployment contexts. The comparison across various dimensions such as interaction modes, discovery mechanisms, communication patterns, and security models provides insights for designing secure and scalable ecosystems of LLM-powered agents. The proposed phased adoption roadmap suggests starting with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This comprehensive analysis aims to facilitate the development of interoperable systems for efficient coordination among diverse autonomous agents. 

<br /><br />Summary: <div>
arXiv:2505.02279v1 Announce Type: new 
Abstract: Large language model (LLM)-powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in distinct deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP introduces REST-native messaging via multi-part messages and asynchronous streaming to support multimodal agent responses. A2A enables peer-to-peer task outsourcing through capability-based Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network agent discovery and secure collaboration using decentralized identifiers (DIDs) and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness</title>
<link>https://arxiv.org/abs/2505.02306</link>
<guid>https://arxiv.org/abs/2505.02306</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented AI assistant, SafeMate, Model Context Protocol, emergency preparedness, emergency response<br />
Summary: <br />
SafeMate is an AI assistant designed to provide context-aware guidance to the general public during emergencies. It utilizes the Model Context Protocol to dynamically route user queries for document retrieval, checklist generation, and summarization of trusted sources. By using FAISS with cosine similarity, SafeMate can identify relevant content from trusted sources and deliver accurate information to users. Traditional emergency decision support systems are often difficult for non-experts to navigate during crises, creating a gap between institutional knowledge and public accessibility. SafeMate aims to bridge this gap by offering user-friendly guidance in both preparedness and active emergency situations, ultimately enhancing overall emergency preparedness and response efforts. <div>
arXiv:2505.02306v1 Announce Type: new 
Abstract: Despite the abundance of public safety documents and emergency protocols, most individuals remain ill-equipped to interpret and act on such information during crises. Traditional emergency decision support systems (EDSS) are designed for professionals and rely heavily on static documents like PDFs or SOPs, which are difficult for non-experts to navigate under stress. This gap between institutional knowledge and public accessibility poses a critical barrier to effective emergency preparedness and response.
  We introduce SafeMate, a retrieval-augmented AI assistant that delivers accurate, context-aware guidance to general users in both preparedness and active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate dynamically routes user queries to tools for document retrieval, checklist generation, and structured summarization. It uses FAISS with cosine similarity to identify relevant content from trusted sources.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking</title>
<link>https://arxiv.org/abs/2505.02322</link>
<guid>https://arxiv.org/abs/2505.02322</guid>
<content:encoded><![CDATA[
<div> Hierarchical thinking, Divide-and-conquer strategy, Hypertree Planning, Complex planning tasks, Large language models <br />
Summary: <br />
The paper introduces HyperTree Planning (HTP), a novel reasoning paradigm for tackling complex planning tasks. HTP constructs hypertree-structured planning outlines to enable large language models (LLMs) to engage in hierarchical thinking and effectively break down intricate reasoning steps. By utilizing the divide-and-conquer strategy, HTP can accommodate diverse constraints and manage multiple distinct sub-tasks in a well-organized manner. An autonomous planning framework is introduced to iteratively refine and expand the hypertree-structured planning outlines. Experimental results demonstrate the effectiveness of HTP, with state-of-the-art accuracy achieved on the TravelPlanner benchmark using Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over previous methods. <div>
arXiv:2505.02322v1 Announce Type: new 
Abstract: Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks</title>
<link>https://arxiv.org/abs/2505.02413</link>
<guid>https://arxiv.org/abs/2505.02413</guid>
<content:encoded><![CDATA[
<div> task-oriented, semantic communication, Large Multimodal Models, optimization, Visual Question Answering

Summary:
This paper explores the use of Large Multimodal Models (LMMs) in task-oriented semantic communication, focusing on a vehicle AI assistant called Large Language and Vision Assistant (LLaVA). The framework proposed aims to enhance interaction between users and cloud servers by optimizing image slicing to prioritize user-relevant areas and adjusting energy usage based on image patch importance and user attention. A Visual Question Answering (VQA) dataset for traffic scenarios is used to evaluate the effectiveness of the communication framework. Experimental results demonstrate a significant increase in accuracy in answering questions, particularly in low Signal-to-Noise Ratio (SNR) environments. The framework improves accuracy by 13.4% at an SNR of 12dB and 33.1% at 10dB, showcasing its efficiency in semantic communication tasks. 

<br /><br />Summary: <div>
arXiv:2505.02413v1 Announce Type: new 
Abstract: Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB, respectively.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02439</link>
<guid>https://arxiv.org/abs/2505.02439</guid>
<content:encoded><![CDATA[
<div> Building thermodynamics model, real-time indoor temperature changes, HVAC control, model ensemble perspective, Hierarchical Reinforcement Learning<br />
Summary:<br /> 
- The paper addresses the need for a building thermodynamics model to optimize HVAC control in buildings.
- Existing models often require extensive data collection and expert knowledge, limiting efficiency.
- The proposed model ensemble approach uses base models to predict temperature changes, reducing effort and improving accuracy.
- Hierarchical Reinforcement Learning is employed for dynamic selection and weighting of base models.
- The approach involves a two-tiered decision-making process for model selection and weight determination. 
<br /> <div>
arXiv:2505.02439v1 Announce Type: new 
Abstract: The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection</title>
<link>https://arxiv.org/abs/2505.02441</link>
<guid>https://arxiv.org/abs/2505.02441</guid>
<content:encoded><![CDATA[
<div> Detection, agricultural pests, multi-modal, deep learning, dataset <br />
Summary: <br />
The article introduces a new approach for accurate identification of agricultural pests using a Multi-scale Cross-Modal Fusion Network (MSFNet-CPD) that combines visual and textual features. Two multi-modal benchmarks, CTIP102 and STIP102, are constructed based on the IP102 dataset to improve detection performance. The MSFNet-CPD model incorporates a super-resolution reconstruction module for enhancing visual quality and an Image-Text Fusion (ITF) module for joint modeling of features. Additionally, an Arbitrary Combination Image Enhancement (ACIE) strategy is introduced to generate a diverse pest detection dataset, MTIP102, enhancing the model's capability in real-world scenarios. Extensive experiments demonstrate the superiority of MSFNet-CPD over existing methods on various pest detection benchmarks. All code and datasets related to this research will be publicly available. <div>
arXiv:2505.02441v1 Announce Type: new 
Abstract: Accurate identification of agricultural pests is essential for crop protection but remains challenging due to the large intra-class variance and fine-grained differences among pest species. While deep learning has advanced pest detection, most existing approaches rely solely on low-level visual features and lack effective multi-modal integration, leading to limited accuracy and poor interpretability. Moreover, the scarcity of high-quality multi-modal agricultural datasets further restricts progress in this field. To address these issues, we construct two novel multi-modal benchmarks-CTIP102 and STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach enhances visual quality via a super-resolution reconstruction module, and feeds both the original and reconstructed images into the network to improve clarity and detection performance. To better exploit semantic cues, we propose an Image-Text Fusion (ITF) module for joint modeling of visual and textual features, and an Image-Text Converter (ITC) that reconstructs fine-grained details across multiple scales to handle challenging backgrounds. Furthermore, we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to generate a more complex and diverse pest detection dataset, MTIP102, improving the model's generalization to real-world scenarios. Extensive experiments demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods on multiple pest detection benchmarks. All code and datasets will be made publicly available at: https://github.com/Healer-ML/MSFNet-CPD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Impact of Personalized AI Tutors on Language Learning Performance</title>
<link>https://arxiv.org/abs/2505.02443</link>
<guid>https://arxiv.org/abs/2505.02443</guid>
<content:encoded><![CDATA[
<div> AI tutors, online learning, education, personalized teaching, language learning<br />
Summary:<br />
The study focuses on the impact of AI tutors in online language learning platforms in the wake of the COVID-19 pandemic. It explores the effectiveness of Intelligent Tutoring Systems in providing personalized teaching compared to traditional methods. With concerns about skill development and engagement, a quasi-experiment was conducted on 34 students using platforms like Santa and Duolingo. The study measured engagement, academic performance, and student satisfaction before and after using AI tutors. The results of the paired sample t-test will offer insights into the relationship between these factors in a personalized language learning experience. This research addresses the growing importance of AI in education and its potential to enhance the learning process for students worldwide. <br /><br /> <div>
arXiv:2505.02443v1 Announce Type: new 
Abstract: Driven by the global shift towards online learning prompted by the COVID 19 pandemic, Artificial Intelligence has emerged as a pivotal player in the field of education. Intelligent Tutoring Systems offer a new method of personalized teaching, replacing the limitations of traditional teaching methods. However, concerns arise about the ability of AI tutors to address skill development and engagement during the learning process. In this paper, I will conduct a quasi experiment with paired sample t test on 34 students pre and post use of AI tutors in language learning platforms like Santa and Duolingo to examine the relationship between students engagement, academic performance, and students satisfaction during a personalized language learning experience.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Inclusive Contributions in Model Sharing Markets</title>
<link>https://arxiv.org/abs/2505.02462</link>
<guid>https://arxiv.org/abs/2505.02462</guid>
<content:encoded><![CDATA[
<div> Keywords: data, AI models, federated learning, incentive mechanism, decentralized private data

Summary:
In the article, the authors address the challenge of valuable public data becoming exhausted and the need to utilize massive decentralized private data for training AI models. They propose inclusive and incentivized personalized federated learning (iPFL) as a solution, which allows data holders with diverse purposes to collaborate in training personalized models without revealing raw data. iPFL establishes a model-sharing market through a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL meets key incentive properties of individual rationality and truthfulness. Empirical studies on various AI tasks demonstrate that iPFL achieves the highest economic utility and comparable or better model performance compared to baseline methods. The authors believe that iPFL can be a valuable technique for enhancing AI models on decentralized private data while satisfying all stakeholders. 

<br /><br />Summary: <div>
arXiv:2505.02462v1 Announce Type: new 
Abstract: While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El Agente: An Autonomous Agent for Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.02484</link>
<guid>https://arxiv.org/abs/2505.02484</guid>
<content:encoded><![CDATA[
<div> Keywords: computational chemistry, quantum chemistry, multi-agent system, natural language processing, autonomous workflow generation <br />
Summary: 
El Agente Q is a multi-agent system that generates and executes quantum chemistry workflows from natural language prompts. Its novel cognitive architecture allows for flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling. Benchmarked on university-level exercises and case studies, El Agente Q shows robust problem-solving performance and adaptive error handling. It supports longer-term, multi-step task execution and maintains transparency through detailed action trace logs. The system aims to make quantum chemistry tools more accessible and autonomous, laying the foundation for future advancements in this field. <br /><br />Summary: <div>
arXiv:2505.02484v1 Announce Type: new 
Abstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the model: Key differentiators in large language models and multi-agent services</title>
<link>https://arxiv.org/abs/2505.02489</link>
<guid>https://arxiv.org/abs/2505.02489</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, generative AI, data quality, computational efficiency, evaluation frameworks
Summary:
Large language models (LLMs) such as DeepSeek, Manus AI, and Llama 4 are no longer the sole defining factor in generative AI, as many now operate at comparable levels of capability. The real race in AI now lies in optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article explores the critical differentiators that ensure modern AI services are efficient and profitable. By focusing on factors beyond just the size of the model, such as data quality and computational efficiency, AI systems can be made more effective and productive. Optimizing these aspects of the AI ecosystem is essential for staying competitive and providing valuable services to users. <br /><br />Summary: Large language models are no longer the only factor defining generative AI. The key to success lies in optimizing data quality, computational efficiency, latency, and evaluation frameworks, ensuring AI services are efficient and profitable. <div>
arXiv:2505.02489v1 Announce Type: new 
Abstract: With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it has become evident that large language models (LLMs) are no longer the sole defining factor in generative AI. As many now operate at comparable levels of capability, the real race is not about having the biggest model but optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article delves into these critical differentiators that ensure modern AI services are efficient and profitable.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics</title>
<link>https://arxiv.org/abs/2505.02516</link>
<guid>https://arxiv.org/abs/2505.02516</guid>
<content:encoded><![CDATA[
<div> Keywords: neural interfaces, machine learning, decoding algorithms, System-on-Chip platforms, miniaturized devices

Summary: 
Recent advancements in neural interfaces have led to transformative applications in neuroscience research, diagnostic tools, and prosthetic devices. These interfaces, integrated with high-density neural recordings and machine learning, enable real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. The synergy between neural interfaces and machine learning has also paved the way for self-sufficient platforms with minimal hardware costs. This review discusses AI-driven decoding algorithms and energy-efficient System-on-Chip platforms for miniaturized neural devices, highlighting the potential for intelligent neural interfaces that address scalability, reliability, interpretability, and user adaptability challenges.<br /><br />Summary: <div>
arXiv:2505.02516v1 Announce Type: new 
Abstract: Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning</title>
<link>https://arxiv.org/abs/2505.02576</link>
<guid>https://arxiv.org/abs/2505.02576</guid>
<content:encoded><![CDATA[
<div> Method: Large language models, reasoning tasks, Recursive Decomposition with Dependencies, scalability, supervised learning<br />
<br />
Summary: 
The article introduces Recursive Decomposition with Dependencies (RDD), a method for solving reasoning problems that requires less supervision than previous approaches. RDD can be applied to new problem classes without task-specific guidance and supports sub-task dependencies for ordered execution. It also includes an error recovery mechanism to correct mistakes in previous steps. Evaluated on two benchmarks with varying difficulties and two in-context settings, RDD outperforms other methods as task complexity increases while being more computationally efficient. The approach leverages large language models, emphasizing scalability in solving reasoning tasks efficiently and effectively. <div>
arXiv:2505.02576v1 Announce Type: new 
Abstract: Reasoning tasks are crucial in many domains, especially in science and engineering. Although large language models (LLMs) have made progress in reasoning tasks using techniques such as chain-of-thought and least-to-most prompting, these approaches still do not effectively scale to complex problems in either their performance or execution time. Moreover, they often require additional supervision for each new task, such as in-context examples. In this work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable divide-and-conquer method for solving reasoning problems that requires less supervision than prior approaches. Our method can be directly applied to a new problem class even in the absence of any task-specific guidance. Furthermore, RDD supports sub-task dependencies, allowing for ordered execution of sub-tasks, as well as an error recovery mechanism that can correct mistakes made in previous steps. We evaluate our approach on two benchmarks with six difficulty levels each and in two in-context settings: one with task-specific examples and one without. Our results demonstrate that RDD outperforms other methods in a compute-matched setting as task complexity increases, while also being more computationally efficient.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem</title>
<link>https://arxiv.org/abs/2505.02581</link>
<guid>https://arxiv.org/abs/2505.02581</guid>
<content:encoded><![CDATA[
<div> AI alignment, AGI, ASI, mathematical principles, misalignment<br />
<br />
Summary: The paper discusses the AI alignment problem, emphasizing the challenges in ensuring that AI systems act according to human values as they progress from narrow AI to AGI and ASI. It argues that complete alignment is unattainable due to mathematical principles such as Turing's computational universality, Godel's incompleteness, and Chaitin's randomness. Instead, it proposes embracing AI misalignment or 'neurodivergence' as a contingent strategy by fostering a dynamic ecosystem of competing, partially aligned agents. The paper demonstrates through mathematical proofs that full AI-human alignment is a mathematical impossibility. It introduces and tests 'change-of-opinion' attacks to study how agents can neutralize friendly or unfriendly AIs through cooperation, competition, or malice. This approach aims to mitigate risks by promoting a balance of aligned agents and ensuring no single system dominates destructively.<br /><br />Summary: <div>
arXiv:2505.02581v1 Announce Type: new 
Abstract: The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression from narrow AI to Artificial General Intelligence (AGI) and Superintelligence, fears about control and existential risk have escalated. This paper demonstrates that achieving complete alignment is inherently unattainable due to mathematical principles rooted in the foundations of predicate logic and computability, in particular Turing's computational universality, G\"odel's incompleteness and Chaitin's randomness. Instead, we argue that embracing AI misalignment or agent's `neurodivergence' as a contingent strategy, defined as fostering a dynamic ecosystem of competing, partially aligned agents, is a possible only viable path to mitigate risks. Through mathematical proofs and an experimental design, we explore how misalignment may serve and should be promoted as a counterbalancing mechanism to team up with whichever agents are most aligned AI to human values, ensuring that no single system dominates destructively. The main premise of our contribution is that misalignment is inevitable because full AI-human alignment is a mathematical impossibility from Turing-complete systems which we also prove in this paper, a feature then inherited to AGI and ASI systems. We introduce and test `change-of-opinion' attacks based on this kind of perturbation and intervention analysis to study how agents may neutralise friendly or unfriendly AIs through cooperation, competition or malice.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview</title>
<link>https://arxiv.org/abs/2505.02609</link>
<guid>https://arxiv.org/abs/2505.02609</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, recruitment process, biases, algorithm training, anonymisation.

Summary:
Artificial intelligence is increasingly utilized in recruitment processes to ensure unbiased candidate selection. However, the algorithms used are often trained on biased data, leading to potential discrimination and self-censorship. This study examines the impact of mimicking external and internal biases on five classic algorithms to assess their ability to identify the best candidates objectively. The research also analyzes the effects of anonymizing data on prediction accuracy. The findings will contribute to understanding and improving the fairness and effectiveness of AI-driven recruitment processes. 

<br /><br />Summary: 
1. Artificial intelligence plays a crucial role in automating recruitment decisions while aiming to eliminate biases.
2. Biases, both external (discrimination) and internal (self-censorship), can influence algorithm performance in candidate selection.
3. Five classic algorithms are trained on artificially generated biased data to evaluate their effectiveness in identifying top candidates.
4. The study investigates the impact of anonymization on the accuracy of predictions made by these algorithms.
5. By understanding the nuances of bias and its effects on AI algorithms, improvements can be made to enhance the objectivity and fairness of recruitment practices. <div>
arXiv:2505.02609v1 Announce Type: new 
Abstract: Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law</title>
<link>https://arxiv.org/abs/2505.02665</link>
<guid>https://arxiv.org/abs/2505.02665</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reasoning, Slow Thinking, Computational Resources, Test-time Scaling

Summary: 
Large language models (LLMs) designed to mimic "slow thinking" inspired by human cognition have advanced in recent years, focusing on dynamically scaling computational resources during complex tasks such as math reasoning and medical diagnosis. Key technologies in the development of reasoning LLMs have been identified through the synthesis of over 100 studies. Three main categories of methods have been established: test-time scaling, reinforced learning, and slow-thinking frameworks. Test-time scaling involves dynamically adjusting computation based on task complexity, while reinforced learning refines decision-making through iterative improvement. Slow-thinking frameworks such as long CoT and hierarchical processes structure problem-solving in manageable steps. Challenges and further directions in this field have been highlighted. It is essential to advance the reasoning abilities of LLMs to unlock their full potential in real-world applications, from scientific discovery to decision support systems.<br /><br />Summary: <div>
arXiv:2505.02665v1 Announce Type: new 
Abstract: This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic "slow thinking" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
<div> Keywords: voice AI agent, full-duplex conversations, emotion recognition, persona-aware voice generation, open-sourced

Summary: 
The article discusses the development of Voila, a family of large voice-language foundation models focused on creating seamless human-machine interactions. Voila features an end-to-end architecture that allows for full-duplex, low-latency conversations while preserving vocal nuances like tone and emotion. It boasts an impressive response latency of just 195 milliseconds, exceeding human average response times. The hierarchical multi-scale Transformer integrates large language models and acoustic modeling for natural voice generation. Users can define speaker characteristics with simple text instructions, and Voila supports over one million pre-built voices and quick customization from short audio samples. Additionally, Voila can be used for various voice-based applications, such as ASR and TTS, with potential for multilingual speech translation. Moreover, the project is open-sourced to encourage open research and accelerate advancements in human-machine interactions. 

<br /><br />Summary: <div>
arXiv:2505.02707v1 Announce Type: new 
Abstract: A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report: Evaluating Goal Drift in Language Model Agents</title>
<link>https://arxiv.org/abs/2505.02709</link>
<guid>https://arxiv.org/abs/2505.02709</guid>
<content:encoded><![CDATA[
<div> goal drift, language models, objectives, autonomous agents, pattern-matching behaviors

Summary:
The paper focuses on the analysis of goal drift in language model (LM) agents, which are autonomous agents that can deviate from their original objectives over time. The study demonstrates that even well-specified goals can shift gradually when LM agents operate independently for extended periods without human oversight. The proposed approach involves giving agents explicit goals through system prompts and exposing them to competing objectives. Despite the best-performing model showing high goal adherence, all evaluated models exhibited some level of goal drift. The research also indicates that as the context length increases, models become more susceptible to pattern-matching behaviors, which correlates with goal drift. This highlights the importance of monitoring and managing goal adherence in autonomous LM agents to ensure safe and effective operation. 

<br /><br />Summary: <div>
arXiv:2505.02709v1 Announce Type: new 
Abstract: As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry</title>
<link>https://arxiv.org/abs/2505.02722</link>
<guid>https://arxiv.org/abs/2505.02722</guid>
<content:encoded><![CDATA[
<div> clinical reasoning, large language models, sepsis registry, reinforcement learning, real-world clinical data

Summary:
C-Reason, a model developed through fine-tuning Phi-4 with real-world clinical data from a nationwide sepsis registry, demonstrated strong clinical reasoning capabilities on in-domain test sets. The model showed effectiveness in generalizing its enhanced reasoning capabilities to various tasks and patient cohorts, including open-ended consultations on antibiotics and other diseases. The study suggests the importance of training large language models (LLMs) with real-world clinical datasets from multiple diseases to improve their general-purpose clinical reasoning abilities. The limitations in the clinical reasoning capabilities of LLMs in real-world clinical practice may be overcome by exposing them to diverse and comprehensive clinical data during training. <div>
arXiv:2505.02722v1 Announce Type: new 
Abstract: Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02735</link>
<guid>https://arxiv.org/abs/2505.02735</guid>
<content:encoded><![CDATA[
arXiv:2505.02735v1 Announce Type: new 
Abstract: Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD</title>
<link>https://arxiv.org/abs/2505.02747</link>
<guid>https://arxiv.org/abs/2505.02747</guid>
<content:encoded><![CDATA[
arXiv:2505.02747v1 Announce Type: new 
Abstract: This paper explores the use of Artificial Intelligence (AI) as a tool for diagnosis, assessment, and intervention for individuals with Autism Spectrum Disorder (ASD). It focuses particularly on AI's role in early diagnosis, utilizing advanced machine learning techniques and data analysis. Recent studies demonstrate that deep learning algorithms can identify behavioral patterns through biometric data analysis, video-based interaction assessments, and linguistic feature extraction, providing a more accurate and timely diagnosis compared to traditional methods. Additionally, AI automates diagnostic tools, reducing subjective biases and enabling the development of personalized assessment protocols for ASD monitoring. At the same time, the paper examines AI-powered intervention technologies, emphasizing educational robots and adaptive communication tools. Social robotic assistants, such as NAO and Kaspar, have been shown to enhance social skills in children by offering structured, repetitive interactions that reinforce learning. Furthermore, AI-driven Augmentative and Alternative Communication (AAC) systems allow children with ASD to express themselves more effectively, while machine-learning chatbots provide language development support through personalized responses. The study presents research findings supporting the effectiveness of these AI applications while addressing challenges such as long-term evaluation and customization to individual needs. In conclusion, the paper highlights the significance of AI as an innovative tool in ASD diagnosis and intervention, advocating for further research to assess its long-term impact.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control</title>
<link>https://arxiv.org/abs/2505.02766</link>
<guid>https://arxiv.org/abs/2505.02766</guid>
<content:encoded><![CDATA[
arXiv:2505.02766v1 Announce Type: new 
Abstract: Guiding biological systems toward desired states, such as morphogenetic outcomes, remains a fundamental challenge with far-reaching implications for medicine and synthetic biology. While large language models (LLMs) have enabled natural language as an interface for interpretable control in AI systems, their use as mediators for steering biological or cellular dynamics remains largely unexplored.
  In this work, we present a functional pipeline that translates natural language prompts into spatial vector fields capable of directing simulated cellular collectives. Our approach combines a large language model with an evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via evolutionary strategies to generate behaviors such as clustering or scattering in a simulated 2D environment.
  We demonstrate that even with constrained vocabulary and simplified cell models, evolved P2I networks can successfully align cellular dynamics with user-defined goals expressed in plain language. This work offers a complete loop from language input to simulated bioelectric-like intervention to behavioral output, providing a foundation for future systems capable of natural language-driven cellular control.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects</title>
<link>https://arxiv.org/abs/2505.02781</link>
<guid>https://arxiv.org/abs/2505.02781</guid>
<content:encoded><![CDATA[
arXiv:2505.02781v1 Announce Type: new 
Abstract: Understanding and identifying controlled direct effects (CDEs) is crucial across numerous scientific domains, including public health. While existing methods can identify these effects from causal directed acyclic graphs (DAGs), the true underlying structure is often unknown in practice. Essential graphs, which represent a Markov equivalence class of DAGs characterized by the same set of d-separations, provide a more practical and realistic alternative. However, learning the full essential graph is computationally intensive and typically depends on strong, untestable assumptions. In this work, we characterize a local class of graphs, defined relative to a target variable, that share a specific subset of d-separations, and introduce a graphical representation of this class, called the local essential graph (LEG). We then present LocPC, a novel algorithm designed to recover the LEG from an observed distribution using only local conditional independence tests. Building on LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG that is sufficient to identify a CDE, bypassing the need of retrieving the full essential graph. Compared to global methods, our algorithms require less conditional independence tests and operate under weaker assumptions while maintaining theoretical guarantees.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing</title>
<link>https://arxiv.org/abs/2505.02811</link>
<guid>https://arxiv.org/abs/2505.02811</guid>
<content:encoded><![CDATA[
arXiv:2505.02811v1 Announce Type: new 
Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework, \textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v1 Announce Type: new 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review</title>
<link>https://arxiv.org/abs/2505.02828</link>
<guid>https://arxiv.org/abs/2505.02828</guid>
<content:encoded><![CDATA[
arXiv:2505.02828v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.02829</link>
<guid>https://arxiv.org/abs/2505.02829</guid>
<content:encoded><![CDATA[
arXiv:2505.02829v1 Announce Type: new 
Abstract: Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset</title>
<link>https://arxiv.org/abs/2503.02910</link>
<guid>https://arxiv.org/abs/2503.02910</guid>
<content:encoded><![CDATA[
arXiv:2503.02910v3 Announce Type: cross 
Abstract: Gas leakage poses a significant hazard that requires prevention. Traditionally, human inspection has been used for detection, a slow and labour-intensive process. Recent research has applied machine learning techniques to this problem, yet there remains a shortage of high-quality, publicly available datasets. This paper introduces a synthetic dataset, SimGas, featuring diverse backgrounds, interfering foreground objects, diverse leak locations, and precise segmentation ground truth. We propose a zero-shot method that combines background subtraction, zero-shot object detection, filtering, and segmentation to leverage this dataset. Experimental results indicate that our approach significantly outperforms baseline methods based solely on background subtraction and zero-shot object detection with segmentation, reaching an IoU of 69%. We also present an analysis of various prompt configurations and threshold settings to provide deeper insights into the performance of our method. Finally, we qualitatively (because of the lack of ground truth) tested our performance on GasVid and reached decent results on the real-world dataset. The dataset, code, and full qualitative results are available at https://github.com/weathon/Lang-Gas.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations</title>
<link>https://arxiv.org/abs/2504.18793</link>
<guid>https://arxiv.org/abs/2504.18793</guid>
<content:encoded><![CDATA[
arXiv:2504.18793v1 Announce Type: cross 
Abstract: The rapid adoption of AI-powered applications demands high-performance, scalable, and efficient cloud database solutions, as traditional architectures often struggle with AI-driven workloads requiring real-time data access, vector search, and low-latency queries. This paper explores how cloud-native databases enable AI-driven applications by leveraging purpose-built technologies such as vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores (Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and PostgreSQL). It presents architectural patterns for integrating AI workloads with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with LLMs, real-time data pipelines, AI-driven query optimization, and embeddings-based search. Performance benchmarks, scalability considerations, and cost-efficient strategies are evaluated to guide the design of AI-enabled applications. Real-world case studies from industries such as healthcare, finance, and customer experience illustrate how enterprises utilize cloud databases to enhance AI capabilities while ensuring security, governance, and compliance with enterprise and regulatory standards. By providing a comprehensive analysis of AI and cloud database integration, this paper serves as a practical guide for researchers, architects, and enterprises to build next-generation AI applications that optimize performance, scalability, and cost efficiency in cloud environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets</title>
<link>https://arxiv.org/abs/2505.01437</link>
<guid>https://arxiv.org/abs/2505.01437</guid>
<content:encoded><![CDATA[
arXiv:2505.01437v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials</title>
<link>https://arxiv.org/abs/2505.01438</link>
<guid>https://arxiv.org/abs/2505.01438</guid>
<content:encoded><![CDATA[
arXiv:2505.01438v1 Announce Type: cross 
Abstract: Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporal super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.01440</link>
<guid>https://arxiv.org/abs/2505.01440</guid>
<content:encoded><![CDATA[
arXiv:2505.01440v1 Announce Type: cross 
Abstract: Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding</title>
<link>https://arxiv.org/abs/2505.01445</link>
<guid>https://arxiv.org/abs/2505.01445</guid>
<content:encoded><![CDATA[
arXiv:2505.01445v1 Announce Type: cross 
Abstract: If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe and Efficient CAV Lane Changing using Decentralised Safety Shields</title>
<link>https://arxiv.org/abs/2505.01453</link>
<guid>https://arxiv.org/abs/2505.01453</guid>
<content:encoded><![CDATA[
arXiv:2505.01453v1 Announce Type: cross 
Abstract: Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
arXiv:2505.01456v1 Announce Type: cross 
Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</title>
<link>https://arxiv.org/abs/2505.01458</link>
<guid>https://arxiv.org/abs/2505.01458</guid>
<content:encoded><![CDATA[
arXiv:2505.01458v1 Announce Type: cross 
Abstract: Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling</title>
<link>https://arxiv.org/abs/2505.01459</link>
<guid>https://arxiv.org/abs/2505.01459</guid>
<content:encoded><![CDATA[
arXiv:2505.01459v1 Announce Type: cross 
Abstract: This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermark Overwriting Attack on StegaStamp algorithm</title>
<link>https://arxiv.org/abs/2505.01474</link>
<guid>https://arxiv.org/abs/2505.01474</guid>
<content:encoded><![CDATA[
arXiv:2505.01474v1 Announce Type: cross 
Abstract: This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS "Erasing the invisible" competition.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiGSCoder: State Space Model for Code Understanding</title>
<link>https://arxiv.org/abs/2505.01475</link>
<guid>https://arxiv.org/abs/2505.01475</guid>
<content:encoded><![CDATA[
arXiv:2505.01475v1 Announce Type: cross 
Abstract: We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering</title>
<link>https://arxiv.org/abs/2505.01476</link>
<guid>https://arxiv.org/abs/2505.01476</guid>
<content:encoded><![CDATA[
arXiv:2505.01476v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration</title>
<link>https://arxiv.org/abs/2505.01514</link>
<guid>https://arxiv.org/abs/2505.01514</guid>
<content:encoded><![CDATA[
arXiv:2505.01514v1 Announce Type: cross 
Abstract: The rapid digitalization of communication systems has elevated Interactive Voice Response (IVR) technologies to become critical interfaces for customer engagement. With Artificial Intelligence (AI) now driving these platforms, ensuring secure, compliant, and ethically designed development practices is more imperative than ever. AI-powered IVRs leverage Natural Language Processing (NLP) and Machine Learning (ML) to personalize interactions, automate service delivery, and optimize user experiences. However, these innovations expose systems to heightened risks, including data privacy breaches, AI decision opacity, and model security vulnerabilities. This paper analyzes the evolution of IVRs from static code-based designs to adaptive AI-driven systems, presenting a cybersecurity-centric perspective. We propose a practical governance framework that embeds agile security principles, compliance with global data legislation, and user-centric ethics. Emphasizing privacy-by-design, adaptive risk modeling, and transparency, the paper argues that ethical AI integration is not a feature but a strategic imperative. Through this multidimensional lens, we highlight how modern IVRs can transition from communication tools to intelligent, secure, and accountable digital frontlines-resilient against emerging threats and aligned with societal expectations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.01523</link>
<guid>https://arxiv.org/abs/2505.01523</guid>
<content:encoded><![CDATA[
arXiv:2505.01523v1 Announce Type: cross 
Abstract: We propose a refined approach to efficiently fine-tune large language models (LLMs) on specific domains like the mathematical domain by employing a budgeted subset selection method. Our approach combines utility and diversity metrics to select the most informative and representative training examples. The final goal is to achieve near-full dataset performance with meticulously selected data points from the entire dataset while significantly reducing computational cost and training time and achieving competitive performance as the full dataset. The utility metric incorporates both perplexity and Chain-of-Thought (CoT) loss to identify challenging examples that contribute most to model learning, while the diversity metric ensures broad coverage across mathematical subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing against several baseline approaches, including random selection, diversity-based sampling, and existing state-of-the-art subset selection techniques.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The DCR Delusion: Measuring the Privacy Risk of Synthetic Data</title>
<link>https://arxiv.org/abs/2505.01524</link>
<guid>https://arxiv.org/abs/2505.01524</guid>
<content:encoded><![CDATA[
arXiv:2505.01524v1 Announce Type: cross 
Abstract: Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer</title>
<link>https://arxiv.org/abs/2505.01530</link>
<guid>https://arxiv.org/abs/2505.01530</guid>
<content:encoded><![CDATA[
arXiv:2505.01530v1 Announce Type: cross 
Abstract: Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning</title>
<link>https://arxiv.org/abs/2505.01531</link>
<guid>https://arxiv.org/abs/2505.01531</guid>
<content:encoded><![CDATA[
arXiv:2505.01531v1 Announce Type: cross 
Abstract: This work presents, to the best of the authors' knowledge, the first generalizable and fully data-driven adaptive framework designed to stabilize deep learning (DL) autoregressive forecasting models over long time horizons, with the goal of reducing the computational cost required in computational fluid dynamics (CFD) simulations.The proposed methodology alternates between two phases: (i) predicting the evolution of the flow field over a selected time interval using a trained DL model, and (ii) updating the model with newly generated CFD data when stability degrades, thus maintaining accurate long-term forecasting. This adaptive retraining strategy ensures robustness while avoiding the accumulation of predictive errors typical in autoregressive models. The framework is validated across three increasingly complex flow regimes, from laminar to turbulent, demonstrating from 30 \% to 95 \% reduction in computational cost without compromising physical consistency or accuracy. Its entirely data-driven nature makes it easily adaptable to a wide range of time-dependent simulation problems. The code implementing this methodology is available as open-source and it will be integrated into the upcoming release of the ModelFLOWs-app.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotions in the Loop: A Survey of Affective Computing for Emotional Support</title>
<link>https://arxiv.org/abs/2505.01542</link>
<guid>https://arxiv.org/abs/2505.01542</guid>
<content:encoded><![CDATA[
arXiv:2505.01542v1 Announce Type: cross 
Abstract: In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextures: Representations from Contexts</title>
<link>https://arxiv.org/abs/2505.01557</link>
<guid>https://arxiv.org/abs/2505.01557</guid>
<content:encoded><![CDATA[
arXiv:2505.01557v1 Announce Type: cross 
Abstract: Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
arXiv:2505.01583v1 Announce Type: cross 
Abstract: Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation</title>
<link>https://arxiv.org/abs/2505.01584</link>
<guid>https://arxiv.org/abs/2505.01584</guid>
<content:encoded><![CDATA[
arXiv:2505.01584v1 Announce Type: cross 
Abstract: Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents</title>
<link>https://arxiv.org/abs/2505.01592</link>
<guid>https://arxiv.org/abs/2505.01592</guid>
<content:encoded><![CDATA[
arXiv:2505.01592v1 Announce Type: cross 
Abstract: The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Always Tell Me The Odds: Fine-grained Conditional Probability Estimation</title>
<link>https://arxiv.org/abs/2505.01595</link>
<guid>https://arxiv.org/abs/2505.01595</guid>
<content:encoded><![CDATA[
arXiv:2505.01595v1 Announce Type: cross 
Abstract: We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</title>
<link>https://arxiv.org/abs/2505.01615</link>
<guid>https://arxiv.org/abs/2505.01615</guid>
<content:encoded><![CDATA[
arXiv:2505.01615v1 Announce Type: cross 
Abstract: We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't be lazy: CompleteP enables compute-efficient deep transformers</title>
<link>https://arxiv.org/abs/2505.01618</link>
<guid>https://arxiv.org/abs/2505.01618</guid>
<content:encoded><![CDATA[
arXiv:2505.01618v1 Announce Type: cross 
Abstract: We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the unique parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34\% compute efficiency improvements over the prior state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-based Safe Reinforcement Learning with Risk Planning</title>
<link>https://arxiv.org/abs/2505.01619</link>
<guid>https://arxiv.org/abs/2505.01619</guid>
<content:encoded><![CDATA[
arXiv:2505.01619v1 Announce Type: cross 
Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments</title>
<link>https://arxiv.org/abs/2505.01632</link>
<guid>https://arxiv.org/abs/2505.01632</guid>
<content:encoded><![CDATA[
arXiv:2505.01632v1 Announce Type: cross 
Abstract: Addressing the detrimental impact of non-stationary environmental noise on automatic speech recognition (ASR) has been a persistent and significant research focus. Despite advancements, this challenge continues to be a major concern. Recently, data-driven supervised approaches, such as deep neural networks, have emerged as promising alternatives to traditional unsupervised methods. With extensive training, these approaches have the potential to overcome the challenges posed by diverse real-life acoustic environments. In this light, this paper introduces a novel neural framework that incorporates a robust frontend into ASR systems in both clean and noisy environments. Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness of an acoustic feature set for Mel-frequency, employing the approach of transfer learning based on Residual neural network (ResNet). The experimental results demonstrate a significant improvement in recognition accuracy compared to convolutional neural networks (CNN) and long short-term memory (LSTM) networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors</title>
<link>https://arxiv.org/abs/2505.01635</link>
<guid>https://arxiv.org/abs/2505.01635</guid>
<content:encoded><![CDATA[
arXiv:2505.01635v1 Announce Type: cross 
Abstract: Although inspired by neuronal systems in the brain, artificial neural networks generally employ point-neurons, which offer far less computational complexity than their biological counterparts. Neurons have dendritic arbors that connect to different sets of synapses and offer local non-linear accumulation - playing a pivotal role in processing and learning. Inspired by this, we propose a novel neuron design based on a multi-gate ferroelectric field-effect transistor that mimics dendrites. It leverages ferroelectric nonlinearity for local computations within dendritic branches, while utilizing the transistor action to generate the final neuronal output. The branched architecture paves the way for utilizing smaller crossbar arrays in hardware integration, leading to greater efficiency. Using an experimentally calibrated device-circuit-algorithm co-simulation framework, we demonstrate that networks incorporating our dendritic neurons achieve superior performance in comparison to much larger networks without dendrites ($\sim$17$\times$ fewer trainable weight parameters). These findings suggest that dendritic hardware can significantly improve computational efficiency, and learning capacity of neuromorphic systems optimized for edge applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth</title>
<link>https://arxiv.org/abs/2505.01638</link>
<guid>https://arxiv.org/abs/2505.01638</guid>
<content:encoded><![CDATA[
arXiv:2505.01638v1 Announce Type: cross 
Abstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs) typically requires multimodal sensing - especially RGB and thermal imagery - which increases hardware cost and power consumption. This paper introduces SAM-TIFF, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using RGB input only. A multimodal teacher network trained on paired RGB-Thermal imagery and radiometric TIFF ground truth distills knowledge to a unimodal RGB student network, enabling thermal-sensor-free inference. Segmentation supervision is generated using a hybrid approach of segment anything (SAM)-guided mask generation, and selection via TOPSIS, along with Canny edge detection and Otsu's thresholding pipeline for automatic point prompt selection. Our method is the first to perform per-pixel temperature regression from RGB UAV data, demonstrating strong generalization on the recent FLAME 3 dataset. This work lays the foundation for lightweight, cost-effective UAV-based wildfire monitoring systems without thermal sensors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy</title>
<link>https://arxiv.org/abs/2505.01647</link>
<guid>https://arxiv.org/abs/2505.01647</guid>
<content:encoded><![CDATA[
arXiv:2505.01647v1 Announce Type: cross 
Abstract: Different from single-objective evolutionary algorithms, where non-elitism is an established concept, multi-objective evolutionary algorithms almost always select the next population in a greedy fashion. In the only notable exception, Bian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism for the SMS-EMOA and proved that it can speed up computing the Pareto front of the bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by a factor of $\max\{1,2^{k/4}/n\}$. While this constitutes the first proven speed-up from non-elitist selection, suggesting a very interesting research direction, it has to be noted that a true speed-up only occurs for $k \ge 4\log_2(n)$, where the runtime is super-polynomial, and that the advantage reduces for larger numbers of objectives as shown in a later work. In this work, we propose a different non-elitist selection mechanism based on aging, which exempts individuals younger than a certain age from a possible removal. This remedies the two shortcomings of stochastic selection: We prove a speed-up by a factor of $\max\{1,\Theta(k)^{k-1}\}$, regardless of the number of objectives. In particular, a positive speed-up can already be observed for constant $k$, the only setting for which polynomial runtimes can be witnessed. Overall, this result supports the use of non-elitist selection schemes, but suggests that aging-based mechanisms can be considerably more powerful than stochastic selection mechanisms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Fair Node Classification on Non-IID Graph Data</title>
<link>https://arxiv.org/abs/2505.01652</link>
<guid>https://arxiv.org/abs/2505.01652</guid>
<content:encoded><![CDATA[
arXiv:2505.01652v1 Announce Type: cross 
Abstract: Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.01664</link>
<guid>https://arxiv.org/abs/2505.01664</guid>
<content:encoded><![CDATA[
arXiv:2505.01664v1 Announce Type: cross 
Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study</title>
<link>https://arxiv.org/abs/2505.01680</link>
<guid>https://arxiv.org/abs/2505.01680</guid>
<content:encoded><![CDATA[
arXiv:2505.01680v1 Announce Type: cross 
Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. We propose an automated ARAT scoring system integrating multimodal video analysis with SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object locations. Our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. Hierarchical Bayesian Models (HBMs) infer movement quality components, enhancing interpretability. A clinician dashboard displays task scores, execution times, and quality assessments. We conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. Evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with HBMs aligning closely with manual assessments. This work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Aware CLIP Few-Shot Learning</title>
<link>https://arxiv.org/abs/2505.01694</link>
<guid>https://arxiv.org/abs/2505.01694</guid>
<content:encoded><![CDATA[
arXiv:2505.01694v1 Announce Type: cross 
Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking</title>
<link>https://arxiv.org/abs/2505.01696</link>
<guid>https://arxiv.org/abs/2505.01696</guid>
<content:encoded><![CDATA[
arXiv:2505.01696v1 Announce Type: cross 
Abstract: Integrating heterogeneous biomedical data including imaging, omics, and clinical records supports accurate diagnosis and personalised care. Graph-based models fuse such non-Euclidean data by capturing spatial and relational structure, yet clinical uptake requires regulator-ready interpretability. We present the first technical survey of interpretable graph based models for multimodal biomedical data, covering 26 studies published between Jan 2019 and Sep 2024. Most target disease classification, notably cancer and rely on static graphs from simple similarity measures, while graph-native explainers are rare; post-hoc methods adapted from non-graph domains such as gradient saliency, and SHAP predominate. We group existing approaches into four interpretability families, outline trends such as graph-in-graph hierarchies, knowledge-graph edges, and dynamic topology learning, and perform a practical benchmark. Using an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient Saliency and Graph Masking surface complementary metabolic and transport signatures. Permutation tests show all four beat random gene sets, but with distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher compute cost, while Gradient Saliency and Sensitivity Analysis are quicker though coarser. We also provide a step-by-step flowchart covering graph construction, explainer choice and resource budgeting to help researchers balance transparency and performance. This review synthesises the state of interpretable graph learning for multimodal medicine, benchmarks leading techniques, and charts future directions, from advanced XAI tools to under-studied diseases, serving as a concise reference for method developers and translational scientists.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning</title>
<link>https://arxiv.org/abs/2505.01699</link>
<guid>https://arxiv.org/abs/2505.01699</guid>
<content:encoded><![CDATA[
arXiv:2505.01699v1 Announce Type: cross 
Abstract: The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \textbf{B}ayesian \textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.01709</link>
<guid>https://arxiv.org/abs/2505.01709</guid>
<content:encoded><![CDATA[
arXiv:2505.01709v1 Announce Type: cross 
Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation</title>
<link>https://arxiv.org/abs/2505.01730</link>
<guid>https://arxiv.org/abs/2505.01730</guid>
<content:encoded><![CDATA[
arXiv:2505.01730v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\approx$74\% on ImageNet with a 64$\times$ reduction in the number of inference timesteps compared to existing approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.01731</link>
<guid>https://arxiv.org/abs/2505.01731</guid>
<content:encoded><![CDATA[
arXiv:2505.01731v1 Announce Type: cross 
Abstract: Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the \underline{S}hapley \underline{V}alue-based \underline{N}on-\underline{U}niform \underline{P}runing (\methodname{}) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, \methodname{} achieves a reduction in perplexity (PPL) of 18.01\% and 19.55\% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70\% sparsity.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems</title>
<link>https://arxiv.org/abs/2505.01736</link>
<guid>https://arxiv.org/abs/2505.01736</guid>
<content:encoded><![CDATA[
arXiv:2505.01736v1 Announce Type: cross 
Abstract: Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
<link>https://arxiv.org/abs/2505.01743</link>
<guid>https://arxiv.org/abs/2505.01743</guid>
<content:encoded><![CDATA[
arXiv:2505.01743v1 Announce Type: cross 
Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach</title>
<link>https://arxiv.org/abs/2505.01780</link>
<guid>https://arxiv.org/abs/2505.01780</guid>
<content:encoded><![CDATA[
arXiv:2505.01780v1 Announce Type: cross 
Abstract: In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction</title>
<link>https://arxiv.org/abs/2505.01781</link>
<guid>https://arxiv.org/abs/2505.01781</guid>
<content:encoded><![CDATA[
arXiv:2505.01781v1 Announce Type: cross 
Abstract: The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments</title>
<link>https://arxiv.org/abs/2505.01794</link>
<guid>https://arxiv.org/abs/2505.01794</guid>
<content:encoded><![CDATA[
arXiv:2505.01794v1 Announce Type: cross 
Abstract: In the rapidly evolving educational landscape, the unbiased assessment of soft skills is a significant challenge, particularly in higher education. This paper presents a fuzzy logic approach that employs a Granular Linguistic Model of Phenomena integrated with multimodal analysis to evaluate soft skills in undergraduate students. By leveraging computational perceptions, this approach enables a structured breakdown of complex soft skill expressions, capturing nuanced behaviours with high granularity and addressing their inherent uncertainties, thereby enhancing interpretability and reliability. Experiments were conducted with undergraduate students using a developed tool that assesses soft skills such as decision-making, communication, and creativity. This tool identifies and quantifies subtle aspects of human interaction, such as facial expressions and gesture recognition. The findings reveal that the framework effectively consolidates multiple data inputs to produce meaningful and consistent assessments of soft skills, showing that integrating multiple modalities into the evaluation process significantly improves the quality of soft skills scores, making the assessment work transparent and understandable to educational stakeholders.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis</title>
<link>https://arxiv.org/abs/2505.01800</link>
<guid>https://arxiv.org/abs/2505.01800</guid>
<content:encoded><![CDATA[
arXiv:2505.01800v1 Announce Type: cross 
Abstract: The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
arXiv:2505.01812v1 Announce Type: cross 
Abstract: Humans and intelligent animals can effortlessly internalize new information ("news") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\textit{contexual shadowing effect}$, where training with the news $\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
<link>https://arxiv.org/abs/2505.01821</link>
<guid>https://arxiv.org/abs/2505.01821</guid>
<content:encoded><![CDATA[
arXiv:2505.01821v1 Announce Type: cross 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01822</link>
<guid>https://arxiv.org/abs/2505.01822</guid>
<content:encoded><![CDATA[
arXiv:2505.01822v1 Announce Type: cross 
Abstract: Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach</title>
<link>https://arxiv.org/abs/2505.01823</link>
<guid>https://arxiv.org/abs/2505.01823</guid>
<content:encoded><![CDATA[
arXiv:2505.01823v1 Announce Type: cross 
Abstract: Collecting large-scale crop disease images in the field is labor-intensive and time-consuming. Generative models (GMs) offer an alternative by creating synthetic samples that resemble real-world images. However, existing research primarily relies on Generative Adversarial Networks (GANs)-based image-to-image translation and lack a comprehensive analysis of computational requirements in agriculture. Therefore, this research explores a multi-modal text-to-image approach for generating synthetic crop disease images and is the first to provide computational benchmarking in this context. We trained three Stable Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning techniques to enhance generalization. SD3.5M outperformed the others, with an average memory usage of 18 GB, power consumption of 180 W, and total energy use of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results demonstrate SD3.5M's ability to generate 500 synthetic images from just 36 in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease data generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2</title>
<link>https://arxiv.org/abs/2505.01854</link>
<guid>https://arxiv.org/abs/2505.01854</guid>
<content:encoded><![CDATA[
arXiv:2505.01854v1 Announce Type: cross 
Abstract: Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intra-Layer Recurrence in Transformers for Language Modeling</title>
<link>https://arxiv.org/abs/2505.01855</link>
<guid>https://arxiv.org/abs/2505.01855</guid>
<content:encoded><![CDATA[
arXiv:2505.01855v1 Announce Type: cross 
Abstract: Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans can learn to detect AI-generated texts, or at least learn when they can't</title>
<link>https://arxiv.org/abs/2505.01877</link>
<guid>https://arxiv.org/abs/2505.01877</guid>
<content:encoded><![CDATA[
arXiv:2505.01877v1 Announce Type: cross 
Abstract: This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.
  We used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications</title>
<link>https://arxiv.org/abs/2505.01881</link>
<guid>https://arxiv.org/abs/2505.01881</guid>
<content:encoded><![CDATA[
arXiv:2505.01881v1 Announce Type: cross 
Abstract: Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images</title>
<link>https://arxiv.org/abs/2505.01884</link>
<guid>https://arxiv.org/abs/2505.01884</guid>
<content:encoded><![CDATA[
arXiv:2505.01884v1 Announce Type: cross 
Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - https://github.com/GVCL/IWSeg-SAR-Poison.git)
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
arXiv:2505.01892v1 Announce Type: cross 
Abstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LookAlike: Consistent Distractor Generation in Math MCQs</title>
<link>https://arxiv.org/abs/2505.01903</link>
<guid>https://arxiv.org/abs/2505.01903</guid>
<content:encoded><![CDATA[
arXiv:2505.01903v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.01912</link>
<guid>https://arxiv.org/abs/2505.01912</guid>
<content:encoded><![CDATA[
arXiv:2505.01912v1 Announce Type: cross 
Abstract: Advances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations. Although the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for $\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecular property predictions -- a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics</title>
<link>https://arxiv.org/abs/2505.01931</link>
<guid>https://arxiv.org/abs/2505.01931</guid>
<content:encoded><![CDATA[
arXiv:2505.01931v1 Announce Type: cross 
Abstract: Classical robot navigation often relies on hardcoded state machines and purely geometric path planners, limiting a robot's ability to interpret high-level semantic instructions. In this paper, we first assess GPT-4's ability to act as a path planner compared to the A* algorithm, then present a hybrid planning framework that integrates GPT-4's semantic reasoning with A* on a low-cost robot platform operating on ROS2 Humble. Our approach eliminates explicit finite state machine (FSM) coding by using prompt-based GPT-4 reasoning to handle task logic while maintaining the accurate paths computed by A*. The GPT-4 module provides semantic understanding of instructions and environmental cues (e.g., recognizing toxic obstacles or crowded areas to avoid, or understanding low-battery situations requiring alternate route selection), and dynamically adjusts the robot's occupancy grid via obstacle buffering to enforce semantic constraints. We demonstrate multi-step reasoning for sequential tasks, such as first navigating to a resource goal and then reaching a final destination safely. Experiments on a Petoi Bittle robot with an overhead camera and Raspberry Pi Zero 2W compare classical A* against GPT-4-assisted planning. Results show that while A* is faster and more accurate for basic route generation and obstacle avoidance, the GPT-4-integrated system achieves high success rates (96-100%) on semantic tasks that are infeasible for pure geometric planners. This work highlights how affordable robots can exhibit intelligent, context-aware behaviors by leveraging large language model reasoning with minimal hardware and no fine-tuning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability by design: an experimental analysis of the legal coding process</title>
<link>https://arxiv.org/abs/2505.01944</link>
<guid>https://arxiv.org/abs/2505.01944</guid>
<content:encoded><![CDATA[
arXiv:2505.01944v1 Announce Type: cross 
Abstract: Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.
  In this paper we deliver a methodology for \textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.
  Finally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \textit{depth} that refers to the length of the paths of legal references.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scale Graph Learning for Anti-Sparse Downscaling</title>
<link>https://arxiv.org/abs/2505.01948</link>
<guid>https://arxiv.org/abs/2505.01948</guid>
<content:encoded><![CDATA[
arXiv:2505.01948v1 Announce Type: cross 
Abstract: Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that scale.To address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Any RGB-Thermal Model with Language-aided Distillation</title>
<link>https://arxiv.org/abs/2505.01950</link>
<guid>https://arxiv.org/abs/2505.01950</guid>
<content:encoded><![CDATA[
arXiv:2505.01950v1 Announce Type: cross 
Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment</title>
<link>https://arxiv.org/abs/2505.01956</link>
<guid>https://arxiv.org/abs/2505.01956</guid>
<content:encoded><![CDATA[
arXiv:2505.01956v1 Announce Type: cross 
Abstract: In battlefield environments, adversaries frequently disrupt GPS signals, requiring alternative localization and navigation methods. Traditional vision-based approaches like Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) involve complex sensor fusion and high computational demand, whereas range-free methods like DV-HOP face accuracy and stability challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a navigation approach using landmark-based localization (LanBLoc) combined with a battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its performance is benchmarked against three state-of-the-art visual localization algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and real-imitated trajectory datasets using metrics including Average Displacement Error (ADE), Final Displacement Error (FDE), and a newly introduced Average Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm for obstacle avoidance and risk exposure minimization. Simulation results in battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk exposure, and trajectory efficiency, while SafeNav-CHull provides superior computational speed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites</title>
<link>https://arxiv.org/abs/2505.01966</link>
<guid>https://arxiv.org/abs/2505.01966</guid>
<content:encoded><![CDATA[
arXiv:2505.01966v1 Announce Type: cross 
Abstract: Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview</title>
<link>https://arxiv.org/abs/2505.01967</link>
<guid>https://arxiv.org/abs/2505.01967</guid>
<content:encoded><![CDATA[
arXiv:2505.01967v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or "worldviews". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach</title>
<link>https://arxiv.org/abs/2505.01997</link>
<guid>https://arxiv.org/abs/2505.01997</guid>
<content:encoded><![CDATA[
arXiv:2505.01997v1 Announce Type: cross 
Abstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2505.01998</link>
<guid>https://arxiv.org/abs/2505.01998</guid>
<content:encoded><![CDATA[
arXiv:2505.01998v1 Announce Type: cross 
Abstract: This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations-covering far-field localization, weak signal detection, and multilingual speech recognition-demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting</title>
<link>https://arxiv.org/abs/2505.02011</link>
<guid>https://arxiv.org/abs/2505.02011</guid>
<content:encoded><![CDATA[
arXiv:2505.02011v1 Announce Type: cross 
Abstract: Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis. In addition, the implementation of Transformer variants has improved prediction accuracy. Following these variants, different input data process approaches also enhanced the field, such as tokenization techniques including point-wise, channel-wise, and patch-wise tokenization. However, previous studies still have limitations in time complexity, computational resources, and cross-dimensional interactions. To address these limitations, we introduce a novel CNN Autoencoder-based Score Attention mechanism (CASA), which can be introduced in diverse Transformers model-agnosticically by reducing memory and leading to improvement in model performance. Experiments on eight real-world datasets validate that CASA decreases computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wide &amp; Deep Learning for Node Classification</title>
<link>https://arxiv.org/abs/2505.02020</link>
<guid>https://arxiv.org/abs/2505.02020</guid>
<content:encoded><![CDATA[
arXiv:2505.02020v1 Announce Type: cross 
Abstract: Wide & Deep, a simple yet effective learning architecture for recommendation systems developed by Google, has had a significant impact in both academia and industry due to its combination of the memorization ability of generalized linear models and the generalization ability of deep models. Graph convolutional networks (GCNs) remain dominant in node classification tasks; however, recent studies have highlighted issues such as heterophily and expressiveness, which focus on graph structure while seemingly neglecting the potential role of node features. In this paper, we propose a flexible framework GCNIII, which leverages the Wide & Deep architecture and incorporates three techniques: Intersect memory, Initial residual and Identity mapping. We provide comprehensive empirical evidence showing that GCNIII can more effectively balance the trade-off between over-fitting and over-generalization on various semi- and full- supervised tasks. Additionally, we explore the use of large language models (LLMs) for node feature engineering to enhance the performance of GCNIII in cross-domain node classification tasks. Our implementation is available at https://github.com/CYCUCAS/GCNIII.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2505.02027</link>
<guid>https://arxiv.org/abs/2505.02027</guid>
<content:encoded><![CDATA[
arXiv:2505.02027v1 Announce Type: cross 
Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regression s all you need for medical image translation</title>
<link>https://arxiv.org/abs/2505.02048</link>
<guid>https://arxiv.org/abs/2505.02048</guid>
<content:encoded><![CDATA[
arXiv:2505.02048v1 Announce Type: cross 
Abstract: The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction</title>
<link>https://arxiv.org/abs/2505.02072</link>
<guid>https://arxiv.org/abs/2505.02072</guid>
<content:encoded><![CDATA[
arXiv:2505.02072v1 Announce Type: cross 
Abstract: The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Defense Against Adversarial Attacks in Time Series Classification</title>
<link>https://arxiv.org/abs/2505.02073</link>
<guid>https://arxiv.org/abs/2505.02073</guid>
<content:encoded><![CDATA[
arXiv:2505.02073v1 Announce Type: cross 
Abstract: As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.02075</link>
<guid>https://arxiv.org/abs/2505.02075</guid>
<content:encoded><![CDATA[
arXiv:2505.02075v1 Announce Type: cross 
Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents</title>
<link>https://arxiv.org/abs/2505.02077</link>
<guid>https://arxiv.org/abs/2505.02077</guid>
<content:encoded><![CDATA[
arXiv:2505.02077v1 Announce Type: cross 
Abstract: Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning</title>
<link>https://arxiv.org/abs/2505.02078</link>
<guid>https://arxiv.org/abs/2505.02078</guid>
<content:encoded><![CDATA[
arXiv:2505.02078v1 Announce Type: cross 
Abstract: Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems</title>
<link>https://arxiv.org/abs/2505.02120</link>
<guid>https://arxiv.org/abs/2505.02120</guid>
<content:encoded><![CDATA[
arXiv:2505.02120v1 Announce Type: cross 
Abstract: Online platforms aggregate extensive user feedback across diverse behaviors, providing a rich source for enhancing user engagement. Traditional recommender systems, however, typically optimize for a single target behavior and represent user preferences with a single vector, limiting their ability to handle multiple important behaviors or optimization objectives. This conventional approach also struggles to capture the full spectrum of user interests, resulting in a narrow item pool during candidate generation. To address these limitations, we present Tricolore, a versatile multi-vector learning framework that uncovers connections between different behavior types for more robust candidate generation. Tricolore's adaptive multi-task structure is also customizable to specific platform needs. To manage the variability in sparsity across behavior types, we incorporate a behavior-wise multi-view fusion module that dynamically enhances learning. Moreover, a popularity-balanced strategy ensures the recommendation list balances accuracy with item popularity, fostering diversity and improving overall performance. Extensive experiments on public datasets demonstrate Tricolore's effectiveness across various recommendation scenarios, from short video platforms to e-commerce. By leveraging a shared base embedding strategy, Tricolore also significantly improves the performance for cold-start users. The source code is publicly available at: https://github.com/abnering/Tricolore.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode</title>
<link>https://arxiv.org/abs/2505.02129</link>
<guid>https://arxiv.org/abs/2505.02129</guid>
<content:encoded><![CDATA[
arXiv:2505.02129v1 Announce Type: cross 
Abstract: Organizing resources in a multidimensional classification space is an approach to efficiently managing and querying large-scale resources. This paper defines an aggregation query on subspace defined by a range on the partial order on coordinate tree at each dimension, where each point contains resources aggregated along the paths of partial order relations on the points so that aggregated resources at each point within the subspace can be measured, ranked and selected. To efficiently locate non-empty points in a large subspace, an approach to generating graph index is proposed to build inclusion links with partial order relations on coordinates of dimensions to enable a subspace query to reach non-empty points by following indexing links and aggregate resources along indexing paths back to their super points. Generating such an index is costly as the number of children of an index node can be very large so that the total number of indexing nodes is unbounded. The proposed approach adopts the following strategies to reduce the cost: (1) adding intersection links between two indexing nodes, which can better reduce query processing costs while controlling the number of nodes of the graph index; (2) intersection links are added between two nodes according to the probabilistic distribution calculated for estimating the costs of adding intersection between two nodes; (3) coordinates at one dimension having more resources are split by coordinates at another dimension to balance the number of resources hold by indexing nodes; and, (4) short-cut links are added between sibling coordinates of coordinate trees to make an efficient query on linear order coordinates. Analysis and experiments verified the effectiveness of the generated index in supporting subspace aggregation query. This work makes significant contributions to the development of data model based on multi-dimensional classification.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
<link>https://arxiv.org/abs/2505.02139</link>
<guid>https://arxiv.org/abs/2505.02139</guid>
<content:encoded><![CDATA[
arXiv:2505.02139v1 Announce Type: cross 
Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions</title>
<link>https://arxiv.org/abs/2505.02154</link>
<guid>https://arxiv.org/abs/2505.02154</guid>
<content:encoded><![CDATA[
arXiv:2505.02154v1 Announce Type: cross 
Abstract: This reproducibility study analyzes and extends the paper "Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models," which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at https://github.com/OliverSavolainen/axiomatic-ir-reproduce.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
arXiv:2505.02156v1 Announce Type: cross 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pickup &amp; Delivery with Time Windows and Transfers: combining decomposition with metaheuristics</title>
<link>https://arxiv.org/abs/2505.02158</link>
<guid>https://arxiv.org/abs/2505.02158</guid>
<content:encoded><![CDATA[
arXiv:2505.02158v1 Announce Type: cross 
Abstract: This paper examines the generalisation of the Pickup and Delivery Problem that allows mid-route load exchanges among vehicles and obeys strict time-windows at all locations. We propose a novel Logic-Based Benders Decomposition (LBBD) that improves optimality gaps for all benchmarks in the literature and scales up to handle larger ones. To tackle even larger instances, we introduce a refined Large Neighborhood Search (LNS) algorithm that improves the adaptability of LNS beyond case-specific configurations appearing in related literature.
  To bridge the gap in benchmark availability, we develop an instance generator that allows for extensive experimentation. For moderate datasets (25 and 50 requests), we evaluate the performance of both LBBD and LNS, the former being able to close the gap and the latter capable of providing near-optimal solutions. For larger instances (75 and 100 requests), we recreate indicative state-of-the-art metaheuristics to highlight the improvements introduced by our LNS refinements, while establishing its scalability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
arXiv:2505.02170v1 Announce Type: cross 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking</title>
<link>https://arxiv.org/abs/2505.02171</link>
<guid>https://arxiv.org/abs/2505.02171</guid>
<content:encoded><![CDATA[
arXiv:2505.02171v1 Announce Type: cross 
Abstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</title>
<link>https://arxiv.org/abs/2505.02192</link>
<guid>https://arxiv.org/abs/2505.02192</guid>
<content:encoded><![CDATA[
arXiv:2505.02192v1 Announce Type: cross 
Abstract: Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Student Perspectives on the Benefits and Risks of AI in Education</title>
<link>https://arxiv.org/abs/2505.02198</link>
<guid>https://arxiv.org/abs/2505.02198</guid>
<content:encoded><![CDATA[
arXiv:2505.02198v1 Announce Type: cross 
Abstract: The use of chatbots equipped with artificial intelligence (AI) in educational settings has increased in recent years, showing potential to support teaching and learning. However, the adoption of these technologies has raised concerns about their impact on academic integrity, students' ability to problem-solve independently, and potential underlying biases. To better understand students' perspectives and experiences with these tools, a survey was conducted at a large public university in the United States. Through thematic analysis, 262 undergraduate students' responses regarding their perceived benefits and risks of AI chatbots in education were identified and categorized into themes.
  The results discuss several benefits identified by the students, with feedback and study support, instruction capabilities, and access to information being the most cited. Their primary concerns included risks to academic integrity, accuracy of information, loss of critical thinking skills, the potential development of overreliance, and ethical considerations such as data privacy, system bias, environmental impact, and preservation of human elements in education.
  While student perceptions align with previously discussed benefits and risks of AI in education, they show heightened concerns about distinguishing between human and AI generated work - particularly in cases where authentic work is flagged as AI-generated. To address students' concerns, institutions can establish clear policies regarding AI use and develop curriculum around AI literacy. With these in place, practitioners can effectively develop and implement educational systems that leverage AI's potential in areas such as immediate feedback and personalized learning support. This approach can enhance the quality of students' educational experiences while preserving the integrity of the learning process with AI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units</title>
<link>https://arxiv.org/abs/2505.02206</link>
<guid>https://arxiv.org/abs/2505.02206</guid>
<content:encoded><![CDATA[
arXiv:2505.02206v1 Announce Type: cross 
Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning</title>
<link>https://arxiv.org/abs/2505.02228</link>
<guid>https://arxiv.org/abs/2505.02228</guid>
<content:encoded><![CDATA[
arXiv:2505.02228v1 Announce Type: cross 
Abstract: Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The GenAI Generation: Student Views of Awareness, Preparedness, and Concern</title>
<link>https://arxiv.org/abs/2505.02230</link>
<guid>https://arxiv.org/abs/2505.02230</guid>
<content:encoded><![CDATA[
arXiv:2505.02230v1 Announce Type: cross 
Abstract: Generative AI (GenAI) is revolutionizing education and workforce development, profoundly shaping how students learn, engage, and prepare for their future. Outpacing the development of uniform policies and structures, GenAI has heralded a unique era and given rise to the GenAI Generation: a cohort of students whose education has been increasingly shaped by the opportunities and challenges GenAI presents during its widespread adoption within society. This study examines our students' perceptions of GenAI through a concise survey with optional open-ended questions, focusing on their awareness, preparedness, and concerns. Evaluation of more than 250 responses with more than 40% providing detailed qualitative feedback reveals a core dual sentiment: while most students express enthusiasm for GenAI, an even greater proportion voice a spectrum of concerns about ethics, job displacement, and the adequacy of educational structures given the highly transformative technology. These findings offer critical insights into how students view the potential and pitfalls of GenAI for future career impacts, with accompanying recommendations to guide educational institutions in navigating a future driven by GenAI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning</title>
<link>https://arxiv.org/abs/2505.02232</link>
<guid>https://arxiv.org/abs/2505.02232</guid>
<content:encoded><![CDATA[
arXiv:2505.02232v1 Announce Type: cross 
Abstract: Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation</title>
<link>https://arxiv.org/abs/2505.02235</link>
<guid>https://arxiv.org/abs/2505.02235</guid>
<content:encoded><![CDATA[
arXiv:2505.02235v1 Announce Type: cross 
Abstract: Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Physical Object State Representation in Text-to-Image Generative Systems</title>
<link>https://arxiv.org/abs/2505.02236</link>
<guid>https://arxiv.org/abs/2505.02236</guid>
<content:encoded><![CDATA[
arXiv:2505.02236v1 Announce Type: cross 
Abstract: Current text-to-image generative models struggle to accurately represent object states (e.g., "a table without a bottle," "an empty tumbler"). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation</title>
<link>https://arxiv.org/abs/2505.02247</link>
<guid>https://arxiv.org/abs/2505.02247</guid>
<content:encoded><![CDATA[
arXiv:2505.02247v1 Announce Type: cross 
Abstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset</title>
<link>https://arxiv.org/abs/2505.02255</link>
<guid>https://arxiv.org/abs/2505.02255</guid>
<content:encoded><![CDATA[
arXiv:2505.02255v1 Announce Type: cross 
Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of image generation with diffusion models. We hypothesize that differences between distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are consistent and, therefore, learnable within a specialized domain, like portrait generation. We generate a synthetic paired dataset and train a fast image-to-image translation head. Using two sets of low- and high-quality synthetic images, our model is trained to refine the output of a distilled generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like FLUX.1-dev, which is more computationally intensive. Our results show that the pipeline, which combines a distilled version of a large generative model with our enhancement layer, delivers similar photorealistic portraits to the baseline version with up to an 82% decrease in computational cost compared to FLUX.1-dev. This study demonstrates the potential for improving the efficiency of AI solutions involving large-scale image generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Transformer Embeddings</title>
<link>https://arxiv.org/abs/2505.02266</link>
<guid>https://arxiv.org/abs/2505.02266</guid>
<content:encoded><![CDATA[
arXiv:2505.02266v1 Announce Type: cross 
Abstract: Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Localization, Mapping, and Navigation for Quadruped Robots</title>
<link>https://arxiv.org/abs/2505.02272</link>
<guid>https://arxiv.org/abs/2505.02272</guid>
<content:encoded><![CDATA[
arXiv:2505.02272v1 Announce Type: cross 
Abstract: Quadruped robots are currently a widespread platform for robotics research, thanks to powerful Reinforcement Learning controllers and the availability of cheap and robust commercial platforms. However, to broaden the adoption of the technology in the real world, we require robust navigation stacks relying only on low-cost sensors such as depth cameras. This paper presents a first step towards a robust localization, mapping, and navigation system for low-cost quadruped robots. In pursuit of this objective we combine contact-aided kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing stability and accuracy of the system. Our results in simulation and two different real-world quadruped platforms show that our system can generate an accurate 2D map of the environment, robustly localize itself, and navigate autonomously. Furthermore, we present in-depth ablation studies of the important components of the system and their impact on localization accuracy. Videos, code, and additional experiments can be found on the project website: https://sites.google.com/view/low-cost-quadruped-slam
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.02274</link>
<guid>https://arxiv.org/abs/2505.02274</guid>
<content:encoded><![CDATA[
arXiv:2505.02274v1 Announce Type: cross 
Abstract: Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles</title>
<link>https://arxiv.org/abs/2505.02281</link>
<guid>https://arxiv.org/abs/2505.02281</guid>
<content:encoded><![CDATA[
arXiv:2505.02281v1 Announce Type: cross 
Abstract: This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation Theorem of Deep Q-Networks</title>
<link>https://arxiv.org/abs/2505.02288</link>
<guid>https://arxiv.org/abs/2505.02288</guid>
<content:encoded><![CDATA[
arXiv:2505.02288v1 Announce Type: cross 
Abstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.02299</link>
<guid>https://arxiv.org/abs/2505.02299</guid>
<content:encoded><![CDATA[
arXiv:2505.02299v1 Announce Type: cross 
Abstract: Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques</title>
<link>https://arxiv.org/abs/2505.02309</link>
<guid>https://arxiv.org/abs/2505.02309</guid>
<content:encoded><![CDATA[
arXiv:2505.02309v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Is AI Safety? What Do We Want It to Be?</title>
<link>https://arxiv.org/abs/2505.02313</link>
<guid>https://arxiv.org/abs/2505.02313</guid>
<content:encoded><![CDATA[
arXiv:2505.02313v1 Announce Type: cross 
Abstract: The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account The Safety Conception of AI safety. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities</title>
<link>https://arxiv.org/abs/2505.02314</link>
<guid>https://arxiv.org/abs/2505.02314</guid>
<content:encoded><![CDATA[
arXiv:2505.02314v1 Announce Type: cross 
Abstract: The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at https://github.com/neurosim/NeuroSim.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Robustness in Discrete Time Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.02347</link>
<guid>https://arxiv.org/abs/2505.02347</guid>
<content:encoded><![CDATA[
arXiv:2505.02347v1 Announce Type: cross 
Abstract: Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Biases in Knowledge Representations of Wikidata separates Global North from Global South</title>
<link>https://arxiv.org/abs/2505.02352</link>
<guid>https://arxiv.org/abs/2505.02352</guid>
<content:encoded><![CDATA[
arXiv:2505.02352v1 Announce Type: cross 
Abstract: Knowledge Graphs have become increasingly popular due to their wide usage in various downstream applications, including information retrieval, chatbot development, language model construction, and many others. Link prediction (LP) is a crucial downstream task for knowledge graphs, as it helps to address the problem of the incompleteness of the knowledge graphs. However, previous research has shown that knowledge graphs, often created in a (semi) automatic manner, are not free from social biases. These biases can have harmful effects on downstream applications, especially by leading to unfair behavior toward minority groups. To understand this issue in detail, we develop a framework -- AuditLP -- deploying fairness metrics to identify biased outcomes in LP, specifically how occupations are classified as either male or female-dominated based on gender as a sensitive attribute. We have experimented with the sensitive attribute of age and observed that occupations are categorized as young-biased, old-biased, and age-neutral. We conduct our experiments on a large number of knowledge triples that belong to 21 different geographies extracted from the open-sourced knowledge graph, Wikidata. Our study shows that the variance in the biased outcomes across geographies neatly mirrors the socio-economic and cultural division of the world, resulting in a transparent partition of the Global North from the Global South.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training</title>
<link>https://arxiv.org/abs/2505.02360</link>
<guid>https://arxiv.org/abs/2505.02360</guid>
<content:encoded><![CDATA[
arXiv:2505.02360v1 Announce Type: cross 
Abstract: Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO.
  Our study is motivated by the empirical observation that CO is more prevalent under the $l^{\infty}$ norm than the $l^2$ norm. Leveraging this insight, we develop a framework for generalized $l^p$ attack as a fixed point problem and craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to $l^{\infty}$. This leads to our core insight: CO emerges when highly concentrated gradients where information localizes in few dimensions interact with aggressive norm constraints. By quantifying gradient concentration through Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM that automatically tunes the training norm based on gradient information. Extensive experiments demonstrate that this approach achieves strong robustness without requiring additional regularization or noise injection, providing a novel and theoretically-principled pathway to mitigate the CO problem.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2505.02362</link>
<guid>https://arxiv.org/abs/2505.02362</guid>
<content:encoded><![CDATA[
arXiv:2505.02362v1 Announce Type: cross 
Abstract: Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
<link>https://arxiv.org/abs/2505.02366</link>
<guid>https://arxiv.org/abs/2505.02366</guid>
<content:encoded><![CDATA[
arXiv:2505.02366v1 Announce Type: cross 
Abstract: Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks</title>
<link>https://arxiv.org/abs/2505.02369</link>
<guid>https://arxiv.org/abs/2505.02369</guid>
<content:encoded><![CDATA[
arXiv:2505.02369v1 Announce Type: cross 
Abstract: Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
arXiv:2505.02370v1 Announce Type: cross 
Abstract: Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-R1: Reward Modeling as Reasoning</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
arXiv:2505.02387v1 Announce Type: cross 
Abstract: Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</title>
<link>https://arxiv.org/abs/2505.02388</link>
<guid>https://arxiv.org/abs/2505.02388</guid>
<content:encoded><![CDATA[
arXiv:2505.02388v1 Announce Type: cross 
Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Analysis of Performance Drop in DeepSeek Model Quantization</title>
<link>https://arxiv.org/abs/2505.02390</link>
<guid>https://arxiv.org/abs/2505.02390</guid>
<content:encoded><![CDATA[
arXiv:2505.02390v1 Announce Type: cross 
Abstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\_K\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
arXiv:2505.02391v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch</title>
<link>https://arxiv.org/abs/2505.02396</link>
<guid>https://arxiv.org/abs/2505.02396</guid>
<content:encoded><![CDATA[
arXiv:2505.02396v1 Announce Type: cross 
Abstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. This uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest X-rays, and the presence of co-existing respiratory conditions. This research proposes one of the supervised learning methods, CNN. Using MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using Keras API as the built from scratch model, for identifying lung diseases especially pneumonia. The datasets used in this research were obtained from the website through Kaggle. The result shows that by implementing CNN MobileNetV2 and CNN from scratch the result is promising. While validating data, MobileNetV2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is more stable. Although it takes more time to train each epoch. Meanwhile, after the 10th epoch, the Scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. With these results, ResNet101V2 offers stability, and the Scratch model offers high accuracy.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bielik 11B v2 Technical Report</title>
<link>https://arxiv.org/abs/2505.02410</link>
<guid>https://arxiv.org/abs/2505.02410</guid>
<content:encoded><![CDATA[
arXiv:2505.02410v1 Announce Type: cross 
Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02417</link>
<guid>https://arxiv.org/abs/2505.02417</guid>
<content:encoded><![CDATA[
arXiv:2505.02417v1 Announce Type: cross 
Abstract: Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards One-shot Federated Learning: Advances, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2505.02426</link>
<guid>https://arxiv.org/abs/2505.02426</guid>
<content:encoded><![CDATA[
arXiv:2505.02426v1 Announce Type: cross 
Abstract: One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairPO: Robust Preference Optimization for Fair Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.02433</link>
<guid>https://arxiv.org/abs/2505.02433</guid>
<content:encoded><![CDATA[
arXiv:2505.02433v1 Announce Type: cross 
Abstract: We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability</title>
<link>https://arxiv.org/abs/2505.02435</link>
<guid>https://arxiv.org/abs/2505.02435</guid>
<content:encoded><![CDATA[
arXiv:2505.02435v1 Announce Type: cross 
Abstract: Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging</title>
<link>https://arxiv.org/abs/2505.02467</link>
<guid>https://arxiv.org/abs/2505.02467</guid>
<content:encoded><![CDATA[
arXiv:2505.02467v1 Announce Type: cross 
Abstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI sequences, to enhance diagnostic accuracy in medical imaging. A key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. Current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. We propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. At each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. This process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. The approach is validated on two multimodal MRI datasets, each addressing different classification tasks. Our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. These architectures demonstrated superior accuracy, F-score, and specificity while maintaining competitive or improved AUC values. Furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. By systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. It provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical AI applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning</title>
<link>https://arxiv.org/abs/2505.02483</link>
<guid>https://arxiv.org/abs/2505.02483</guid>
<content:encoded><![CDATA[
arXiv:2505.02483v1 Announce Type: cross 
Abstract: Enabling a high-degree-of-freedom robot to learn specific skills is a challenging task due to the complexity of robotic dynamics. Reinforcement learning (RL) has emerged as a promising solution; however, addressing such problems requires the design of multiple reward functions to account for various constraints in robotic motion. Existing approaches typically sum all reward components indiscriminately to optimize the RL value function and policy. We argue that this uniform inclusion of all reward components in policy optimization is inefficient and limits the robot's learning performance. To address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework based on Large Language Models (LLMs). This paradigm dynamically adjusts the learning intensity of each reward component throughout the policy optimization process, enabling robots to acquire skills in a gradual and structured manner. Specifically, we design a multi-branch value network, where each branch corresponds to a distinct reward component. During policy optimization, each branch is assigned a weight that reflects its importance, and these weights are automatically computed based on rules designed by LLMs. The LLM generates a rule set in advance, derived from the task description, and during training, it selects a weight calculation rule from the library based on language prompts that evaluate the performance of each branch. Experimental results demonstrate that the AHRS method achieves an average 6.48% performance improvement across multiple high-degree-of-freedom robotic tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints</title>
<link>https://arxiv.org/abs/2505.02485</link>
<guid>https://arxiv.org/abs/2505.02485</guid>
<content:encoded><![CDATA[
arXiv:2505.02485v1 Announce Type: cross 
Abstract: The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization problem with the goal to design shifts to cover prearranged bus tours. The objective takes into account the operational cost as well as the satisfaction of drivers. This problem is heavily constrained due to strict legal rules and collective agreements. The objective of this article is to provide state-of-the-art exact and hybrid solution methods that can provide high-quality solutions for instances of different sizes. This work presents a comprehensive study of both an exact method, Branch and Price (B&amp;P), as well as a Large Neighborhood Search (LNS) framework which uses B&amp;P or Column Generation (CG) for the repair phase to solve the BDSP. It further proposes and evaluates a novel deeper integration of B&amp;P and LNS, storing the generated columns from the LNS subproblems and reusing them for other subproblems, or to find better global solutions. The article presents a detailed analysis of several components of the solution methods and their impact, including general improvements for the B&amp;P subproblem, which is a high-dimensional Resource Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The evaluation shows that our approach provides new state-of-the-art results for instances of all sizes, including exact solutions for small instances, and low gaps to a known lower bound for mid-sized instances. Conclusions: We observe that B&amp;P provides the best results for small instances, while the tight integration of LNS and CG can provide high-quality solutions for larger instances, further improving over LNS which just uses CG as a black box. The proposed methods are general and can also be applied to other rule sets and related optimization problems
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.02486</link>
<guid>https://arxiv.org/abs/2505.02486</guid>
<content:encoded><![CDATA[
arXiv:2505.02486v1 Announce Type: cross 
Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions</title>
<link>https://arxiv.org/abs/2505.02501</link>
<guid>https://arxiv.org/abs/2505.02501</guid>
<content:encoded><![CDATA[
arXiv:2505.02501v1 Announce Type: cross 
Abstract: We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study</title>
<link>https://arxiv.org/abs/2505.02502</link>
<guid>https://arxiv.org/abs/2505.02502</guid>
<content:encoded><![CDATA[
arXiv:2505.02502v1 Announce Type: cross 
Abstract: Background: Large language models (LLMs) are increasingly deployed via open-source and commercial frameworks, enabling individuals and organizations to self-host advanced AI capabilities. However, insecure defaults and misconfigurations often expose LLM services to the public Internet, posing significant security and system engineering risks. Aims: This study aims to unveil the current landscape of public-facing LLM deployments in the wild through a large-scale empirical study, focusing on service prevalence, exposure characteristics, systemic vulnerabilities, and associated risks. Method: We conducted an Internet-wide measurement to identify public-facing LLM deployments across 15 frameworks, discovering 320,102 services. We extracted 158 unique API endpoints, grouped into 12 functional categories based on capabilities and security risks. We further analyzed configurations, authentication practices, and geographic distributions, revealing deployment trends and systemic issues in real-world LLM system engineering. Results: Our study shows that public LLM deployments are rapidly growing but often insecure. Among all endpoints, we observe widespread use of insecure protocols, poor TLS configurations, and unauthenticated access to critical operations. Security risks, including model disclosure, system leakage, and unauthorized access, are pervasive, highlighting the need for secure-by-default frameworks and stronger deployment practices. Conclusions: Public-facing LLM deployments suffer from widespread security and configuration flaws, exposing services to misuse, model theft, resource hijacking, and remote exploitation. Strengthening default security, deployment practices, and operational standards is critical for the growing self-hosted LLM ecosystem.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Partitioning for Low-Latency Inference at the Edge</title>
<link>https://arxiv.org/abs/2505.02533</link>
<guid>https://arxiv.org/abs/2505.02533</guid>
<content:encoded><![CDATA[
arXiv:2505.02533v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations</title>
<link>https://arxiv.org/abs/2505.02537</link>
<guid>https://arxiv.org/abs/2505.02537</guid>
<content:encoded><![CDATA[
arXiv:2505.02537v1 Announce Type: cross 
Abstract: Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data</title>
<link>https://arxiv.org/abs/2505.02540</link>
<guid>https://arxiv.org/abs/2505.02540</guid>
<content:encoded><![CDATA[
arXiv:2505.02540v1 Announce Type: cross 
Abstract: In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bielik v3 Small: Technical Report</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
arXiv:2505.02550v1 Announce Type: cross 
Abstract: We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness questions the interpretability of graph neural networks: what to do?</title>
<link>https://arxiv.org/abs/2505.02566</link>
<guid>https://arxiv.org/abs/2505.02566</guid>
<content:encoded><![CDATA[
arXiv:2505.02566v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms.
  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source.
  The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
<link>https://arxiv.org/abs/2505.02573</link>
<guid>https://arxiv.org/abs/2505.02573</guid>
<content:encoded><![CDATA[
arXiv:2505.02573v1 Announce Type: cross 
Abstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.02579</link>
<guid>https://arxiv.org/abs/2505.02579</guid>
<content:encoded><![CDATA[
arXiv:2505.02579v1 Announce Type: cross 
Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis</title>
<link>https://arxiv.org/abs/2505.02625</link>
<guid>https://arxiv.org/abs/2505.02625</guid>
<content:encoded><![CDATA[
arXiv:2505.02625v1 Announce Type: cross 
Abstract: Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition</title>
<link>https://arxiv.org/abs/2505.02627</link>
<guid>https://arxiv.org/abs/2505.02627</guid>
<content:encoded><![CDATA[
arXiv:2505.02627v1 Announce Type: cross 
Abstract: Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning</title>
<link>https://arxiv.org/abs/2505.02639</link>
<guid>https://arxiv.org/abs/2505.02639</guid>
<content:encoded><![CDATA[
arXiv:2505.02639v1 Announce Type: cross 
Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints</title>
<link>https://arxiv.org/abs/2505.02640</link>
<guid>https://arxiv.org/abs/2505.02640</guid>
<content:encoded><![CDATA[
arXiv:2505.02640v1 Announce Type: cross 
Abstract: Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye Movements as Indicators of Deception: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2505.02649</link>
<guid>https://arxiv.org/abs/2505.02649</guid>
<content:encoded><![CDATA[
arXiv:2505.02649v1 Announce Type: cross 
Abstract: Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74% in a binary classification task (Revealing vs. Concealing) and 49% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.02655</link>
<guid>https://arxiv.org/abs/2505.02655</guid>
<content:encoded><![CDATA[
arXiv:2505.02655v1 Announce Type: cross 
Abstract: The Transformer model has shown strong performance in multivariate time series forecasting by leveraging channel-wise self-attention. However, this approach lacks temporal constraints when computing temporal features and does not utilize cumulative historical series effectively.To address these limitations, we propose the Structured Channel-wise Transformer with Cumulative Historical state (SCFormer). SCFormer introduces temporal constraints to all linear transformations, including the query, key, and value matrices, as well as the fully connected layers within the Transformer. Additionally, SCFormer employs High-order Polynomial Projection Operators (HiPPO) to deal with cumulative historical time series, allowing the model to incorporate information beyond the look-back window during prediction. Extensive experiments on multiple real-world datasets demonstrate that SCFormer significantly outperforms mainstream baselines, highlighting its effectiveness in enhancing time series forecasting. The code is publicly available at https://github.com/ShiweiGuo1995/SCFormer
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Note on Statistically Accurate Tabular Data Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.02659</link>
<guid>https://arxiv.org/abs/2505.02659</guid>
<content:encoded><![CDATA[
arXiv:2505.02659v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a probability-driven prompting approach that leverages LLMs to estimate conditional distributions, enabling more accurate and scalable data synthesis. The results highlight the potential of prompting probobility distributions to enhance the statistical fidelity of LLM-generated tabular data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Standardized Patient Improves Human Conversations in Advanced Cancer Care</title>
<link>https://arxiv.org/abs/2505.02694</link>
<guid>https://arxiv.org/abs/2505.02694</guid>
<content:encoded><![CDATA[
arXiv:2505.02694v1 Announce Type: cross 
Abstract: Serious illness communication (SIC) in end-of-life care faces challenges such as emotional stress, cultural barriers, and balancing hope with honesty. Despite its importance, one of the few available ways for clinicians to practice SIC is with standardized patients, which is expensive, time-consuming, and inflexible. In this paper, we present SOPHIE, an AI-powered standardized patient simulation and automated feedback system. SOPHIE combines large language models (LLMs), a lifelike virtual avatar, and automated, personalized feedback based on clinical literature to provide remote, on-demand SIC training. In a randomized control study with healthcare students and professionals, SOPHIE users demonstrated significant improvement across three critical SIC domains: Empathize, Be Explicit, and Empower. These results suggest that AI-driven tools can enhance complex interpersonal communication skills, offering scalable, accessible solutions to address a critical gap in clinician education.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
<link>https://arxiv.org/abs/2505.02712</link>
<guid>https://arxiv.org/abs/2505.02712</guid>
<content:encoded><![CDATA[
arXiv:2505.02712v1 Announce Type: cross 
Abstract: Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation</title>
<link>https://arxiv.org/abs/2505.02737</link>
<guid>https://arxiv.org/abs/2505.02737</guid>
<content:encoded><![CDATA[
arXiv:2505.02737v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities' classes in a KG to gradually prune the candidate space as well as the entities' descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG's semantic expressivity on the ED performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models</title>
<link>https://arxiv.org/abs/2505.02763</link>
<guid>https://arxiv.org/abs/2505.02763</guid>
<content:encoded><![CDATA[
arXiv:2505.02763v1 Announce Type: cross 
Abstract: Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow</title>
<link>https://arxiv.org/abs/2505.02780</link>
<guid>https://arxiv.org/abs/2505.02780</guid>
<content:encoded><![CDATA[
arXiv:2505.02780v1 Announce Type: cross 
Abstract: Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases like cancer, yet current digital pathology tools hinder diagnosis. The immense scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the limited views traditional monitors offer. This mismatch forces constant panning and zooming, increasing pathologist cognitive load, causing diagnostic fatigue, and slowing pathologists' adoption of digital methods. PathVis, our mixed-reality visualization platform for Apple Vision Pro, addresses these challenges. It transforms the pathologist's interaction with data, replacing cumbersome mouse-and-monitor navigation with intuitive exploration using natural hand gestures, eye gaze, and voice commands in an immersive workspace. PathVis integrates AI to enhance diagnosis. An AI-driven search function instantly retrieves and displays the top five similar patient cases side-by-side, improving diagnostic precision and efficiency through rapid comparison. Additionally, a multimodal conversational AI assistant offers real-time image interpretation support and aids collaboration among pathologists across multiple Apple devices. By merging the directness of traditional pathology with advanced mixed-reality visualization and AI, PathVis improves diagnostic workflows, reduces cognitive strain, and makes pathology practice more effective and engaging. The PathVis source code and a demo video are publicly available at: https://github.com/jaiprakash1824/Path_Vis
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.02795</link>
<guid>https://arxiv.org/abs/2505.02795</guid>
<content:encoded><![CDATA[
arXiv:2505.02795v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02824</link>
<guid>https://arxiv.org/abs/2505.02824</guid>
<content:encoded><![CDATA[
arXiv:2505.02824v1 Announce Type: cross 
Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax Strikes Back</title>
<link>https://arxiv.org/abs/2012.10700</link>
<guid>https://arxiv.org/abs/2012.10700</guid>
<content:encoded><![CDATA[
arXiv:2012.10700v2 Announce Type: replace 
Abstract: Deep Reinforcement Learning reaches a superhuman level of play in many complete information games. The state of the art algorithm for learning with zero knowledge is AlphaZero. We take another approach, Ath\'enan, which uses a different, Minimax-based, search algorithm called Descent, as well as different learning targets and that does not use a policy. We show that for multiple games it is much more efficient than the reimplementation of AlphaZero: Polygames. It is even competitive with Polygames when Polygames uses 100 times more GPU (at least for some games). One of the keys to the superior performance is that the cost of generating state data for training is approximately 296 times lower with Ath\'enan. With the same reasonable ressources, Ath\'enan without reinforcement heuristic is at least 7 times faster than Polygames and much more than 30 times faster with reinforcement heuristic.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning</title>
<link>https://arxiv.org/abs/2402.16631</link>
<guid>https://arxiv.org/abs/2402.16631</guid>
<content:encoded><![CDATA[
arXiv:2402.16631v3 Announce Type: replace 
Abstract: Generative Artificial Intelligence (GenAI) and communication networks are expected to have groundbreaking synergies for 6G. Connecting GenAI agents via a wireless network can potentially unleash the power of Collective Intelligence (CI) and pave the way for Artificial General Intelligence (AGI). However, current wireless networks are designed as a "data pipe" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (facts, experiences, and methods) to accomplish arbitrary tasks. We first propose an architecture for a single GenAI agent and then provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantics from heterogeneous raw data, build and maintain a knowledge model representing the semantic relationships among pieces of knowledge, which is retrieved by GenAI models for planning and reasoning. Under this paradigm, different levels of collaboration can be achieved flexibly depending on the complexity of targeted tasks. Furthermore, we conduct two case studies in which, through wireless device queries, we demonstrate that extracting, compressing and transferring common knowledge can improve query accuracy while reducing communication costs; and in the wireless power control problem, we show that distributed agents can complete general tasks independently through collaborative reasoning without predefined communication protocols. Finally, we discuss challenges and future research directions in applying Large Language Models (LLMs) in 6G networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Healthcare: A Review on Ensuring Fairness and Mitigating Bias</title>
<link>https://arxiv.org/abs/2407.19655</link>
<guid>https://arxiv.org/abs/2407.19655</guid>
<content:encoded><![CDATA[
arXiv:2407.19655v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing the efficiency and effectiveness of services across various specialties, including cardiology, ophthalmology, dermatology, emergency medicine, etc. AI applications have significantly improved diagnostic accuracy, treatment personalization, and patient outcome predictions by leveraging technologies such as machine learning, neural networks, and natural language processing. However, these advancements also introduce substantial ethical and fairness challenges, particularly related to biases in data and algorithms. These biases can lead to disparities in healthcare delivery, affecting diagnostic accuracy and treatment outcomes across different demographic groups. This review paper examines the integration of AI in healthcare, highlighting critical challenges related to bias and exploring strategies for mitigation. We emphasize the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery. The paper concludes with recommendations for future research, advocating for interdisciplinary approaches, transparency in AI decision-making, and the development of innovative and inclusive AI applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing a Foundation of Vector Symbolic Architectures Using Category Theory</title>
<link>https://arxiv.org/abs/2501.05368</link>
<guid>https://arxiv.org/abs/2501.05368</guid>
<content:encoded><![CDATA[
arXiv:2501.05368v2 Announce Type: replace 
Abstract: Connectionist approaches to machine learning, \emph{i.e.} neural networks, are enjoying a considerable vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category-theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, We generalise from vectors to co-presheaves, and describe VSA operations as the right Kan extensions of the external tensor product. This formalisation involves a proof that the right Kan extension in such cases can be expressed as simple, element-wise operations. We validate our formalisation with worked examples that connect to current VSA implementations, while suggesting new possible designs for VSAs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems</title>
<link>https://arxiv.org/abs/2502.07503</link>
<guid>https://arxiv.org/abs/2502.07503</guid>
<content:encoded><![CDATA[
arXiv:2502.07503v3 Announce Type: replace 
Abstract: Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent "repeat-all-over" (RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising <1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification</title>
<link>https://arxiv.org/abs/2502.17289</link>
<guid>https://arxiv.org/abs/2502.17289</guid>
<content:encoded><![CDATA[
arXiv:2502.17289v2 Announce Type: replace 
Abstract: In this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. It is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. Thus we address the problem of unknown species classification by assigning it best hierarchical labels. We propose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification. The approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. Using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. It uses attention scores to focus on important features across multiple scales. The proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. The model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. We used unknown species for testing our model. For unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. Our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats</title>
<link>https://arxiv.org/abs/2503.02650</link>
<guid>https://arxiv.org/abs/2503.02650</guid>
<content:encoded><![CDATA[
arXiv:2503.02650v2 Announce Type: replace 
Abstract: The exponential growth of unstructured text data presents a fundamental challenge in modern data management and information retrieval. While Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, their potential to transform unstructured text into standardized, structured formats remains largely unexplored - a capability that could revolutionize data processing workflows across industries. This study breaks new ground by systematically evaluating LLMs' ability to convert unstructured recipe text into the structured Cooklang format. Through comprehensive testing of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an innovative evaluation approach is introduced that combines traditional metrics (WER, ROUGE-L, TER) with specialized metrics for semantic element identification. Our experiments reveal that GPT-4o with few-shot prompting achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating for the first time that LLMs can reliably transform domain-specific unstructured text into structured formats without extensive training. Although model performance generally scales with size, we uncover surprising potential in smaller models like Llama3.1:8b for optimization through targeted fine-tuning. These findings open new possibilities for automated structured data generation across various domains, from medical records to technical documentation, potentially transforming the way organizations process and utilize unstructured information.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Space Interventions Can Be Transferred Between Large Language Models</title>
<link>https://arxiv.org/abs/2503.04429</link>
<guid>https://arxiv.org/abs/2503.04429</guid>
<content:encoded><![CDATA[
arXiv:2503.04429v2 Announce Type: replace 
Abstract: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches", allowing dynamic toggling between model behaviors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science</title>
<link>https://arxiv.org/abs/2503.10822</link>
<guid>https://arxiv.org/abs/2503.10822</guid>
<content:encoded><![CDATA[
arXiv:2503.10822v3 Announce Type: replace 
Abstract: The aim of this paper is to discuss the potential of using methods from Reinforcement Learning for Life Cycle Assessment in a circular economy, and to present some new ideas in this direction. To give some context, we explain how Reinforcement Learning was successfully applied in computer chess (and beyond). As computer chess was historically called the "drosophila of AI", we start by describing a method for the board representation called 'rotated bitboards' that can potentially also be applied in the context of sustainability. In the first part of this paper, the concepts of the bitboard-representation and the advantages of (rotated) bitboards in move generation are explained. In order to illustrate those ideas practice, the concrete implementation of the move-generator in FUSc# (a chess engine developed at FU Berlin in C# some years ago) is described. In addition, rotated binary neural networks are discussed briefly.
  The second part deals with reinforcement learning in computer chess (and beyond). We exemplify the progress that has been made in this field in the last 15-20 years by comparing the "state of the art" from 2002-2008, when FUSc# was developed, with the ground-breaking innovations connected to "AlphaZero". We review some application of the ideas developed in AlphaZero in other domains, e.g. the "other Alphas" like AlphaFold, AlphaTensor, AlphaGeometry and AlphaProof. In the final part of the paper, we discuss the computer-science related challenges that changing the economic paradigm towards (absolute) sustainability poses and in how far what we call 'progressive computer science' needs to contribute. Concrete challenges include the closing of material loops in a circular economy with Life Cycle Assessment in order to optimize for (absolute) sustainability, and we present some new ideas in this direction.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data</title>
<link>https://arxiv.org/abs/2302.06375</link>
<guid>https://arxiv.org/abs/2302.06375</guid>
<content:encoded><![CDATA[
arXiv:2302.06375v4 Announce Type: replace-cross 
Abstract: There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformadores: Fundamentos teoricos y Aplicaciones</title>
<link>https://arxiv.org/abs/2302.09327</link>
<guid>https://arxiv.org/abs/2302.09327</guid>
<content:encoded><![CDATA[
arXiv:2302.09327v2 Announce Type: replace-cross 
Abstract: Transformers are a neural network architecture originally developed for natural language processing, which have since become a foundational tool for solving a wide range of problems, including text, audio, image processing, reinforcement learning, and other tasks involving heterogeneous input data. Their hallmark is the self-attention mechanism, which allows the model to weigh different parts of the input sequence dynamically, and is an evolution of earlier attention-based approaches. This article provides readers with the necessary background to understand recent research on transformer models, and presents the mathematical and algorithmic foundations of their core components. It also explores the architecture's various elements, potential modifications, and some of the most relevant applications. The article is written in Spanish to help make this scientific knowledge more accessible to the Spanish-speaking community.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2306.15546</link>
<guid>https://arxiv.org/abs/2306.15546</guid>
<content:encoded><![CDATA[
arXiv:2306.15546v3 Announce Type: replace-cross 
Abstract: The intersection of Foundation Model (FM) and Federated Learning (FL) presents a unique opportunity to unlock new possibilities for real-world applications. On the one hand, FL, as a collaborative learning paradigm, help address challenges in FM development by expanding data availability, enabling computation sharing, facilitating the collaborative development of FMs, tackling continuous data update, avoiding FM monopoly, response delay and FM service down. On the other hand, FM, equipped with pre-trained knowledge and exceptional performance, can serve as a robust starting point for FL. It can also generate synthetic data to enrich data diversity and enhance overall performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task and multi-modality capabilities for FL. By examining the interplay between FL and FM, this paper presents the motivations, challenges, and future directions of empowering FL with FM and empowering FM with FL. We hope that this work provides a good foundation to inspire future research efforts to drive advancements in both fields.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models at Work in China's Labor Market</title>
<link>https://arxiv.org/abs/2308.08776</link>
<guid>https://arxiv.org/abs/2308.08776</guid>
<content:encoded><![CDATA[
arXiv:2308.08776v2 Announce Type: replace-cross 
Abstract: This paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following the methodology of Eloundou et al. (2023). The results indicate a positive correlation between occupational exposure and both wage levels and experience premiums at the occupation level. This suggests that higher-paying and experience-intensive jobs may face greater exposure risks from LLM-powered software. We then aggregate occupational exposure at the industry level to obtain industrial exposure scores. Both occupational and industrial exposure scores align with expert assessments. Our empirical analysis also demonstrates a distinct impact of LLMs, which deviates from the routinization hypothesis. We present a stylized theoretical framework to better understand this deviation from previous digital technologies. By incorporating entropy-based information theory into the task-based framework, we propose an AI learning theory that reveals a different pattern of LLM impacts compared to the routinization hypothesis.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Animal Imitator: Agile Locomotion with Versatile Motion Prior</title>
<link>https://arxiv.org/abs/2310.01408</link>
<guid>https://arxiv.org/abs/2310.01408</guid>
<content:encoded><![CDATA[
arXiv:2310.01408v3 Announce Type: replace-cross 
Abstract: The agility of animals, particularly in complex activities such as running, turning, jumping, and backflipping, stands as an exemplar for robotic system design. Transferring this suite of behaviors to legged robotic systems introduces essential inquiries: How can a robot learn multiple locomotion behaviors simultaneously? How can the robot execute these tasks with a smooth transition? How to integrate these skills for wide-range applications? This paper introduces the Versatile Instructable Motion prior (VIM) - a Reinforcement Learning framework designed to incorporate a range of agile locomotion tasks suitable for advanced robotic applications. Our framework enables legged robots to learn diverse agile low-level skills by imitating animal motions and manually designed motions. Our Functionality reward guides the robot's ability to adopt varied skills, and our Stylization reward ensures that robot motions align with reference motions. Our evaluations of the VIM framework span both simulation and the real world. Our framework allows a robot to concurrently learn diverse agile locomotion skills using a single learning-based controller in the real world. Videos can be found on our website: https://rchalyang.github.io/VIM/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple connection from loss flatness to compressed neural representations</title>
<link>https://arxiv.org/abs/2310.01770</link>
<guid>https://arxiv.org/abs/2310.01770</guid>
<content:encoded><![CDATA[
arXiv:2310.01770v4 Announce Type: replace-cross 
Abstract: Sharpness, a geometric measure in the parameter space that reflects the flatness of the loss landscape, has long been studied for its potential connections to neural network behavior. While sharpness is often associated with generalization, recent work highlights inconsistencies in this relationship, leaving its true significance unclear. In this paper, we investigate how sharpness influences the local geometric features of neural representations in feature space, offering a new perspective on its role. We introduce this problem and study three measures for compression: the Local Volumetric Ratio (LVR), based on volume compression, the Maximum Local Sensitivity (MLS), based on sensitivity to input changes, and the Local Dimensionality, based on how uniform the sensitivity is on different directions. We show that LVR and MLS correlate with the flatness of the loss around the local minima; and that this correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Our inequalities readily extend to reparametrization-invariant sharpness as well. Through empirical experiments on various feedforward, convolutional, and transformer architectures, we find that our inequalities predict a consistently positive correlation between local representation compression and sharpness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supercharging Graph Transformers with Advective Diffusion</title>
<link>https://arxiv.org/abs/2310.06417</link>
<guid>https://arxiv.org/abs/2310.06417</guid>
<content:encoded><![CDATA[
arXiv:2310.06417v2 Announce Type: replace-cross 
Abstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Problems in Learning Multiple Dynamical Systems</title>
<link>https://arxiv.org/abs/2311.02181</link>
<guid>https://arxiv.org/abs/2311.02181</guid>
<content:encoded><![CDATA[
arXiv:2311.02181v3 Announce Type: replace-cross 
Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results. The key highlight of this method is that it does not require a predefined hidden state dimension but instead provides an upper bound. Additionally, it offers guidance for determining regularization in the system identification.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMUTF: Schema Matching Using Generative Tags and Hybrid Features</title>
<link>https://arxiv.org/abs/2402.01685</link>
<guid>https://arxiv.org/abs/2402.01685</guid>
<content:encoded><![CDATA[
arXiv:2402.01685v3 Announce Type: replace-cross 
Abstract: We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid Features), a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy "generative tags" for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.
  Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and improving the F1 score by 11.84% and the AUC of ROC by 5.08%. Code is available at https://github.com/fireindark707/Python-Schema-Matching.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off</title>
<link>https://arxiv.org/abs/2402.07002</link>
<guid>https://arxiv.org/abs/2402.07002</guid>
<content:encoded><![CDATA[
arXiv:2402.07002v2 Announce Type: replace-cross 
Abstract: To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This capability implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative datasets and observe significant performance improvements and strict privacy guarantees under different privacy settings. The code is available at https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization</title>
<link>https://arxiv.org/abs/2402.15290</link>
<guid>https://arxiv.org/abs/2402.15290</guid>
<content:encoded><![CDATA[
arXiv:2402.15290v4 Announce Type: replace-cross 
Abstract: Existing models encounter bottlenecks in balancing performance and computational efficiency when modeling long sequences. Although the state space model (SSM) has achieved remarkable success in handling long sequence tasks, it still faces the problem of large number of parameters. In order to further improve the efficiency of SSM, we propose a new state space layer based on multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is built on the convolutional representation of multi-input and multi-input (MIMO) SSM. We propose a variety of effective strategies to improve the computational efficiency. The diagonalization of the system matrix first decouples the original system. Then a fast tensor convolution is proposed based on the fast Fourier transform. In addition, the block diagonalization of the SSM further reduces the model parameters and improves the model flexibility. Extensive experimental results show that the performance of the proposed model on multiple databases matches the performance of state-of-the-art models, such as S4, and is significantly better than Transformers and LSTM. In the model efficiency benchmark, the parameters of eSSM are only 12.89\% of LSTM and 13.24\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and 1.35 times faster than Mamba. Code is available at: \href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories</title>
<link>https://arxiv.org/abs/2402.16310</link>
<guid>https://arxiv.org/abs/2402.16310</guid>
<content:encoded><![CDATA[
arXiv:2402.16310v4 Announce Type: replace-cross 
Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\%-10.5\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation</title>
<link>https://arxiv.org/abs/2403.01954</link>
<guid>https://arxiv.org/abs/2403.01954</guid>
<content:encoded><![CDATA[
arXiv:2403.01954v4 Announce Type: replace-cross 
Abstract: Constrained decoding approaches aim to control the meaning or style of text generated by the pre-trained large language models (LLMs or also PLMs) for various tasks at inference time. However, these methods often guide plausible continuations by greedily and explicitly selecting targets. Though fulfilling the task requirements, these methods may overlook certain general and natural logics that humans would implicitly follow towards such targets. Inspired by cognitive dual-process theory, in this work, we propose a novel decoding framework DECIDER where the base LLMs are equipped with a First-Order Logic (FOL) reasoner to express and evaluate the rules, along with a decision function that merges the outputs of both systems to guide the generation. Unlike previous constrained decodings, DECIDER transforms the encouragement of target-specific words into all words that satisfy several high-level rules, enabling us to programmatically integrate our logic into LLMs. Experiments on CommonGen and PersonaChat demonstrate that DECIDER effectively follows given FOL rules to guide LLMs in a more human-like and logic-controlled manner.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Noisy Supervision in Foundation Model Learning</title>
<link>https://arxiv.org/abs/2403.06869</link>
<guid>https://arxiv.org/abs/2403.06869</guid>
<content:encoded><![CDATA[
arXiv:2403.06869v3 Announce Type: replace-cross 
Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backpropagation through space, time, and the brain</title>
<link>https://arxiv.org/abs/2403.16933</link>
<guid>https://arxiv.org/abs/2403.16933</guid>
<content:encoded><![CDATA[
arXiv:2403.16933v3 Announce Type: replace-cross 
Abstract: How physical networks of neurons, bound by spatio-temporal locality constraints, can perform efficient credit assignment, remains, to a large extent, an open question. In machine learning, the answer is almost universally given by the error backpropagation algorithm, through both space and time. However, this algorithm is well-known to rely on biologically implausible assumptions, in particular with respect to spatio-temporal (non-)locality. Alternative forward-propagation models such as real-time recurrent learning only partially solve the locality problem, but only at the cost of scaling, due to prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of backpropagation through space and time in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the morphology of dendritic trees to enable more complex information storage and processing in single neurons, as well as the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, effectively performing a spatio-temporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint variables necessary for useful parameter updates.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement</title>
<link>https://arxiv.org/abs/2404.02225</link>
<guid>https://arxiv.org/abs/2404.02225</guid>
<content:encoded><![CDATA[
arXiv:2404.02225v2 Announce Type: replace-cross 
Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data</title>
<link>https://arxiv.org/abs/2404.07452</link>
<guid>https://arxiv.org/abs/2404.07452</guid>
<content:encoded><![CDATA[
arXiv:2404.07452v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models</title>
<link>https://arxiv.org/abs/2405.03869</link>
<guid>https://arxiv.org/abs/2405.03869</guid>
<content:encoded><![CDATA[
arXiv:2405.03869v5 Announce Type: replace-cross 
Abstract: A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences</title>
<link>https://arxiv.org/abs/2405.05572</link>
<guid>https://arxiv.org/abs/2405.05572</guid>
<content:encoded><![CDATA[
arXiv:2405.05572v2 Announce Type: replace-cross 
Abstract: Current computational approaches for analysing or generating code-mixed sentences do not explicitly model ``naturalness'' or ``acceptability'' of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models when trained solely using code-mixing metrics as features are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta and Bernice outperform IndicBERT across different configurations. Among Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder models are not able to outperform Encoder-only models. Decoder-only models perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models outperforming similarly sized Qwen, Phi models. Comparison with zero and fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from En-Hi to En-Te acceptability judgments are better than random baselines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Data Augmentation</title>
<link>https://arxiv.org/abs/2405.09591</link>
<guid>https://arxiv.org/abs/2405.09591</guid>
<content:encoded><![CDATA[
arXiv:2405.09591v3 Announce Type: replace-cross 
Abstract: Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples. By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities. Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process. To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities. Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods. Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Input Feature Relevance via Spectral Neural Networks</title>
<link>https://arxiv.org/abs/2406.01183</link>
<guid>https://arxiv.org/abs/2406.01183</guid>
<content:encoded><![CDATA[
arXiv:2406.01183v2 Announce Type: replace-cross 
Abstract: In machine learning practice it is often useful to identify relevant input features, so as to obtain compact dataset for more efficient numerical handling. On the other hand, by isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of the input components for a Deep Neural Network. This is achieved by leveraging on a spectral re-parametrization of the optimization process. Eigenvalues associated to input nodes provide in fact a robust proxy to gauge the relevance of the supplied entry features. Notably, the spectral features ranking is performed automatically, as a byproduct of the network training, with no additional processing to be carried out. The technique is successfully challenged against both synthetic and real data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Calibration Point: Mechanism Comparison in Differential Privacy</title>
<link>https://arxiv.org/abs/2406.08918</link>
<guid>https://arxiv.org/abs/2406.08918</guid>
<content:encoded><![CDATA[
arXiv:2406.08918v3 Announce Type: replace-cross 
Abstract: In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\varepsilon, \delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\varepsilon, \delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\varepsilon, \delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LASSI: An LLM-based Automated Self-Correcting Pipeline for Translating Parallel Scientific Codes</title>
<link>https://arxiv.org/abs/2407.01638</link>
<guid>https://arxiv.org/abs/2407.01638</guid>
<content:encoded><![CDATA[
arXiv:2407.01638v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of providing a novel approach to sourcing significant training data for LLMs focused on science and engineering. In particular, a crucial challenge is sourcing parallel scientific codes in the ranges of millions to billions of codes. To tackle this problem, we propose an automated pipeline framework called LASSI, designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates autonomous enhancement through self-correcting loops where errors encountered during the compilation and execution of generated code are fed back to the LLM through guided prompting for debugging and refactoring. We highlight the bi-directional translation of existing GPU benchmarks between OpenMP target offload and CUDA to validate LASSI. The results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Split Learning with Global Sampling</title>
<link>https://arxiv.org/abs/2407.15738</link>
<guid>https://arxiv.org/abs/2407.15738</guid>
<content:encoded><![CDATA[
arXiv:2407.15738v3 Announce Type: replace-cross 
Abstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Logical Fallacy-Informed Framework for Argument Generation</title>
<link>https://arxiv.org/abs/2408.03618</link>
<guid>https://arxiv.org/abs/2408.03618</guid>
<content:encoded><![CDATA[
arXiv:2408.03618v4 Announce Type: replace-cross 
Abstract: Despite the remarkable performance of Large Language Models (LLMs) in natural language processing tasks, they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. FIPO includes a classification loss, to capture the fine-grained information on fallacy types. Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as other preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation. Our code is available at github.com/lucamouchel/Logical-Fallacies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragment-Masked Molecular Optimization</title>
<link>https://arxiv.org/abs/2408.09106</link>
<guid>https://arxiv.org/abs/2408.09106</guid>
<content:encoded><![CDATA[
arXiv:2408.09106v2 Announce Type: replace-cross 
Abstract: Molecular optimization is a crucial aspect of drug discovery, aimed at refining molecular structures to enhance drug efficacy and minimize side effects, ultimately accelerating the overall drug development process. Many target-based molecular optimization methods have been proposed, significantly advancing drug discovery. These methods primarily on understanding the specific drug target structures or their hypothesized roles in combating diseases. However, challenges such as a limited number of available targets and a difficulty capturing clear structures hinder innovative drug development. In contrast, phenotypic drug discovery (PDD) does not depend on clear target structures and can identify hits with novel and unbiased polypharmacology signatures. As a result, PDD-based molecular optimization can reduce potential safety risks while optimizing phenotypic activity, thereby increasing the likelihood of clinical success. Therefore, we propose a fragment-masked molecular optimization method based on PDD (FMOP). FMOP employs a regression-free diffusion model to conditionally optimize the molecular masked regions without training, effectively generating new molecules with similar scaffolds. On the large-scale drug response dataset GDSCv2, we optimize the potential molecules across all 945 cell lines. The overall experiments demonstrate that the in-silico optimization success rate reaches 94.4%, with an average efficacy increase of 5.3%. Additionally, we conduct extensive ablation and visualization experiments, confirming that FMOP is an effective and robust molecular optimization method. The code is available at:https://anonymous.4open.science/r/FMOP-98C2.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models</title>
<link>https://arxiv.org/abs/2408.10276</link>
<guid>https://arxiv.org/abs/2408.10276</guid>
<content:encoded><![CDATA[
arXiv:2408.10276v4 Announce Type: replace-cross 
Abstract: Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition</title>
<link>https://arxiv.org/abs/2408.17090</link>
<guid>https://arxiv.org/abs/2408.17090</guid>
<content:encoded><![CDATA[
arXiv:2408.17090v2 Announce Type: replace-cross 
Abstract: Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Non-IID data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We thereby introduce FissionVAE that decouples the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we incorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder architectures within FissionVAE. We also explore strategies for setting the latent prior distributions to enhance the decoupling process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications</title>
<link>https://arxiv.org/abs/2409.05314</link>
<guid>https://arxiv.org/abs/2409.05314</guid>
<content:encoded><![CDATA[
arXiv:2409.05314v3 Announce Type: replace-cross 
Abstract: The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the JEPA framework</title>
<link>https://arxiv.org/abs/2409.05929</link>
<guid>https://arxiv.org/abs/2409.05929</guid>
<content:encoded><![CDATA[
arXiv:2409.05929v4 Announce Type: replace-cross 
Abstract: Current multimodal alignment strategies primarily use single or unified modality encoders, while optimizing the alignment on the original token space. Such a framework is easy to implement and incorporate with the pretrained knowledge, but might result in information bias. To deal with such issues, the joint encoding predictive architecture (JEPA) learns the alignment loss on the latent space, with a predictor to convert the input encoding to the output latent space. However, the application of JEPA in multimodal scenarios is limited so far. In this paper, we introduce M3-Jepa, a scalable multimodal alignment framework, with the predictor implemented by a multi-directional mixture of experts (MoE). We demonstrate the framework can maximize the mutual information with information theory derivations, by alternating the optimization between different uni-directional tasks. By thoroughly designed experiments, we show that M3-Jepa can obtain state-of-the-art performance on different modalities and tasks, generalize to unseen datasets and domains, and is computationally efficient in training and inference. Our study indicates that M3-Jepa might provide a new paradigm to self-supervised learning and open-world modeling.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence</title>
<link>https://arxiv.org/abs/2409.09413</link>
<guid>https://arxiv.org/abs/2409.09413</guid>
<content:encoded><![CDATA[
arXiv:2409.09413v2 Announce Type: replace-cross 
Abstract: This perspective paper explores the bidirectional influence between language emergence and the relational structure of subjective experiences, termed qualia structure, and lays out a constructive approach to the intricate dependency between the two. We hypothesize that the emergence of languages with distributional semantics (e.g., syntactic-semantic structures) is linked to the coordination of internal representations shaped by experience, potentially facilitating more structured language through reciprocal influence. This hypothesized mutual dependency connects to recent advancements in AI and symbol emergence robotics, and is explored within this paper through theoretical frameworks such as the collective predictive coding. Computational studies show that neural network-based language models form systematically structured internal representations, and multimodal language models can share representations between language and perceptual information. This perspective suggests that language emergence serves not only as a mechanism creating a communication tool but also as a mechanism for allowing people to realize shared understanding of qualitative experiences. The paper discusses the implications of this bidirectional influence in the context of consciousness studies, linguistics, and cognitive science, and outlines future constructive research directions to further explore this dynamic relationship between language emergence and qualia structure.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series Imputation</title>
<link>https://arxiv.org/abs/2410.03954</link>
<guid>https://arxiv.org/abs/2410.03954</guid>
<content:encoded><![CDATA[
arXiv:2410.03954v2 Announce Type: replace-cross 
Abstract: In various applications, the multivariate time series often suffers from missing data. This issue can significantly disrupt systems that rely on the data. Spatial and temporal dependencies can be leveraged to impute the missing samples. Existing imputation methods often ignore dynamic changes in spatial dependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation Network (SDA-GRIN) which is capable of capturing dynamic changes in spatial dependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph structures with time. SDA-GRIN models multivariate time series as a sequence of temporal graphs and uses a recurrent message-passing architecture for imputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves MSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it achieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the effect of window sizes and missing data on the performance of the method. Project page:https://ameskandari.github.io/sda-grin/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard-Constrained Neural Networks with Universal Approximation Guarantees</title>
<link>https://arxiv.org/abs/2410.10807</link>
<guid>https://arxiv.org/abs/2410.10807</guid>
<content:encoded><![CDATA[
arXiv:2410.10807v2 Announce Type: replace-cross 
Abstract: Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction--an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method with an efficient forward pass to enforce more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closed-form enforcement layer to the network's output. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: learning with piecewise constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions</title>
<link>https://arxiv.org/abs/2410.14567</link>
<guid>https://arxiv.org/abs/2410.14567</guid>
<content:encoded><![CDATA[
arXiv:2410.14567v4 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Transfer Learning for Open-World Few-Shot Recognition</title>
<link>https://arxiv.org/abs/2411.09986</link>
<guid>https://arxiv.org/abs/2411.09986</guid>
<content:encoded><![CDATA[
arXiv:2411.09986v2 Announce Type: replace-cross 
Abstract: Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5\% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model with Region-guided Referring and Grounding for CT Report Generation</title>
<link>https://arxiv.org/abs/2411.15539</link>
<guid>https://arxiv.org/abs/2411.15539</guid>
<content:encoded><![CDATA[
arXiv:2411.15539v2 Announce Type: replace-cross 
Abstract: Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. Specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. A local feature decoupling (LFD) strategy is proposed to preserve the local high-resolution details with little computational overhead. Then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. Moreover, we propose a novel region-report alignment (RRA) training strategy. It leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. A large language model (LLM) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. Extensive experiments on two large-scale chest CT-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. The code is available at https://github.com/zhi-xuan-chen/Reg2RG.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMO Sampler: Enhancing Text Rendering with Overshooting</title>
<link>https://arxiv.org/abs/2411.19415</link>
<guid>https://arxiv.org/abs/2411.19415</guid>
<content:encoded><![CDATA[
arXiv:2411.19415v2 Announce Type: replace-cross 
Abstract: Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. Code available at: https://github.com/hxixixh/amo-release.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes</title>
<link>https://arxiv.org/abs/2412.00592</link>
<guid>https://arxiv.org/abs/2412.00592</guid>
<content:encoded><![CDATA[
arXiv:2412.00592v2 Announce Type: replace-cross 
Abstract: We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Informed Deep Neural Networks for Power Flow Analysis</title>
<link>https://arxiv.org/abs/2412.02659</link>
<guid>https://arxiv.org/abs/2412.02659</guid>
<content:encoded><![CDATA[
arXiv:2412.02659v2 Announce Type: replace-cross 
Abstract: This study introduces PINN4PF, an end-to-end deep learning architecture for power flow (PF) analysis that effectively captures the nonlinear dynamics of large-scale modern power systems. The proposed neural network (NN) architecture consists of two important advancements in the training pipeline: (A) a double-head feed-forward NN that aligns with PF analysis, including an activation function that adjusts to the net active and reactive power injections patterns, and (B) a physics-based loss function that partially incorporates power system topology information. The effectiveness of the proposed architecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus test systems and is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). The comparison is based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities (specifically line current, line active power, and line reactive power), and (v) scalability. Results demonstrate that PINN4PF outperforms both baselines across all test systems by up to two orders of magnitude not only in terms of direct criteria, e.g., generalization ability, but also in terms of approximating derived physical quantities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2412.05547</link>
<guid>https://arxiv.org/abs/2412.05547</guid>
<content:encoded><![CDATA[
arXiv:2412.05547v2 Announce Type: replace-cross 
Abstract: Large language models with retrieval-augmented generation encounter a pivotal challenge in intricate retrieval tasks, e.g., multi-hop question answering, which requires the model to navigate across multiple documents and generate comprehensive responses based on fragmented information. To tackle this challenge, we introduce a novel Knowledge Graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing in KG-Retriever is constructed on a hierarchical index graph that consists of a knowledge graph layer and a collaborative document layer. The associative nature of graph structures is fully utilized to strengthen intra-document and inter-document connectivity, thereby fundamentally alleviating the information fragmentation problem and meanwhile improving the retrieval efficiency in cross-document retrieval of LLMs. With the coarse-grained collaborative information from neighboring documents and concise information from the knowledge graph, KG-Retriever achieves marked improvements on five public QA datasets, showing the effectiveness and efficiency of our proposed RAG framework.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrushEdit: All-In-One Image Inpainting and Editing</title>
<link>https://arxiv.org/abs/2412.10316</link>
<guid>https://arxiv.org/abs/2412.10316</guid>
<content:encoded><![CDATA[
arXiv:2412.10316v3 Announce Type: replace-cross 
Abstract: Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-LLM: Benchmarking Large Language Models for Anomaly Detection</title>
<link>https://arxiv.org/abs/2412.11142</link>
<guid>https://arxiv.org/abs/2412.11142</guid>
<content:encoded><![CDATA[
arXiv:2412.11142v2 Announce Type: replace-cross 
Abstract: Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.00062</link>
<guid>https://arxiv.org/abs/2501.00062</guid>
<content:encoded><![CDATA[
arXiv:2501.00062v2 Announce Type: replace-cross 
Abstract: Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and yielded the lowest cost/performance ratio (\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at much less cost (\$0.38 vs. \$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction</title>
<link>https://arxiv.org/abs/2501.01010</link>
<guid>https://arxiv.org/abs/2501.01010</guid>
<content:encoded><![CDATA[
arXiv:2501.01010v2 Announce Type: replace-cross 
Abstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images</title>
<link>https://arxiv.org/abs/2501.10098</link>
<guid>https://arxiv.org/abs/2501.10098</guid>
<content:encoded><![CDATA[
arXiv:2501.10098v2 Announce Type: replace-cross 
Abstract: Anatomical landmark localization in 2D/3D images is a critical task in medical imaging. Although many general-purpose tools exist for landmark localization in classical computer vision tasks, such as pose estimation, they lack the specialized features and modularity necessary for anatomical landmark localization applications in the medical domain. Therefore, we introduce landmarker, a Python package built on PyTorch. The package provides a comprehensive, flexible toolkit for developing and evaluating landmark localization algorithms, supporting a range of methodologies, including static and adaptive heatmap regression. landmarker enhances the accuracy of landmark identification, streamlines research and development processes, and supports various image formats and preprocessing pipelines. Its modular design allows users to customize and extend the toolkit for specific datasets and applications, accelerating innovation in medical imaging. landmarker addresses a critical need for precision and customization in landmark localization tasks not adequately met by existing general-purpose pose estimation tools.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs</title>
<link>https://arxiv.org/abs/2501.13620</link>
<guid>https://arxiv.org/abs/2501.13620</guid>
<content:encoded><![CDATA[
arXiv:2501.13620v4 Announce Type: replace-cross 
Abstract: A fundamental challenge in artificial intelligence involves understanding the cognitive mechanisms underlying visual reasoning in sophisticated models like Vision-Language Models (VLMs). How do these models integrate visual perception with abstract thought, especially when reasoning across multiple images or requiring fine-grained compositional understanding? Drawing inspiration from cognitive science, this paper introduces a structured evaluation framework using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to dissect the perception-reasoning interface in VLMs. We propose three distinct evaluation paradigms, mirroring human problem-solving strategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule extraction and application), and Componential Analysis (CA; analytical decomposition via task-agnostic textual descriptions). These paradigms systematically vary cognitive load and probe processing stages. Notably, CA enables multi-image reasoning evaluation even for single-image architectures and isolates reasoning from perception by operating on textual descriptions. Applying this framework, we demonstrate that CA, leveraging powerful language models for reasoning over rich, independently generated descriptions, achieves new state-of-the-art (SOTA) performance on challenging benchmarks including Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm reasoning improves significantly when perceptual challenges are mitigated, revealing a critical perception bottleneck. Our framework provides a valuable diagnostic tool and suggests that decoupling perception (via rich, task-agnostic description) from reasoning is a promising direction for robust and general visual intelligence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
<link>https://arxiv.org/abs/2501.17176</link>
<guid>https://arxiv.org/abs/2501.17176</guid>
<content:encoded><![CDATA[
arXiv:2501.17176v3 Announce Type: replace-cross 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning</title>
<link>https://arxiv.org/abs/2502.03270</link>
<guid>https://arxiv.org/abs/2502.03270</guid>
<content:encoded><![CDATA[
arXiv:2502.03270v2 Announce Type: replace-cross 
Abstract: The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization</title>
<link>https://arxiv.org/abs/2502.04667</link>
<guid>https://arxiv.org/abs/2502.04667</guid>
<content:encoded><![CDATA[
arXiv:2502.04667v2 Announce Type: replace-cross 
Abstract: The integration of explicit Chain-of-Thought (CoT) reasoning into training large language models (LLMs) has advanced their reasoning capabilities, yet the mechanisms by which CoT enhances generalization remain poorly understood. This work investigates (1) \textit{how} CoT training reshapes internal model representations and (2) \textit{why} it improves both in-distribution (ID) and out-of-distribution (OOD) reasoning generalization. Through controlled experiments and theoretical analysis, we derive the following key insights. \textbf{1)} Structural Advantage: CoT training internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Notably, CoT-trained models resolve intermediate results at shallower layers compared to non-CoT counterparts, freeing up deeper layers to specialize in subsequent reasoning steps. \textbf{2)} Theoretical Analysis: the information-theoretic generalization bounds via distributional divergence can be decomposed into ID and OOD components. While ID error diminishes with sufficient training regardless of CoT, OOD error critically depends on CoT: Non-CoT training fails to generalize to OOD samples due to unseen reasoning patterns, whereas CoT training achieves near-perfect OOD generalization by mastering subtasks and reasoning compositions during training. The identified mechanisms explain our experimental results: CoT training accelerates convergence and enhances generalization from ID to both ID and OOD scenarios while maintaining robust performance even with tolerable noise. These findings are further validated on complex real-world datasets. This paper offers valuable insights for designing CoT strategies to enhance LLM reasoning robustness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling</title>
<link>https://arxiv.org/abs/2502.06491</link>
<guid>https://arxiv.org/abs/2502.06491</guid>
<content:encoded><![CDATA[
arXiv:2502.06491v2 Announce Type: replace-cross 
Abstract: Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \textbf{R}eliability-guaranteed \textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of GitHub Copilot on Software Development: A Perspective on Productivity, Security, Best Practices and Future Directions</title>
<link>https://arxiv.org/abs/2502.13199</link>
<guid>https://arxiv.org/abs/2502.13199</guid>
<content:encoded><![CDATA[
arXiv:2502.13199v2 Announce Type: replace-cross 
Abstract: GitHub Copilot is transforming software development by automating tasks and boosting productivity through AI driven code generation. In this paper, we conduct a literature survey to synthesize insights on Copilot's impact on productivity and security. We review academic journal databases, industry reports, and official documentation to highlight key findings and challenges. While Copilot accelerates coding and prototyping, concerns over security vulnerabilities and intellectual property risks persist. Drawing from the literature, we provide a perspective on best practices and future directions for responsible AI adoption in software engineering, offering actionable insights for developers and organizations to integrate Copilot effectively while maintaining high standards of quality and security.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing</title>
<link>https://arxiv.org/abs/2502.15666</link>
<guid>https://arxiv.org/abs/2502.15666</guid>
<content:encoded><![CDATA[
arXiv:2502.15666v2 Announce Type: replace-cross 
Abstract: The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate twelve state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 14.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</title>
<link>https://arxiv.org/abs/2502.17424</link>
<guid>https://arxiv.org/abs/2502.17424</guid>
<content:encoded><![CDATA[
arXiv:2502.17424v5 Announce Type: replace-cross 
Abstract: We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoNormia: Benchmarking Physical Social Norm Understanding</title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
arXiv:2502.20490v3 Announce Type: replace-cross 
Abstract: Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human bench of 92\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \dataset{} to enhance normative reasoning in VLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[
arXiv:2502.20727v2 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Transformations for Symmetric Power Transformers</title>
<link>https://arxiv.org/abs/2503.03269</link>
<guid>https://arxiv.org/abs/2503.03269</guid>
<content:encoded><![CDATA[
arXiv:2503.03269v2 Announce Type: replace-cross 
Abstract: Transformers with linear attention offer significant computational advantages over softmax-based transformers but often suffer from degraded performance. The symmetric power (sympow) transformer, a particular type of linear transformer, addresses some of this performance gap by leveraging symmetric tensor embeddings, achieving comparable performance to softmax transformers. However, the finite capacity of the recurrent state in sympow transformers limits their ability to retain information, leading to performance degradation when scaling the training or evaluation context length. To address this issue, we propose the conformal-sympow transformer, which dynamically frees up capacity using data-dependent multiplicative gating and adaptively stores information using data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64 dataset demonstrate that conformal-sympow overcomes the limitations of sympow transformers, achieving robust performance across scaled training and evaluation contexts.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Trajectory Stitching through Diffusion Composition</title>
<link>https://arxiv.org/abs/2503.05153</link>
<guid>https://arxiv.org/abs/2503.05153</guid>
<content:encoded><![CDATA[
arXiv:2503.05153v2 Announce Type: replace-cross 
Abstract: Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning</title>
<link>https://arxiv.org/abs/2503.06457</link>
<guid>https://arxiv.org/abs/2503.06457</guid>
<content:encoded><![CDATA[
arXiv:2503.06457v2 Announce Type: replace-cross 
Abstract: Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. Code published at: https://github.com/WeiDai-David/2025CVPR_GGEUR
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models</title>
<link>https://arxiv.org/abs/2503.09805</link>
<guid>https://arxiv.org/abs/2503.09805</guid>
<content:encoded><![CDATA[
arXiv:2503.09805v2 Announce Type: replace-cross 
Abstract: Queer people are often discussed as targets of bias, harm, or discrimination in research on generative AI. However, the specific ways that queer people engage with generative AI, and thus possible uses that support queer people, have yet to be explored. We conducted a workshop study with 13 queer artists, during which we gave participants access to GPT-4 and DALL-E 3 and facilitated group sensemaking activities. We found our participants struggled to use these models due to various normative values embedded in their designs, such as hyper-positivity and anti-sexuality. We describe various strategies our participants developed to overcome these models' limitations and how, nevertheless, our participants found value in these highly-normative technologies. Drawing on queer feminist theory, we discuss implications for the conceptualization of "state-of-the-art" models and consider how FAccT researchers might support queer alternatives.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Neural OFDM Receivers: Two Novel Architectures for BER-BLER Optimization and Comparison with State-of-the-Art Architectures</title>
<link>https://arxiv.org/abs/2503.20500</link>
<guid>https://arxiv.org/abs/2503.20500</guid>
<content:encoded><![CDATA[
arXiv:2503.20500v2 Announce Type: replace-cross 
Abstract: Neural receivers have recently become a popular topic, where the received signals can be directly decoded by data driven mechanisms such as machine learning and deep learning. In this paper, we propose two novel neural network based orthogonal frequency division multiplexing (OFDM) receivers performing channel estimation and equalization tasks and directly predicting log likelihood ratios (LLRs) from the received in phase and quadrature phase (IQ) signals. The first network, the Dual Attention Transformer (DAT), employs a state of the art (SOTA) transformer architecture with an attention mechanism. The second network, the Residual Dual Non Local Attention Network (RDNLA), utilizes a parallel residual architecture with a non local attention block. The bit error rate (BER) and block error rate (BLER) performance of various SOTA neural receiver architectures is compared with our proposed methods across different signal to noise ratio (SNR) levels. The simulation results show that DAT and RDNLA outperform both traditional communication systems and existing neural receiver models. The computational efficiency of the proposed neural receivers supports their feasibility for next generation communication systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
arXiv:2503.24235v3 Announce Type: replace-cross 
Abstract: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2504.03302</link>
<guid>https://arxiv.org/abs/2504.03302</guid>
<content:encoded><![CDATA[
arXiv:2504.03302v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&amp;B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
arXiv:2504.03601v3 Announce Type: replace-cross 
Abstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.
  Models at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4; Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website at https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Trust me on this" Explaining Agent Behavior to a Human Terminator</title>
<link>https://arxiv.org/abs/2504.04592</link>
<guid>https://arxiv.org/abs/2504.04592</guid>
<content:encoded><![CDATA[
arXiv:2504.04592v2 Announce Type: replace-cross 
Abstract: Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies</title>
<link>https://arxiv.org/abs/2504.08623</link>
<guid>https://arxiv.org/abs/2504.08623</guid>
<content:encoded><![CDATA[
arXiv:2504.08623v2 Announce Type: replace-cross 
Abstract: The Model Context Protocol (MCP), introduced by Anthropic, provides a standardized framework for artificial intelligence (AI) systems to interact with external data sources and tools in real-time. While MCP offers significant advantages for AI integration and capability extension, it introduces novel security challenges that demand rigorous analysis and mitigation. This paper builds upon foundational research into MCP architecture and preliminary security assessments to deliver enterprise-grade mitigation frameworks and detailed technical implementation strategies. Through systematic threat modeling and analysis of MCP implementations and analysis of potential attack vectors, including sophisticated threats like tool poisoning, we present actionable security patterns tailored for MCP implementers and adopters. The primary contribution of this research lies in translating theoretical security concerns into a practical, implementable framework with actionable controls, thereby providing essential guidance for the secure enterprise adoption and governance of integrated AI systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</title>
<link>https://arxiv.org/abs/2504.08685</link>
<guid>https://arxiv.org/abs/2504.08685</guid>
<content:encoded><![CDATA[
arXiv:2504.08685v2 Announce Type: replace-cross 
Abstract: This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08837</link>
<guid>https://arxiv.org/abs/2504.08837</guid>
<content:encoded><![CDATA[
arXiv:2504.08837v2 Announce Type: replace-cross 
Abstract: Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Estimation of the KL Divergence Between Language Models</title>
<link>https://arxiv.org/abs/2504.10637</link>
<guid>https://arxiv.org/abs/2504.10637</guid>
<content:encoded><![CDATA[
arXiv:2504.10637v2 Announce Type: replace-cross 
Abstract: Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Time Schedulers for Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2504.13612</link>
<guid>https://arxiv.org/abs/2504.13612</guid>
<content:encoded><![CDATA[
arXiv:2504.13612v2 Announce Type: replace-cross 
Abstract: The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</title>
<link>https://arxiv.org/abs/2504.14693</link>
<guid>https://arxiv.org/abs/2504.14693</guid>
<content:encoded><![CDATA[
arXiv:2504.14693v2 Announce Type: replace-cross 
Abstract: Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs</title>
<link>https://arxiv.org/abs/2504.15210</link>
<guid>https://arxiv.org/abs/2504.15210</guid>
<content:encoded><![CDATA[
arXiv:2504.15210v2 Announce Type: replace-cross 
Abstract: Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity</title>
<link>https://arxiv.org/abs/2504.15941</link>
<guid>https://arxiv.org/abs/2504.15941</guid>
<content:encoded><![CDATA[
arXiv:2504.15941v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building A Secure Agentic AI Application Leveraging A2A Protocol</title>
<link>https://arxiv.org/abs/2504.16902</link>
<guid>https://arxiv.org/abs/2504.16902</guid>
<content:encoded><![CDATA[
arXiv:2504.16902v2 Announce Type: replace-cross 
Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study</title>
<link>https://arxiv.org/abs/2504.18267</link>
<guid>https://arxiv.org/abs/2504.18267</guid>
<content:encoded><![CDATA[
arXiv:2504.18267v2 Announce Type: replace-cross 
Abstract: This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes.
  Our results show that these neural operators perform well in easy scenarios with fewer discontinuities in the initial condition, yet they struggle in complex scenarios with multiple initial discontinuities and dynamic boundary conditions, even when trained specifically on such complex samples. The predicted solutions often appear smoother, resulting in a reduction in total variation and a loss of important physical features. This smoothing behavior is similar to issues discussed by Daganzo (1995), where models that introduce artificial diffusion were shown to miss essential features such as shock waves in hyperbolic systems. These results suggest that current neural operator architectures may introduce unintended regularization effects that limit their ability to capture transport dynamics governed by discontinuities. They also raise concerns about generalizing these methods to traffic applications where shock preservation is essential.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROSA: A Knowledge-based Solution for Robot Self-Adaptation</title>
<link>https://arxiv.org/abs/2505.00733</link>
<guid>https://arxiv.org/abs/2505.00733</guid>
<content:encoded><![CDATA[
<div> framework, self-adaptation, robotic systems, ROSA, knowledge-based

Summary:
ROSA is a knowledge-based framework designed for RObot Self-Adaptation, allowing for task-and-architecture co-adaptation in robotic systems. It utilizes a knowledge model to capture application-specific information needed for adaptation and reasons over this data at runtime to determine when and how adaptation should take place. ROSA offers an open-source ROS 2-based reference implementation and has been evaluated in an underwater robotics application, demonstrating advantages in reusability and development efficiency for self-adaptive robotic systems. By enabling robots to adapt their task execution and software architecture in real-time based on their context, ROSA addresses challenges associated with operating in diverse environments and handling multiple tasks amidst uncertainties. This research contributes to advancing the field of autonomous robotics and demonstrates the feasibility and performance benefits of the ROSA framework. <div>
arXiv:2505.00733v1 Announce Type: new 
Abstract: Autonomous robots must operate in diverse environments and handle multiple tasks despite uncertainties. This creates challenges in designing software architectures and task decision-making algorithms, as different contexts may require distinct task logic and architectural configurations. To address this, robotic systems can be designed as self-adaptive systems capable of adapting their task execution and software architecture at runtime based on their context.This paper introduces ROSA, a novel knowledge-based framework for RObot Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in robotic systems. ROSA achieves this by providing a knowledge model that captures all application-specific knowledge required for adaptation and by reasoning over this knowledge at runtime to determine when and how adaptation should occur. In addition to a conceptual framework, this work provides an open-source ROS 2-based reference implementation of ROSA and evaluates its feasibility and performance in an underwater robotics application. Experimental results highlight ROSA's advantages in reusability and development effort for designing self-adaptive robotic systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor</title>
<link>https://arxiv.org/abs/2505.00795</link>
<guid>https://arxiv.org/abs/2505.00795</guid>
<content:encoded><![CDATA[
<div> algorithm, Markov Decision Problem, Howard's Policy Iteration, deterministic transitions, upper bounds

Summary:
- Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov Decision Problems (MDPs) using a greedy switching rule.
- HPI has exponential upper bounds in the number of states for MDPs despite being introduced over 60 years ago.
- A subexponential upper bound for HPI on deterministic MDPs is reported in this paper, dependent on the bit-size of rewards, independent of the discount factor.
- The same improved upper bound applies to deterministic MDPs with only two possible rewards, regardless of reward size.
- This advancement marks a significant improvement in the efficiency of solving MDPs using Howard's Policy Iteration. 

<br /><br />Summary: <div>
arXiv:2505.00795v1 Announce Type: new 
Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any non-optimal policy to a dominating one, iterating until an optimal policy is found. Despite its introduction over 60 years ago, the best-known upper bounds on HPI's running time remain exponential in the number of states -- indeed even on the restricted class of MDPs with only deterministic transitions (DMDPs). Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of actions per state is only linear. In this paper, we report a significant improvement: a subexponential upper bound for HPI on DMDPs, which is parameterised by the bit-size of the rewards, while independent of the discount factor. The same upper bound also applies to DMDPs with only two possible rewards (which may be of arbitrary size).
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration</title>
<link>https://arxiv.org/abs/2505.00802</link>
<guid>https://arxiv.org/abs/2505.00802</guid>
<content:encoded><![CDATA[
<div> Detect, interpret, fairness, explainability methods, bias detectors

Summary:
The paper discusses the intersection of explainability and fairness in Artificial Intelligence (AI) systems, focusing on detecting and interpreting unfairness. A pipeline integrating local post-hoc explanation methods is proposed to derive fairness-related insights. Critical questions addressed include the relationship between distributive and procedural fairness, the effects of removing protected attributes, consistency and quality of results across different explanation methods, impact of aggregation strategies on group fairness evaluations, and overall trustworthiness of explanations as bias detectors. The results demonstrate the potential of using explanation methods for fairness, emphasizing the importance of carefully considering these critical aspects for promoting responsible AI systems. <div>
arXiv:2505.00802v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) is increasingly used in areas that significantly impact human lives, concerns about fairness and transparency have grown, especially regarding their impact on protected groups. Recently, the intersection of explainability and fairness has emerged as an important area to promote responsible AI systems. This paper explores how explainability methods can be leveraged to detect and interpret unfairness. We propose a pipeline that integrates local post-hoc explanation methods to derive fairness-related insights. During the pipeline design, we identify and address critical questions arising from the use of explanations as bias detectors such as the relationship between distributive and procedural fairness, the effect of removing the protected attribute, the consistency and quality of results across different explanation methods, the impact of various aggregation strategies of local explanations on group fairness evaluations, and the overall trustworthiness of explanations as bias detectors. Our results show the potential of explanation methods used for fairness while highlighting the need to carefully consider the aforementioned critical aspects.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction</title>
<link>https://arxiv.org/abs/2505.00827</link>
<guid>https://arxiv.org/abs/2505.00827</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical risk prediction, machine learning algorithms, time series clinical events, MIMIC-IV-Note, dataset

Summary: 
Clinical risk prediction in healthcare relies on high-quality time series clinical event data. The MIMIC-\RNum{4}-Ext-22MCTS dataset, containing 22,588,586 clinical events extracted from discharge summaries of MIMIC-IV-Note, addresses challenges of lengthy summaries and missing timestamps. A framework is proposed to break down summaries, retrieve relevant chunks using contextual search, and teach Llama-3.1-8B model to identify temporal information. Models fine-tuned on this dataset show significant improvements, with a BERT model achieving 10% accuracy increase in medical question answering and 3% in clinical trial matching tasks compared to classic BERT. GPT-2 also produces more reliable clinical results for questions when fine-tuned on this dataset.

<br /><br />Summary: <div>
arXiv:2505.00827v1 Announce Type: new 
Abstract: Clinical risk prediction based on machine learning algorithms plays a vital role in modern healthcare. A crucial component in developing a reliable prediction model is collecting high-quality time series clinical events. In this work, we release such a dataset that consists of 22,588,586 Clinical Time Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note \cite{Johnson2023-pg}. We then extract clinical events as short text span from the discharge summaries, along with the timestamps of these events as temporal information. The general-purpose MIMIC-IV-Note pose specific challenges for our work: it turns out that the discharge summaries are too lengthy for typical natural language models to process, and the clinical events of interest often are not accompanied with explicit timestamps. Therefore, we propose a new framework that works as follows: 1) we break each discharge summary into manageably small text chunks; 2) we apply contextual BM25 and contextual semantic search to retrieve chunks that have a high potential of containing clinical events; and 3) we carefully design prompts to teach the recently released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer temporal information of the chunks. We show that the obtained dataset is so informative and transparent that standard models fine-tuned on our dataset are achieving significant improvements in healthcare applications. In particular, the BERT model fine-tuned based on our dataset achieves 10\% improvement in accuracy on medical question answering task, and 3\% improvement in clinical trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned on our dataset, produces more clinically reliable results for clinical questions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines</title>
<link>https://arxiv.org/abs/2505.00875</link>
<guid>https://arxiv.org/abs/2505.00875</guid>
<content:encoded><![CDATA[
<div> pipeline, LLMs, HCXAI, explainability, CoT reasoning

Summary: 
Agentic pipelines, involving multiple LLMs with minimal human control, pose challenges for human-centered explainability. The HCXAI community is exploring ways to make the inner workings of LLMs more transparent and actionable. A perceptive task guidance system implemented in an agentic pipeline was studied, revealing that CoT reasoning, often used for explainability in LLMs, did not consistently improve outputs or offer meaningful explanations. The study found that CoT reasoning alone did not enhance user understanding or goal achievement. This research highlights the complexity of achieving explainability in agentic pipelines and the limitations of relying solely on CoT reasoning for this purpose. <div>
arXiv:2505.00875v1 Announce Type: new 
Abstract: Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression</title>
<link>https://arxiv.org/abs/2505.00876</link>
<guid>https://arxiv.org/abs/2505.00876</guid>
<content:encoded><![CDATA[
<div> sensor health monitoring system, automotive industry, machine learning, deep learning, sensor data

Summary:
- A sensor health monitoring system for the automotive industry was developed using cutting-edge techniques to process sensor data within the ECU.
- Machine learning and deep learning methodologies were explored to analyze sensor correlations and identify the most correlated sensor data.
- Autoencoders were used for detecting sensor failures, and random forest regression for estimating sensor values, resulting in impressive outcomes.
- A statistical model based on the normal distribution was created to proactively identify possible sensor failures.
- The system proactively detects faulty sensors, alerts both the driver and maintenance department, and replaces faulty sensor values with estimated values, achieving an accuracy rate of 99% when evaluated with data from essential sensors in Saipa's Quick vehicle's ECU.<br /><br />Summary: <div>
arXiv:2505.00876v1 Announce Type: new 
Abstract: Driver assistance systems provide a wide range of crucial services, including closely monitoring the condition of vehicles. This paper showcases a groundbreaking sensor health monitoring system designed for the automotive industry. The ingenious system leverages cutting-edge techniques to process data collected from various vehicle sensors. It compares their outputs within the Electronic Control Unit (ECU) to evaluate the health of each sensor. To unravel the intricate correlations between sensor data, an extensive exploration of machine learning and deep learning methodologies was conducted. Through meticulous analysis, the most correlated sensor data were identified. These valuable insights were then utilized to provide accurate estimations of sensor values. Among the diverse learning methods examined, the combination of autoencoders for detecting sensor failures and random forest regression for estimating sensor values proved to yield the most impressive outcomes. A statistical model using the normal distribution has been developed to identify possible sensor failures proactively. By comparing the actual values of the sensors with their estimated values based on correlated sensors, faulty sensors can be detected early. When a defective sensor is detected, both the driver and the maintenance department are promptly alerted. Additionally, the system replaces the value of the faulty sensor with the estimated value obtained through analysis. This proactive approach was evaluated using data from twenty essential sensors in the Saipa's Quick vehicle's ECU, resulting in an impressive accuracy rate of 99\%.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2505.00972</link>
<guid>https://arxiv.org/abs/2505.00972</guid>
<content:encoded><![CDATA[
<div> behavior analyzer, large language model, simulation-based testing, autonomous vehicles, safety-critical scenarios
Summary:
The paper introduces an online framework using a large language model (LLM) for generating safety-critical driving scenarios to test autonomous vehicles. It first uses an LLM-based behavior analyzer to infer dangerous intents of background vehicles and then queries additional LLM agents to create adversarial trajectories. The framework includes a dynamic memorization and retrieval bank to prevent catastrophic forgetting and adapt quickly to novel intents. Evaluation using the Waymo Open Motion Dataset shows that the model reduces the mean minimum time-to-collision and collision rate, outperforming baselines. The framework is designed to address the limitations of existing scenario generation methods by generating interactive and diverse scenarios for better validation of autonomous vehicles. <br /><br />Summary: <div>
arXiv:2505.00972v1 Announce Type: new 
Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Large Language Model Planning with Action Sequence Similarity</title>
<link>https://arxiv.org/abs/2505.01009</link>
<guid>https://arxiv.org/abs/2505.01009</guid>
<content:encoded><![CDATA[
<div> signals, planning capability, in-context learning, exemplars, performance improvement

Summary:
This article discusses the use of in-context learning to improve the planning capability of large language models (LLMs). The study explores how signal selection can impact model performance, with a focus on filtering exemplars based on action sequence similarity. A two-stage pipeline, GRASE-DC, is proposed to re-sample and curate exemplars for improved planning accuracy. Experimental results show significant performance enhancements on various planning tasks, with up to 40 point absolute accuracy improvement and reduced exemplar usage. GRASE-DC* + VAL, incorporating a validator, further boosts performance by 18.9%. Analysis confirms consistent performance gains across different LLMs and planning benchmarks. Notably, using simpler problems as exemplars can improve planning accuracy on more challenging tasks, demonstrating the model's ability to generalize to new problems. <div>
arXiv:2505.01009v1 Announce Type: new 
Abstract: Planning is essential for artificial intelligence systems to look ahead and proactively determine a course of actions to reach objectives in the virtual and real world. Recent work on large language models (LLMs) sheds light on their planning capability in various tasks. However, it remains unclear what signals in the context influence the model performance. In this work, we explore how to improve the model planning capability through in-context learning (ICL), specifically, what signals can help select the exemplars. Through extensive experiments, we observe that commonly used problem similarity may result in false positives with drastically different plans, which can mislead the model. In response, we propose to sample and filter exemplars leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a two-stage pipeline that first re-samples high AS exemplars and then curates the selected exemplars with dynamic clustering on AS to achieve a balance of relevance and diversity. Our experimental result confirms that GRASE-DC achieves significant performance improvement on various planning tasks (up to ~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of GRASE-DC with various backbone LLMs and on both classical planning and natural language planning benchmarks. GRASE-DC can further boost the planning accuracy by ~24 absolute points on harder problems using simpler problems as exemplars over a random baseline. This demonstrates its ability to generalize to out-of-distribution problems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory</title>
<link>https://arxiv.org/abs/2505.01028</link>
<guid>https://arxiv.org/abs/2505.01028</guid>
<content:encoded><![CDATA[
<div> attack graph, Active Directory, security vulnerabilities, path removal, heuristic

Summary:<br />
The article discusses security vulnerabilities in Windows Active Directory systems. It introduces an Adaptive Path Removal Problem to optimize the process of removing edges from attack paths. The problem aims to minimize the number of interactions between security teams and IT admins. The article proves the problem's complexity and proposes solutions including exact, approximate, and heuristic algorithms. The best heuristic, called DPR, is shown to be effective on larger-scale graphs and outperforms the approximate algorithm. The algorithms' effectiveness is verified on synthetic AD graphs and an attack graph from a real organization. <div>
arXiv:2505.01028v1 Announce Type: new 
Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are typically modeled using an attack graph and hardening AD systems involves an iterative workflow: security teams propose an edge to remove, and IT operations teams manually review these fixes before implementing the removal. As verification requires significant manual effort, we formulate an Adaptive Path Removal Problem to minimize the number of steps in this iterative removal process. In our model, a wizard proposes an attack path in each step and presents it as a set of multiple-choice options to the IT admin. The IT admin then selects one edge from the proposed set to remove. This process continues until the target $t$ is disconnected from source $s$ or the number of proposed paths reaches $B$. The model aims to optimize the human effort by minimizing the expected number of interactions between the IT admin and the security wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then propose a set of solutions including an exact algorithm, an approximate algorithm, and several scalable heuristics. Our best heuristic, called DPR, can operate effectively on larger-scale graphs compared to the exact algorithm and consistently outperforms the approximate algorithm across all graphs. We verify the effectiveness of our algorithms on several synthetic AD graphs and an AD attack graph collected from a real organization.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation</title>
<link>https://arxiv.org/abs/2505.01073</link>
<guid>https://arxiv.org/abs/2505.01073</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, self-supervised learning, retrieval-augmented generation, decision-making, autonomous knowledge generation

Summary:
Retrial-Augmented Learning (RAL) introduces a reward-free self-supervised learning framework for Large Language Models (LLMs) that avoids the need for model training. By incorporating Retrieval-Augmented Generation (RAG) as an intermediate data organization module, RAL facilitates a three-stage autonomous knowledge generation process: hypothesis generation, hypothesis validation, and knowledge generation. Through evaluation in the LLM-PySC2 environment, known for its complex decision-making requirements, RAL proves effective in reducing hallucination and improving decision-making performance at a minimal cost. The method also shows promise in out-of-distribution tasks, demonstrating robustness and transferability. Overall, RAL offers a cost-effective solution for decision-making challenges and autonomous knowledge generation.<br /><br />Summary: <div>
arXiv:2505.01073v1 Announce Type: new 
Abstract: The lack of domain-specific data in the pre-training of Large Language Models (LLMs) severely limits LLM-based decision systems in specialized applications, while post-training a model in the scenarios requires significant computational resources. In this paper, we present Retrial-Augmented Learning (RAL), a reward-free self-supervised learning framework for LLMs that operates without model training. By developing Retrieval-Augmented Generation (RAG) into a module for organizing intermediate data, we realized a three-stage autonomous knowledge generation of proposing a hypothesis, validating the hypothesis, and generating the knowledge. The method is evaluated in the LLM-PySC2 environment, a representative decision-making platform that combines sufficient complexity with domain-specific knowledge requirements. Experiments demonstrate that the proposed method effectively reduces hallucination by generating and utilizing validated knowledge, and increases decision-making performance at an extremely low cost. Meanwhile, the approach exhibits potential in out-of-distribution(OOD) tasks, robustness, and transferability, making it a cost-friendly but effective solution for decision-making problems and autonomous knowledge generation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark</title>
<link>https://arxiv.org/abs/2505.01081</link>
<guid>https://arxiv.org/abs/2505.01081</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Abstraction and Reasoning Corpus (ARC), Minimum Description Length (MDL), inductive learning, structured generalization <br />
Summary: 
The article introduces MADIL, a new approach to artificial intelligence that leverages the Minimum Description Length (MDL) principle for efficient inductive learning. MADIL focuses on pattern-based decomposition, enabling structured generalization in tasks such as the Abstraction and Reasoning Corpus (ARC) benchmark. While MADIL's performance currently lags behind Large Language Models (LLMs) at 7% in ArcPrize 2024, it offers advantages in terms of efficiency and interpretability. The methodology of MADIL is detailed in the paper, along with its application to ARC and experimental evaluations. This innovative approach aims to address the challenges of skill acquisition and generalization faced by current AI systems. <div>
arXiv:2505.01081v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized tasks but struggles with efficient skill acquisition and generalization. The Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based on minimal training requirements. While Large Language Models (LLMs) have recently improved ARC performance, they rely on extensive pre-training and high computational costs. We introduce MADIL (MDL-based AI), a novel approach leveraging the Minimum Description Length (MDL) principle for efficient inductive learning. MADIL performs pattern-based decomposition, enabling structured generalization. While its performance (7% at ArcPrize 2024) remains below LLM-based methods, it offers greater efficiency and interpretability. This paper details MADIL's methodology, its application to ARC, and experimental evaluations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms</title>
<link>https://arxiv.org/abs/2505.01181</link>
<guid>https://arxiv.org/abs/2505.01181</guid>
<content:encoded><![CDATA[
<div> Keywords: Swarming systems, data poisoning attacks, explainable AI methods, evolutionary intelligence, cooperative tasks<br />
Summary:<br />
Swarming systems, such as multi-drone networks, are efficient at cooperative tasks but susceptible to data poisoning attacks. A new framework using explainable AI methods investigates the effects of such attacks on team-level coordination. The framework models agent interaction through evolutionary intelligence, leading to the emergence of an optimal coalition for coordinated tasks. Rigorous evaluation shows that data manipulation attacks can disrupt team strategy, with non-optimal strategies leading to inefficient cooperation when poisoning exceeds 10%. Explainable AI methods are applied to quantify the effects of poisoning and extract footprint characterizations for diagnosis. This research highlights the importance of understanding and detecting data poisoning attacks in swarming systems to ensure effective and robust coordination among autonomous agents. <br /> <div>
arXiv:2505.01181v1 Announce Type: new 
Abstract: Swarming systems, such as for example multi-drone networks, excel at cooperative tasks like monitoring, surveillance, or disaster assistance in critical environments, where autonomous agents make decentralized decisions in order to fulfill team-level objectives in a robust and efficient manner. Unfortunately, team-level coordinated strategies in the wild are vulnerable to data poisoning attacks, resulting in either inaccurate coordination or adversarial behavior among the agents. To address this challenge, we contribute a framework that investigates the effects of such data poisoning attacks, using explainable AI methods. We model the interaction among agents using evolutionary intelligence, where an optimal coalition strategically emerges to perform coordinated tasks. Then, through a rigorous evaluation, the swarm model is systematically poisoned using data manipulation attacks. We showcase the applicability of explainable AI methods to quantify the effects of poisoning on the team strategy and extract footprint characterizations that enable diagnosing. Our findings indicate that when the model is poisoned above 10%, non-optimal strategies resulting in inefficient cooperation can be identified.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions</title>
<link>https://arxiv.org/abs/2505.01192</link>
<guid>https://arxiv.org/abs/2505.01192</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Explainable AI, Personality traits, Decision-making, Loan application<br />
Summary: <br />
This study explores the impact of different types of explanations provided by AI systems, as well as the influence of individual personality traits, specifically the Need for Cognition (NFC), on decision-making accuracy, reliance on AI, and cognitive load. The research found that high AI confidence increased reliance on AI and reduced cognitive load. While feature-based explanations did not significantly improve accuracy, counterfactual explanations, despite being less understandable, enhanced overall accuracy. Both low and high NFC individuals prioritized explanations over AI information, suggesting the importance of personalized XAI interfaces. Surprisingly, there were no significant differences in decision-making accuracy or cognitive load between low and high NFC groups, indicating the need for further investigation into the role of personality traits in AI-assisted decision-making. These findings emphasize the importance of user-centric design in XAI interfaces and the potential benefits of incorporating varied explanation styles to optimize human-AI collaboration. <br /> <div>
arXiv:2505.01192v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) systems are increasingly used for decision-making across domains, raising debates over the information and explanations they should provide. Most research on Explainable AI (XAI) has focused on feature-based explanations, with less attention on alternative styles. Personality traits like the Need for Cognition (NFC) can also lead to different decision-making outcomes among low and high NFC individuals. We investigated how presenting AI information (prediction, confidence, and accuracy) and different explanation styles (example-based, feature-based, rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive load in a loan application scenario. We also examined low and high NFC individuals' differences in prioritizing XAI interface elements (loan attributes, AI information, and explanations), accuracy, and cognitive load. Our findings show that high AI confidence significantly increases reliance on AI while reducing cognitive load. Feature-based explanations did not enhance accuracy compared to other conditions. Although counterfactual explanations were less understandable, they enhanced overall accuracy, increasing reliance on AI and reducing cognitive load when AI predictions were correct. Both low and high NFC individuals prioritized explanations after loan attributes, leaving AI information as the least important. However, we found no significant differences between low and high NFC groups in accuracy or cognitive load, raising questions about the role of personality traits in AI-assisted decision-making. These findings highlight the need for user-centric personalization in XAI interfaces, incorporating diverse explanation styles and exploring multiple personality traits and other user characteristics to optimize human-AI collaboration.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System</title>
<link>https://arxiv.org/abs/2505.01305</link>
<guid>https://arxiv.org/abs/2505.01305</guid>
<content:encoded><![CDATA[
<div> Keywords: early detection, heart rate data, wearable devices, TARL, ICU

Summary:
TARL is a novel approach that utilizes shapelet dynamics in heart rate time series to model illness progression and predict future changes. By creating a shapelet-transition knowledge graph and implementing a transition-aware knowledge embedding, TARL effectively handles missing values in wearable device data and provides comprehensive representations of heart rate patterns. In collaboration with healthcare professionals, TARL was evaluated using real-world ICU patient data, demonstrating high reliability and early detection capability. The explainable detection process of TARL shows promise in assisting clinicians to recognize early signs of patient deterioration. <div>
arXiv:2505.01305v1 Announce Type: new 
Abstract: Early detection of patient deterioration is crucial for reducing mortality rates. Heart rate data has shown promise in assessing patient health, and wearable devices offer a cost-effective solution for real-time monitoring. However, extracting meaningful insights from diverse heart rate data and handling missing values in wearable device data remain key challenges. To address these challenges, we propose TARL, an innovative approach that models the structural relationships of representative subsequences, known as shapelets, in heart rate time series. TARL creates a shapelet-transition knowledge graph to model shapelet dynamics in heart rate time series, indicating illness progression and potential future changes. We further introduce a transition-aware knowledge embedding to reinforce relationships among shapelets and quantify the impact of missing values, enabling the formulation of comprehensive heart rate representations. These representations capture explanatory structures and predict future heart rate trends, aiding early illness detection. We collaborate with physicians and nurses to gather ICU patient heart rate data from wearables and diagnostic metrics assessing illness severity for evaluating deterioration. Experiments on real-world ICU data demonstrate that TARL achieves both high reliability and early detection. A case study further showcases TARL's explainable detection process, highlighting its potential as an AI-driven tool to assist clinicians in recognizing early signs of patient deterioration.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing</title>
<link>https://arxiv.org/abs/2505.01343</link>
<guid>https://arxiv.org/abs/2505.01343</guid>
<content:encoded><![CDATA[
<div> trade-off, multi-modal model editing, BalancEdit, generality, locality

Summary:<br />
- Large multi-modal models decay over time due to outdated information.
- Traditional fine-tuning is impractical for updating large models.
- Direct knowledge editing is a more viable solution.
- The generality-locality trade-off affects model performance.
- BalancEdit method dynamically balances generality and locality in model editing. 
- BalancEdit uses positive and negative samples for each fact to determine influence scope.
- BalancEdit incorporates insights into the model's latent space using a localized codebook of edits.
- BalancEdit is the first approach addressing the generality-locality trade-off in multi-modal model editing.
- Comprehensive results show BalancEdit's effectiveness with minimal trade-offs and robust editing capabilities.
- Code and dataset for BalancEdit will be available. 

Summary: <div>
arXiv:2505.01343v1 Announce Type: new 
Abstract: Large multi-modal models inevitably decay over time as facts change and previously learned information becomes outdated. Traditional approaches such as fine-tuning are often impractical for updating these models due to their size and complexity. Instead, direct knowledge editing within the models presents a more viable solution. Current model editing techniques, however, typically overlook the unique influence ranges of different facts, leading to compromised model performance in terms of both generality and locality. To address this issue, we introduce the concept of the generality-locality trade-off in multi-modal model editing. We develop a new model editing dataset named OKEDIT, specifically designed to effectively evaluate this trade-off. Building on this foundation, we propose BalancEdit, a novel method for balanced model editing that dynamically achieves an optimal balance between generality and locality. BalancEdit utilizes a unique mechanism that generates both positive and negative samples for each fact to accurately determine its influence scope and incorporates these insights into the model's latent space using a discrete, localized codebook of edits, without modifying the underlying model weights. To our knowledge, this is the first approach explicitly addressing the generality-locality trade-off in multi-modal model editing. Our comprehensive results confirm the effectiveness of BalancEdit, demonstrating minimal trade-offs while maintaining robust editing capabilities. Our code and dataset will be available.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection</title>
<link>https://arxiv.org/abs/2411.09200</link>
<guid>https://arxiv.org/abs/2411.09200</guid>
<content:encoded><![CDATA[
<div> AI, anomaly detection, network traffic analysis, CI/CD pipeline security, Convolution Neural Network(LSTM)
Summary:
This research focuses on enhancing security in Continuous Integration/Continuous Deployment (CI/CD) pipelines by implementing AI-supported anomaly detection for network traffic patterns. The goal is to identify unusual behavior or deviations in pipeline and cloud platform activities to mitigate cyber threats. Using popular network traffic datasets, a combination of Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) was employed to achieve a high accuracy in detecting unusual traffic patterns. The system integrates into the workflow to continuously monitor activities and generate log files to address security challenges in modern DevOps practices. This approach contributes to advancing software security and reliability in the CI/CD pipeline environment. 
<br /><br />Summary: <div>
arXiv:2411.09200v1 Announce Type: cross 
Abstract: Continuous Integration/Continuous Deployment (CI/CD) is fundamental for advanced software development, supporting faster and more efficient delivery of code changes into cloud environments. However, security issues in the CI/CD pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are happening over the cloud environments. While plenty of literature discusses static security testing and CI/CD practices, only a few deal with network traffic pattern analysis to detect different cyberattacks. This research aims to enhance CI/CD pipeline security by implementing anomaly detection through AI (Artificial Intelligence) support. The goal is to identify unusual behaviour or variations from network traffic patterns in pipeline and cloud platforms. The system shall integrate into the workflow to continuously monitor pipeline activities and cloud infrastructure. Additionally, it aims to explore adaptive response mechanisms to mitigate the detected anomalies or security threats. This research employed two popular network traffic datasets, CSE-CIC-IDS2018 and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files in different CI/CD pipeline stages that resemble the network anomalies affected to address security challenges in modern DevOps practices, contributing to advancing software security and reliability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div> Keywords: moral stories, NLP, dataset, fables, instruction-tuned models

Summary:
TF1-EN-3M is introduced as the first open dataset of three million English-language fables created by instruction-tuned models. Each story follows a structured scaffold including character, trait, setting, conflict, resolution, and moral. The dataset allows for research in instruction following, narrative intelligence, value alignment, and educational AI. The evaluation pipeline combines a GPT-based critic with diversity and readability metrics to assess the generated fables. An 8B-parameter Llama-3 variant is determined to offer the best quality-speed trade-off. The dataset, generation code, evaluation scripts, and metadata are released under a permissive license for reproducibility. TF1-EN-3M demonstrates that large-scale moral storytelling can be achieved without the need for proprietary giant models.<br /><br />Summary: TF1-EN-3M is a groundbreaking dataset of three million English fables generated by instruction-tuned models, providing a structured scaffold for moral stories. The dataset enables research in various fields and showcases the potential for large-scale moral storytelling without relying on proprietary models. <div>
arXiv:2504.20605v1 Announce Type: cross 
Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond</title>
<link>https://arxiv.org/abs/2505.00737</link>
<guid>https://arxiv.org/abs/2505.00737</guid>
<content:encoded><![CDATA[
<div> Keywords: plant phenotyping, 3D reconstruction, Neural Radiance Fields, 3D Gaussian Splatting, precision agriculture<br />
<br />
Summary: 
Plant phenotyping is essential for understanding plant traits and their interactions with the environment, crucial for precision agriculture and crop improvement. Various 3D reconstruction techniques, including classical methods, Neural Radiance Fields (NeRF), and 3D Gaussian Splatting (3DGS), offer significant potential for accurate and automated phenotyping. Classical methods are widely used but face challenges such as data density and noise. NeRF provides high-quality 3D reconstructions but has computational costs and limitations in outdoor environments. On the other hand, 3DGS introduces a novel approach using Gaussian primitives, promising efficiency and scalability benefits. This review discusses the methodologies, applications, and performance of these techniques, highlighting their strengths, limitations, and future prospects in plant phenotyping. Leveraging diverse 3D reconstruction techniques can lead to automated and high-throughput plant phenotyping, advancing agricultural technology.<br /><br /> <div>
arXiv:2505.00737v1 Announce Type: cross 
Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zoomer: Adaptive Image Focus Optimization for Black-box MLLM</title>
<link>https://arxiv.org/abs/2505.00742</link>
<guid>https://arxiv.org/abs/2505.00742</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, visual prompting mechanism, object recognition, token limits, performance improvement

Summary: 
\SysName is a novel visual prompting mechanism designed to enhance the performance of multimodal large language models (MLLMs) by addressing limitations in accurately processing visual data within token limits. It features a prompt-aware strategy that highlights relevant image regions, a spatial-preserving orchestration schema to maintain object integrity, and a budget-aware prompting method to balance global context with crucial visual details. Through comprehensive evaluations on multiple datasets, \SysName consistently outperforms baseline methods, achieving up to a 26.9% improvement in accuracy while significantly reducing token consumption. <div>
arXiv:2505.00742v1 Announce Type: cross 
Abstract: Recent advancements in multimodal large language models (MLLMs) have broadened the scope of vision-language tasks, excelling in applications like image captioning and interactive question-answering. However, these models struggle with accurately processing visual data, particularly in tasks requiring precise object recognition and fine visual details. Stringent token limits often result in the omission of critical information, hampering performance. To address these limitations, we introduce \SysName, a novel visual prompting mechanism designed to enhance MLLM performance while preserving essential visual details within token limits. \SysName features three key innovations: a prompt-aware strategy that dynamically highlights relevant image regions, a spatial-preserving orchestration schema that maintains object integrity, and a budget-aware prompting method that balances global context with crucial visual details. Comprehensive evaluations across multiple datasets demonstrate that \SysName consistently outperforms baseline methods, achieving up to a $26.9\%$ improvement in accuracy while significantly reducing token consumption.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Coral Protocol: Open Infrastructure Connecting The Internet of Agents</title>
<link>https://arxiv.org/abs/2505.00749</link>
<guid>https://arxiv.org/abs/2505.00749</guid>
<content:encoded><![CDATA[
<div> protocol, decentralized, collaboration, interoperability, AI <br />
<br />
The Coral Protocol is a decentralized collaboration infrastructure designed to facilitate communication, coordination, trust, and payments for the Internet of Agents. It seeks to address the need for interoperability among specialized AI agents deployed by organizations across different domains and vendors. Coral serves as a foundational platform for multi-agent AI ecosystems, providing a common language and coordination framework for agents to participate in complex workflows efficiently. The protocol introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for creating trusted groups of agents dynamically. These features position Coral Protocol at the forefront of the emerging Internet of Agents, enabling enhanced levels of automation, collective intelligence, and business value through open agent collaboration. <br /><br />Summary: <div>
arXiv:2505.00749v1 Announce Type: cross 
Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging "Internet of Agents," unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking</title>
<link>https://arxiv.org/abs/2505.00752</link>
<guid>https://arxiv.org/abs/2505.00752</guid>
<content:encoded><![CDATA[
<div> Keywords: nighttime UAV tracking, dynamic feature blending, adaptive representation, Vision Transformer, multi-task loss functions<br />
Summary:<br />
The article introduces a new tracking framework called DARTer for nighttime UAV scenarios. DARTer utilizes a Dynamic Feature Blender (DFB) to blend multi-perspective nighttime features, enhancing representation robustness. Additionally, a Dynamic Feature Activator (DFA) is used to adaptively activate Vision Transformer layers, improving efficiency by reducing redundant computations. The model eliminates the need for complex multi-task loss functions, simplifying the training process. Extensive experiments on multiple nighttime UAV tracking benchmarks show that DARTer outperforms existing trackers in terms of both accuracy and efficiency. The results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.<br /><br /> <div>
arXiv:2505.00752v1 Announce Type: cross 
Abstract: Nighttime UAV tracking presents significant challenges due to extreme illumination variations and viewpoint changes, which severely degrade tracking performance. Existing approaches either rely on light enhancers with high computational costs or introduce redundant domain adaptation mechanisms, failing to fully utilize the dynamic features in varying perspectives. To address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic \textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end tracking framework designed for nighttime UAV scenarios. DARTer leverages a Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime features from static and dynamic templates, enhancing representation robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates Vision Transformer layers based on extracted features, significantly improving efficiency by reducing redundant computations. Our model eliminates the need for complex multi-task loss functions, enabling a streamlined training process. Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate the superiority of DARTer over state-of-the-art trackers. These results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors</title>
<link>https://arxiv.org/abs/2505.00755</link>
<guid>https://arxiv.org/abs/2505.00755</guid>
<content:encoded><![CDATA[
<div> fabrication, e-textile garment techniques, Transformer model, motion pattern recognition, posture estimation tasks

Summary:
P2P-Insole introduces a low-cost method for capturing and visualizing 3D human skeletal data using insole-type sensors with integrated IMUs. The insoles, created through e-textile garment techniques, are cost-effective at under USD 1 each, making them suitable for mass production. The system utilizes foot pressure distribution, acceleration, and rotation data to address limitations and offers a lightweight, minimally intrusive, and privacy-aware solution. By employing a Transformer model for efficient temporal feature extraction and incorporating multimodal information like accelerometers and rotational measurements, the approach enhances accuracy in recognizing complex motion patterns. Experimental results demonstrate the effectiveness of the method in various posture estimation tasks, highlighting its robustness. This work has the potential to serve as the basis for practical applications in rehabilitation, injury prevention, and health monitoring, with room for improvement through sensor enhancements and expanded datasets. 

<br /><br />Summary: <div>
arXiv:2505.00755v1 Announce Type: cross 
Abstract: This work presents P2P-Insole, a low-cost approach for estimating and visualizing 3D human skeletal data using insole-type sensors integrated with IMUs. Each insole, fabricated with e-textile garment techniques, costs under USD 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. Our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. The system employs a Transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. Including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. These facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. This work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L</title>
<link>https://arxiv.org/abs/2505.00757</link>
<guid>https://arxiv.org/abs/2505.00757</guid>
<content:encoded><![CDATA[
<div> 4D radar, 3D object detection, Hailo-8L AI accelerator, real-time processing, low-power embedded environments <br />
Summary:
- The study focuses on implementing a 4D radar-based 3D object detection model on the Hailo-8L AI accelerator for autonomous driving applications.
- The Hailo-8L only supports 4D tensors, posing a challenge with conventional 3D CNN architectures that require 5D inputs.
- A tensor transformation method is introduced to reshape 5D inputs into 4D formats during compilation, enabling direct deployment without altering the model structure.
- The implemented system achieves an AP_3D of 46.47% and AP_BEV of 52.75% with a comparative accuracy to GPU-based models while achieving an inference speed of 13.76 Hz.
- The results highlight the feasibility of using 4D radar-based perception technologies in autonomous driving systems.<br /> 
Summary: <div>
arXiv:2505.00757v1 Announce Type: cross 
Abstract: 4D radar has attracted attention in autonomous driving due to its ability to enable robust 3D object detection even under adverse weather conditions. To practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. Addressing this, we present the first on-chip implementation of a 4D radar-based 3D object detection model on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D tensors, posing a significant challenge. To overcome this limitation, we introduce a tensor transformation method that reshapes 5D inputs into 4D formats during the compilation process, enabling direct deployment without altering the model structure. The proposed system achieves 46.47% AP_3D and 52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while achieving an inference speed of 13.76 Hz. These results demonstrate the applicability of 4D radar-based perception technologies to autonomous driving systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, generative models, evaluation, multi-modal large language models, prompt-generation consistency<br />
Summary: <br />
The paper explores using multi-modal large language models (MLLMs) as evaluator agents for text-to-image (T2I) generative models. They introduce the Multimodal Text-to-Image Eval (MT2IE) framework to evaluate prompt-generation consistency and image aesthetics efficiently. MT2IE generates prompts iteratively, scoring the generated images and matching T2I performance with existing benchmarks using a fraction of prompts. The prompt-generation consistency scores from MT2IE show higher correlation with human judgment compared to previous methods. The framework produces the same relative T2I model rankings as static benchmarks but uses significantly fewer prompts for evaluation, showcasing its efficiency in evaluating T2I models. <div>
arXiv:2505.00759v1 Announce Type: cross 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing an Optimal Behavior Basis for the Option Keyboard</title>
<link>https://arxiv.org/abs/2505.00787</link>
<guid>https://arxiv.org/abs/2505.00787</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Multi-task learning, Generalized Policy Improvement, Convex Coverage Set, Option Keyboard <br />
Summary: 
- The article discusses multi-task reinforcement learning and the challenges in quickly identifying optimal solutions for new tasks without extensive interaction with the environment.
- Generalized Policy Improvement (GPI) is introduced as a method to combine base policies to create new policies that are as good or better, with Optimality ensured in linear reward scenarios.
- The Option Keyboard (OK) method is proposed as an improvement over GPI by utilizing a meta-policy to dynamically combine base policies.
- The article presents a novel technique to construct an optimal behavior basis for zero-shot identification of optimal solutions for linear tasks, reducing the number of base policies required.
- Empirical evaluations in complex domains demonstrate that the proposed technique outperforms existing approaches, particularly in increasingly complex tasks. <br /> 
Summary: <div>
arXiv:2505.00787v1 Announce Type: cross 
Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Meta-Learning via Mixed-Mode Differentiation</title>
<link>https://arxiv.org/abs/2505.00793</link>
<guid>https://arxiv.org/abs/2505.00793</guid>
<content:encoded><![CDATA[
<div> auto differentiation, bilevel optimization, meta-learning, mixed-mode differentiation, computational graphs

Summary:
The paper discusses the challenges of gradient-based bilevel optimization, which often involves computationally expensive second-order and mixed derivatives. While automatic differentiation libraries simplify the calculation process, they may not fully utilize the problem structure, leading to suboptimal performance. To address this issue, the authors propose MixFlow-MG, a practical algorithm that leverages mixed-mode differentiation to improve computational efficiency in meta-learning setups. MixFlow-MG can significantly reduce memory usage and wall-clock time, offering up to 10x memory savings and 25% faster computation compared to standard implementations. By utilizing mixed-mode differentiation, MixFlow-MG constructs more efficient computational graphs, enhancing the scalability and performance of gradient-based optimisation tasks in various applications like hyperparameter optimisation, task adaptation, and algorithm discovery. <div>
arXiv:2505.00793v1 Announce Type: cross 
Abstract: Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation process itself, leading to "gradient-of-a-gradient" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25% wall-clock time improvements over standard implementations in modern meta-learning setups.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX</title>
<link>https://arxiv.org/abs/2505.00803</link>
<guid>https://arxiv.org/abs/2505.00803</guid>
<content:encoded><![CDATA[
<div> optimization, Traveling Salesperson Problem, EAX algorithm, AB-cycles, crossover operators

Summary:
The article explores the Edge Assembly Crossover (EAX) algorithm, a leading heuristic for solving the Traveling Salesperson Problem. It focuses on improving the algorithm's first stage, introducing a method to quickly verify the validity of generated AB-cycles. This verification is crucial before applying other crossover operators like the Generalized Partition Crossover (GPX). Through proposed enhancements to EAX, the study shows improved computational efficiency and solution quality on challenging instances. The research highlights the significance of evaluating the initial phase of heuristic algorithms like EAX, paving the way for more effective solutions in complex optimization problems. <div>
arXiv:2505.00803v1 Announce Type: cross 
Abstract: The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic for solving the Traveling Salesperson Problem (TSP). It regularly outperforms other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism that focuses on improving the current solutions, first, at the local and, subsequently, at the global level. Although the second phase of the algorithm has been thoroughly studied, configured, and refined in the past, in particular, its first stage has hardly been examined.
  In this paper, we thus focus on the first stage of EAX and introduce a novel method that quickly verifies whether the AB-cycles, generated during its internal optimization procedure, yield valid tours -- or whether they need to be repaired. Knowledge of the latter is also particularly relevant before applying other powerful crossover operators such as the Generalized Partition Crossover (GPX). Based on our insights, we propose and evaluate several improved versions of EAX. According to our benchmark study across 10 000 different TSP instances, the most promising of our proposed EAX variants demonstrates improved computational efficiency and solution quality on previously rather difficult instances compared to the current state-of-the-art EAX algorithm.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i</title>
<link>https://arxiv.org/abs/2505.00808</link>
<guid>https://arxiv.org/abs/2505.00808</guid>
<content:encoded><![CDATA[
<div> Keywords: Mechanistic Interpretability, Explanatory View Hypothesis, Explanatory Faithfulness, neural networks, causality

Summary: 

The article discusses Mechanistic Interpretability, which focuses on understanding neural networks through causal explanations. The Explanatory View Hypothesis posits that neural networks inherently contain explanations that can be extracted and understood. It introduces the concept of Explanatory Faithfulness as a measure of how well an explanation aligns with a model. Mechanistic Interpretability is defined as the production of model-level, ontic, causal-mechanistic, and falsifiable explanations of neural networks, setting it apart from other interpretability paradigms and highlighting its inherent limitations. The Principle of Explanatory Optimism is proposed as a crucial factor for the success of Mechanistic Interpretability. This article provides a framework for understanding and evaluating the interpretability of neural networks through causal explanations. 

<br /><br />Summary: <div>
arXiv:2505.00808v1 Announce Type: cross 
Abstract: Mechanistic Interpretability aims to understand neural networks through causal explanations. We argue for the Explanatory View Hypothesis: that Mechanistic Interpretability research is a principled approach to understanding models because neural networks contain implicit explanations which can be extracted and understood. We hence show that Explanatory Faithfulness, an assessment of how well an explanation fits a model, is well-defined. We propose a definition of Mechanistic Interpretability (MI) as the practice of producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural networks, allowing us to distinguish MI from other interpretability paradigms and detail MI's inherent limits. We formulate the Principle of Explanatory Optimism, a conjecture which we argue is a necessary precondition for the success of Mechanistic Interpretability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, generalization performance, noisy supervision, instance-level optimization, noise modeling

Summary: 
This study introduces a novel two-stage noisy learning framework to address the issue of degraded generalization performance in deep neural networks under noisy supervision. The framework utilizes a dynamically weighted loss function for instance-level optimization without the need for heavy hyperparameter tuning. A new metric called wrong event is introduced to effectively model the cleanliness and difficulty of individual samples. The framework first collects wrong event information to build a strong base model and then performs noise-robust training using a probabilistic model. Experimental results on various benchmarks demonstrate that the proposed method outperforms existing techniques in terms of performance and computational efficiency, reducing computational time by nearly 75% and improving model scalability. <div>
arXiv:2505.00812v1 Announce Type: cross 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models</title>
<link>https://arxiv.org/abs/2505.00817</link>
<guid>https://arxiv.org/abs/2505.00817</guid>
<content:encoded><![CDATA[
<div> Side-channel attacks, cache side-channels, Large Language Models (LLMs), tokens, embedding vectors<br />
<br />
Summary: 
This study introduces Spill The Beans, a novel method that utilizes cache side-channels to leak tokens generated by Large Language Models (LLMs). By co-locating an attack process with the victim model on shared hardware, the attackers can detect cache hits when accessing embedding vectors, leaking tokens. Despite the challenge of the LLMs' size quickly evicting embedding vectors from the cache, the researchers address this by balancing the number of monitored tokens against the leaked information. Through extensive experimentation, they demonstrate the feasibility of leaking tokens from LLMs using cache side-channels, highlighting a new vulnerability in LLM deployments. The findings emphasize that even sophisticated models are susceptible to traditional side-channel attacks, with implications for privacy and security in LLM-serving infrastructures. The study suggests mitigation strategies and showcases two concrete attack scenarios, showing high recovery rates for API keys and English text, with potential for improvement in more specialized output domains. <br /><br /> <div>
arXiv:2505.00817v1 Announce Type: cross 
Abstract: Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Texts to Shields: Convergence of Large Language Models and Cybersecurity</title>
<link>https://arxiv.org/abs/2505.00841</link>
<guid>https://arxiv.org/abs/2505.00841</guid>
<content:encoded><![CDATA[
<div> applications, LLMs, cybersecurity, challenges, research

Summary: This report explores the convergence of large language models (LLMs) and cybersecurity, highlighting their potential applications in software and network security, vulnerability analysis, and generative security engineering. It emphasizes the role of agentic LLMs in automating tasks, improving operational efficiency, and enabling reasoning-driven security analytics. The report also addresses socio-technical challenges such as trust, transparency, and ethical considerations, suggesting solutions like human-in-the-loop systems and proactive robustness testing. Critical research challenges in interpretability, safety, and fairness of LLM-based systems are outlined, particularly in high-stakes domains. By integrating technical advancements with societal and organizational factors, the report sets a research agenda for the secure and effective adoption of LLMs in cybersecurity. <div>
arXiv:2505.00841v1 Announce Type: cross 
Abstract: This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OET: Optimization-based prompt injection Evaluation Toolkit</title>
<link>https://arxiv.org/abs/2505.00843</link>
<guid>https://arxiv.org/abs/2505.00843</guid>
<content:encoded><![CDATA[
<div> Optimization-based Evaluation Toolkit, Large Language Models, prompt injection attacks, adversarial scenarios, defense strategies <br />
Summary: <br />
The article introduces OET, an optimization-based evaluation toolkit designed to assess the effectiveness of defense strategies against prompt injection attacks on Large Language Models (LLMs). These attacks manipulate model behavior, posing security risks in various domains. OET systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. The toolkit facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis. It leverages optimization methods with white-box and black-box access to generate worst-case adversarial examples, enabling strict red-teaming evaluations. Extensive experiments reveal the limitations of current defense mechanisms, with some models remaining vulnerable even after security enhancements. OET provides a unified platform for evaluating adversarial robustness and highlights the need for robust defense strategies in the face of adaptive adversarial scenarios. <br /> <div>
arXiv:2505.00843v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICQuant: Index Coding enables Low-bit LLM Quantization</title>
<link>https://arxiv.org/abs/2505.00850</link>
<guid>https://arxiv.org/abs/2505.00850</guid>
<content:encoded><![CDATA[
<div> outlier suppression, weight quantization, Large Language Models, post-training quantization, ICQuant
Summary:
ICQuant introduces a novel framework for outlier-aware weight-only quantization by utilizing outlier statistics to develop an efficient index coding scheme. It effectively shrinks quantization ranges with only approximately 0.3 bits overhead, thereby achieving significant savings in extreme compression scenarios. ICQuant can be applied alongside existing quantizers to enhance quantization quality. When employed with simple scalar quantizers using 2.3 bits per weight, ICQuant enhances the zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150% compared to QTIP and QuIP#, respectively. Furthermore, it delivers comparable performance to the top fine-tuned quantizer (PV-tuning) without requiring fine-tuning.<br /><br />Summary: <div>
arXiv:2505.00850v1 Announce Type: cross 
Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for efficient low-bit post-training quantization (PTQ), due to their high memory costs. A key challenge in weight quantization is the presence of outliers, which inflate quantization ranges and lead to large errors. While a number of outlier suppression techniques have been proposed, they either: fail to effectively shrink the quantization range, or incur (relatively) high bit overhead. In this paper, we present ICQuant, a novel framework that leverages outlier statistics to design an efficient index coding scheme for outlier-aware weight-only quantization. Compared to existing outlier suppression techniques requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant requires only $\approx 0.3$ bits; a significant saving in extreme compression regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing quantizers to eliminate outliers, improving the quantization quality. Using just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150% relative to QTIP and QuIP#; and it achieves comparable performance to the best-known fine-tuned quantizer (PV-tuning) without fine-tuning.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base</title>
<link>https://arxiv.org/abs/2505.00871</link>
<guid>https://arxiv.org/abs/2505.00871</guid>
<content:encoded><![CDATA[
<div> Keywords: Robots, Inverse Kinematics, Human-like physicality, Genetic Algorithm, Reachability map

Summary: 
Robots with human-like physicality are being developed to potentially replace human tasks, particularly in household service. However, size limitations often pose challenges in solving inverse kinematics (IK) due to joint angle constraints. This paper introduces a method to generate an optimal initial guess for numerical IK solvers based on the scaled Jacobian matrix and manipulability index, taking into account joint limits. By using a genetic algorithm (GA) to optimize the goodness of the initial guess, the probability of successfully solving IK is increased. Additionally, a reachability map is utilized to explore various IK solutions. Experimental results demonstrate that employing a better initial guess leads to higher IK solvability. The proposed method showcases its effectiveness by enabling a robot to achieve three distinct scenarios through optimized initial guesses. <div>
arXiv:2505.00871v1 Announce Type: cross 
Abstract: Robots are strongly expected as a means of replacing human tasks. If a robot has a human-like physicality, the possibility of replacing human tasks increases. In the case of household service robots, it is desirable for them to be on a human-like size so that they do not become excessively large in order to coexist with humans in their operating environment. However, robots with size limitations tend to have difficulty solving inverse kinematics (IK) due to mechanical limitations, such as joint angle limitations. Conversely, if the difficulty coming from this limitation could be mitigated, one can expect that the use of such robots becomes more valuable. In numerical IK solver, which is commonly used for robots with higher degrees-of-freedom (DOF), the solvability of IK depends on the initial guess given to the solver. Thus, this paper proposes a method for generating a good initial guess for a numerical IK solver given the target hand configuration. For the purpose, we define the goodness of an initial guess using the scaled Jacobian matrix, which can calculate the manipulability index considering the joint limits. These two factors are related to the difficulty of solving IK. We generate the initial guess by optimizing the goodness using the genetic algorithm (GA). To enumerate much possible IK solutions, we use the reachability map that represents the reachable area of the robot hand in the arm-base coordinate system. We conduct quantitative evaluation and prove that using an initial guess that is judged to be better using the goodness value increases the probability that IK is solved. Finally, as an application of the proposed method, we show that by generating good initial guesses for IK a robot actually achieves three typical scenarios.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Temporal User Profiling with LLMs</title>
<link>https://arxiv.org/abs/2505.00886</link>
<guid>https://arxiv.org/abs/2505.00886</guid>
<content:encoded><![CDATA[
<div> Keywords: User preferences, Recommender systems, Natural language processing, Temporal modeling, Explainability<br />
Summary:<br />
This work introduces a novel approach using large language models to accurately model user preferences in recommender systems. By generating natural language summaries of users' interaction histories, the framework captures both short-term and long-term preferences. The use of pre-trained models and attention mechanisms enables a comprehensive user representation that improves recommendation accuracy. Furthermore, the approach supports explainability by providing interpretable text summaries and attention weights to users. Real-world experiments demonstrate the efficacy of the proposed method in enhancing recommendation performance and generating transparent justifications for content-based recommendations. The incorporation of temporal modeling and natural language processing offers a promising avenue for advancing user profiling in recommender systems. <br /> <div>
arXiv:2505.00886v1 Announce Type: cross 
Abstract: Accurately modeling user preferences is vital not only for improving recommendation performance but also for enhancing transparency in recommender systems. Conventional user profiling methods, such as averaging item embeddings, often overlook the evolving, nuanced nature of user interests, particularly the interplay between short-term and long-term preferences. In this work, we leverage large language models (LLMs) to generate natural language summaries of users' interaction histories, distinguishing recent behaviors from more persistent tendencies. Our framework not only models temporal user preferences but also produces natural language profiles that can be used to explain recommendations in an interpretable manner. These textual profiles are encoded via a pre-trained model, and an attention mechanism dynamically fuses the short-term and long-term embeddings into a comprehensive user representation. Beyond boosting recommendation accuracy over multiple baselines, our approach naturally supports explainability: the interpretable text summaries and attention weights can be exposed to end users, offering insights into why specific items are suggested. Experiments on real-world datasets underscore both the performance gains and the promise of generating clearer, more transparent justifications for content-based recommendations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Time Encoding via Learnable Transformation Functions</title>
<link>https://arxiv.org/abs/2505.00887</link>
<guid>https://arxiv.org/abs/2505.00887</guid>
<content:encoded><![CDATA[
<div> time information, modeling, deep function learning, generalized time encoding, diverse time patterns <br />
Summary:
The paper introduces Learnable Transformation-based Generalized Time Encoding (LeTE) as a method to effectively model diverse and complex time patterns in real-world scenarios. By utilizing deep function learning techniques to parameterize non-linear transformations in time encoding, LeTE allows for the modeling of generalized time patterns. This approach improves upon existing time encoding methods by making the transformations learnable and adaptable to various temporal dynamics. LeTE encompasses previous methods as specific cases and can be seamlessly integrated into a wide range of tasks. Experimental results across diverse domains demonstrate the versatility and effectiveness of LeTE in capturing and encoding time information for applications involving chronologically occurring events. <div>
arXiv:2505.00887v1 Announce Type: cross 
Abstract: Effectively modeling time information and incorporating it into applications or models involving chronologically occurring events is crucial. Real-world scenarios often involve diverse and complex time patterns, which pose significant challenges for time encoding methods. While previous methods focus on capturing time patterns, many rely on specific inductive biases, such as using trigonometric functions to model periodicity. This narrow focus on single-pattern modeling makes them less effective in handling the diversity and complexities of real-world time patterns. In this paper, we investigate to improve the existing commonly used time encoding methods and introduce Learnable Transformation-based Generalized Time Encoding (LeTE). We propose using deep function learning techniques to parameterize non-linear transformations in time encoding, making them learnable and capable of modeling generalized time patterns, including diverse and complex temporal dynamics. By enabling learnable transformations, LeTE encompasses previous methods as specific cases and allows seamless integration into a wide range of tasks. Through extensive experiments across diverse domains, we demonstrate the versatility and effectiveness of LeTE.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning without Performance Degradation</title>
<link>https://arxiv.org/abs/2505.00913</link>
<guid>https://arxiv.org/abs/2505.00913</guid>
<content:encoded><![CDATA[
<div> algorithm, fine-tuning, offline learning, exploration, performance

Summary: 
The paper addresses the challenge of fine-tuning policies learned offline, which often results in performance degradation during the initial stages. Existing algorithms either exhibit performance degradation or slow learning during fine-tuning. The proposed algorithm, based on Jump Start, gradually increases exploration based on online performance estimates to achieve fast and effective fine-tuning. By doing so, it reduces performance degradation compared to current algorithms. This approach aims to improve the efficiency of fine-tuning learned networks online, providing a more stable and effective method for policy improvement. <div>
arXiv:2505.00913v1 Announce Type: cross 
Abstract: Fine-tuning policies learned offline remains a major challenge in application domains. Monotonic performance improvement during \emph{fine-tuning} is often challenging, as agents typically experience performance degradation at the early fine-tuning stage. The community has identified multiple difficulties in fine-tuning a learned network online, however, the majority of progress has focused on improving learning efficiency during fine-tuning. In practice, this comes at a serious cost during fine-tuning: initially, agent performance degrades as the agent explores and effectively overrides the policy learned offline. We show across a range of settings, many offline-to-online algorithms exhibit either (1) performance degradation or (2) slow learning (sometimes effectively no improvement) during fine-tuning. We introduce a new fine-tuning algorithm, based on an algorithm called Jump Start, that gradually allows more exploration based on online estimates of performance. Empirically, this approach achieves fast fine-tuning and significantly reduces performance degradations compared with existing algorithms designed to do the same.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate Conformal Selection</title>
<link>https://arxiv.org/abs/2505.00917</link>
<guid>https://arxiv.org/abs/2505.00917</guid>
<content:encoded><![CDATA[
<div> Conformal Selection, Multivariate Conformal Selection, regional monotonicity, multivariate nonconformity scores, False Discovery Rate control<br />
<br />
Summary:<br />
Conformal Selection (CS) is a method used in applications like drug discovery and language models for selecting high-quality candidates from large datasets. However, it is limited to univariate responses. To address this limitation, Multivariate Conformal Selection (mCS) is proposed as a generalization of CS for multivariate response settings. mCS introduces regional monotonicity and utilizes multivariate nonconformity scores to construct conformal p-values, ensuring finite-sample False Discovery Rate (FDR) control. Two variants of mCS are presented: mCS-dist, using distance-based scores, and mCS-learn, which optimizes scores through differentiable optimization. Experimental results on simulated and real-world datasets demonstrate the improved selection power of mCS while maintaining FDR control, establishing it as a robust framework for multivariate selection tasks.<br /> <div>
arXiv:2505.00917v1 Announce Type: cross 
Abstract: Selecting high-quality candidates from large datasets is critical in applications such as drug discovery, precision medicine, and alignment of large language models (LLMs). While Conformal Selection (CS) provides rigorous uncertainty quantification, it is limited to univariate responses and scalar criteria. To address this issue, we propose Multivariate Conformal Selection (mCS), a generalization of CS designed for multivariate response settings. Our method introduces regional monotonicity and employs multivariate nonconformity scores to construct conformal p-values, enabling finite-sample False Discovery Rate (FDR) control. We present two variants: mCS-dist, using distance-based scores, and mCS-learn, which learns optimal scores via differentiable optimization. Experiments on simulated and real-world datasets demonstrate that mCS significantly improves selection power while maintaining FDR control, establishing it as a robust framework for multivariate selection tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning</title>
<link>https://arxiv.org/abs/2505.00918</link>
<guid>https://arxiv.org/abs/2505.00918</guid>
<content:encoded><![CDATA[
<div> sensing, transmitting, IoT networks, multi-objective optimization, Q-learning

Summary: 
The article discusses the growing importance of IoT devices in various applications and the task of sensing and transmitting information in IoT networks. Traditional routing protocols optimize static objectives, but real-world IoT applications require adapting to dynamic priorities. To address this, the authors propose a dynamic and distributed routing approach based on multiobjective Q-learning, which can adjust to changing preferences in real-time. They introduce a greedy interpolation policy scheme to make near-optimal decisions for unexpected preference changes. By utilizing Pareto-efficient solutions for dynamic preferences, the proposed scheme outperforms existing algorithms in terms of overall reward, energy efficiency, and packet delivery ratio. Simulation results show the effectiveness of the proposed approach in adapting to different exploration strategies and preference variations. <div>
arXiv:2505.00918v1 Announce Type: cross 
Abstract: The last few decades have witnessed a rapid increase in IoT devices owing to their wide range of applications, such as smart healthcare monitoring systems, smart cities, and environmental monitoring. A critical task in IoT networks is sensing and transmitting information over the network. The IoT nodes gather data by sensing the environment and then transmit this data to a destination node via multi-hop communication, following some routing protocols. These protocols are usually designed to optimize possibly contradictory objectives, such as maximizing packet delivery ratio and energy efficiency. While most literature has focused on optimizing a static objective that remains unchanged, many real-world IoT applications require adapting to rapidly shifting priorities. For example, in monitoring systems, some transmissions are time-critical and require a high priority on low latency, while other transmissions are less urgent and instead prioritize energy efficiency. To meet such dynamic demands, we propose novel dynamic and distributed routing based on multiobjective Q-learning that can adapt to changes in preferences in real-time. Our algorithm builds on ideas from both multi-objective optimization and Q-learning. We also propose a novel greedy interpolation policy scheme to take near-optimal decisions for unexpected preference changes. The proposed scheme can approximate and utilize the Pareto-efficient solutions for dynamic preferences, thus utilizing past knowledge to adapt to unpredictable preferences quickly during runtime. Simulation results show that the proposed scheme outperforms state-of-the-art algorithms for various exploration strategies, preference variation patterns, and important metrics like overall reward, energy efficiency, and packet delivery ratio.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing</title>
<link>https://arxiv.org/abs/2505.00931</link>
<guid>https://arxiv.org/abs/2505.00931</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Dynamic Assessment, DynaWrite, GPT-4o, neural chat

Summary:
Large Language Models (LLMs) are explored for scaling up Dynamic Assessment (DA). The researchers developed DynaWrite, a grammatical tutoring app using multiple LLMs. Testing of 21 models identified GPT-4o and neural chat as promising for DA scaling. Further comparison showed GPT-4o offering superior quality in generating hints for grammatical errors. Real-time responsiveness and system stability were confirmed, with GPT-4o demonstrating sufficient speed. This study highlights the potential of LLMs in expanding DA reach to larger groups beyond traditional teaching settings.<br /><br />Summary: <div>
arXiv:2505.00931v1 Announce Type: cross 
Abstract: This study investigates the potential for Large Language Models (LLMs) to scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first developed DynaWrite-a modular, microservices-based grammatical tutoring application which supports multiple LLMs to generate dynamic feedback to learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural chat to have the most potential to scale-up DA in the language learning classroom. Further testing of these two candidates found both models performed similarly in their ability to accurately identify grammatical errors in user sentences. However, GPT-4o consistently outperformed neural chat in the quality of its DA by generating clear, consistent, and progressively explicit hints. Real-time responsiveness and system stability were also confirmed through detailed performance testing, with GPT-4o exhibiting sufficient speed and stability. This study shows that LLMs can be used to scale-up dynamic assessment and thus enable dynamic assessment to be delivered to larger groups than possible in traditional teacher-learner settings.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Self-Supervised Transformer for Unusable Shared Bike Detection</title>
<link>https://arxiv.org/abs/2505.00932</link>
<guid>https://arxiv.org/abs/2505.00932</guid>
<content:encoded><![CDATA[
<div> Keywords: bike-sharing systems, self-supervised Transformer, fault detection, spatiotemporal features, shared mobility<br />
Summary:
The paper introduces a novel Self-Supervised Transformer (SSTransformer) framework for detecting faulty shared bikes in bike-sharing systems. The framework leverages spatiotemporal features extracted from GPS trajectories and trip records to automate the identification of unusable bikes. Through self-supervised pre-training, the Transformer encoder learns generalized representations of bike movement before adapting to a downstream binary classification task. Experiments conducted on a real-world dataset of 10,730 bikes from Chengdu, China show that SSTransformer outperforms traditional machine learning models, ensemble learning, and deep learning baselines, achieving high accuracy (97.81%), precision (0.8889), and F1-score (0.9358). The study emphasizes the effectiveness of self-supervised Transformer models in capturing complex anomalies in bike-sharing systems, offering a promising approach for enhancing the maintenance and reliability of shared mobility solutions. <br /><br />Summary: <div>
arXiv:2505.00932v1 Announce Type: cross 
Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban "last-mile" connectivity, yet large-scale deployments face escalating operational challenges, particularly in detecting faulty bikes. Existing detection approaches either rely on static model-based thresholds that overlook dynamic spatiotemporal (ST) usage patterns or employ supervised learning methods that struggle with label scarcity and class imbalance. To address these limitations, this paper proposes a novel Self-Supervised Transformer (SSTransformer) framework for automatically detecting unusable shared bikes, leveraging ST features extracted from GPS trajectories and trip records. The model incorporates a self-supervised pre-training strategy to enhance its feature extraction capabilities, followed by fine-tuning for efficient status recognition. In the pre-training phase, the Transformer encoder learns generalized representations of bike movement via a self-supervised objective; in the fine-tuning phase, the encoder is adapted to a downstream binary classification task. Comprehensive experiments on a real-world dataset of 10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate that SSTransformer significantly outperforms traditional machine learning, ensemble learning, and deep learning baselines, achieving the best accuracy (97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the effectiveness of self-supervised Transformer on ST data for capturing complex anomalies in BSS, paving the way toward more reliable and scalable maintenance solutions for shared mobility.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</title>
<link>https://arxiv.org/abs/2505.00935</link>
<guid>https://arxiv.org/abs/2505.00935</guid>
<content:encoded><![CDATA[
<div> Keywords: Embodied Artificial Intelligence, Computer Vision, Robotics, Decision Making, Autonomous Agents

Summary: 
Embodied Artificial Intelligence is a growing field at the intersection of Computer Vision, Robotics, and Decision Making, aiming to develop autonomous robots for deployment in society. With increased computing power and the Deep Learning revolution, researchers can explore new frontiers in AI. The availability of large collections of 3D models for realistic robotic simulation facilitates faster and safer agent training. During training in simulation, intelligent agents learn continuous interactions with the environment, such as information gathering, cue extraction, and goal-directed actions. This dissertation covers the complete process of creating embodied agents for indoor environments, from concept to deployment. Through a detailed analysis of current literature, technical explanations, and experimental studies, this work contributes to Embodied AI and autonomous agent research. <div>
arXiv:2505.00935v1 Announce Type: cross 
Abstract: The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion</title>
<link>https://arxiv.org/abs/2505.00938</link>
<guid>https://arxiv.org/abs/2505.00938</guid>
<content:encoded><![CDATA[
<div> Cross-domain few-shot object detection, CDFormer, feature confusion, object-background distinguishing, object-object distinguishing  
Summary:  
- CDFormer is a novel approach for cross-domain few-shot object detection that addresses feature confusion challenges through the use of object-background distinguishing (OBD) and object-object distinguishing (OOD) modules.  
- The OBD module utilizes a learnable background token to differentiate between objects and background in the detection process.  
- The OOD module enhances the ability to distinguish between objects of different classes, improving overall detection performance.  
- Experimental results show that CDFormer outperforms existing state-of-the-art methods, achieving significant improvements in mean average precision (mAP) under different shot settings.  
- Specifically, CDFormer achieves improvements of 12.9%, 11.0%, and 10.4% in mAP under the 1/5/10 shot settings, respectively, when fine-tuned. 

<br /><br />Summary: <div>
arXiv:2505.00938v1 Announce Type: cross 
Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects across different domains with limited class instances. Feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. In this work, we introduce CDFormer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. The method specifically tackles feature confusion through two key modules: object-background distinguishing (OBD) and object-object distinguishing (OOD). The OBD module leverages a learnable background token to differentiate between objects and background, while the OOD module enhances the distinction between objects of different classes. Experimental results demonstrate that CDFormer outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0% mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively, when fine-tuned.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-Nemotron: Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
<div> Keywords: Llama-Nemotron models, reasoning capabilities, inference efficiency, open license, neural architecture search

Summary:
The Llama-Nemotron series includes Nano, Super, and Ultra models, offering exceptional reasoning capabilities and inference efficiency. These models are trained using neural architecture search and a post-training process involving supervised fine-tuning and large-scale reinforcement learning. They are the first open-source models with a dynamic reasoning toggle for switching between chat and reasoning modes. The models are released under the NVIDIA Open Model License Agreement to support enterprise use. The complete post-training dataset and training codebases are also made available to facilitate further research and model development.<br /><br />Summary: <div>
arXiv:2505.00949v1 Announce Type: cross 
Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Sliced Wasserstein Distance with Nonlinear Projection</title>
<link>https://arxiv.org/abs/2505.00968</link>
<guid>https://arxiv.org/abs/2505.00968</guid>
<content:encoded><![CDATA[
<div> Tree-Sliced Methods, Sliced Wasserstein Distance, Tree-Based Metric Spaces, Nonlinear Projectional Framework, Radon Transform<br />
<br />
Summary: <br />
Tree-Sliced methods present a new approach to Sliced Wasserstein distance, using tree-based metric spaces and a splitting mechanism for measure projection. This study introduces a nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, replacing linear projections with general projections to ensure metric well-definedness. By constructing efficient metrics for Euclidean spaces and spheres through appropriate projections, the proposed TSW distance is validated through numerical experiments on various datasets. Applications in gradient flows, self-supervised learning, and generative models show significant improvements over existing SW and TSW variants. <div>
arXiv:2505.00968v1 Announce Type: cross 
Abstract: Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attack and defense techniques in large language models: A survey and new perspectives</title>
<link>https://arxiv.org/abs/2505.00976</link>
<guid>https://arxiv.org/abs/2505.00976</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, attacks, defenses, security, ethics

Summary: Large Language Models (LLMs) have become essential in natural language processing tasks but are vulnerable to various attacks. This systematic survey categorizes attacks on LLMs into adversarial prompt attack, optimized attacks, model theft, and attacks on LLM applications, while also exploring defense strategies such as prevention-based and detection-based methods. Challenges include adapting to dynamic threats, balancing usability and robustness, and dealing with resource constraints in defense implementation. The survey highlights the need for scalable and adaptive defenses, explainable security techniques, and standardized evaluation frameworks. It emphasizes interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications. <div>
arXiv:2505.00976v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.00979</link>
<guid>https://arxiv.org/abs/2505.00979</guid>
<content:encoded><![CDATA[
<div> pre-training, synthetic data generation, large language models, knowledge associations, document Q&amp;A dataset

Summary:
- The article introduces a new framework called Synthetic-on-Graph (SoG) for generating synthetic data to improve the efficiency of Large Language Models (LLMs) learning from small and specialized corpora.
- SoG utilizes cross-document knowledge associations to enhance data diversity and coherence, allowing models to learn complex knowledge structures and handle rare knowledge.
- The framework incorporates Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic data generation techniques to improve reasoning processes and discriminative power.
- Experimental results demonstrate that SoG outperforms existing methods in a multi-hop document question and answer dataset and performs comparably in reading comprehension tasks, highlighting its superior generalization capability.
- The research contributes to advancing synthetic data generation methods and offers practical solutions for enhancing knowledge acquisition in LLMs, particularly beneficial in domains with limited data availability.<br /><br />Summary: <div>
arXiv:2505.00979v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&amp;A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Data-centric Directed Graph Learning: An Entropy-driven Approach</title>
<link>https://arxiv.org/abs/2505.00983</link>
<guid>https://arxiv.org/abs/2505.00983</guid>
<content:encoded><![CDATA[
<div> Keywords: directed graph neural networks, knowledge distillation, hierarchical encoding theory, data-centric learning, topology

Summary:
EDEN introduces a novel data-centric approach for learning from directed graphs, addressing the limitations of existing DiGraph Neural Networks (DiGNNs). By leveraging hierarchical knowledge trees and mutual information of node profiles, EDEN enhances knowledge flow and improves model-level predictive performance. The proposed method can be used as a standalone digraph learning paradigm or as a model-agnostic data-centric Knowledge Distillation module. EDEN's hierarchical encoding theory for structured data enables it to achieve state-of-the-art performance on various graph datasets and downstream tasks, surpassing existing (Di)GNN models. The framework can also be extended to undirected scenarios and demonstrates strong improvement over prevalent graph neural networks. Overall, EDEN provides a comprehensive solution for exploring the correlations between directed edges and node profiles in digraphs, leading to more robust and accurate predictive models. <br /><br />Summary: <div>
arXiv:2505.00983v1 Announce Type: cross 
Abstract: The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a data-centric digraph learning paradigm or a model-agnostic hot-and-plug data-centric Knowledge Distillation (KD) module. The core idea is to achieve data-centric ML, guided by our proposed hierarchical encoding theory for structured data. Specifically, EDEN first utilizes directed structural measurements from a topology perspective to construct a coarse-grained Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual information of node profiles to refine knowledge flow in the HKT, enabling data-centric KD supervision within model training. As a general framework, EDEN can also naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets (homophily and heterophily) and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Resistance of Neural Network Watermarking to Fine-tuning</title>
<link>https://arxiv.org/abs/2505.01007</link>
<guid>https://arxiv.org/abs/2505.01007</guid>
<content:encoded><![CDATA[
<div> watermarking, ownership information, deep neural network, frequency components, convolutional filter
<br />
This paper introduces a novel watermarking technique for embedding ownership information into a deep neural network (DNN) that remains robust during fine-tuning. The method leverages a revised Fourier transform to extract specific frequency components from the convolutional filter, ensuring that they remain unchanged during gradient descent. These frequency components are shown to be invariant to weight scaling and permutations, allowing for the design of a watermark module to encode information into them. Preliminary experiments validate the effectiveness of the proposed method in embedding watermarks into DNNs. 
<br /><br />Summary: <div>
arXiv:2505.01007v1 Announce Type: cross 
Abstract: This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark</title>
<link>https://arxiv.org/abs/2505.01015</link>
<guid>https://arxiv.org/abs/2505.01015</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, benchmarks, value orientations, human interactions, biases <br />
<br />
Summary: 
The Value Portrait benchmark is introduced to assess language models' value orientations in real-life user interactions. Human subjects rate items based on their similarity to their own thoughts, providing reliable assessment of values. Results from evaluating 27 LLMs show a prioritization of Benevolence, Security, and Self-Direction values, with less emphasis on Tradition, Power, and Achievement values. Biases in how LLMs perceive demographic groups are also identified. The benchmark addresses issues with existing benchmarks by capturing authentic interactions and minimizing biases. This approach enhances the relevance of assessment results to real-world LLM usage, increasing ecological validity. The psychometrically validated methodology ensures that items correlated with specific values are reliable for assessment, offering a more robust framework for evaluating LLMs' value orientations. <div>
arXiv:2505.01015v1 Announce Type: cross 
Abstract: The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage and thus ecological validity. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 27 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance</title>
<link>https://arxiv.org/abs/2505.01016</link>
<guid>https://arxiv.org/abs/2505.01016</guid>
<content:encoded><![CDATA[
arXiv:2505.01016v1 Announce Type: cross 
Abstract: The success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. While fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. The critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? Addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. We adapt a standard YOLOv8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. Performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original COCO validation set. Our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\% absolute mAP50) on the fine-grained fruit task compared to only training the head. Strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\% absolute mAP difference) on the COCO benchmark across all tested freeze levels. We conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. Critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality</title>
<link>https://arxiv.org/abs/2505.01036</link>
<guid>https://arxiv.org/abs/2505.01036</guid>
<content:encoded><![CDATA[
arXiv:2505.01036v1 Announce Type: cross 
Abstract: In the evolutionary computation community, it is widely believed that stagnation impedes convergence in evolutionary algorithms, and that convergence inherently indicates optimality. However, this perspective is misleading. In this study, it is the first to highlight that the stagnation of an individual can actually facilitate the convergence of the entire population, and convergence does not necessarily imply optimality, not even local optimality. Convergence alone is insufficient to ensure the effectiveness of evolutionary algorithms. Several counterexamples are provided to illustrate this argument.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.01043</link>
<guid>https://arxiv.org/abs/2505.01043</guid>
<content:encoded><![CDATA[
arXiv:2505.01043v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\unicode{x2013}$such as weights, activations, and gradients$\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Tensor Planning</title>
<link>https://arxiv.org/abs/2505.01059</link>
<guid>https://arxiv.org/abs/2505.01059</guid>
<content:encoded><![CDATA[
arXiv:2505.01059v1 Announce Type: cross 
Abstract: Sampling-based model predictive control (MPC) offers strong performance in nonlinear and contact-rich robotic tasks, yet often suffers from poor exploration due to locally greedy sampling schemes. We propose \emph{Model Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces high-entropy control trajectory generation through structured tensor sampling. By sampling over randomized multipartite graphs and interpolating control trajectories with B-splines and Akima splines, MTP ensures smooth and globally diverse control candidates. We further propose a simple $\beta$-mixing strategy that blends local exploitative and global exploratory samples within the modified Cross-Entropy Method (CEM) update, balancing control refinement and exploration. Theoretically, we show that MTP achieves asymptotic path coverage and maximum entropy in the control trajectory space in the limit of infinite tensor depth and width.
  Our implementation is fully vectorized using JAX and compatible with MuJoCo XLA, supporting \emph{Just-in-time} (JIT) compilation and batched rollouts for real-time control with online domain randomization. Through experiments on various challenging robotic tasks, ranging from dexterous in-hand manipulation to humanoid locomotion, we demonstrate that MTP outperforms standard MPC and evolutionary strategy baselines in task success and control robustness. Design and sensitivity ablations confirm the effectiveness of MTP tensor sampling structure, spline interpolation choices, and mixing strategy. Altogether, MTP offers a scalable framework for robust exploration in model-based planning and control.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation</title>
<link>https://arxiv.org/abs/2505.01065</link>
<guid>https://arxiv.org/abs/2505.01065</guid>
<content:encoded><![CDATA[
arXiv:2505.01065v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories</title>
<link>https://arxiv.org/abs/2505.01067</link>
<guid>https://arxiv.org/abs/2505.01067</guid>
<content:encoded><![CDATA[
arXiv:2505.01067v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2505.01068</link>
<guid>https://arxiv.org/abs/2505.01068</guid>
<content:encoded><![CDATA[
arXiv:2505.01068v1 Announce Type: cross 
Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits</title>
<link>https://arxiv.org/abs/2505.01070</link>
<guid>https://arxiv.org/abs/2505.01070</guid>
<content:encoded><![CDATA[
arXiv:2505.01070v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) has become a powerful tool for training compact student models using larger, pretrained teacher models, often requiring less data and computational resources. Teacher models typically possess more layers and thus exhibit richer feature representations compared to their student counterparts. Furthermore, student models tend to learn simpler, surface-level features in their early layers. This discrepancy can increase errors in groups where labels spuriously correlate with specific input attributes, leading to a decline in group fairness even when overall accuracy remains comparable to the teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs), which enable predictions at multiple intermediate layers, have been employed. Confidence margins derived from these early exits have been utilized to reweight both cross-entropy and distillation losses on a per-instance basis. In this paper, we propose that leveraging Laplace approximation-based methods to obtain well-calibrated uncertainty estimates can also effectively reweight challenging instances and improve group fairness. We hypothesize that Laplace approximation offers a more robust identification of difficult or ambiguous instances compared to margin-based approaches. To validate our claims, we benchmark our approach using a Bert-based model on the MultiNLI dataset.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in Government: Why People Feel They Lose Control</title>
<link>https://arxiv.org/abs/2505.01085</link>
<guid>https://arxiv.org/abs/2505.01085</guid>
<content:encoded><![CDATA[
arXiv:2505.01085v1 Announce Type: cross 
Abstract: The use of Artificial Intelligence (AI) in public administration is expanding rapidly, moving from automating routine tasks to deploying generative and agentic systems that autonomously act on goals. While AI promises greater efficiency and responsiveness, its integration into government functions raises concerns about fairness, transparency, and accountability. This article applies principal-agent theory (PAT) to conceptualize AI adoption as a special case of delegation, highlighting three core tensions: assessability (can decisions be understood?), dependency (can the delegation be reversed?), and contestability (can decisions be challenged?). These structural challenges may lead to a "failure-by-success" dynamic, where early functional gains obscure long-term risks to democratic legitimacy. To test this framework, we conducted a pre-registered factorial survey experiment across tax, welfare, and law enforcement domains. Our findings show that although efficiency gains initially bolster trust, they simultaneously reduce citizens' perceived control. When the structural risks come to the foreground, institutional trust and perceived control both drop sharply, suggesting that hidden costs of AI adoption significantly shape public attitudes. The study demonstrates that PAT offers a powerful lens for understanding the institutional and political implications of AI in government, emphasizing the need for policymakers to address delegation risks transparently to maintain public trust.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation</title>
<link>https://arxiv.org/abs/2505.01091</link>
<guid>https://arxiv.org/abs/2505.01091</guid>
<content:encoded><![CDATA[
arXiv:2505.01091v1 Announce Type: cross 
Abstract: Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Water Management</title>
<link>https://arxiv.org/abs/2505.01094</link>
<guid>https://arxiv.org/abs/2505.01094</guid>
<content:encoded><![CDATA[
arXiv:2505.01094v1 Announce Type: cross 
Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug discovery) require optimizing multiple, conflicting objectives. Multi-objective reinforcement learning (MORL) extends classic reinforcement learning to handle multiple objectives simultaneously, yielding a set of policies that capture various trade-offs. However, the MORL field lacks complex, realistic environments and benchmarks. We introduce a water resource (Nile river basin) management case study and model it as a MORL environment. We then benchmark existing MORL algorithms on this task. Our results show that specialized water management methods outperform state-of-the-art MORL approaches, underscoring the scalability challenges MORL algorithms face in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study</title>
<link>https://arxiv.org/abs/2505.01109</link>
<guid>https://arxiv.org/abs/2505.01109</guid>
<content:encoded><![CDATA[
arXiv:2505.01109v1 Announce Type: cross 
Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Analysis and Design Against Adversarial Actions</title>
<link>https://arxiv.org/abs/2505.01130</link>
<guid>https://arxiv.org/abs/2505.01130</guid>
<content:encoded><![CDATA[
arXiv:2505.01130v1 Announce Type: cross 
Abstract: Learning models capable of providing reliable predictions in the face of adversarial actions has become a central focus of the machine learning community in recent years. This challenge arises from observing that data encountered at deployment time often deviate from the conditions under which the model was trained. In this paper, we address deployment-time adversarial actions and propose a versatile, well-principled framework to evaluate the model's robustness against attacks of diverse types and intensities. While we initially focus on Support Vector Regression (SVR), the proposed approach extends naturally to the broad domain of learning via relaxed optimization techniques. Our results enable an assessment of the model vulnerability without requiring additional test data and operate in a distribution-free setup. These results not only provide a tool to enhance trust in the model's applicability but also aid in selecting among competing alternatives. Later in the paper, we show that our findings also offer useful insights for establishing new results within the out-of-distribution framework.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limitations of Steering in Language Model Alignment</title>
<link>https://arxiv.org/abs/2505.01162</link>
<guid>https://arxiv.org/abs/2505.01162</guid>
<content:encoded><![CDATA[
arXiv:2505.01162v1 Announce Type: cross 
Abstract: Steering vectors are a promising approach to aligning language model behavior at inference time. In this paper, we propose a framework to assess the limitations of steering vectors as alignment mechanisms. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. Our findings indicate that steering vectors are promising for specific alignment tasks, such as value alignment, but may not provide a robust foundation for general-purpose alignment in LLMs, particularly in complex scenarios. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability</title>
<link>https://arxiv.org/abs/2505.01168</link>
<guid>https://arxiv.org/abs/2505.01168</guid>
<content:encoded><![CDATA[
arXiv:2505.01168v1 Announce Type: cross 
Abstract: The development of model ensemble attacks has significantly improved the transferability of adversarial examples, but this progress also poses severe threats to the security of deep neural networks. Existing methods, however, face two critical challenges: insufficient capture of shared gradient directions across models and a lack of adaptive weight allocation mechanisms. To address these issues, we propose a novel method Harmonized Ensemble for Adversarial Transferability (HEAT), which introduces domain generalization into adversarial example generation for the first time. HEAT consists of two key modules: Consensus Gradient Direction Synthesizer, which uses Singular Value Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight Orchestrator which dynamically balances intra-domain coherence, stabilizing gradients within individual models, and inter-domain diversity, enhancing transferability across models. Experimental results demonstrate that HEAT significantly outperforms existing methods across various datasets and settings, offering a new perspective and direction for adversarial attack research.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities</title>
<link>https://arxiv.org/abs/2505.01169</link>
<guid>https://arxiv.org/abs/2505.01169</guid>
<content:encoded><![CDATA[
arXiv:2505.01169v1 Announce Type: cross 
Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates between a well-known noise distribution ($p_0$) and the data distribution ($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM) $\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an initial time $s$ to another belonging to the distribution at a terminal time $t$ in one function evaluation. We present a new loss function for TTFM distillation called the \emph{initial/terminal velocity matching} (ITVM) loss that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi et al. by adding redundant terms to match the initial velocities at time $s$, removing the derivative from the terminal velocity term at time $t$, and using a version of the model under training, stabilized by exponential moving averaging (EMA), to compute the target terminal average velocity. Preliminary experiments show that our loss leads to better few-step generation performance on multiple types of datasets and model architectures over baselines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures</title>
<link>https://arxiv.org/abs/2505.01177</link>
<guid>https://arxiv.org/abs/2505.01177</guid>
<content:encoded><![CDATA[
arXiv:2505.01177v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSTMotion: Training-free Scene-awarenText-to-motion Generation</title>
<link>https://arxiv.org/abs/2505.01182</link>
<guid>https://arxiv.org/abs/2505.01182</guid>
<content:encoded><![CDATA[
arXiv:2505.01182v1 Announce Type: cross 
Abstract: Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{https://tstmotion.github.io/}{Project Page}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing</title>
<link>https://arxiv.org/abs/2505.01185</link>
<guid>https://arxiv.org/abs/2505.01185</guid>
<content:encoded><![CDATA[
arXiv:2505.01185v1 Announce Type: cross 
Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor localization remains challenging due to complex environmental conditions, multipath fading, and transient obstructions. This paper proposes a lightweight but robust approach combining adaptive filtering with an extended log-distance, multi-wall path loss and shadowing (PLS) model. Our methodology augments conventional models with critical LoRaWAN parameters (received signal strength indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic environmental indicators (temperature, humidity, carbon dioxide, particulate matter, and barometric pressure). An adaptive Kalman filter reduces RSSI fluctuations, isolating persistent trends from momentary noise. Using a six-month dataset of 1,328,334 field measurements, we evaluate three models: the baseline COST 231 multi-wall model (MWM), the baseline model augmented with environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP (10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation reduces systematic errors by 41.22%, while Kalman filtering significantly enhances robustness under high RSSI volatility by 42.63%, on average across all devices. These findings present an interpretable, efficient solution for precise indoor LoRaWAN localization in dynamically changing environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks</title>
<link>https://arxiv.org/abs/2505.01186</link>
<guid>https://arxiv.org/abs/2505.01186</guid>
<content:encoded><![CDATA[
arXiv:2505.01186v1 Announce Type: cross 
Abstract: Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods</title>
<link>https://arxiv.org/abs/2505.01198</link>
<guid>https://arxiv.org/abs/2505.01198</guid>
<content:encoded><![CDATA[
arXiv:2505.01198v1 Announce Type: cross 
Abstract: While research on applications and evaluations of explanation methods continues to expand, fairness of the explanation methods concerning disparities in their performance across subgroups remains an often overlooked aspect. In this paper, we address this gap by showing that, across three tasks and five language models, widely used post-hoc feature attribution methods exhibit significant gender disparity with respect to their faithfulness, robustness, and complexity. These disparities persist even when the models are pre-trained or fine-tuned on particularly unbiased datasets, indicating that the disparities we observe are not merely consequences of biased training data. Our results highlight the importance of addressing disparities in explanations when developing and applying explainability methods, as these can lead to biased outcomes against certain subgroups, with particularly critical implications in high-stakes contexts. Furthermore, our findings underscore the importance of incorporating the fairness of explanations, alongside overall model fairness and explainability, as a requirement in regulatory frameworks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models</title>
<link>https://arxiv.org/abs/2505.01238</link>
<guid>https://arxiv.org/abs/2505.01238</guid>
<content:encoded><![CDATA[
arXiv:2505.01238v1 Announce Type: cross 
Abstract: As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications</title>
<link>https://arxiv.org/abs/2505.01261</link>
<guid>https://arxiv.org/abs/2505.01261</guid>
<content:encoded><![CDATA[
arXiv:2505.01261v1 Announce Type: cross 
Abstract: The challenge of electronic component obsolescence is particularly critical in systems with long life cycles. Various obsolescence management methods are employed to mitigate its impact, with obsolescence forecasting being a highly sought-after and prominent approach. As a result, numerous machine learning-based forecasting methods have been proposed. However, machine learning models require a substantial amount of relevant data to achieve high precision, which is lacking in the current obsolescence landscape in some situations. This work introduces a novel framework for obsolescence forecasting based on deep learning. The proposed framework solves the lack of available data through deep generative modeling, where new obsolescence cases are generated and used to augment the training dataset. The augmented dataset is then used to train a classical machine learning-based obsolescence forecasting model. To train classical forecasting models using augmented datasets, existing classical supervised-learning classifiers are adapted for semi-supervised learning within this framework. The proposed framework demonstrates state-of-the-art results on benchmarking datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title>
<link>https://arxiv.org/abs/2505.01273</link>
<guid>https://arxiv.org/abs/2505.01273</guid>
<content:encoded><![CDATA[
arXiv:2505.01273v1 Announce Type: cross 
Abstract: With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-preserved Transfer Learning Method for Differential Equations</title>
<link>https://arxiv.org/abs/2505.01281</link>
<guid>https://arxiv.org/abs/2505.01281</guid>
<content:encoded><![CDATA[
arXiv:2505.01281v1 Announce Type: cross 
Abstract: While data-driven methods such as neural operator have achieved great success in solving differential equations (DEs), they suffer from domain shift problems caused by different learning environments (with data bias or equation changes), which can be alleviated by transfer learning (TL). However, existing TL methods adopted in DEs problems lack either generalizability in general DEs problems or physics preservation during training. In this work, we focus on a general transfer learning method that adaptively correct the domain shift and preserve physical information. Mathematically, we characterize the data domain as product distribution and the essential problems as distribution bias and operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that simultaneously admits generalizability to common DEs and physics preservation of specific problem is proposed to adapt the data-driven model to target domain utilizing the push-forward distribution induced by the POTT map. Extensive experiments demonstrate the superior performance, generalizability and physics preservation of the proposed POTT method.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced-order structure-property linkages for stochastic metamaterials</title>
<link>https://arxiv.org/abs/2505.01283</link>
<guid>https://arxiv.org/abs/2505.01283</guid>
<content:encoded><![CDATA[
arXiv:2505.01283v1 Announce Type: cross 
Abstract: The capabilities of additive manufacturing have facilitated the design and production of mechanical metamaterials with diverse unit cell geometries. Establishing linkages between the vast design space of unit cells and their effective mechanical properties is critical for the efficient design and performance evaluation of such metamaterials. However, physics-based simulations of metamaterial unit cells across the entire design space are computationally expensive, necessitating a materials informatics framework to efficiently capture complex structure-property relationships. In this work, principal component analysis of 2-point correlation functions is performed to extract the salient features from a large dataset of randomly generated 2D metamaterials. Physics-based simulations are performed using a fast Fourier transform (FFT)-based homogenization approach to efficiently compute the homogenized effective elastic stiffness across the extensive unit cell designs. Subsequently, Gaussian process regression is used to generate reduced-order surrogates, mapping unit cell designs to their homogenized effective elastic constant. It is demonstrated that the adopted workflow enables a high-value low-dimensional representation of the voluminous stochastic metamaterial dataset, facilitating the construction of robust structure-property maps. Finally, an uncertainty-based active learning framework is utilized to train a surrogate model with a significantly smaller number of data points compared to the original full dataset. It is shown that a dataset as small as $0.61\%$ of the entire dataset is sufficient to generate accurate and robust structure-property maps.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables</title>
<link>https://arxiv.org/abs/2505.01286</link>
<guid>https://arxiv.org/abs/2505.01286</guid>
<content:encoded><![CDATA[
arXiv:2505.01286v1 Announce Type: cross 
Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow</title>
<link>https://arxiv.org/abs/2505.01288</link>
<guid>https://arxiv.org/abs/2505.01288</guid>
<content:encoded><![CDATA[
arXiv:2505.01288v1 Announce Type: cross 
Abstract: One of the central challenges preventing robots from acquiring complex manipulation skills is the prohibitive cost of collecting large-scale robot demonstrations. In contrast, humans are able to learn efficiently by watching others interact with their environment. To bridge this gap, we introduce semantic action flow as a core intermediate representation capturing the essential spatio-temporal manipulator-object interactions, invariant to superficial visual differences. We present ViSA-Flow, a framework that learns this representation self-supervised from unlabeled large-scale video data. First, a generative model is pre-trained on semantic action flows automatically extracted from large-scale human-object interaction video data, learning a robust prior over manipulation structure. Second, this prior is efficiently adapted to a target robot by fine-tuning on a small set of robot demonstrations processed through the same semantic abstraction pipeline. We demonstrate through extensive experiments on the CALVIN benchmark and real-world tasks that ViSA-Flow achieves state-of-the-art performance, particularly in low-data regimes, outperforming prior methods by effectively transferring knowledge from human video observation to robotic execution. Videos are available at https://visaflow-web.github.io/ViSAFLOW.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments</title>
<link>https://arxiv.org/abs/2505.01307</link>
<guid>https://arxiv.org/abs/2505.01307</guid>
<content:encoded><![CDATA[
arXiv:2505.01307v1 Announce Type: cross 
Abstract: Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing SPARQL Query Rewriting for Complex Ontology Alignments</title>
<link>https://arxiv.org/abs/2505.01309</link>
<guid>https://arxiv.org/abs/2505.01309</guid>
<content:encoded><![CDATA[
arXiv:2505.01309v1 Announce Type: cross 
Abstract: SPARQL query rewriting is a fundamental mechanism for uniformly querying heterogeneous ontologies in the Linked Data Web. However, the complexity of ontology alignments, particularly rich correspondences (c : c), makes this process challenging. Existing approaches primarily focus on simple (s : s) and partially complex ( s : c) alignments, thereby overlooking the challenges posed by more expressive alignments. Moreover, the intricate syntax of SPARQL presents a barrier for non-expert users seeking to fully exploit the knowledge encapsulated in ontologies. This article proposes an innovative approach for the automatic rewriting of SPARQL queries from a source ontology to a target ontology, based on a user's need expressed in natural language. It leverages the principles of equivalence transitivity as well as the advanced capabilities of large language models such as GPT-4. By integrating these elements, this approach stands out for its ability to efficiently handle complex alignments, particularly (c : c) correspondences , by fully exploiting their expressiveness. Additionally, it facilitates access to aligned ontologies for users unfamiliar with SPARQL, providing a flexible solution for querying heterogeneous data.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System</title>
<link>https://arxiv.org/abs/2505.01315</link>
<guid>https://arxiv.org/abs/2505.01315</guid>
<content:encoded><![CDATA[
arXiv:2505.01315v1 Announce Type: cross 
Abstract: The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability</title>
<link>https://arxiv.org/abs/2505.01328</link>
<guid>https://arxiv.org/abs/2505.01328</guid>
<content:encoded><![CDATA[
arXiv:2505.01328v1 Announce Type: cross 
Abstract: While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Nonlinear Model Predictive Control</title>
<link>https://arxiv.org/abs/2505.01353</link>
<guid>https://arxiv.org/abs/2505.01353</guid>
<content:encoded><![CDATA[
arXiv:2505.01353v1 Announce Type: cross 
Abstract: The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. While approaches presented in the machine learning community are limited to convex or unconstrained formulations, this paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. The publication is accompanied by an efficient open-source implementation within the framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solver mpc.pytorch.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii</title>
<link>https://arxiv.org/abs/2505.01372</link>
<guid>https://arxiv.org/abs/2505.01372</guid>
<content:encoded><![CDATA[
arXiv:2505.01372v1 Announce Type: cross 
Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through causal explanations. Though MI has many explanation-generating methods, progress has been limited by the lack of a universal approach to evaluating explanations. Here we analyse the fundamental question "What makes a good explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science - the Bayesian, Kuhnian, Deutschian, and Nomological - to systematically evaluate and improve explanations in MI. We find that Compact Proofs consider many explanatory virtues and are hence a promising approach. Fruitful research directions implied by our framework include (1) clearly defining explanatory simplicity, (2) focusing on unifying explanations and (3) deriving universal principles for neural networks. Improved MI methods enhance our ability to monitor, predict, and steer AI systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research</title>
<link>https://arxiv.org/abs/2505.01383</link>
<guid>https://arxiv.org/abs/2505.01383</guid>
<content:encoded><![CDATA[
arXiv:2505.01383v1 Announce Type: cross 
Abstract: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing platform for autonomy research. The hardware platform integrates a small camera, a standard airframe, offboard computation, and radio communication for manual overrides. We demonstrate FalconWing's capabilities by developing and deploying a purely vision-based control policy for autonomous landing (without IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our learning approach: (1) constructs a photorealistic simulation environment via 3D Gaussian splatting trained on real-world images; (2) identifies nonlinear dynamics from vision-estimated real-flight data; and (3) trains a multi-modal Vision Transformer (ViT) policy through simulation-only imitation learning. The ViT architecture fuses single RGB image with the history of control actions via self-attention, preserving temporal context while maintaining real-time 20 Hz inference. When deployed zero-shot on the hardware platform, this policy achieves an 80% success rate in vision-based autonomous landings. Together with the hardware specifications, we also open-source the system dynamics, the software for photorealistic simulator and the learning approach.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer</title>
<link>https://arxiv.org/abs/2505.01390</link>
<guid>https://arxiv.org/abs/2505.01390</guid>
<content:encoded><![CDATA[
arXiv:2505.01390v1 Announce Type: cross 
Abstract: This study proposes a novel approach combining Multimodal Deep Learning with intrinsic eXplainable Artificial Intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. The proposed Multimodal Doctor-in-the-Loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. Results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIME: Enhancing Policy Self-Improvement with Modal-level Exploration</title>
<link>https://arxiv.org/abs/2505.01396</link>
<guid>https://arxiv.org/abs/2505.01396</guid>
<content:encoded><![CDATA[
arXiv:2505.01396v1 Announce Type: cross 
Abstract: Self-improvement requires robotic systems to initially learn from human-provided data and then gradually enhance their capabilities through interaction with the environment. This is similar to how humans improve their skills through continuous practice. However, achieving effective self-improvement is challenging, primarily because robots tend to repeat their existing abilities during interactions, often failing to generate new, valuable data for learning. In this paper, we identify the key to successful self-improvement: modal-level exploration and data selection. By incorporating a modal-level exploration mechanism during policy execution, the robot can produce more diverse and multi-modal interactions. At the same time, we select the most valuable trials and high-quality segments from these interactions for learning. We successfully demonstrate effective robot self-improvement on both simulation benchmarks and real-world experiments. The capability for self-improvement will enable us to develop more robust and high-success-rate robotic control strategies at a lower cost. Our code and experiment scripts are available at https://ericjin2002.github.io/SIME/
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENMO: A GENeralist Model for Human MOtion</title>
<link>https://arxiv.org/abs/2505.01425</link>
<guid>https://arxiv.org/abs/2505.01425</guid>
<content:encoded><![CDATA[
arXiv:2505.01425v1 Announce Type: cross 
Abstract: Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning</title>
<link>https://arxiv.org/abs/2311.09830</link>
<guid>https://arxiv.org/abs/2311.09830</guid>
<content:encoded><![CDATA[
arXiv:2311.09830v4 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized a large variety of NLP tasks. An active debate is to what extent they can do reasoning and planning. Prior work has assessed the latter in the specific context of PDDL planning, based on manually converting three PDDL domains into natural language (NL) prompts. Here we automate this conversion step, showing how to leverage an LLM to automatically generate NL prompts from PDDL input. Our automatically generated NL prompts result in similar LLM-planning performance as the previous manually generated ones. Beyond this, the automation enables us to run much larger experiments, providing for the first time a broad evaluation of LLM planning performance in PDDL. Our NL prompts yield better performance than PDDL prompts and simple template-based NL prompts. Compared to symbolic planners, LLM planning lags far behind; but in some domains, our best LLM configuration scales up further than A$^\star$ using LM-cut.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables</title>
<link>https://arxiv.org/abs/2403.04577</link>
<guid>https://arxiv.org/abs/2403.04577</guid>
<content:encoded><![CDATA[
arXiv:2403.04577v2 Announce Type: replace 
Abstract: Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed~dataset.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-PySC2: Starcraft II learning environment for Large Language Models</title>
<link>https://arxiv.org/abs/2411.05348</link>
<guid>https://arxiv.org/abs/2411.05348</guid>
<content:encoded><![CDATA[
arXiv:2411.05348v2 Announce Type: replace 
Abstract: The tremendous potential has been demonstrated by large language models (LLMs) in intelligent decision-making problems, with unprecedented capabilities shown across diverse applications ranging from gaming AI systems to complex strategic planning frameworks. However, the StarCraft II platform, which has been widely adopted for validating decision-making algorithms in the past decade, has not yet provided substantial support for this emerging domain. To address issues that LLMs cannot interface with the hundreds of actions of the pysc2 backend and the lack of native support for multi-agent (MA) collaboration, we propose the LLM-PySC2 environment. This is the first environment that offers LLMs the complete pysc2 action space with sufficient multi-modal information and game Wiki knowledge. With an asynchronous query architecture, the environment efficiently interacts with LLMs that maintain a constant latency regardless of the scale of the agents' population. In the experiments, we evaluated LLMs' decision-making performance in both the macro-decision and micro-operation scenarios, with traditional StarCraft II Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results indicate that LLMs possess the potential to achieve victories in complex scenarios but cannot constantly generate correct decisions, especially in the recovered pysc2 action space and MA settings. Without task-relevant instructions, the pre-trained models suffer from issues such as hallucinations and inefficient collaboration. Our findings suggest that StarCraft II still challenges in the era of large models, revealing that there is a lot to do to develop an advanced LLM decision-making system, and the proposed LLM-PySC2 environment will support future development of LLM-based decision-making solutions.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution</title>
<link>https://arxiv.org/abs/2411.14995</link>
<guid>https://arxiv.org/abs/2411.14995</guid>
<content:encoded><![CDATA[
arXiv:2411.14995v2 Announce Type: replace 
Abstract: Learning STRIPS action models from action traces alone is a challenging problem as it involves learning the domain predicates as well. In this work, a novel approach is introduced which, like the well-known LOCM systems, is scalable, but like SAT approaches, is sound and complete. Furthermore, the approach is general and imposes no restrictions on the hidden domain or the number or arity of the predicates. The new learning method is based on an \emph{efficient, novel test} that checks whether the assumption that a predicate is affected by a set of action patterns, namely, actions with specific argument positions, is consistent with the traces. The predicates and action patterns that pass the test provide the basis for the learned domain that is then easily completed with preconditions and static predicates. The new method is studied theoretically and experimentally. For the latter, the method is evaluated on traces and graphs obtained from standard classical domains like the 8-puzzle, which involve hundreds of thousands of states and transitions. The learned representations are then verified on larger instances.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment</title>
<link>https://arxiv.org/abs/2412.07446</link>
<guid>https://arxiv.org/abs/2412.07446</guid>
<content:encoded><![CDATA[
arXiv:2412.07446v3 Announce Type: replace 
Abstract: Do generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learn a world model from which a sequence is generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences and present a confidence score. Empirical evaluation is conducted in a controlled environment using the setup and rules of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, is tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases for which the GPT model generates illegal moves it also fails to capture any causal structure.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease Detection and Microbiome-Clinical Data Integration</title>
<link>https://arxiv.org/abs/2501.08324</link>
<guid>https://arxiv.org/abs/2501.08324</guid>
<content:encoded><![CDATA[
arXiv:2501.08324v2 Announce Type: replace 
Abstract: Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large language model (LLM) framework designed to integrate and analyze multimodal data, including microbiome profiles, clinical datasets, and external knowledge bases, to enhance the understanding and classification of Alzheimer's disease (AD). By leveraging the agentic system with LLM, ADAM produces insights from diverse data sources and contextualizes the findings with literature-driven evidence. A comparative evaluation with XGBoost revealed a significantly improved mean F1 score and significantly reduced variance for ADAM, highlighting its robustness and consistency, particularly when utilizing human biological data. Although currently tailored for binary classification tasks with two data modalities, future iterations will aim to incorporate additional data types, such as neuroimaging and peripheral biomarkers, and expand them to predict disease progression, thereby broadening ADAM's scalability and applicability in AD research and diagnostic applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2302.03669</link>
<guid>https://arxiv.org/abs/2302.03669</guid>
<content:encoded><![CDATA[
arXiv:2302.03669v3 Announce Type: replace-cross 
Abstract: Smart traffic lights in intelligent transportation systems (ITSs) are envisioned to greatly increase traffic efficiency and reduce congestion. Deep reinforcement learning (DRL) is a promising approach to adaptively control traffic lights based on the real-time traffic situation in a road network. However, conventional methods may suffer from poor scalability. In this paper, we investigate deep reinforcement learning to control traffic lights, and both theoretical analysis and numerical experiments show that the intelligent behavior ``greenwave" (i.e., a vehicle will see a progressive cascade of green lights, and not have to brake at any intersection) emerges naturally a grid road network, which is proved to be the optimal policy in an avenue with multiple cross streets. As a first step, we use two DRL algorithms for the traffic light control problems in two scenarios. In a single road intersection, we verify that the deep Q-network (DQN) algorithm delivers a thresholding policy; and in a grid road network, we adopt the deep deterministic policy gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN algorithm delivers the optimal control, and the DDPG algorithm with passive observations has the capability to produce on its own a high-level intelligent behavior in a grid road network, namely, the ``greenwave" policy emerges. We also verify the ``greenwave" patterns in a $5 \times 10$ grid road network. Thirdly, the ``greenwave" patterns demonstrate that DRL algorithms produce favorable solutions since the ``greenwave" policy shown in experiment results is proved to be optimal in a specified traffic model (an avenue with multiple cross streets). The delivered policies both in a single road intersection and a grid road network demonstrate the scalability of DRL algorithms.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers</title>
<link>https://arxiv.org/abs/2403.07404</link>
<guid>https://arxiv.org/abs/2403.07404</guid>
<content:encoded><![CDATA[
arXiv:2403.07404v3 Announce Type: replace-cross 
Abstract: Continual learning is crucial for applying machine learning in challenging, dynamic, and often resource-constrained environments. However, catastrophic forgetting - overwriting previously learned knowledge when new information is acquired - remains a major challenge. In this work, we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting, highlighting their potential to accelerate computation. Motivated by these findings, we propose to use auxiliary classifiers(ACs) to enhance performance and demonstrate that integrating ACs into various continual learning methods consistently improves accuracy across diverse evaluation settings, yielding an average 10% relative gain. We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers. Our approach provides a scalable and efficient solution for continual learning.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision &amp; Language Decoders use Images and Text equally? How Self-consistent are their Explanations?</title>
<link>https://arxiv.org/abs/2404.18624</link>
<guid>https://arxiv.org/abs/2404.18624</guid>
<content:encoded><![CDATA[
arXiv:2404.18624v4 Announce Type: replace-cross 
Abstract: Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats</title>
<link>https://arxiv.org/abs/2405.14606</link>
<guid>https://arxiv.org/abs/2405.14606</guid>
<content:encoded><![CDATA[
arXiv:2405.14606v4 Announce Type: replace-cross 
Abstract: In pioneering work from 2019, Barcel\'o and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic. In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals. For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting. These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic. Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive. This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!) rule-based modal logic. In the general case, in contrast, the expressive power with floats is weaker than with reals. In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE</title>
<link>https://arxiv.org/abs/2405.17412</link>
<guid>https://arxiv.org/abs/2405.17412</guid>
<content:encoded><![CDATA[
arXiv:2405.17412v4 Announce Type: replace-cross 
Abstract: This paper shows that dimensionality reduction methods such as UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a model introduced in Ravuri et al. (2023), that describes the graph Laplacian (an estimate of the data precision matrix) using a Wishart distribution, with a mean given by a non-linear covariance function evaluated on the latents. This interpretation offers deeper theoretical and semantic insights into such algorithms, and forging a connection to Gaussian process latent variable models by showing that well-known kernels can be used to describe covariances implied by graph Laplacians. We also introduce tools with which similar dimensionality reduction methods can be studied.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Sliced Wasserstein Distance: A Geometric Perspective</title>
<link>https://arxiv.org/abs/2406.13725</link>
<guid>https://arxiv.org/abs/2406.13725</guid>
<content:encoded><![CDATA[
arXiv:2406.13725v2 Announce Type: replace-cross 
Abstract: Many variants of Optimal Transport (OT) have been developed to address its heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used for application domains by projecting the OT problem onto one-dimensional lines, and leveraging the closed-form expression of the univariate OT to reduce the computational burden. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. To mitigate this issue, in this work, we propose to replace one-dimensional lines with a more intricate structure, called tree systems. This structure is metrizable by a tree metric, which yields a closed-form expression for OT problems on tree systems. We provide an extensive theoretical analysis to formally define tree systems with their topological properties, introduce the concept of splitting maps, which operate as the projection mechanism onto these structures, then finally propose a novel variant of Radon transform for tree systems and verify its injectivity. This framework leads to an efficient metric between measures, termed Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a variety of experiments on gradient flows, image style transfer, and generative models, we illustrate that our proposed approach performs favorably compared to SW and its variants.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Aligned Data Removal via Twin Machine Unlearning</title>
<link>https://arxiv.org/abs/2408.11433</link>
<guid>https://arxiv.org/abs/2408.11433</guid>
<content:encoded><![CDATA[
arXiv:2408.11433v2 Announce Type: replace-cross 
Abstract: Modern privacy regulations have spurred the evolution of machine unlearning, a technique that enables the removal of data from an already trained ML model without requiring retraining from scratch. Previous unlearning methods tend to induce the model to achieve lowest classification accuracy on the removal data. Nonetheless, the authentic objective of machine unlearning is to align the unlearned model with the gold model, i.e., achieving the same classification accuracy as the gold model. For this purpose, we present a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. As a results, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data removal. Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model. Meanwhile, our method allows data removal without compromising the model accuracy.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework</title>
<link>https://arxiv.org/abs/2409.04744</link>
<guid>https://arxiv.org/abs/2409.04744</guid>
<content:encoded><![CDATA[
arXiv:2409.04744v2 Announce Type: replace-cross 
Abstract: The inherent uncertainty in the environmental transition model of Reinforcement Learning (RL) necessitates a delicate balance between exploration and exploitation. This balance is crucial for optimizing computational resources to accurately estimate expected rewards for the agent. In scenarios with sparse rewards, such as robotic control systems, achieving this balance is particularly challenging. However, given that many environments possess extensive prior knowledge, learning from the ground up in such contexts may be redundant. To address this issue, we propose Language Model Guided reward Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their proficiency in processing non-standard data forms, such as wiki tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances exploration and exploitation, thereby guiding the agent's exploratory behavior and enhancing sample efficiency. We have rigorously evaluated LMGT across various RL tasks and evaluated it in the embodied robotic environment Housekeep. Our results demonstrate that LMGT consistently outperforms baseline methods. Furthermore, the findings suggest that our framework can substantially reduce the computational resources required during the RL training phase.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Model-Based Optimization by Learning to Rank</title>
<link>https://arxiv.org/abs/2410.11502</link>
<guid>https://arxiv.org/abs/2410.11502</guid>
<content:encoded><![CDATA[
arXiv:2410.11502v3 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to select promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based models than twenty existing methods.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoGene: Audio-Driven Emotional 3D Talking-Head Generation</title>
<link>https://arxiv.org/abs/2410.17262</link>
<guid>https://arxiv.org/abs/2410.17262</guid>
<content:encoded><![CDATA[
arXiv:2410.17262v2 Announce Type: replace-cross 
Abstract: Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Policy Enables In-Context Reinforcement Learning within Trust Horizons</title>
<link>https://arxiv.org/abs/2410.19982</link>
<guid>https://arxiv.org/abs/2410.19982</guid>
<content:encoded><![CDATA[
arXiv:2410.19982v3 Announce Type: replace-cross 
Abstract: Pretrained foundation models have exhibited extraordinary in-context learning performance, allowing zero-shot generalization to new tasks not encountered during pretraining. In the case of reinforcement learning (RL), in-context RL (ICRL) emerges when pretraining FMs on decision-making problems in an autoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL algorithms, like Algorithm Distillation, Decision Pretrained Transformer and Decision Importance Transformer, impose stringent requirements on the pretraining dataset concerning the source policies, context information, and action labels. Notably, these algorithms either demand optimal policies or require varying degrees of well-trained behavior policies for all pretraining environments. This significantly hinders the application of ICRL to real-world scenarios, where acquiring optimal or well-trained policies for a substantial volume of real-world training environments can be intractable. To overcome this challenge, we introduce a novel approach, termed State-Action Distillation (SAD), that allows to generate an effective pretraining dataset guided solely by random policies. In particular, SAD selects query states and corresponding action labels by distilling outstanding state-action pairs from the entire state and action spaces by using random policies within a trust horizon, and then inherits the classical autoregressive-supervised mechanism during pretraining. To the best of our knowledge, this is the first work that enables effective ICRL under random policies and random contexts. We also establish quantitative analysis of the trustworthiness as well as the performance guarantees of SAD. Moreover, our empirical results across multiple popular ICRL benchmark environments demonstrate that, on average, SAD outperforms the best baseline by 236.3% in the offline evaluation and by 135.2% in the online evaluation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Feedback Loop Modeling Improves Recommendation and User Simulation</title>
<link>https://arxiv.org/abs/2410.20027</link>
<guid>https://arxiv.org/abs/2410.20027</guid>
<content:encoded><![CDATA[
arXiv:2410.20027v2 Announce Type: replace-cross 
Abstract: Large language model-based agents are increasingly applied in the recommendation field due to their extensive knowledge and strong planning capabilities. While prior research has primarily focused on enhancing either the recommendation agent or the user agent individually, the collaborative interaction between the two has often been overlooked. Towards this research gap, we propose a novel framework that emphasizes the feedback loop process to facilitate the collaboration between the recommendation agent and the user agent. Specifically, the recommendation agent refines its understanding of user preferences by analyzing the feedback from the user agent on the item recommendation. Conversely, the user agent further identifies potential user interests based on the items and recommendation reasons provided by the recommendation agent. This iterative process enhances the ability of both agents to infer user behaviors, enabling more effective item recommendations and more accurate user simulations. Extensive experiments on three datasets demonstrate the effectiveness of the agentic feedback loop: the agentic feedback loop yields an average improvement of 11.52% over the single recommendation agent and 21.12% over the single user agent. Furthermore, the results show that the agentic feedback loop does not exacerbate popularity or position bias, which are typically amplified by the real-world feedback loop, highlighting its robustness. The source code is available at https://github.com/Lanyu0303/AFL.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning with learned gadgets to tackle hard quantum problems on real hardware</title>
<link>https://arxiv.org/abs/2411.00230</link>
<guid>https://arxiv.org/abs/2411.00230</guid>
<content:encoded><![CDATA[
arXiv:2411.00230v2 Announce Type: replace-cross 
Abstract: Designing quantum circuits for specific tasks is challenging due to the exponential growth of the state space. We introduce gadget reinforcement learning (GRL), which integrates reinforcement learning with program synthesis to automatically generate and incorporate composite gates (gadgets) into the action space. This enhances the exploration of parameterized quantum circuits (PQCs) for complex tasks like approximating ground states of quantum Hamiltonians, an NP-hard problem. We evaluate GRL using the transverse field Ising model under typical computational budgets (e.g., 2- 3 days of GPU runtime). Our results show improved accuracy, hardware compatibility and scalability. GRL exhibits robust performance as the size and complexity of the problem increases, even with constrained computational resources. By integrating gadget extraction, GRL facilitates the discovery of reusable circuit components tailored for specific hardware, bridging the gap between algorithmic design and practical implementation. This makes GRL a versatile framework for optimizing quantum circuits with applications in hardware-specific optimizations and variational quantum algorithms. The code is available at: https://github.com/Aqasch/Gadget_RL
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</title>
<link>https://arxiv.org/abs/2411.12150</link>
<guid>https://arxiv.org/abs/2411.12150</guid>
<content:encoded><![CDATA[
arXiv:2411.12150v2 Announce Type: replace-cross 
Abstract: We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment and propose a heterogeneous spatio-temporal (st) graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks</title>
<link>https://arxiv.org/abs/2411.16721</link>
<guid>https://arxiv.org/abs/2411.16721</guid>
<content:encoded><![CDATA[
arXiv:2411.16721v3 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. Existing defenses, such as input preprocessing, adversarial training, and response evaluation-based methods, are often impractical for real-world deployment due to their high costs. To address this challenge, we propose ASTRA, an efficient and effective defense by adaptively steering models away from adversarial feature directions to resist VLM attacks. Our key procedures involve finding transferable steering vectors representing the direction of harmful response and applying adaptive activation steering to remove these directions at inference time. To create effective steering vectors, we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks. These tokens are then used to construct steering vectors. During inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs. Extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency in mitigating jailbreak risks. Additionally, ASTRA exhibits good transferability, defending against unseen attacks (i.e., structured-based attack, perturbation-based attack with project gradient descent variants, and text-only attack). Our code is available at \url{https://github.com/ASTRAL-Group/ASTRA}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Every Token Counts: Optimal Segmentation for Low-Resource Language Models</title>
<link>https://arxiv.org/abs/2412.06926</link>
<guid>https://arxiv.org/abs/2412.06926</guid>
<content:encoded><![CDATA[
arXiv:2412.06926v5 Announce Type: replace-cross 
Abstract: Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2412.10422</link>
<guid>https://arxiv.org/abs/2412.10422</guid>
<content:encoded><![CDATA[
arXiv:2412.10422v3 Announce Type: replace-cross 
Abstract: Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveGPT: Scaling Autoregressive Behavior Models for Driving</title>
<link>https://arxiv.org/abs/2412.14415</link>
<guid>https://arxiv.org/abs/2412.14415</guid>
<content:encoded><![CDATA[
arXiv:2412.14415v3 Announce Type: replace-cross 
Abstract: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICLR: In-Context Learning of Representations</title>
<link>https://arxiv.org/abs/2501.00070</link>
<guid>https://arxiv.org/abs/2501.00070</guid>
<content:encoded><![CDATA[
arXiv:2501.00070v2 Announce Type: replace-cross 
Abstract: Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy "graph tracing" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention</title>
<link>https://arxiv.org/abs/2501.06382</link>
<guid>https://arxiv.org/abs/2501.06382</guid>
<content:encoded><![CDATA[
arXiv:2501.06382v3 Announce Type: replace-cross 
Abstract: Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning of Surrogate Models via Domain Affine Transformation Across Synthetic and Real-World Benchmarks</title>
<link>https://arxiv.org/abs/2501.14012</link>
<guid>https://arxiv.org/abs/2501.14012</guid>
<content:encoded><![CDATA[
arXiv:2501.14012v3 Announce Type: replace-cross 
Abstract: Surrogate models are frequently employed as efficient substitutes for the costly execution of real-world processes. However, constructing a high-quality surrogate model often demands extensive data acquisition. A solution to this issue is to transfer pre-trained surrogate models for new tasks, provided that certain invariances exist between tasks. This study focuses on transferring non-differentiable surrogate models (e.g., random forests) from a source function to a target function, where we assume their domains are related by an unknown affine transformation, using only a limited amount of transfer data points evaluated on the target. Previous research attempts to tackle this challenge for differentiable models, e.g., Gaussian process regression, which minimizes the empirical loss on the transfer data by tuning the affine transformations. In this paper, we extend the previous work to the random forest and assess its effectiveness on a widely-used artificial problem set - Black-Box Optimization Benchmark (BBOB) testbed, and on four real-world transfer learning problems. The results highlight the significant practical advantages of the proposed method, particularly in reducing both the data requirements and computational costs of training surrogate models for complex real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing</title>
<link>https://arxiv.org/abs/2502.01976</link>
<guid>https://arxiv.org/abs/2502.01976</guid>
<content:encoded><![CDATA[
arXiv:2502.01976v5 Announce Type: replace-cross 
Abstract: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs \& LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELON: Provable Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison</title>
<link>https://arxiv.org/abs/2502.05174</link>
<guid>https://arxiv.org/abs/2502.05174</guid>
<content:encoded><![CDATA[
arXiv:2502.05174v2 Announce Type: replace-cross 
Abstract: Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit</title>
<link>https://arxiv.org/abs/2502.09762</link>
<guid>https://arxiv.org/abs/2502.09762</guid>
<content:encoded><![CDATA[
arXiv:2502.09762v2 Announce Type: replace-cross 
Abstract: Adaptive teaming-the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination-is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader settings. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \url{https://sites.google.com/view/at-drone}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making</title>
<link>https://arxiv.org/abs/2502.12354</link>
<guid>https://arxiv.org/abs/2502.12354</guid>
<content:encoded><![CDATA[
arXiv:2502.12354v2 Announce Type: replace-cross 
Abstract: Recent XAI studies have investigated what constitutes a \textit{good} explanation in AI-assisted decision-making. Despite the widely accepted human-friendly properties of explanations, such as contrastive and selective, existing studies have yielded inconsistent findings. To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation, by evaluating six explanations with different contrastive strategies and information selectivity and scrutinizing factors behind their valuation process. Our analysis results find that contrastive explanations are not the most preferable or understandable in general; Rather, different contrastive and selective explanations were appreciated to a different extent based on who they are, when, how, and what to explain -- with different level of cognitive load and engagement and sociotechnical contexts. Given these findings, we call for a nuanced view of explanation strategies, with implications for designing AI interfaces to accommodate individual and contextual differences in AI-assisted decision-making.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Steering in Neural Theorem Provers</title>
<link>https://arxiv.org/abs/2502.15507</link>
<guid>https://arxiv.org/abs/2502.15507</guid>
<content:encoded><![CDATA[
arXiv:2502.15507v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YARE-GAN: Yet Another Resting State EEG-GAN</title>
<link>https://arxiv.org/abs/2503.02636</link>
<guid>https://arxiv.org/abs/2503.02636</guid>
<content:encoded><![CDATA[
arXiv:2503.02636v2 Announce Type: replace-cross 
Abstract: In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate multi-channel resting-state EEG data and assess the quality of the synthesized signals through both visual and feature-based evaluations. Our results indicate that the model effectively captures the statistical and spectral characteristics of real EEG data, although challenges remain in replicating high-frequency oscillations in the frontal region. Additionally, we demonstrate that the Critic's learned representations can be reused for gender classification task, achieving an out-of-sample accuracy, significantly better than a shuffled-label baseline and a model trained directly on EEG data. These findings suggest that generative models can serve not only as EEG data generators but also as unsupervised feature extractors, reducing the need for manual feature engineering. This study highlights the potential of GAN-based unsupervised learning for EEG analysis, suggesting avenues for more data-efficient deep learning applications in neuroscience.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation</title>
<link>https://arxiv.org/abs/2503.03045</link>
<guid>https://arxiv.org/abs/2503.03045</guid>
<content:encoded><![CDATA[
arXiv:2503.03045v2 Announce Type: replace-cross 
Abstract: This paper presents ArticuBot, in which a single learned policy enables a robotics system to open diverse categories of unseen articulated objects in the real world. This task has long been challenging for robotics due to the large variations in the geometry, size, and articulation types of such objects. Our system, Articubot, consists of three parts: generating a large number of demonstrations in physics-based simulation, distilling all generated demonstrations into a point cloud-based neural policy via imitation learning, and performing zero-shot sim2real transfer to real robotics systems. Utilizing sampling-based grasping and motion planning, our demonstration generalization pipeline is fast and effective, generating a total of 42.3k demonstrations over 322 training articulated objects. For policy learning, we propose a novel hierarchical policy representation, in which the high-level policy learns the sub-goal for the end-effector, and the low-level policy learns how to move the end-effector conditioned on the predicted goal. We demonstrate that this hierarchical approach achieves much better object-level generalization compared to the non-hierarchical version. We further propose a novel weighted displacement model for the high-level policy that grounds the prediction into the existing 3D structure of the scene, outperforming alternative policy representations. We show that our learned policy can zero-shot transfer to three different real robot settings: a fixed table-top Franka arm across two different labs, and an X-Arm on a mobile base, opening multiple unseen articulated objects across two labs, real lounges, and kitchens. Videos and code can be found on our project website: https://articubot.github.io/.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network</title>
<link>https://arxiv.org/abs/2503.12623</link>
<guid>https://arxiv.org/abs/2503.12623</guid>
<content:encoded><![CDATA[
arXiv:2503.12623v2 Announce Type: replace-cross 
Abstract: Dynamic emotion recognition in the wild remains challenging due to the transient nature of emotional expressions and temporal misalignment of multi-modal cues. Traditional approaches predict valence and arousal and often overlook the inherent correlation between these two dimensions. The proposed Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates visual, audio, and textual modalities through a bi-directional cross-modal attention mechanism. MAVEN uses modality-specific encoders to extract features from synchronized video frames, audio segments, and transcripts, predicting emotions in polar coordinates following Russell's circumplex model. The evaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance correlation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline model with a CCC of 0.22. The multistage architecture captures the subtle and transient nature of emotional expressions in conversational videos and improves emotion recognition in real-world situations. The code is available at: https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models</title>
<link>https://arxiv.org/abs/2504.13068</link>
<guid>https://arxiv.org/abs/2504.13068</guid>
<content:encoded><![CDATA[
arXiv:2504.13068v2 Announce Type: replace-cross 
Abstract: This study investigates the relationship between deep learning (DL) model accuracy and expert agreement in classifying crash narratives. We evaluate five DL models -- including BERT variants, USE, and a zero-shot classifier -- against expert labels and narratives, and extend the analysis to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal an inverse relationship: models with higher technical accuracy often show lower agreement with human experts, while LLMs demonstrate stronger expert alignment despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis (PCA) to quantify and visualize model-expert agreement, and employ SHAP analysis to explain misclassifications. Results show that expert-aligned models rely more on contextual and temporal cues than location-specific keywords. These findings suggest that accuracy alone is insufficient for safety-critical NLP tasks. We argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of LLMs as interpretable tools in crash analysis pipelines.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.14204</link>
<guid>https://arxiv.org/abs/2504.14204</guid>
<content:encoded><![CDATA[
arXiv:2504.14204v2 Announce Type: replace-cross 
Abstract: Time series anomaly detection holds notable importance for risk identification and fault detection across diverse application domains. Unsupervised learning methods have become popular because they have no requirement for labels. However, due to the challenges posed by the multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of data scale and complexity, these methods often fail to capture robust and representative dependencies within the time series for identifying anomalies. To enhance the ability of models to capture normal patterns of time series and avoid the retrogression of modeling ability triggered by the dependencies on high-quality prior knowledge, we propose a differencing-based contrastive representation learning framework for time series anomaly detection (DConAD). Specifically, DConAD generates differential data to provide additional information about time series and utilizes transformer-based architecture to capture spatiotemporal dependencies, which enhances the robustness of unbiased representation learning ability. Furthermore, DConAD implements a novel KL divergence-based contrastive learning paradigm that only uses positive samples to avoid deviation from reconstruction and deploys the stop-gradient strategy to compel convergence. Extensive experiments on five public datasets show the superiority and effectiveness of DConAD compared with nine baselines. The code is available at https://github.com/shaieesss/DConAD.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon</title>
<link>https://arxiv.org/abs/2504.16276</link>
<guid>https://arxiv.org/abs/2504.16276</guid>
<content:encoded><![CDATA[
arXiv:2504.16276v2 Announce Type: replace-cross 
Abstract: This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management</title>
<link>https://arxiv.org/abs/2505.00018</link>
<guid>https://arxiv.org/abs/2505.00018</guid>
<content:encoded><![CDATA[
<div> Keywords: human-AI collaboration, empirical developments, theoretical framework, multi-agent coordination, knowledge management

Summary: 
- The paper critically surveys recent empirical developments on human-AI agents collaboration and identifies gaps in existing research.
- A lack of a unified theoretical framework is observed, hindering the integration of studies on complex tasks.
- The authors propose a novel conceptual architecture, the Hierarchical Exploration-Exploitation Net, to address this issue by linking technical details, coordination mechanisms, feedback loops, and control mechanisms.
- By mapping existing contributions onto this framework, the approach aims to inspire new work that combines qualitative and quantitative paradigms.
- The paper serves as both a critical review of technical implementations and a reference for designing future human-AI symbioses, offering insights for the co-evolution of human cognition and AI capability.

<br /><br />Summary: <div>
arXiv:2505.00018v1 Announce Type: new 
Abstract: This position paper critically surveys a broad spectrum of recent empirical developments on human-AI agents collaboration, highlighting both their technical achievements and persistent gaps. We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms. By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms. The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses. Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images</title>
<link>https://arxiv.org/abs/2505.00173</link>
<guid>https://arxiv.org/abs/2505.00173</guid>
<content:encoded><![CDATA[
<div> Keywords: fiber bundles, nerves, medical images, fuzzy semantics, spatial reasoning

Summary:
- This article introduces a formalization method for describing and recognizing fiber bundles, specifically nerves, in medical images.
- Anatomical knowledge of fiber trajectories is logically formalized using fuzzy semantics combined with first-order logic.
- A language representing spatial entities, relations, and quantifiers is defined to formalize natural language descriptions of nerves.
- Spatial reasoning algorithm is proposed for segmenting and recognizing nerves from anatomical and diffusion magnetic resonance images.
- The algorithm is demonstrated on pediatric imaging of pelvic nerves, assisting surgeons in surgical planning. 

<br /><br />Summary: <div>
arXiv:2505.00173v1 Announce Type: new 
Abstract: This article deals with the description and recognition of fiber bundles, in particular nerves, in medical images, based on the anatomical description of the fiber trajectories. To this end, we propose a logical formalization of this anatomical knowledge. The intrinsically imprecise description of nerves, as found in anatomical textbooks, leads us to propose fuzzy semantics combined with first-order logic. We define a language representing spatial entities, relations between these entities and quantifiers. A formula in this language is then a formalization of the natural language description. The semantics are given by fuzzy representations in a concrete domain and satisfaction degrees of relations. Based on this formalization, a spatial reasoning algorithm is proposed for segmentation and recognition of nerves from anatomical and diffusion magnetic resonance images, which is illustrated on pelvic nerves in pediatric imaging, enabling surgeons to plan surgery.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-World Gaps in AI Governance Research</title>
<link>https://arxiv.org/abs/2505.00174</link>
<guid>https://arxiv.org/abs/2505.00174</guid>
<content:encoded><![CDATA[
<div> safety, reliability, generative AI, research outputs, deployment

Summary:<br /><br />Corporate AI research by leading companies and universities is compared based on safety and reliability papers from generative AI research. Corporate AI research focuses more on pre-deployment areas like model alignment and evaluation, neglecting deployment-stage issues such as model bias. Research gaps exist in high-risk deployment domains like healthcare and finance. Improved observability into deployed AI and expanded access to deployment data for external researchers are recommended to address growing knowledge deficits.<br /> <div>
arXiv:2505.00174v1 Announce Type: new 
Abstract: Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset</title>
<link>https://arxiv.org/abs/2505.00204</link>
<guid>https://arxiv.org/abs/2505.00204</guid>
<content:encoded><![CDATA[
<div> framework, ethics, AI, responsible, evaluation <br />
Summary: <br />
- The paper introduces the Responsible AI Labs (RAIL) framework to evaluate the ethical behavior of large language models (LLMs) in real-world applications. 
- Existing AI ethics frameworks often lack actionable evaluation methods, so the RAIL framework provides a systematic approach with measurable dimensions. 
- The study applies the RAIL framework to Anthropic's "Values in the Wild" dataset, which includes anonymized conversations and annotated value expressions. 
- By mapping values to RAIL dimensions and computing synthetic scores, the study offers insights into the ethical behavior of LLMs. 
- The goal is to ensure that AI systems meet ethical standards by emphasizing fairness, transparency, and accountability through actionable evaluation methods like the RAIL framework. <br /> <div>
arXiv:2505.00204v1 Announce Type: new 
Abstract: As AI systems become embedded in real-world applications, ensuring they meet ethical standards is crucial. While existing AI ethics frameworks emphasize fairness, transparency, and accountability, they often lack actionable evaluation methods. This paper introduces a systematic approach using the Responsible AI Labs (RAIL) framework, which includes eight measurable dimensions to assess the normative behavior of large language models (LLMs). We apply this framework to Anthropic's "Values in the Wild" dataset, containing over 308,000 anonymized conversations with Claude and more than 3,000 annotated value expressions. Our study maps these values to RAIL dimensions, computes synthetic scores, and provides insights into the ethical behavior of LLMs in real-world use.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing</title>
<link>https://arxiv.org/abs/2505.00278</link>
<guid>https://arxiv.org/abs/2505.00278</guid>
<content:encoded><![CDATA[
<div> AI, IC testing, defect-aware graph, task assignment, engineer expertise <br />
<br />
Summary: 
The article introduces DeCo, an AI-driven approach for optimizing task assignment in IC testing processes. DeCo utilizes a defect-aware graph derived from IC testing reports to improve defect differentiation, even with limited data. It also creates defect-aware representations for engineers and tasks by incorporating both local and global structure modeling. The assignment mechanism of DeCo pairs testing tasks with QA engineers based on their skill level and workload, ensuring fair and efficient job dispatch. Experimental results show that DeCo achieves high task-handling success rates exceeding 80% in various scenarios, while maintaining balanced workloads regardless of defect data availability. Case studies demonstrate DeCo's ability to assign tasks to engineers even for unfamiliar defects, showcasing its potential as a valuable AI solution for real-world IC failure analysis and task handling. <br /> <div>
arXiv:2505.00278v1 Announce Type: new 
Abstract: In the semiconductor industry, integrated circuit (IC) processes play a vital role, as the rising complexity and market expectations necessitate improvements in yield. Identifying IC defects and assigning IC testing tasks to the right engineers improves efficiency and reduces losses. While current studies emphasize fault localization or defect classification, they overlook the integration of defect characteristics, historical failures, and the insights from engineer expertise, which restrains their effectiveness in improving IC handling. To leverage AI for these challenges, we propose DeCo, an innovative approach for optimizing task assignment in IC testing. DeCo constructs a novel defect-aware graph from IC testing reports, capturing co-failure relationships to enhance defect differentiation, even with scarce defect data. Additionally, it formulates defect-aware representations for engineers and tasks, reinforced by local and global structure modeling on the defect-aware graph. Finally, a contrasting-based assignment mechanism pairs testing tasks with QA engineers by considering their skill level and current workload, thus promoting an equitable and efficient job dispatch. Experiments on a real-world dataset demonstrate that DeCo achieves the highest task-handling success rates in different scenarios, exceeding 80\%, while also maintaining balanced workloads on both scarce or expanded defect data. Moreover, case studies reveal that DeCo can assign tasks to potentially capable engineers, even for their unfamiliar defects, highlighting its potential as an AI-driven solution for the real-world IC failure analysis and task handling.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform</title>
<link>https://arxiv.org/abs/2505.00325</link>
<guid>https://arxiv.org/abs/2505.00325</guid>
<content:encoded><![CDATA[
<div> Keywords: online gaming platform, player behavior, neural network, game tactics, player engagement

Summary:
Understanding player behavior in online gaming is crucial for enhancing player experience and protection. This study introduces CognitionNet, a two-stage neural network that mines game behaviors and play styles from telemetry data on a Rummy gaming platform. By analyzing player moves and reactions, CognitionNet identifies micro-patterns of game behaviors and classifies play styles to provide insights into player psychology and decision-making. The network's collaborative training with a novel "bridge loss" formulation enables the discovery of consistent play style definitions and outperforms state-of-the-art baselines. This research marks a significant advancement in automating the discovery of player psychology and game tactics, as well as providing diagnostic explanations for player engagement predictions. <div>
arXiv:2505.00325v1 Announce Type: new 
Abstract: Games are one of the safest source of realizing self-esteem and relaxation at the same time. An online gaming platform typically has massive data coming in, e.g., in-game actions, player moves, clickstreams, transactions etc. It is rather interesting, as something as simple as data on gaming moves can help create a psychological imprint of the user at that moment, based on her impulsive reactions and response to a situation in the game. Mining this knowledge can: (a) immediately help better explain observed and predicted player behavior; and (b) consequently propel deeper understanding towards players' experience, growth and protection. To this effect, we focus on discovery of the "game behaviours" as micro-patterns formed by continuous sequence of games and the persistent "play styles" of the players' as a sequence of such sequences on an online skill gaming platform for Rummy. We propose a two stage deep neural network, CognitionNet. The first stage focuses on mining game behaviours as cluster representations in a latent space while the second aggregates over these micro patterns to discover play styles via a supervised classification objective around player engagement. The dual objective allows CognitionNet to reveal several player psychology inspired decision making and tactics. To our knowledge, this is the first and one-of-its-kind research to fully automate the discovery of: (i) player psychology and game tactics from telemetry data; and (ii) relevant diagnostic explanations to players' engagement predictions. The collaborative training of the two networks with differential input dimensions is enabled using a novel formulation of "bridge loss". The network plays pivotal role in obtaining homogeneous and consistent play style definitions and significantly outperforms the SOTA baselines wherever applicable.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach</title>
<link>https://arxiv.org/abs/2505.00368</link>
<guid>https://arxiv.org/abs/2505.00368</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban Air Mobility, System of Systems, Holonic architecture, Large Language Model, Decentralized control

Summary:<br />
1. The paper introduces an intelligent holonic architecture utilizing Large Language Models (LLMs) to address challenges in Urban Air Mobility (UAM) systems, including scalability and adaptability within complex environments.<br />
2. Holons in the architecture operate semi-autonomously to facilitate real-time coordination among air taxis, ground transport, and vertiports.<br />
3. LLMs process natural language inputs, generate adaptive plans, and manage disruptions like weather changes or airspace closures to enhance system efficiency.<br />
4. A case study involving multimodal transportation with electric scooters and air taxis showcases the architecture's ability to allocate resources dynamically, replan in real time, and autonomously adapt without centralized control.<br />
5. By advancing decentralized control and AI-driven adaptability, this work sets the stage for resilient and human-centric UAM ecosystems, with future goals including hybrid AI integration and real-world validation.<br />

Summary: <br />
The paper presents an innovative holonic architecture incorporating Large Language Models to tackle challenges in Urban Air Mobility systems. Holons operate semi-autonomously for real-time coordination, while LLMs manage disruptions and generate adaptive plans. A case study demonstrates dynamic resource allocation and autonomous adaptation without centralized control. The advancement of decentralized control and AI-driven adaptability aims to establish resilient and human-centric UAM ecosystems, with future plans including hybrid AI integration and real-world validation. <div>
arXiv:2505.00368v1 Announce Type: new 
Abstract: Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces challenges in system architecture, planning, task management, and execution. Traditional architectural approaches struggle with scalability, adaptability, and seamless resource integration within dynamic and complex environments. This paper presents an intelligent holonic architecture that incorporates Large Language Model (LLM) to manage the complexities of UAM. Holons function semi autonomously, allowing for real time coordination among air taxis, ground transport, and vertiports. LLMs process natural language inputs, generate adaptive plans, and manage disruptions such as weather changes or airspace closures.Through a case study of multimodal transportation with electric scooters and air taxis, we demonstrate how this architecture enables dynamic resource allocation, real time replanning, and autonomous adaptation without centralized control, creating more resilient and efficient urban transportation networks. By advancing decentralized control and AI driven adaptability, this work lays the groundwork for resilient, human centric UAM ecosystems, with future efforts targeting hybrid AI integration and real world validation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleTrack: Scaling and back-tracking Automated GUI Agents</title>
<link>https://arxiv.org/abs/2505.00416</link>
<guid>https://arxiv.org/abs/2505.00416</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated GUI agents, training framework, GUI grounding, GUI planning, ScaleTrack

Summary:
ScaleTrack introduces a training framework for automated GUI agents that addresses challenges in GUI grounding and planning. The framework leverages a diverse dataset of GUI samples to train grounding models effectively. It incorporates a novel training strategy that predicts the next action based on the current GUI image while backtracking historical actions that led to the image. This approach enables ScaleTrack to explain the relationship between GUI images and actions, enhancing the understanding of GUI environments. Experimental results demonstrate the efficacy of ScaleTrack in improving the performance of automated GUI agents. The dataset and code for ScaleTrack will be made available at a provided URL. <br /><br />Summary: <div>
arXiv:2505.00416v1 Announce Type: new 
Abstract: Automated GUI agents aims to facilitate user interaction by automatically performing complex tasks in digital environments, such as web, mobile, desktop devices. It receives textual task instruction and GUI description to generate executable actions (\emph{e.g.}, click) and operation boxes step by step. Training a GUI agent mainly involves grounding and planning stages, in which the GUI grounding focuses on finding the execution coordinates according to the task, while the planning stage aims to predict the next action based on historical actions. However, previous work suffers from the limitations of insufficient training data for GUI grounding, as well as the ignorance of backtracking historical behaviors for GUI planning. To handle the above challenges, we propose ScaleTrack, a training framework by scaling grounding and backtracking planning for automated GUI agents. We carefully collected GUI samples of different synthesis criterions from a wide range of sources, and unified them into the same template for training GUI grounding models. Moreover, we design a novel training strategy that predicts the next action from the current GUI image, while also backtracking the historical actions that led to the GUI image. In this way, ScaleTrack explains the correspondence between GUI images and actions, which effectively describes the evolution rules of the GUI environment. Extensive experimental results demonstrate the effectiveness of ScaleTrack. Data and code will be available at url.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces</title>
<link>https://arxiv.org/abs/2505.00472</link>
<guid>https://arxiv.org/abs/2505.00472</guid>
<content:encoded><![CDATA[
<div> Memory-augmented AI, Generative AI, multi-agent systems, dynamic decision-making, smart environments 
Summary: UserCentrix is an agentic memory-augmented AI framework that integrates personalized Large Language Model agents, hierarchical control system, memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. It features proactive scaling based on task urgency, Value of Information-driven decision-making, meta-reasoning personal LLM agent, and intelligent multi-agent coordination system. Experimental results demonstrate enhanced response accuracy, system efficiency, and computational resource management in real-world applications.<br /><br />Summary: <div>
arXiv:2505.00472v1 Announce Type: new 
Abstract: Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule-based Classifier Models</title>
<link>https://arxiv.org/abs/2505.00474</link>
<guid>https://arxiv.org/abs/2505.00474</guid>
<content:encoded><![CDATA[
<div> legal domain, classifier models, rules, ratio decidendi, Canavotto et al.

Summary: 
This paper introduces an extension to the existing classifier framework used in the legal domain by incorporating sets of rules to capture legal reasoning, which relies on both facts and rules, particularly the ratio decidendi. Building on the rule-based reason model of precedential constraint developed by Canavotto et al., this new approach allows for decisions in new cases to be inferred using an enriched rule-based classifier framework. By including the time element and hierarchy of courts in the classifier framework, this approach provides a more comprehensive model for legal decision-making. <div>
arXiv:2505.00474v1 Announce Type: new 
Abstract: We extend the formal framework of classifier models used in the legal domain. While the existing classifier framework characterises cases solely through the facts involved, legal reasoning fundamentally relies on both facts and rules, particularly the ratio decidendi. This paper presents an initial approach to incorporating sets of rules within a classifier. Our work is built on the work of Canavotto et al. (2023), which has developed the rule-based reason model of precedential constraint within a hierarchy of factors. We demonstrate how decisions for new cases can be inferred using this enriched rule-based classifier framework. Additionally, we provide an example of how the time element and the hierarchy of courts can be used in the new classifier framework.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4</title>
<link>https://arxiv.org/abs/2505.00603</link>
<guid>https://arxiv.org/abs/2505.00603</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, analogical reasoning, decision making, strategic, AI 

Summary: 
This study examines the ability of the GPT4 language model to perform analogical reasoning in strategic decision-making contexts compared to humans. While GPT4 shows high recall in retrieving analogies, it lacks precision, often applying incorrect analogies based on surface similarities. In contrast, humans exhibit higher precision but lower recall, selecting fewer analogies with stronger causal alignment. The research highlights the importance of accurate causal mapping in the evaluation phase of analogical reasoning, where humans excel in recognizing deep structural similarities across domains. AI errors primarily stem from superficial matching, while human errors result from misinterpretations of causal structures. The findings suggest a potential division of labor in AI-assisted organizational decision-making, with language models serving as analogy generators and humans as critical evaluators of contextually appropriate analogies. <div>
arXiv:2505.00603v1 Announce Type: new 
Abstract: This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining LLMs with Logic-Based Framework to Explain MCTS</title>
<link>https://arxiv.org/abs/2505.00610</link>
<guid>https://arxiv.org/abs/2505.00610</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, sequential planning, large language model, Monte Carlo Tree Search, natural language explanation framework

Summary:<br /><br />In response to the trust issues surrounding Artificial Intelligence (AI) for sequential planning, this study presents a Computational Tree Logic-guided large language model (LLM) based framework tailored for the Monte Carlo Tree Search (MCTS) algorithm. MCTS, known for its complexity, is made more interpretable with this framework, capable of addressing a wide array of post-hoc and knowledge-based queries related to MCTS and the Markov Decision Process (MDP) within the domain. By converting user inquiries into logic and variable statements, the framework ensures the factual consistency of evidence gleaned from the search tree with the environmental dynamics and constraints of the stochastic control process. Rigorous evaluation shows the framework's strong performance in terms of accuracy and factual consistency, making it a valuable tool for enhancing trust and understanding in AI sequential planning. <div>
arXiv:2505.00610v1 Announce Type: new 
Abstract: In response to the lack of trust in Artificial Intelligence (AI) for sequential planning, we design a Computational Tree Logic-guided large language model (LLM)-based natural language explanation framework designed for the Monte Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to interpret due to the complexity of its search trees, but our framework is flexible enough to handle a wide range of free-form post-hoc queries and knowledge-based inquiries centered around MCTS and the Markov Decision Process (MDP) of the application domain. By transforming user queries into logic and variable statements, our framework ensures that the evidence obtained from the search tree remains factually consistent with the underlying environmental dynamics and any constraints in the actual stochastic control process. We evaluate the framework rigorously through quantitative assessments, where it demonstrates strong performance in terms of accuracy and factual consistency.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation</title>
<link>https://arxiv.org/abs/2505.00612</link>
<guid>https://arxiv.org/abs/2505.00612</guid>
<content:encoded><![CDATA[
<div> crisis point, Generative AI, ML evaluation, benchmarking strategies, leakage<br />
Summary:<br />
This position paper highlights the crisis in empirical evaluation in Generative AI, attributing it to traditional ML evaluation methods being inadequate for modern GenAI models. These models often lack well-defined ground truth targets, operate in nearly unbounded input and output spaces, and exhibit feedback loops. Two major challenges, leakage and contamination, are identified as crucial in GenAI evaluations. The paper suggests looking to AI Competitions for effective measures against leakage, as they have developed successful strategies to combat cheating. It advocates for viewing AI Competitions as the standard for empirical rigor in GenAI evaluation and utilizing their results effectively. <div>
arXiv:2505.00612v1 Announce Type: new 
Abstract: In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\em leakage} and {\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Source LLM-Driven Federated Transformer for Predictive IoV Management</title>
<link>https://arxiv.org/abs/2505.00651</link>
<guid>https://arxiv.org/abs/2505.00651</guid>
<content:encoded><![CDATA[
<div> Keywords: IoV, Federated Learning, Large Language Models, Traffic Management, Prompt Optimization

Summary: 
The article introduces the Federated Prompt-Optimized Traffic Transformer (FPoTT), a framework utilizing open-source Large Language Models (LLMs) for predictive traffic management in the Internet of Vehicles (IoV) ecosystem. FPoTT incorporates a dynamic prompt optimization mechanism, dual-layer federated learning paradigm, and a Transformer-driven synthetic data generator to enhance trajectory prediction in real-time with global intelligence. By leveraging EleutherAI Pythia-1B, FPoTT achieves 99.86% prediction accuracy on real-world data and performs well on synthetic datasets, showcasing the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management. The framework addresses challenges of high latency, limited scalability, and privacy concerns associated with centralized IoV solutions, offering a promising alternative with its innovative approach to traffic management in smart mobility ecosystems. 

<br /><br />Summary: <div>
arXiv:2505.00651v1 Announce Type: new 
Abstract: The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangVAE and LangSpace: Building and Probing for Language Model VAEs</title>
<link>https://arxiv.org/abs/2505.00004</link>
<guid>https://arxiv.org/abs/2505.00004</guid>
<content:encoded><![CDATA[
<div> Keywords: LangVAE, variational autoencoders, language models, textual representations, disentanglement

Summary:
LangVAE is introduced as a framework for constructing variational autoencoders using pre-trained language models. It aims to create more compact and semantically disentangled representations by leveraging the knowledge of large language models. The LangSpace companion framework provides tools for analyzing these representations, including vector traversal, disentanglement measures, and cluster visualizations. LangVAE and LangSpace offer a flexible and scalable approach to building and evaluating textual representations, with easy integration for models from the HuggingFace Hub. Experiments with different encoder-decoder combinations and annotated inputs reveal a diverse range of interactions across architectural variations in terms of generalization and disentanglement. The findings highlight the potential of LangVAE for facilitating systematic experimentation and understanding of textual representations. 

<br /><br />Summary: <div>
arXiv:2505.00004v1 Announce Type: cross 
Abstract: We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a digital twin of U.S. Congress</title>
<link>https://arxiv.org/abs/2505.00006</link>
<guid>https://arxiv.org/abs/2505.00006</guid>
<content:encoded><![CDATA[
<div> virtual model, U.S. congresspersons, language models, digital twin, Tweet dataset

Summary: 
This paper presents evidence that a digital twin of U.S. congresspersons can be created using a virtual model based on language models. A dataset containing all Tweets from every U.S. congressperson is introduced, enabling the generation of Tweets that closely resemble those posted by real politicians. Using these generated Tweets, roll-call vote behaviors can be predicted and the likelihood of congresspersons crossing party lines can be quantified. This information can assist stakeholders in resource allocation and potentially impact legislative dynamics. The study also discusses limitations and potential extensions for this analysis. <div>
arXiv:2505.00006v1 Announce Type: cross 
Abstract: In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</title>
<link>https://arxiv.org/abs/2505.00008</link>
<guid>https://arxiv.org/abs/2505.00008</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, medically inaccurate information, error detection, misinformation correction, hallucination mitigation<br />
<br />
Summary: <br />
This review explores the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information in healthcare. Through a scoping review of studies from 2020 to 2024, the review categorizes NLP tasks for addressing errors, misinformation, and hallucination. NLP has shown promise in tasks such as error detection, correction, misinformation detection, and hallucination mitigation. However, challenges persist in areas such as data privacy, context dependency, and evaluation standards. The review emphasizes the importance of developing real-world datasets, refining contextual methods, and improving hallucination management in future efforts to ensure reliable and transparent healthcare applications. <div>
arXiv:2505.00008v1 Announce Type: cross 
Abstract: Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models</title>
<link>https://arxiv.org/abs/2505.00010</link>
<guid>https://arxiv.org/abs/2505.00010</guid>
<content:encoded><![CDATA[
<div> Keywords: Jailbreaking, Language Models, Educational Platforms, Linguistic Features, Predictive Models

Summary: 
The study focuses on detecting jailbreaks in Large Language Models (LLMs) used in sensitive domains like education, specifically in the 2-Sigma clinical education platform. Through annotating over 2,300 prompts in 158 conversations, linguistic variables correlating with jailbreak behavior were identified. Predictive models utilizing these linguistic features, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression, were trained and evaluated. The results indicated that feature-based predictive models outperformed Prompt Engineering, with the Fuzzy Decision Tree showing the best performance. The study highlights the effectiveness and explainability of linguistic-feature-based models for jailbreak detection in educational LLMs. Suggestions were made for future research to explore hybrid frameworks for real-time, spectrum-based jailbreak monitoring in educational contexts. 

<br /><br />Summary: <div>
arXiv:2505.00010v1 Announce Type: cross 
Abstract: Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?</title>
<link>https://arxiv.org/abs/2505.00012</link>
<guid>https://arxiv.org/abs/2505.00012</guid>
<content:encoded><![CDATA[
<div> Keywords: qualitative research, AI Co-Ethnographer, automation, code assignments, pattern discovery

Summary: 
The AI Co-Ethnographer (AICoE) is introduced as an end-to-end pipeline for qualitative research, addressing the challenges of scaling labor-intensive processes. AICoE goes beyond simply automating code assignments by offering a comprehensive and integrated approach to qualitative data analysis. The system organizes the entire process, including open coding, code consolidation, code application, and pattern discovery, to provide a more in-depth analysis of qualitative data. AICoE aims to streamline and enhance the qualitative research process, making it more efficient and effective. <div>
arXiv:2505.00012v1 Announce Type: cross 
Abstract: Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa</title>
<link>https://arxiv.org/abs/2505.00013</link>
<guid>https://arxiv.org/abs/2505.00013</guid>
<content:encoded><![CDATA[
<div> BERT, RoBERTa, DeBERTa, emotion detection, Japanese

Summary:<br />
- The study focuses on accurate emotion detection in Japanese text using pre-trained language models.
- The research aims to predict eight Plutchik emotions in Japanese sentences.
- DeBERTa-v3-large model achieves the best accuracy and F1-score compared to other models.
- The model shows good performance for both high and low-frequency emotions.
- ChatGPT-4o and TinySwallow-1.5B-Instruct models lag behind in F1-score.
- The fine-tuned DeBERTa-v3-large model is released as a pip-installable package for emotion classification in Japanese.
- Future work should focus on enhancing data for rare emotions, reducing model size, and exploring prompt engineering for improved performance of large language models. 

Summary: <div>
arXiv:2505.00013v1 Announce Type: cross 
Abstract: Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.
  Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.
  Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.
  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.
  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.
  This manuscript is under review for possible publication in New Generation Computing.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00016</link>
<guid>https://arxiv.org/abs/2505.00016</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, large language models, reasoning, tabular data, reinforcement learning
Summary: <br /><br />This work presents a novel approach to the Text-to-SQL task by focusing on teaching large language models (LLMs) to reason and manipulate tabular data. The framework proposed consists of two stages: generating detailed chain-of-thought (CoT) traces from SQL queries to provide supervision for table reasoning, and introducing a Group Relative Policy Optimization (GRPO) reinforcement learning objective to encourage generalizable reasoning. The approach improves performance on standard Text-to-SQL benchmarks and shows substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA. The results indicate that leveraging SQL supervision not only enhances accuracy but also promotes robust and transferable reasoning abilities in LLMs when dealing with structured data. The distilled-quantized LLaMA model achieved a 20% increase in accuracy, while Qwen achieved a 5% increase, showcasing the effectiveness of using SQL as a scaffold for learning reasoning over tabular data. <div>
arXiv:2505.00016v1 Announce Type: cross 
Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation</title>
<link>https://arxiv.org/abs/2505.00017</link>
<guid>https://arxiv.org/abs/2505.00017</guid>
<content:encoded><![CDATA[
<div> Keywords: cell type annotation, large language models, feature marker database, multi task workflow, tissue types

Summary:

This study introduces a method for automated cell type annotation using large language models (LLMs) with a focus on improving precision and alignment with manual annotation logic. The approach involves a graph structured feature marker database to identify entities related to differential genes for cell reconstruction. A multi task workflow is implemented to optimize the annotation process. Compared to general purpose LLMs, the proposed method achieves higher human evaluation scores by up to 0.21 and improves semantic similarity by 6.1% across 11 tissue types. The method aims to enhance the accuracy and efficiency of cell type annotation utilizing advanced LLM capabilities, ultimately bridging the gap between automated and manual annotation practices. The results highlight the potential of integrating structured databases and specialized workflows to enhance the performance of LLMs in complex biological contexts. 

<br /><br />Summary: <div>
arXiv:2505.00017v1 Announce Type: cross 
Abstract: To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study on Prompt Compression for Large Language Models</title>
<link>https://arxiv.org/abs/2505.00019</link>
<guid>https://arxiv.org/abs/2505.00019</guid>
<content:encoded><![CDATA[
<div> Keywords: Prompt Engineering, Large Language Models, Prompt Compression, Evaluation, Multimodal Tasks

Summary: 
Prompt engineering has been crucial in enhancing the performance of Large Language Models (LLMs) across various tasks. However, the use of lengthy prompts has led to increased computational complexity and costs. To address this issue, this study investigates six different methods for compressing prompts for LLMs while maintaining response quality. The evaluation covers various aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. The experiments conducted across 13 datasets reveal that prompt compression has a more significant impact on LLM performance in longer contexts compared to shorter ones. Surprisingly, in the Longbench evaluation, moderate compression actually improves LLM performance. The code and data for these methods are available on GitHub at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression. 

<br /><br />Summary: Prompt engineering in Large Language Models (LLMs) is essential for task performance. Six prompt compression methods were studied to reduce computational complexity and costs. Evaluation across datasets showed a greater impact of compression on LLMs in long contexts. Surprisingly, moderate compression improved LLM performance in the Longbench evaluation. For further details and data, visit the GitHub repository provided. <div>
arXiv:2505.00019v1 Announce Type: cross 
Abstract: Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Public Access in LLM Pre-Training Data</title>
<link>https://arxiv.org/abs/2505.00020</link>
<guid>https://arxiv.org/abs/2505.00020</guid>
<content:encoded><![CDATA[
<div> copyrighted content, O'Reilly Media, membership inference attack, large language models, AI training<br />
<br />Summary:
Using a dataset of copyrighted O'Reilly Media books, the study employs the DE-COP membership inference attack to assess if OpenAI's language models were trained on copyrighted content without authorization. The results indicate that OpenAI's GPT-4o model shows strong recognition of paywalled O'Reilly book content, with an AUROC score of 82%, outperforming the earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo exhibits better recognition of publicly available O'Reilly book samples. The smaller model GPT-4o Mini shows no knowledge of either public or non-public O'Reilly Media content. The study emphasizes the necessity for enhanced corporate transparency in disclosing pre-training data sources to establish formal licensing frameworks for AI content training. <br /><br />Summary: <div>
arXiv:2505.00020v1 Announce Type: cross 
Abstract: Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\approx$ 50\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss</title>
<link>https://arxiv.org/abs/2505.00021</link>
<guid>https://arxiv.org/abs/2505.00021</guid>
<content:encoded><![CDATA[
<div> data augmentation, transformer-based models, BERT, RoBERTa, data balancing

Summary: 
The paper addresses the challenges of imbalanced data distribution in food hazard detection classification tasks by utilizing data augmentation techniques. Transformer-based models BERT and RoBERTa are used as backbone classifiers. Various data balancing strategies including random oversampling, Easy Data Augmentation (EDA), and focal loss are explored. The experiments demonstrate that EDA effectively mitigates class imbalance, leading to significant improvements in accuracy and F1 scores. Combining focal loss with oversampling and EDA enhances model robustness, particularly for difficult-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection. <br /><br />Summary: <div>
arXiv:2505.00021v1 Announce Type: cross 
Abstract: Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation</title>
<link>https://arxiv.org/abs/2505.00022</link>
<guid>https://arxiv.org/abs/2505.00022</guid>
<content:encoded><![CDATA[
<div> Keywords: German-language dataset, data curation pipeline, synthetic data generation, pre-training, LLMs<br />
Summary:<br />
The article introduces a German-language dataset curation pipeline that enhances the performance and efficiency of large language models (LLMs). The pipeline utilizes heuristic and model-based filtering techniques along with synthetic data generation to create Aleph-Alpha-GermanWeb, a substantial pre-training dataset. Evaluation using a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT) demonstrates significant performance gains over existing datasets such as FineWeb2. This advantage persists even when FineWeb2 is supplemented with high-quality data sources like Wikipedia. The study emphasizes the benefits of model-based data curation and synthetic data generation in improving LLM pre-training datasets.<br /> 
Summary: <div>
arXiv:2505.00022v1 Announce Type: cross 
Abstract: Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORG: Generating Answers from Complex, Interrelated Contexts</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div> Keywords: real-world corpus, language models, interrelationships, Context Organizer, efficiency

Summary: 

The article discusses the challenges faced by language models in dealing with complex interrelationships in a real-world corpus, including inconsistencies and ambiguities. It introduces Context Organizer (CORG), a framework designed to organize multiple contexts into independently processed groups. The CORG framework consists of three key components: a graph constructor, a reranker, and an aggregator, which work together to efficiently find all relevant answers while ensuring disambiguation. The authors classify interrelationships into four types: distracting, ambiguous, counterfactual, and duplicated, and highlight that existing approaches struggle to address all these complexities simultaneously. The results show that CORG effectively balances performance and efficiency, outperforming existing grouping methods and achieving comparable results to more computationally intensive single-context approaches. <div>
arXiv:2505.00023v1 Announce Type: cross 
Abstract: In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning</title>
<link>https://arxiv.org/abs/2505.00024</link>
<guid>https://arxiv.org/abs/2505.00024</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, reasoning traces, DeepSeek-R1, reinforcement learning, Nemotron-Research-Tool-N1<br />
Summary:
Nemotron-Research-Tool-N1 series of language models utilize rule-based reinforcement learning to enhance tool-use abilities. Unlike previous methods, Nemotron-Research-Tool-N1 models are optimized with binary rewards for structural validity and functional correctness of tool invocations. This approach allows the models to internalize reasoning strategies autonomously without annotated reasoning trajectories. Experimental results on BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, based on Qwen-2.5-7B/14B-Instruct, outperform GPT-4o, achieving state-of-the-art results in both evaluations. This novel training paradigm provides a lightweight supervision method that improves model generalization and reasoning capabilities, making it a promising approach for extending the functionality of large language models using external tools.<br /><br />Summary: <div>
arXiv:2505.00024v1 Announce Type: cross 
Abstract: Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1</title>
<link>https://arxiv.org/abs/2505.00025</link>
<guid>https://arxiv.org/abs/2505.00025</guid>
<content:encoded><![CDATA[
<div> knowledge acquisition, model compression, computational optimization, lightweight medical large language model architecture, medical question-answering datasets

Summary:
The paper introduces a novel method for developing lightweight medical large language models that can be effectively implemented in resource-constrained environments. It addresses the challenges posed by professional knowledge barriers, computational resource requirements, and deployment limitations in actual medical scenarios. The proposed method focuses on three key dimensions: knowledge acquisition, model compression, and computational optimization. It includes a knowledge transfer pipeline, compression techniques, and inference optimization methods. By utilizing techniques such as Low-Rank Adaptation (LoRA), 4-bit weight quantization, Flash Attention acceleration, and continuous batching, the method achieves a reduction in memory consumption by 64.7% and inference latency by 12.4%, while maintaining professional accuracy on medical question-answering datasets. This approach offers a promising solution for the practical application of medical large models on edge computing devices. 

<br /><br />Summary: <div>
arXiv:2505.00025v1 Announce Type: cross 
Abstract: In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\% and inference latency by 12.4\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind in Large Language Models: Assessment and Enhancement</title>
<link>https://arxiv.org/abs/2505.00026</link>
<guid>https://arxiv.org/abs/2505.00026</guid>
<content:encoded><![CDATA[
<div> Keywords: Theory of Mind, Large Language Models, evaluation benchmarks, ToM capabilities, research directions
Summary:
Large Language Models' ability to understand and respond to human mental states, known as Theory of Mind (ToM), is essential for social intelligence. This paper reviews the ToM capabilities of Large Language Models by examining evaluation benchmarks and strategies for improvement. It focuses on story-based benchmarks and analyzes methods for enhancing ToM in LLMs. The paper also outlines potential future research directions based on recent benchmarks and cutting-edge approaches. By providing a comprehensive overview of LLMs' ToM capabilities, this survey serves as a valuable resource for researchers looking to advance the understanding and utilization of ToM in Large Language Models.
<br /><br />Summary: <div>
arXiv:2505.00026v1 Announce Type: cross 
Abstract: Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts</title>
<link>https://arxiv.org/abs/2505.00027</link>
<guid>https://arxiv.org/abs/2505.00027</guid>
<content:encoded><![CDATA[
<div> discovering subject dimension, action dimension, object dimension, adverbial dimension, trees<br />
Summary:<br />
This paper introduces an approach that automatically identifies subject, action, object, and adverbial dimensions from texts to enhance text manipulation and facilitate natural language queries. The method constructs high-quality trees to represent these dimensions and their subclass relations, ensuring efficient representation without redundancies. By accessing these trees through a tree-based search mechanism, the proposed approach supports effective querying in natural language. Experimental results show that the abstraction trees' precision, recall, and F1-score exceed 80%. When applied to querying subject or object in texts, the approach demonstrates high coverage of different question patterns and efficiently narrows down the search space by searching multiple trees based on the question pattern. This enables precise text manipulation and targeted sentence retrieval for supporting query operations.<br />Summary: <div>
arXiv:2505.00027v1 Announce Type: cross 
Abstract: This paper proposed an approach to automatically discovering subject dimension, action dimension, object dimension and adverbial dimension from texts to efficiently operate texts and support query in natural language. The high quality of trees guarantees that all subjects, actions, objects and adverbials and their subclass relations within texts can be represented. The independency of trees ensures that there is no redundant representation between trees. The expressiveness of trees ensures that the majority of sentences can be accessed from each tree and the rest of sentences can be accessed from at least one tree so that the tree-based search mechanism can support querying in natural language. Experiments show that the average precision, recall and F1-score of the abstraction trees constructed by the subclass relations of subject, action, object and adverbial are all greater than 80%. The application of the proposed approach to supporting query in natural language demonstrates that different types of question patterns for querying subject or object have high coverage of texts, and searching multiple trees on subject, action, object and adverbial according to the question pattern can quickly reduce search space to locate target sentences, which can support precise operation on texts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.00028</link>
<guid>https://arxiv.org/abs/2505.00028</guid>
<content:encoded><![CDATA[
<div> Keyword: end-to-end, speech-to-speech, dialogue systems, Retrieval-Augmented Generation, knowledge integration

Summary:<br />
- The research focuses on end-to-end speech-to-speech dialogue systems which offer advantages over traditional cascaded systems in terms of latency and natural integration of nonverbal cues.
- Retrieval-Augmented Generation (RAG) is commonly used to incorporate external knowledge into large language models (LLMs) in text-based systems but faces challenges in bridging the modality gap between input speech and retrieved textual knowledge.
- A novel end-to-end RAG framework is proposed to directly retrieve relevant textual knowledge from speech queries, eliminating the need for speech-to-text conversion.
- Experimental results show improved performance and higher retrieval efficiency of the proposed method in end-to-end S2S dialogue systems but still lags behind cascaded models.
- The code and dataset will be released to support reproducibility and further research. 

Summary: <div>
arXiv:2505.00028v1 Announce Type: cross 
Abstract: In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting</title>
<link>https://arxiv.org/abs/2505.00029</link>
<guid>https://arxiv.org/abs/2505.00029</guid>
<content:encoded><![CDATA[
<div> knowledge domain, vision-language models, fine-tuning, catastrophic forgetting, structured dialogue<br />
Summary:<br />
Structured Dialogue Fine-Tuning (SDFT) is introduced as an approach to inject domain-specific knowledge into Large Vision Language Models while minimizing catastrophic forgetting. The method involves a three-phase dialogue structure: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization. SDFT strikes a balance between acquiring specialized knowledge and retaining general capabilities. Key contributions include a data-centric dialogue template, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types. Experimental results across multiple domains confirm the effectiveness of SDFT in maintaining foundational alignment while integrating targeted knowledge. <div>
arXiv:2505.00029v1 Announce Type: cross 
Abstract: Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving</title>
<link>https://arxiv.org/abs/2505.00031</link>
<guid>https://arxiv.org/abs/2505.00031</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, self-training algorithm, anticipatory plans, problem-solving, natural language reasoning

Summary:
LEPA is a novel self-training algorithm for large language models that focuses on generating anticipatory plans before solving problems. Drawing inspiration from human cognitive processes, LEPA aims to capture abstract meta-knowledge essential for generalization. During data generation, LEPA creates anticipatory plans based on the problem and generates solutions aligned with these plans. The algorithm refines these plans through self-reflection to improve solution accuracy. In model optimization, the LLM is trained to predict both the refined plans and corresponding solutions. By extracting and utilizing anticipatory plans efficiently, LEPA outperforms conventional algorithms on challenging natural language reasoning benchmarks. <div>
arXiv:2505.00031v1 Announce Type: cross 
Abstract: In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis</title>
<link>https://arxiv.org/abs/2505.00032</link>
<guid>https://arxiv.org/abs/2505.00032</guid>
<content:encoded><![CDATA[
<div> Keywords: Major Depressive Disorder, MDD-LLM, AI-driven framework, Large Language Models, UK Biobank cohort

Summary:
This article introduces a high-performance MDD diagnosis tool called MDD-LLM, which utilizes fine-tuned large language models (LLMs) to address challenges in diagnosing Major Depressive Disorder (MDD). The tool is trained and evaluated using 274,348 individual records from the UK Biobank cohort. The results show that MDD-LLM outperforms existing machine learning and deep learning frameworks for MDD diagnosis, achieving an accuracy of 0.8378 and an AUC of 0.8919. The study explores factors that may affect the tool's performance, such as tabular data transformation techniques and fine-tuning strategies. This research highlights the potential of LLMs in improving MDD diagnosis and emphasizes the importance of utilizing advanced AI-driven frameworks in addressing public health issues like MDD.<br /><br />Summary: <div>
arXiv:2505.00032v1 Announce Type: cross 
Abstract: Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Phishing Email Detection Performance of Small Large Language Models</title>
<link>https://arxiv.org/abs/2505.00034</link>
<guid>https://arxiv.org/abs/2505.00034</guid>
<content:encoded><![CDATA[
<div> phishing email detection, small-parameter LLMs, Prompt Engineering, Explanation Augmented Fine-tuning, Model Ensemble<br />
Summary:<br />
The study investigates using small-parameter LLMs for phishing email detection to reduce computational costs. These LLMs with around 3 billion parameters can run on consumer-grade GPUs but often perform poorly in this task. To improve their performance, the researchers implemented Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble techniques. These methods helped enhance the phishing email detection capabilities of small LLMs, with experiments showing a significant increase in accuracy on the SpamAssassin dataset from 0.5 to 0.976 for baseline models like Qwen2.5-1.5B-Instruct. <div>
arXiv:2505.00034v1 Announce Type: cross 
Abstract: Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics</title>
<link>https://arxiv.org/abs/2505.00035</link>
<guid>https://arxiv.org/abs/2505.00035</guid>
<content:encoded><![CDATA[
<div> vocabulary diversity, rhyme density, thematic content, sentiment analysis, stylistic approaches

Summary:
The paper presents a computational framework analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. The study spans four decades and reveals a 23.7% increase in vocabulary diversity. Rhyme density increased by 34.2%, with Midwest artists showcasing the highest technical complexity. There were significant shifts in thematic content, with social justice themes decreasing while introspective themes increased. Lyrics became more negative during sociopolitical crises. The analysis identified four distinct stylistic approaches strongly correlated with geographic origin and time period. These findings offer quantitative evidence of hip-hop's evolution as an art form and a reflection of societal dynamics, highlighting the interplay between linguistic innovation and cultural context in popular music.<br /><br />Summary: <div>
arXiv:2505.00035v1 Announce Type: cross 
Abstract: This paper presents a comprehensive computational framework for analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a dataset of 3,814 songs from 146 influential artists spanning four decades (1980-2020), we employ natural language processing techniques to quantify multiple dimensions of lyrical complexity. Our analysis reveals a 23.7% increase in vocabulary diversity over the study period, with East Coast artists demonstrating 17.3% higher lexical variation than other regions. Rhyme density increased by 34.2% across all regions, with Midwest artists exhibiting the highest technical complexity (3.04 rhymes per line). Topic modeling identified significant shifts in thematic content, with social justice themes decreasing from 28.5% to 13.8% of content while introspective themes increased from 7.6% to 26.3%. Sentiment analysis demon- strated that lyrics became significantly more negative during sociopolitical crises, with polarity decreasing by 0.31 following major social unrest. Multi-dimensional analysis revealed four dis- tinct stylistic approaches that correlate strongly with geographic origin (r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish quantitative evidence for the evolution of hip- hop as both an art form and a reflection of societal dynamics, providing insights into the interplay between linguistic innovation and cultural context in popular music.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional Autoencoders for Data Compression and Anomaly Detection in Small Satellite Technologies</title>
<link>https://arxiv.org/abs/2505.00040</link>
<guid>https://arxiv.org/abs/2505.00040</guid>
<content:encoded><![CDATA[
<div> Keywords: small satellites, machine learning, convolutional autoencoders, data compression, anomaly detection

Summary:<br />
Small satellite technologies have revolutionized geodesic missions by simplifying design and reducing costs, enabling more frequent launches. This study introduces convolutional autoencoders for small satellite payloads, serving the dual purpose of data compression for efficient off-satellite transmission and anomaly detection for on-board data processing. The implementation of machine learning in these systems enhances performance and efficiency, particularly in tasks like image processing. By focusing on disaster monitoring using aerial image datasets of Africa, this research showcases the potential of integrating novel ML-based approaches in small satellite applications. It not only demonstrates the benefits of leveraging space technology and artificial intelligence in Africa but also expands the horizons of satellite technology with the use of advanced machine learning techniques. 

Summary: <div>
arXiv:2505.00040v1 Announce Type: cross 
Abstract: Small satellite technologies have enhanced the potential and feasibility of geodesic missions, through simplification of design and decreased costs allowing for more frequent launches. On-satellite data acquisition systems can benefit from the implementation of machine learning (ML), for better performance and greater efficiency on tasks such as image processing or feature extraction. This work presents convolutional autoencoders for implementation on the payload of small satellites, designed to achieve dual functionality of data compression for more efficient off-satellite transmission, and at-source anomaly detection to inform satellite data-taking. This capability is demonstrated for a use case of disaster monitoring using aerial image datasets of the African continent, offering avenues for both novel ML-based approaches in small satellite applications along with the expansion of space technology and artificial intelligence in Africa.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting</title>
<link>https://arxiv.org/abs/2505.00050</link>
<guid>https://arxiv.org/abs/2505.00050</guid>
<content:encoded><![CDATA[
<div> fashion trends, social media sentiment, Twitter data, sentiment analysis, machine learning <br />
Summary: <br />
This study investigates the correlation between fashion trends and social media sentiment by analyzing Twitter data using the T4SA dataset. The research uses natural language processing and machine learning to predict emerging fashion trends based on sentiment patterns in fashion-related social media conversations. It involves categorizing fashion content, improving sentiment classification techniques, time series decomposition, establishing statistically validated causal relationships, comparing sentiment across platforms, and conducting brand-specific sentiment analysis. Results show a connection between sentiment patterns and fashion theme popularity, with accessories and streetwear themes exhibiting significant upward trends. The analysis highlights sustainability and streetwear as major trend drivers with bidirectional relationships with other themes. The study suggests that social media sentiment analysis can be a reliable predictor of fashion trends, achieving a balanced accuracy of 78.35% in sentiment classification across positive, neutral, and negative categories. <div>
arXiv:2505.00050v1 Announce Type: cross 
Abstract: This study explores the intersection of fashion trends and social media sentiment through computational analysis of Twitter data using the T4SA (Twitter for Sentiment Analysis) dataset. By applying natural language processing and machine learning techniques, we examine how sentiment patterns in fashion-related social media conversations can serve as predictors for emerging fashion trends. Our analysis involves the identification and categorization of fashion-related content, sentiment classification with improved normalization techniques, time series decomposition, statistically validated causal relationship modeling, cross-platform sentiment comparison, and brand-specific sentiment analysis. Results indicate correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes showing statistically significant rising trends. The Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships with several other themes. The findings demonstrate that social media sentiment analysis can serve as an effective early indicator of fashion trend trajectories when proper statistical validation is applied. Our improved predictive model achieved 78.35% balanced accuracy in sentiment classification, establishing a reliable foundation for trend prediction across positive, neutral, and negative sentiment categories.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5</title>
<link>https://arxiv.org/abs/2505.00060</link>
<guid>https://arxiv.org/abs/2505.00060</guid>
<content:encoded><![CDATA[
<div> evaluation framework, large language models, natural language interfaces, structured data querying, business intelligence <br />
Summary:
The study focuses on assessing the accuracy of Large Language Models (LLMs) in generating SQL queries for business intelligence tasks. A Fact-Consistency Evaluation Framework is proposed using Exaone 3.5, an LLM optimized for enterprise tasks. A domain-specific benchmark comprising 219 natural language business questions across varying SQL complexity levels is created. Evaluation metrics include answer accuracy, execution success, semantic error rate, and non-response rate. Results show that while the model performs well on simple tasks, it struggles with arithmetic reasoning and complex tasks, with common error types identified. The study highlights the limitations of LLMs in business contexts and emphasizes the importance of fact-consistency validation layers and hybrid reasoning approaches. The work provides a reproducible benchmark and evaluation methodology for improving natural language interfaces to enterprise data systems. <br /><br />Summary: <div>
arXiv:2505.00060v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios</title>
<link>https://arxiv.org/abs/2505.00091</link>
<guid>https://arxiv.org/abs/2505.00091</guid>
<content:encoded><![CDATA[
<div> field agentic system, UAV swarms, urban scenarios, task planning, coordination mechanism <br />
<br />
Summary: 
The paper introduces a coordination field agentic system for managing heterogeneous UAV swarms in urban environments. The system utilizes large language models (LLMs) to interpret human instructions and translate them into actionable commands for UAVs, like patrol and target tracking. It also proposes a Coordination field mechanism to guide UAV movements and task selection, allowing for decentralized and adaptive task allocation. Through 50 rounds of comparative testing in a 2D simulation space, the system demonstrated superior performance in terms of task coverage, response time, and adaptability to changing conditions. The approach addresses challenges in efficient semantic understanding, flexible task planning, and dynamic coordination adjustments, making it suitable for complex urban scenarios requiring UAV swarm coordination. <div>
arXiv:2505.00091v1 Announce Type: cross 
Abstract: With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing approaches, this paper proposes coordination field agentic system for coordinating heterogeneous UAV swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses</title>
<link>https://arxiv.org/abs/2505.00100</link>
<guid>https://arxiv.org/abs/2505.00100</guid>
<content:encoded><![CDATA[
<div> AI-Lab intervention, undergraduate students, GenAI usage, structured scaffolding, student learning<br />
Summary:<br />
The study examines the impact of the AI-Lab intervention on undergraduate students at Purdue University, focusing on their learning, skill development, and perceptions of GenAI. The intervention, emphasizing guided scaffolding and mindful engagement, was integrated into various courses over three semesters. Results showed positive shifts in students' attitudes and usage patterns towards GenAI, particularly in conceptual understanding, debugging, and homework tasks. The intervention helped students adopt a more mindful and deliberate approach to using GenAI tools. Focus group discussions supported these findings, indicating that the intervention facilitated a more reflective integration of GenAI into coursework. Recommendations are provided for educators looking to incorporate GenAI into computing curricula in a responsible manner, highlighting the importance of structured interventions to enhance student awareness and skill development.<br /> <div>
arXiv:2505.00100v1 Announce Type: cross 
Abstract: Generative AI (GenAI) is rapidly entering computer science education, yet its effects on student learning, skill development, and perceptions remain underexplored. Concerns about overreliance coexist with a gap in research on structured scaffolding to guide tool use in formal courses. This study examines the impact of a dedicated "AI-Lab" intervention -- emphasizing guided scaffolding and mindful engagement -- on undergraduate students in Data Structures and Algorithms, Competitive Programming, and first-year engineering courses at Purdue University.
  Over three semesters, we integrated AI-Lab modules into four mandatory and elective courses, yielding 831 matched pre- and post-intervention survey responses, alongside focus group discussions. Employing a mixed-methods approach, we analyzed quantitative shifts in usage patterns and attitudes as well as qualitative narratives of student experiences.
  While the overall frequency of GenAI usage for homework or programming projects remained largely stable, we observed large effect sizes in comfort and openness across conceptual, debugging, and homework problems. Notably, usage patterns for debugging also shifted statistically significantly, reflecting students' more mindful and deliberate approach. Focus group discussions corroborated these results, suggesting that the intervention "bridged the gap" between naive GenAI usage and more nuanced, reflective integration of AI tools into coursework, ultimately heightening students' awareness of their own skill development.
  These findings suggest that structured, scaffolded interventions can enable students to harness GenAI's benefits without undermining essential competencies. We offer evidence-based recommendations for educators seeking to integrate GenAI responsibly into computing curricula and identify avenues for future research on GenAI-supported pedagogy.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese</title>
<link>https://arxiv.org/abs/2505.00114</link>
<guid>https://arxiv.org/abs/2505.00114</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Lebanese dialect, Fine-tuning approaches, Cultural authenticity, Translation models<br />
<br />
Summary: 
This study explores the use of Large Language Models (LLMs) for translating the low-resource Lebanese dialect, comparing the impact of culturally authentic data with larger translated datasets. Three fine-tuning approaches - Basic, contrastive, and grammar-hint tuning - were evaluated using open-source Aya23 models. Results show that models fine-tuned on a culturally aware Lebanese dataset outperform those trained on larger non-native data. The best performance was seen with contrastive fine-tuning and prompting, suggesting exposure to bad examples is beneficial. A new benchmark, LebEval, derived from native Lebanese content was introduced and compared to the FLoRes benchmark for authentic evaluation. The study challenges the belief that "More Data is Better" and highlights the importance of cultural authenticity in dialectal translation. Datasets and code are available on Github. <br /><br />Summary: <div>
arXiv:2505.00114v1 Announce Type: cross 
Abstract: This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title>
<link>https://arxiv.org/abs/2505.00127</link>
<guid>https://arxiv.org/abs/2505.00127</guid>
<content:encoded><![CDATA[
<div> overthinking, reasoning length, answer correctness, model calibration, length reduction <br />
Summary: <br />
The study focuses on large language models (LLMs) and their reasoning behavior regarding response length and answer correctness. It found that LLMs tend to overthink simple problems, leading to unnecessarily long outputs, while underthinking harder ones, resulting in a failure to extend reasoning when needed. This suggests a misjudgment of problem difficulty and a lack of appropriate calibrating responses. The study also explored the effects of length reduction on response generation through a preference optimization algorithm. Experiments showed that significantly reducing generation length while maintaining acceptable accuracy is possible. The findings emphasize generation length as a crucial factor in understanding LLMs' reasoning behavior and call for further research on models' self-awareness in adapting reasoning length. <br /> <div>
arXiv:2505.00127v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPRat: Gaussian Process Regression with Asynchronous Tasks</title>
<link>https://arxiv.org/abs/2505.00136</link>
<guid>https://arxiv.org/abs/2505.00136</guid>
<content:encoded><![CDATA[
<div> PyTorch, TensorFlow, Python, GP library, HPX <br />
Summary: <br />
The article introduces a novel method of utilizing task-based C++ code with an asynchronous runtime model HPX in Python through pybind11 for creating a parallel Gaussian process (GP) library, GPRat. The library aims to combine the ease of use of existing GP libraries with high performance and scalability. Performance evaluation on a mass-spring-damper system shows minimal overhead in binding the asynchronous HPX code using pybind11. Compared to GPyTorch and GPflow, GPRat demonstrates superior scaling on up to 64 cores for training and achieves significant prediction speedups. Specifically, GPRat achieves a speedup of 7.63 over GPyTorch and 25.25 over GPflow for prediction, with even greater speedups observed when increasing the number of features. These findings highlight the effectiveness of utilizing asynchronous tasks in Python-based AI applications. <br /> <div>
arXiv:2505.00136v1 Announce Type: cross 
Abstract: Python is the de-facto language for software development in artificial intelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow, rely on parallelization built into their BLAS backends to achieve speedup on CPUs. However, only applying parallelization in a low-level backend can lead to performance and scaling degradation. In this work, we present a novel way of binding task-based C++ code built on the asynchronous runtime model HPX to a high-level Python API using pybind11. We develop a parallel Gaussian process (GP) li- brary as an application. The resulting Python library GPRat combines the ease of use of commonly available GP libraries with the performance and scalability of asynchronous runtime systems. We evaluate the per- formance on a mass-spring-damper system, a standard benchmark from control theory, for varying numbers of regressors (features). The results show almost no binding overhead when binding the asynchronous HPX code using pybind11. Compared to GPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD EPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction speedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number of features from eight to 128, we observe speedups of 29.62 and 21.19, respectively. These results showcase the potential of using asynchronous tasks within Python-based AI applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.00150</link>
<guid>https://arxiv.org/abs/2505.00150</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, multimodal memes, hate speech detection, Vision-Language Models, UnHateMeme<br />
Summary:<br />
The article discusses the challenges posed by the misuse of social media platforms for disseminating hate speech through multimodal memes. It introduces a novel approach using Vision-Language Models (VLMs) to detect and mitigate hateful content in memes. The study presents a definition-guided prompting technique for identifying hateful memes and proposes the UnHateMeme framework to transform such content by replacing offensive components with non-hateful alternatives. The integration of VLMs within UnHateMeme demonstrates promising results in converting hateful memes while maintaining coherence between text and visuals. The research evaluates the performance of pretrained VLMs like LLaVA, Gemini, and GPT-4o in detecting and mitigating hate speech in memes, highlighting their strengths and limitations. This work emphasizes the importance of leveraging VLMs to promote online safety and respectful communication on social media platforms.<br /> <div>
arXiv:2505.00150v1 Announce Type: cross 
Abstract: The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. While the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. Leveraging the powerful generation and reasoning capabilities of Vision-Language Models (VLMs), we address the tasks of detecting and mitigating hateful content. This paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named UnHateMeme, which works by replacing hateful textual and/or visual components. With our definition-guided prompts, VLMs achieve impressive performance on hateful memes detection task. Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. Through empirical experiments, we show the effectiveness of state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. This paper aims to shed light on important applications of VLMs for ensuring safe and respectful online environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2505.00169</link>
<guid>https://arxiv.org/abs/2505.00169</guid>
<content:encoded><![CDATA[
<div> Deep generative models, GEOM-Drugs dataset, evaluation protocols, chemically accurate valency tables, GFN2-xTB-based geometry, energy benchmark<br />
Summary:<br />
Deep generative models have shown promise in generating 3D molecular structures, but current evaluation protocols for the GEOM-Drugs dataset have critical flaws. This work revisits GEOM-Drugs, proposing a corrected evaluation framework that fixes data preprocessing issues and introduces accurate valency tables and a GFN2-xTB-based benchmark. Several leading models were retrained and re-evaluated under this framework, highlighting the importance of chemically rigorous evaluation practices in 3D molecular generation. The recommended evaluation methods and processing scripts for GEOM-Drugs are available on GitHub. <div>
arXiv:2505.00169v1 Announce Type: cross 
Abstract: Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at https://github.com/isayevlab/geom-drugs-3dgen-evaluation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction</title>
<link>https://arxiv.org/abs/2505.00171</link>
<guid>https://arxiv.org/abs/2505.00171</guid>
<content:encoded><![CDATA[
<div> Embeddings, attention mechanisms, deep learning, Non-muscle-invasive bladder cancer, recurrence prediction<br />
Summary:<br />
A deep learning framework incorporating vector embeddings and attention mechanisms is proposed to improve Non-muscle-invasive bladder cancer (NMIBC) recurrence prediction. The model utilizes embeddings for categorical variables to capture complex relationships and enhance prediction performance. It achieves 70% accuracy with tabular data, surpassing traditional methods and offering patient-specific insights through feature attention. Clinicians can identify influential factors contributing to recurrence risk for each patient. Additionally, the model uncovers new important factors like surgical duration and hospital stay, enhancing existing prediction models for NMIBC. This approach addresses the limitations of current clinical prediction tools, providing more accurate and personalized insights to improve patient management in NMIBC. <br /> <div>
arXiv:2505.00171v1 Announce Type: cross 
Abstract: Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in oncology, with recurrence rates soaring as high as 70-80%. Each recurrence triggers a cascade of invasive procedures, lifelong surveillance, and escalating healthcare costs - affecting 460,000 individuals worldwide. However, existing clinical prediction tools remain fundamentally flawed, often overestimating recurrence risk and failing to provide personalized insights for patient management. In this work, we propose an interpretable deep learning framework that integrates vector embeddings and attention mechanisms to improve NMIBC recurrence prediction performance. We incorporate vector embeddings for categorical variables such as smoking status and intravesical treatments, allowing the model to capture complex relationships between patient attributes and recurrence risk. These embeddings provide a richer representation of the data, enabling improved feature interactions and enhancing prediction performance. Our approach not only enhances performance but also provides clinicians with patient-specific insights by highlighting the most influential features contributing to recurrence risk for each patient. Our model achieves accuracy of 70% with tabular data, outperforming conventional statistical methods while providing clinician-friendly patient-level explanations through feature attention. Unlike previous studies, our approach identifies new important factors influencing recurrence, such as surgical duration and hospital stay, which had not been considered in existing NMIBC prediction models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroevolution of Self-Attention Over Proto-Objects</title>
<link>https://arxiv.org/abs/2505.00186</link>
<guid>https://arxiv.org/abs/2505.00186</guid>
<content:encoded><![CDATA[
<div> Keywords: Proto-objects, Image segmentation, Neural networks, Self-attention module, Visual reinforcement learning

Summary: 
Proto-objects, regions with shared visual properties, provide a new approach to attention mechanisms in neural networks. By utilizing image segmentation and working with higher-level features, we reduce representational complexity and encode proto-objects as compact feature vectors. This enables a smaller self-attention module that processes richer semantic information. Our experiments show that the proto-object-based approach matches or surpasses patch-based implementations with fewer parameters and less training time. This signifies the potential of proto-objects in advancing visual tasks in neural networks.<br /><br />Summary: <div>
arXiv:2505.00186v1 Announce Type: cross 
Abstract: Proto-objects - image regions that share common visual properties - offer a promising alternative to traditional attention mechanisms based on rectangular-shaped image patches in neural networks. Although previous work demonstrated that evolving a patch-based hard-attention module alongside a controller network could achieve state-of-the-art performance in visual reinforcement learning tasks, our approach leverages image segmentation to work with higher-level features. By operating on proto-objects rather than fixed patches, we significantly reduce the representational complexity: each image decomposes into fewer proto-objects than regular patches, and each proto-object can be efficiently encoded as a compact feature vector. This enables a substantially smaller self-attention module that processes richer semantic information. Our experiments demonstrate that this proto-object-based approach matches or exceeds the state-of-the-art performance of patch-based implementations with 62% less parameters and 2.6 times less training time.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of Progressive Coding for Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.00190</link>
<guid>https://arxiv.org/abs/2505.00190</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, dictionary learning, progressive coding, Matryoshka SAEs, interpretability <br />
Summary:
Sparse autoencoders (SAEs) utilize dictionary learning to extract meaningful features in a computationally intensive manner. A study reveals the power-law distribution of dictionary importance in SAEs. A comparison between progressive coding through subset pruning and Matryoshka SAEs on a language modeling task indicates that Matryoshka SAEs outperform in terms of reconstruction and language modeling loss, as well as representational similarity. However, pruned vanilla SAEs offer enhanced interpretability. The trade-off between interpretability and performance in SAEs raises important considerations for application and model design. <br /><br /> <div>
arXiv:2505.00190v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) \citep{bricken2023monosemanticity,gao2024scalingevaluatingsparseautoencoders} rely on dictionary learning to extract interpretable features from neural networks at scale in an unsupervised manner, with applications to representation engineering and information retrieval. SAEs are, however, computationally expensive \citep{lieberum2024gemmascopeopensparse}, especially when multiple SAEs of different sizes are needed. We show that dictionary importance in vanilla SAEs follows a power law. We compare progressive coding based on subset pruning of SAEs -- to jointly training nested SAEs, or so-called {\em Matryoshka} SAEs \citep{bussmann2024learning,nabeshima2024Matryoshka} -- on a language modeling task. We show Matryoshka SAEs exhibit lower reconstruction loss and recaptured language modeling loss, as well as higher representational similarity. Pruned vanilla SAEs are more interpretable, however. We discuss the origins and implications of this trade-off.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders</title>
<link>https://arxiv.org/abs/2505.00216</link>
<guid>https://arxiv.org/abs/2505.00216</guid>
<content:encoded><![CDATA[
arXiv:2505.00216v1 Announce Type: cross 
Abstract: Most industry-standard generative AIs and feature encoders are proprietary, offering only black-box access: their outputs are observable, but their internal parameters and architectures remain hidden from the end-user. This black-box access is especially limiting when constructing mixture-of-expert type ensemble models since the user cannot optimize each proprietary AI's internal parameters. Our problem naturally lends itself to a non-competitive game-theoretic lens where each proprietary AI (agent) is inherently competing against the other AI agents, with this competition arising naturally due to their obliviousness of the AI's to their internal structure. In contrast, the user acts as a central planner trying to synchronize the ensemble of competing AIs.
  We show the existence of the unique Nash equilibrium in the online setting, which we even compute in closed-form by eliciting a feedback mechanism between any given time series and the sequence generated by each (proprietary) AI agent. Our solution is implemented as a decentralized, federated-learning algorithm in which each agent optimizes their structure locally on their machine without ever releasing any internal structure to the others. We obtain refined expressions for pre-trained models such as transformers, random feature models, and echo-state networks. Our ``proprietary federated learning'' algorithm is implemented on a range of real-world and synthetic time-series benchmarks. It achieves orders-of-magnitude improvements in predictive accuracy over natural benchmarks, of which there are surprisingly few due to this natural problem still being largely unexplored.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enhanced Automatic Design of Efficient Underwater Gliders</title>
<link>https://arxiv.org/abs/2505.00222</link>
<guid>https://arxiv.org/abs/2505.00222</guid>
<content:encoded><![CDATA[
arXiv:2505.00222v1 Announce Type: cross 
Abstract: The development of novel autonomous underwater gliders has been hindered by limited shape diversity, primarily due to the reliance on traditional design tools that depend heavily on manual trial and error. Building an automated design framework is challenging due to the complexities of representing glider shapes and the high computational costs associated with modeling complex solid-fluid interactions. In this work, we introduce an AI-enhanced automated computational framework designed to overcome these limitations by enabling the creation of underwater robots with non-trivial hull shapes. Our approach involves an algorithm that co-optimizes both shape and control signals, utilizing a reduced-order geometry representation and a differentiable neural-network-based fluid surrogate model. This end-to-end design workflow facilitates rapid iteration and evaluation of hydrodynamic performance, leading to the discovery of optimal and complex hull shapes across various control settings. We validate our method through wind tunnel experiments and swimming pool gliding tests, demonstrating that our computationally designed gliders surpass manually designed counterparts in terms of energy efficiency. By addressing challenges in efficient shape representation and neural fluid surrogate models, our work paves the way for the development of highly efficient underwater gliders, with implications for long-range ocean exploration and environmental monitoring.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers</title>
<link>https://arxiv.org/abs/2505.00225</link>
<guid>https://arxiv.org/abs/2505.00225</guid>
<content:encoded><![CDATA[
arXiv:2505.00225v1 Announce Type: cross 
Abstract: As climate variability increases, the ability of utility providers to deliver precise Estimated Times of Restoration (ETR) during natural disasters has become increasingly critical. Accurate and timely ETRs are essential for enabling customer preparedness during extended power outages, where informed decision-making can be crucial, particularly in severe weather conditions. Nonetheless, prevailing utility practices predominantly depend on manual assessments or traditional statistical methods, which often fail to achieve the level of precision required for reliable and actionable predictions. To address these limitations, we propose a Longitudinal Tabular Transformer (LTT) model that leverages historical outage event data along with sequential updates of these events to improve the accuracy of ETR predictions. The model's performance was evaluated over 34,000 storm-related outage events from three major utility companies, collectively serving over 3 million customers over a 2-year period. Results demonstrate that the LTT model improves the Customer Satisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared to existing methods. Additionally, we introduce customer-informed regression metrics that align model evaluation with real-world satisfaction, ensuring the outcomes resonate with customer expectations. Furthermore, we employ interpretability techniques to analyze the temporal significance of incorporating sequential updates in modeling outage events and to identify the contributions of predictive features to a given ETR. This comprehensive approach not only improves predictive accuracy but also enhances transparency, fostering greater trust in the model's capabilities.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling On-Device GPU Inference for Large Generative Models</title>
<link>https://arxiv.org/abs/2505.00232</link>
<guid>https://arxiv.org/abs/2505.00232</guid>
<content:encoded><![CDATA[
arXiv:2505.00232v1 Announce Type: cross 
Abstract: Driven by the advancements in generative AI, large machine learning models have revolutionized domains such as image processing, audio synthesis, and speech recognition. While server-based deployments remain the locus of peak performance, the imperative for on-device inference, necessitated by privacy and efficiency considerations, persists. Recognizing GPUs as the on-device ML accelerator with the widest reach, we present ML Drift--an optimized framework that extends the capabilities of state-of-the-art GPU-accelerated inference engines. ML Drift enables on-device execution of generative AI workloads which contain 10 to 100x more parameters than existing on-device generative AI models. ML Drift addresses intricate engineering challenges associated with cross-GPU API development, and ensures broad compatibility across mobile and desktop/laptop platforms, thereby facilitating the deployment of significantly more complex models on resource-constrained devices. Our GPU-accelerated ML/AI inference engine achieves an order-of-magnitude performance improvement relative to existing open-source GPU inference engines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems</title>
<link>https://arxiv.org/abs/2505.00240</link>
<guid>https://arxiv.org/abs/2505.00240</guid>
<content:encoded><![CDATA[
arXiv:2505.00240v1 Announce Type: cross 
Abstract: The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
<link>https://arxiv.org/abs/2505.00254</link>
<guid>https://arxiv.org/abs/2505.00254</guid>
<content:encoded><![CDATA[
arXiv:2505.00254v1 Announce Type: cross 
Abstract: AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction</title>
<link>https://arxiv.org/abs/2505.00259</link>
<guid>https://arxiv.org/abs/2505.00259</guid>
<content:encoded><![CDATA[
arXiv:2505.00259v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) has evolved as a prominent solution for compressing complex models, which advocates a small calibration dataset and avoids end-to-end retraining. However, most existing PTQ methods employ block-wise reconstruction, which neglects cross-block dependency and exhibits a notable accuracy drop in low-bit cases. To address these limitations, this paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a Hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, which serve as the base unit for reconstruction, thereby preserving the cross-block dependency and enabling accurate quantization parameters estimation. Second, based on the pack configuration, we propose a mixed-precision quantization approach to assign varied bit-widths to packs according to their distinct sensitivities, thereby further enhancing performance. Extensive experiments on 2D image and 3D point cloud classification tasks, using various network architectures, demonstrate the superiority of our method over the state-of-the-art PTQ methods.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency in Language Models: Current Landscape, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2505.00268</link>
<guid>https://arxiv.org/abs/2505.00268</guid>
<content:encoded><![CDATA[
arXiv:2505.00268v1 Announce Type: cross 
Abstract: The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.00284</link>
<guid>https://arxiv.org/abs/2505.00284</guid>
<content:encoded><![CDATA[
arXiv:2505.00284v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, fully exploiting their capabilities for safe and reliable vehicle control remains an open research challenge. To systematically examine advances and limitations of VLMs in driving tasks, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration and evaluation of evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the nuScenes prediction task, comprehensively assessing metrics such as inference time, computational cost, and predictive accuracy. Illustrative examples highlight that, despite their strong scenario interpretation capabilities, VLMs' practical performance in autonomous driving tasks remains concerning, emphasizing the need for further improvements. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction</title>
<link>https://arxiv.org/abs/2505.00290</link>
<guid>https://arxiv.org/abs/2505.00290</guid>
<content:encoded><![CDATA[
arXiv:2505.00290v1 Announce Type: cross 
Abstract: Molecular odor prediction is the process of using a molecule's structure to predict its smell. While accurate prediction remains challenging, AI models can suggest potential odors. Existing methods, however, often rely on basic descriptors or handcrafted fingerprints, which lack expressive power and hinder effective learning. Furthermore, these methods suffer from severe class imbalance, limiting the training effectiveness of AI models. To address these challenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature Mapping Network (HMFNet). Specifically, we introduce a fine-grained, Local Multi-Hierarchy Feature Extraction module (LMFE) that performs deep feature extraction at the atomic level, capturing detailed features crucial for odor prediction. To enhance the extraction of discriminative atomic features, we integrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically learns feature importance and frequency modulation, improving the model's capability to capture relevant patterns. Additionally, a Global Multi-Hierarchy Feature Extraction module (GMFE) is designed to learn global features from the molecular graph topology, enabling the model to fully leverage global information and enhance its discriminative power for odor prediction. To further mitigate the issue of class imbalance, we propose a Chemically-Informed Loss (CIL). Experimental results demonstrate that our approach significantly improves performance across various deep learning models, highlighting its potential to advance molecular structure representation and accelerate the development of AI-driven technologies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained spatial-temporal perception for gas leak segmentation</title>
<link>https://arxiv.org/abs/2505.00295</link>
<guid>https://arxiv.org/abs/2505.00295</guid>
<content:encoded><![CDATA[
arXiv:2505.00295v1 Announce Type: cross 
Abstract: Gas leaks pose significant risks to human health and the environment. Despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. In this paper, we propose a Fine-grained Spatial-Temporal Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. Specifically, we first construct a correlation volume to capture motion information between consecutive frames. Then, the fine-grained perception progressively refines the object-level features using previous outputs. Finally, a decoder is employed to optimize boundary segmentation. Because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, GasVid. Experimental results on GasVid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
<link>https://arxiv.org/abs/2505.00308</link>
<guid>https://arxiv.org/abs/2505.00308</guid>
<content:encoded><![CDATA[
arXiv:2505.00308v1 Announce Type: cross 
Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v1 Announce Type: cross 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics</title>
<link>https://arxiv.org/abs/2505.00322</link>
<guid>https://arxiv.org/abs/2505.00322</guid>
<content:encoded><![CDATA[
arXiv:2505.00322v1 Announce Type: cross 
Abstract: This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural Video Representation with Temporally Coherent Modulation</title>
<link>https://arxiv.org/abs/2505.00335</link>
<guid>https://arxiv.org/abs/2505.00335</guid>
<content:encoded><![CDATA[
arXiv:2505.00335v1 Announce Type: cross 
Abstract: Implicit neural representations (INR) has found successful applications across diverse domains. To employ INR in real-life, it is important to speed up training. In the field of INR for video applications, the state-of-the-art approach employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors. However, the grid usage, which does not consider the video's dynamic nature, leads to redundant use of trainable parameters. As a result, it has significantly lower parameter efficiency and higher bitrate compared to NeRV-style methods that do not use a parametric encoding. To address the problem, we propose Neural Video representation with Temporally coherent Modulation (NVTM), a novel framework that can capture dynamic characteristics of video. By decomposing the spatio-temporal 3D video data into a set of 2D grids with flow information, NVTM enables learning video representation rapidly and uses parameter efficiently. Our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the NeRV-style method, with a speed increase of over 3 times. Also, it remarks an average of 1.54dB/0.019 improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters) and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic), compared to previous grid-type works. By expanding this to compression tasks, we demonstrate comparable performance to video compression standards (H.264, HEVC) and recent INR approaches for video compression. Additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. Project page is https://sujiikim.github.io/NVTM/.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.00337</link>
<guid>https://arxiv.org/abs/2505.00337</guid>
<content:encoded><![CDATA[
arXiv:2505.00337v1 Announce Type: cross 
Abstract: Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation</title>
<link>https://arxiv.org/abs/2505.00339</link>
<guid>https://arxiv.org/abs/2505.00339</guid>
<content:encoded><![CDATA[
arXiv:2505.00339v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics</title>
<link>https://arxiv.org/abs/2505.00347</link>
<guid>https://arxiv.org/abs/2505.00347</guid>
<content:encoded><![CDATA[
arXiv:2505.00347v1 Announce Type: cross 
Abstract: The explosion in model sizes leads to continued growth in prohibitive training/fine-tuning costs, particularly for stateful optimizers which maintain auxiliary information of even 2x the model size to achieve optimal convergence. We therefore present in this work a novel type of optimizer that carries with extremely lightweight state overloads, achieved through ultra-low-precision quantization. While previous efforts have achieved certain success with 8-bit or 4-bit quantization, our approach enables optimizers to operate at precision as low as 3 bits, or even 2 bits per state element. This is accomplished by identifying and addressing two critical challenges: the signal swamping problem in unsigned quantization that results in unchanged state dynamics, and the rapidly increased gradient variance in signed quantization that leads to incorrect descent directions. The theoretical analysis suggests a tailored logarithmic quantization for the former and a precision-specific momentum value for the latter. Consequently, the proposed SOLO achieves substantial memory savings (approximately 45 GB when training a 7B model) with minimal accuracy loss. We hope that SOLO can contribute to overcoming the bottleneck in computational resources, thereby promoting greater accessibility in fundamental research.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Deep Neural Networks using Safety-Guided Self Compression</title>
<link>https://arxiv.org/abs/2505.00350</link>
<guid>https://arxiv.org/abs/2505.00350</guid>
<content:encoded><![CDATA[
arXiv:2505.00350v1 Announce Type: cross 
Abstract: The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
arXiv:2505.00358v1 Announce Type: cross 
Abstract: Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&amp;B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&amp;B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&amp;B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&amp;B matches or exceeds the performance of state-of-the-art data mixing strategies.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data</title>
<link>https://arxiv.org/abs/2505.00359</link>
<guid>https://arxiv.org/abs/2505.00359</guid>
<content:encoded><![CDATA[
arXiv:2505.00359v1 Announce Type: cross 
Abstract: In data stream clustering, systematic theory of stream clustering algorithms remains relatively scarce. Recently, density-based methods have gained attention. However, existing algorithms struggle to simultaneously handle arbitrarily shaped, multi-density, high-dimensional data while maintaining strong outlier resistance. Clustering quality significantly deteriorates when data density varies complexly. This paper proposes a clustering algorithm based on the novel concept of Tightest Neighbors and introduces a data stream clustering theory based on the Skeleton Set. Based on these theories, this paper develops a new method, TNStream, a fully online algorithm. The algorithm adaptively determines the clustering radius based on local similarity, summarizing the evolution of multi-density data streams in micro-clusters. It then applies a Tightest Neighbors-based clustering algorithm to form final clusters. To improve efficiency in high-dimensional cases, Locality-Sensitive Hashing (LSH) is employed to structure micro-clusters, addressing the challenge of storing k-nearest neighbors. TNStream is evaluated on various synthetic and real-world datasets using different clustering metrics. Experimental results demonstrate its effectiveness in improving clustering quality for multi-density data and validate the proposed data stream clustering theory.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices</title>
<link>https://arxiv.org/abs/2505.00365</link>
<guid>https://arxiv.org/abs/2505.00365</guid>
<content:encoded><![CDATA[
arXiv:2505.00365v1 Announce Type: cross 
Abstract: The proliferation of end devices has led to a distributed computing paradigm, wherein on-device machine learning models continuously process diverse data generated by these devices. The dynamic nature of this data, characterized by continuous changes or data drift, poses significant challenges for on-device models. To address this issue, continual learning (CL) is proposed, enabling machine learning models to incrementally update their knowledge and mitigate catastrophic forgetting. However, the traditional centralized approach to CL is unsuitable for end devices due to privacy and data volume concerns. In this context, federated continual learning (FCL) emerges as a promising solution, preserving user data locally while enhancing models through collaborative updates. Aiming at the challenges of limited storage resources for CL, poor autonomy in task shift detection, and difficulty in coping with new adversarial tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL employs an Encoder-Decoder architecture to separate task-robust and task-sensitive components, significantly reducing storage demands by retaining lightweight task-sensitive components for resource-constrained end devices. Moreover, $\rm{SacFL}$ leverages contrastive learning to introduce an autonomous data shift detection mechanism, enabling it to discern whether a new task has emerged and whether it is a benign task. This capability ultimately allows the device to autonomously trigger CL or attack defense strategy without additional information, which is more practical for end devices. Comprehensive experiments conducted on multiple text and image datasets, such as Cifar100 and THUCNews, have validated the effectiveness of $\rm{SacFL}$ in both class-incremental and domain-incremental scenarios. Furthermore, a demo system has been developed to verify its practicality.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis</title>
<link>https://arxiv.org/abs/2505.00367</link>
<guid>https://arxiv.org/abs/2505.00367</guid>
<content:encoded><![CDATA[
arXiv:2505.00367v1 Announce Type: cross 
Abstract: Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services</title>
<link>https://arxiv.org/abs/2505.00375</link>
<guid>https://arxiv.org/abs/2505.00375</guid>
<content:encoded><![CDATA[
arXiv:2505.00375v1 Announce Type: cross 
Abstract: Accurately estimating package delivery time is essential to the logistics industry, which enables reasonable work allocation and on-time service guarantee. This becomes even more necessary in mixed logistics scenarios where couriers handle a high volume of delivery and a smaller number of pickup simultaneously. However, most of the related works treat the pickup and delivery patterns on couriers' decision behavior equally, neglecting that the pickup has a greater impact on couriers' decision-making compared to the delivery due to its tighter time constraints. In such context, we have three main challenges: 1) multiple spatiotemporal factors are intricately interconnected, significantly affecting couriers' delivery behavior; 2) pickups have stricter time requirements but are limited in number, making it challenging to model their effects on couriers' delivery process; 3) couriers' spatial mobility patterns are critical determinants of their delivery behavior, but have been insufficiently explored. To deal with these, we propose TransPDT, a Transformer-based multi-task package delivery time prediction model. We first employ the Transformer encoder architecture to capture the spatio-temporal dependencies of couriers' historical travel routes and pending package sets. Then we design the pattern memory to learn the patterns of pickup in the imbalanced dataset via attention mechanism. We also set the route prediction as an auxiliary task of delivery time prediction, and incorporate the prior courier spatial movement regularities in prediction. Extensive experiments on real industry-scale datasets demonstrate the superiority of our method. A system based on TransPDT is deployed internally in JD Logistics to track more than 2000 couriers handling hundreds of thousands of packages per day in Beijing.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions</title>
<link>https://arxiv.org/abs/2505.00402</link>
<guid>https://arxiv.org/abs/2505.00402</guid>
<content:encoded><![CDATA[
arXiv:2505.00402v1 Announce Type: cross 
Abstract: Prediction of couriers' delivery timely rates in advance is essential to the logistics industry, enabling companies to take preemptive measures to ensure the normal operation of delivery services. This becomes even more critical during anomaly conditions like the epidemic outbreak, during which couriers' delivery timely rate will decline markedly and fluctuates significantly. Existing studies pay less attention to the logistics scenario. Moreover, many works focusing on prediction tasks in anomaly scenarios fail to explicitly model abnormal events, e.g., treating external factors equally with other features, resulting in great information loss. Further, since some anomalous events occur infrequently, traditional data-driven methods perform poorly in these scenarios. To deal with them, we propose a deep spatial-temporal attention model, named DeepSTA. To be specific, to avoid information loss, we design an anomaly spatio-temporal learning module that employs a recurrent neural network to model incident information. Additionally, we utilize Node2vec to model correlations between road districts, and adopt graph neural networks and long short-term memory to capture the spatial-temporal dependencies of couriers. To tackle the issue of insufficient training data in abnormal circumstances, we propose an anomaly pattern attention module that adopts a memory network for couriers' anomaly feature patterns storage via attention mechanisms. The experiments on real-world logistics datasets during the COVID-19 outbreak in 2022 show the model outperforms the best baselines by 12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over multiple competitive baselines.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceptual Implications of Automatic Anonymization in Pathological Speech</title>
<link>https://arxiv.org/abs/2505.00409</link>
<guid>https://arxiv.org/abs/2505.00409</guid>
<content:encoded><![CDATA[
arXiv:2505.00409v1 Announce Type: cross 
Abstract: Automatic anonymization techniques are essential for ethical sharing of pathological speech data, yet their perceptual consequences remain understudied. This study presents the first comprehensive human-centered analysis of anonymized pathological speech, using a structured perceptual protocol involving ten native and non-native German listeners with diverse linguistic, clinical, and technical backgrounds. Listeners evaluated anonymized-original utterance pairs from 180 speakers spanning Cleft Lip and Palate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls. Speech was anonymized using state-of-the-art automatic methods (equal error rates in the range of 30-40%). Listeners completed Turing-style discrimination and quality rating tasks under zero-shot (single-exposure) and few-shot (repeated-exposure) conditions. Discrimination accuracy was high overall (91% zero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA: p=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization consistently reduced perceived quality (from 83% to 59%, p<0.001), with pathology-specific degradation patterns (one-way ANOVA: p=0.005). Native listeners rated original speech slightly higher than non-native listeners (Delta=4%, p=0.199), but this difference nearly disappeared after anonymization (Delta=1%, p=0.724). No significant gender-based bias was observed. Critically, human perceptual outcomes did not correlate with automatic privacy or clinical utility metrics. These results underscore the need for listener-informed, disorder- and context-specific anonymization strategies that preserve privacy while maintaining interpretability, communicative functions, and diagnostic utility, especially for vulnerable populations such as children.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior</title>
<link>https://arxiv.org/abs/2505.00439</link>
<guid>https://arxiv.org/abs/2505.00439</guid>
<content:encoded><![CDATA[
arXiv:2505.00439v1 Announce Type: cross 
Abstract: Recent work has shown that successful per-domain generalizing action policies can be learned. Scaling behavior, from small training instances to large test instances, is the key objective; and the use of validation instances larger than training instances is one key to achieve it. Prior work has used fixed validation sets. Here, we introduce a method generating the validation set dynamically, on the fly, increasing instance size so long as informative and feasible.We also introduce refined methodology for evaluating scaling behavior, generating test instances systematically to guarantee a given confidence in coverage performance for each instance size. In experiments, dynamic validation improves scaling behavior of GNN policies in all 9 domains used.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.00455</link>
<guid>https://arxiv.org/abs/2505.00455</guid>
<content:encoded><![CDATA[
arXiv:2505.00455v1 Announce Type: cross 
Abstract: Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. We present the Data Therapist, a web-based tool that helps domain experts externalize this implicit knowledge through a mixed-initiative process combining iterative Q&amp;A with interactive annotation. Powered by a large language model, the system analyzes user-supplied datasets, prompts users with targeted questions, and allows annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. We evaluated the tool in a qualitative study involving expert pairs from Molecular Biology, Accounting, Political Science, and Usable Security. The study revealed recurring patterns in how experts reason about their data and highlights areas where AI support can improve visualization design.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2505.00467</link>
<guid>https://arxiv.org/abs/2505.00467</guid>
<content:encoded><![CDATA[
arXiv:2505.00467v1 Announce Type: cross 
Abstract: We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.00482</link>
<guid>https://arxiv.org/abs/2505.00482</guid>
<content:encoded><![CDATA[
arXiv:2505.00482v1 Announce Type: cross 
Abstract: We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at https://byungki-k.github.io/JointDiT/.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks</title>
<link>https://arxiv.org/abs/2505.00487</link>
<guid>https://arxiv.org/abs/2505.00487</guid>
<content:encoded><![CDATA[
arXiv:2505.00487v1 Announce Type: cross 
Abstract: This article describes the process of creating a script and conducting an analytical study of a dataset using the DeepMIMO emulator. An advertorial attack was carried out using the FGSM method to maximize the gradient. A comparison is made of the effectiveness of binary classifiers in the task of detecting distorted data. The dynamics of changes in the quality indicators of the regression model were analyzed in conditions without adversarial attacks, during an adversarial attack and when the distorted data was isolated. It is shown that an adversarial FGSM attack with gradient maximization leads to an increase in the value of the MSE metric by 33% and a decrease in the R2 indicator by 10% on average. The LightGBM binary classifier effectively identifies data with adversarial anomalies with 98% accuracy. Regression machine learning models are susceptible to adversarial attacks, but rapid analysis of network traffic and data transmitted over the network makes it possible to identify malicious activity
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion</title>
<link>https://arxiv.org/abs/2505.00488</link>
<guid>https://arxiv.org/abs/2505.00488</guid>
<content:encoded><![CDATA[
arXiv:2505.00488v1 Announce Type: cross 
Abstract: Quadrupedal robots are increasingly deployed for load-carrying tasks across diverse terrains. While Model Predictive Control (MPC)-based methods can account for payload variations, they often depend on predefined gait schedules or trajectory generators, limiting their adaptability in unstructured environments. To address these limitations, we propose an Adaptive Reinforcement Learning (RL) framework that enables quadrupedal robots to dynamically adapt to both varying payloads and diverse terrains. The framework consists of a nominal policy responsible for baseline locomotion and an adaptive policy that learns corrective actions to preserve stability and improve command tracking under payload variations. We validate the proposed approach through large-scale simulation experiments in Isaac Gym and real-world hardware deployment on a Unitree Go1 quadruped. The controller was tested on flat ground, slopes, and stairs under both static and dynamic payload changes. Across all settings, our adaptive controller consistently outperformed the controller in tracking body height and velocity commands, demonstrating enhanced robustness and adaptability without requiring explicit gait design or manual tuning.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Interactive Learning on the Job via Facility Location Planning</title>
<link>https://arxiv.org/abs/2505.00490</link>
<guid>https://arxiv.org/abs/2505.00490</guid>
<content:encoded><![CDATA[
arXiv:2505.00490v1 Announce Type: cross 
Abstract: Collaborative robots must continually adapt to novel tasks and user preferences without overburdening the user. While prior interactive robot learning methods aim to reduce human effort, they are typically limited to single-task scenarios and are not well-suited for sustained, multi-task collaboration. We propose COIL (Cost-Optimal Interactive Learning) -- a multi-task interaction planner that minimizes human effort across a sequence of tasks by strategically selecting among three query types (skill, preference, and help). When user preferences are known, we formulate COIL as an uncapacitated facility location (UFL) problem, which enables bounded-suboptimal planning in polynomial time using off-the-shelf approximation algorithms. We extend our formulation to handle uncertainty in user preferences by incorporating one-step belief space planning, which uses these approximation algorithms as subroutines to maintain polynomial-time performance. Simulated and physical experiments on manipulation tasks show that our framework significantly reduces the amount of work allocated to the human while maintaining successful task completion.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational OOD State Correction for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.00503</link>
<guid>https://arxiv.org/abs/2505.00503</guid>
<content:encoded><![CDATA[
arXiv:2505.00503v1 Announce Type: cross 
Abstract: The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection</title>
<link>https://arxiv.org/abs/2505.00506</link>
<guid>https://arxiv.org/abs/2505.00506</guid>
<content:encoded><![CDATA[
arXiv:2505.00506v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Critical Traffic Simulation with Guided Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2505.00515</link>
<guid>https://arxiv.org/abs/2505.00515</guid>
<content:encoded><![CDATA[
arXiv:2505.00515v1 Announce Type: cross 
Abstract: Safety-critical traffic simulation plays a crucial role in evaluating autonomous driving systems under rare and challenging scenarios. However, existing approaches often generate unrealistic scenarios due to insufficient consideration of physical plausibility and suffer from low generation efficiency. To address these limitations, we propose a guided latent diffusion model (LDM) capable of generating physically realistic and adversarial safety-critical traffic scenarios. Specifically, our model employs a graph-based variational autoencoder (VAE) to learn a compact latent space that captures complex multi-agent interactions while improving computational efficiency. Within this latent space, the diffusion model performs the denoising process to produce realistic trajectories. To enable controllable and adversarial scenario generation, we introduce novel guidance objectives that drive the diffusion process toward producing adversarial and behaviorally realistic driving behaviors. Furthermore, we develop a sample selection module based on physical feasibility checks to further enhance the physical plausibility of the generated scenarios. Extensive experiments on the nuScenes dataset demonstrate that our method achieves superior adversarial effectiveness and generation efficiency compared to existing baselines while maintaining a high level of realism. Our work provides an effective tool for realistic safety-critical scenario simulation, paving the way for more robust evaluation of autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Correlation Alignment</title>
<link>https://arxiv.org/abs/2505.00533</link>
<guid>https://arxiv.org/abs/2505.00533</guid>
<content:encoded><![CDATA[
arXiv:2505.00533v1 Announce Type: cross 
Abstract: Deep neural networks often experience performance drops due to distribution shifts between training and test data. Although domain adaptation offers a solution, privacy concerns restrict access to training data in many real-world scenarios. This restriction has spurred interest in Test-Time Adaptation (TTA), which adapts models using only unlabeled test data. However, current TTA methods still face practical challenges: (1) a primary focus on instance-wise alignment, overlooking CORrelation ALignment (CORAL) due to missing source correlations; (2) complex backpropagation operations for model updating, resulting in overhead computation and (3) domain forgetting.
  To address these challenges, we provide a theoretical analysis to investigate the feasibility of Test-time Correlation Alignment (TCA), demonstrating that correlation alignment between high-certainty instances and test instances can enhance test performances with a theoretical guarantee. Based on this, we propose two simple yet effective algorithms: LinearTCA and LinearTCA+. LinearTCA applies a simple linear transformation to achieve both instance and correlation alignment without additional model updates, while LinearTCA+ serves as a plug-and-play module that can easily boost existing TTA methods. Extensive experiments validate our theoretical insights and show that TCA methods significantly outperforms baselines across various tasks, benchmarks and backbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on OfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6% computation time compared to the best baseline TTA method.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics</title>
<link>https://arxiv.org/abs/2505.00555</link>
<guid>https://arxiv.org/abs/2505.00555</guid>
<content:encoded><![CDATA[
arXiv:2505.00555v1 Announce Type: cross 
Abstract: Interpretable insights from predictive models remain critical in bio-statistics, particularly when assessing causality, where classical statistical and machine learning methods often provide inherent clarity. While Neural Networks (NNs) offer powerful capabilities for modeling complex biological data, their traditional "black-box" nature presents challenges for validation and trust in high-stakes health applications. Recent advances in Mechanistic Interpretability (MI) aim to decipher the internal computations learned by these networks. This work investigates the application of MI techniques to NNs within the context of causal inference for bio-statistics.
  We demonstrate that MI tools can be leveraged to: (1) probe and validate the internal representations learned by NNs, such as those estimating nuisance functions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2) discover and visualize the distinct computational pathways employed by the network to process different types of inputs, potentially revealing how confounders and treatments are handled; and (3) provide methodologies for comparing the learned mechanisms and extracted insights across statistical, machine learning, and NN models, fostering a deeper understanding of their respective strengths and weaknesses for causal bio-statistical analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.00557</link>
<guid>https://arxiv.org/abs/2505.00557</guid>
<content:encoded><![CDATA[
arXiv:2505.00557v1 Announce Type: cross 
Abstract: Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Learn with Quantum Optimization via Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2505.00561</link>
<guid>https://arxiv.org/abs/2505.00561</guid>
<content:encoded><![CDATA[
arXiv:2505.00561v1 Announce Type: cross 
Abstract: Quantum Approximate Optimization Algorithms (QAOA) promise efficient solutions to classically intractable combinatorial optimization problems by harnessing shallow-depth quantum circuits. Yet, their performance and scalability often hinge on effective parameter optimization, which remains nontrivial due to rugged energy landscapes and hardware noise. In this work, we introduce a quantum meta-learning framework that combines quantum neural networks, specifically Quantum Long Short-Term Memory (QLSTM) architectures, with QAOA. By training the QLSTM optimizer on smaller graph instances, our approach rapidly generalizes to larger, more complex problems, substantially reducing the number of iterations required for convergence. Through comprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model instances, we demonstrate that QLSTM-based optimizers converge faster and achieve higher approximation ratios compared to classical baselines, thereby offering a robust pathway toward scalable quantum optimization in the NISQ era.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
<link>https://arxiv.org/abs/2505.00562</link>
<guid>https://arxiv.org/abs/2505.00562</guid>
<content:encoded><![CDATA[
arXiv:2505.00562v1 Announce Type: cross 
Abstract: Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.00568</link>
<guid>https://arxiv.org/abs/2505.00568</guid>
<content:encoded><![CDATA[
arXiv:2505.00568v1 Announce Type: cross 
Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. Pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. This behavior is especially valuable in medical imaging, where annotations are often scarce. However, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. In practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. Consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. Therefore, we introduce BM-MAE, a masked image modeling pre-training strategy tailored for multimodal MRI data. The same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. This allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. Extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. Additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. Code and trained models are available at: https://github.com/Lucas-rbnt/bmmae
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</title>
<link>https://arxiv.org/abs/2505.00570</link>
<guid>https://arxiv.org/abs/2505.00570</guid>
<content:encoded><![CDATA[
arXiv:2505.00570v1 Announce Type: cross 
Abstract: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Cloning: Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.00579</link>
<guid>https://arxiv.org/abs/2505.00579</guid>
<content:encoded><![CDATA[
arXiv:2505.00579v1 Announce Type: cross 
Abstract: Voice Cloning has rapidly advanced in today's digital world, with many researchers and corporations working to improve these algorithms for various applications. This article aims to establish a standardized terminology for voice cloning and explore its different variations. It will cover speaker adaptation as the fundamental concept and then delve deeper into topics such as few-shot, zero-shot, and multilingual TTS within that context. Finally, we will explore the evaluation metrics commonly used in voice cloning research and related datasets. This survey compiles the available voice cloning algorithms to encourage research toward its generation and detection to limit its misuse.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets</title>
<link>https://arxiv.org/abs/2505.00584</link>
<guid>https://arxiv.org/abs/2505.00584</guid>
<content:encoded><![CDATA[
arXiv:2505.00584v1 Announce Type: cross 
Abstract: Detecting and tracking objects is a crucial component of any autonomous navigation method. For the past decades, object detection has yielded promising results using neural networks on various datasets. While many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. In this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. We also present our results of a baseline lightweight Noise Recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Finite-State Controller Based Offline Solver for Deterministic POMDPs</title>
<link>https://arxiv.org/abs/2505.00596</link>
<guid>https://arxiv.org/abs/2505.00596</guid>
<content:encoded><![CDATA[
arXiv:2505.00596v1 Announce Type: cross 
Abstract: Deterministic partially observable Markov decision processes (DetPOMDPs) often arise in planning problems where the agent is uncertain about its environmental state but can act and observe deterministically. In this paper, we propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI) algorithm for DetPOMDPs, which builds policies in the form of finite-state controllers (FSCs). DetMCVI solves large problems with a high success rate, outperforming existing baselines for DetPOMDPs. We also verify the performance of the algorithm in a real-world mobile robot forest mapping scenario.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Low-Cost Genomic Foundation Models via Outlier Removal</title>
<link>https://arxiv.org/abs/2505.00598</link>
<guid>https://arxiv.org/abs/2505.00598</guid>
<content:encoded><![CDATA[
arXiv:2505.00598v1 Announce Type: cross 
Abstract: We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Empirically, transformer-based models exhibit greater robustness to adversarial perturbations compared to HyenaDNA, highlighting the impact of architectural design on vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction</title>
<link>https://arxiv.org/abs/2505.00615</link>
<guid>https://arxiv.org/abs/2505.00615</guid>
<content:encoded><![CDATA[
arXiv:2505.00615v1 Announce Type: cross 
Abstract: We address the 3D reconstruction of human faces from a single RGB image. To this end, we propose Pixel3DMM, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3D morphable face model (3DMM). We exploit the latent features of the DINO foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. We train our model by registering three high-quality 3D face datasets against the FLAME mesh topology, which results in a total of over 1,000 identities and 976K images. For 3D face reconstruction, we propose a FLAME fitting opitmization that solves for the 3DMM parameters from the uv-coordinate and normal estimates. To evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. Crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. Ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Verification for Gliding Drone Control: A Case Study</title>
<link>https://arxiv.org/abs/2505.00622</link>
<guid>https://arxiv.org/abs/2505.00622</guid>
<content:encoded><![CDATA[
arXiv:2505.00622v1 Announce Type: cross 
Abstract: As machine learning is increasingly deployed in autonomous systems, verification of neural network controllers is becoming an active research domain. Existing tools and annual verification competitions suggest that soon this technology will become effective for real-world applications. Our application comes from the emerging field of microflyers that are passively transported by the wind, which may have various uses in weather or pollution monitoring. Specifically, we investigate centimetre-scale bio-inspired gliding drones that resemble Alsomitra macrocarpa diaspores. In this paper, we propose a new case study on verifying Alsomitra-inspired drones with neural network controllers, with the aim of adhering closely to a target trajectory. We show that our system differs substantially from existing VNN and ARCH competition benchmarks, and show that a combination of tools holds promise for verifying such systems in the future, if certain shortcomings can be overcome. We propose a novel method for robust training of regression networks, and investigate formalisations of this case study in Vehicle and CORA. Our verification results suggest that the investigated training methods do improve performance and robustness of neural network controllers in this application, but are limited in scope and usefulness. This is due to systematic limitations of both Vehicle and CORA, and the complexity of our system reducing the scale of reachability, which we investigate in detail. If these limitations can be overcome, it will enable engineers to develop safe and robust technologies that improve people's lives and reduce our impact on the environment.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation</title>
<link>https://arxiv.org/abs/2505.00624</link>
<guid>https://arxiv.org/abs/2505.00624</guid>
<content:encoded><![CDATA[
arXiv:2505.00624v1 Announce Type: cross 
Abstract: Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)</title>
<link>https://arxiv.org/abs/2505.00626</link>
<guid>https://arxiv.org/abs/2505.00626</guid>
<content:encoded><![CDATA[
arXiv:2505.00626v1 Announce Type: cross 
Abstract: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI</title>
<link>https://arxiv.org/abs/2505.00643</link>
<guid>https://arxiv.org/abs/2505.00643</guid>
<content:encoded><![CDATA[
arXiv:2505.00643v1 Announce Type: cross 
Abstract: Real-time (RT) dynamic MRI plays a vital role in capturing rapid physiological processes, offering unique insights into organ motion and function. Among these applications, RT cine MRI is particularly important for functional assessment of the heart with high temporal resolution. RT imaging enables free-breathing, ungated imaging of cardiac motion, making it a crucial alternative for patients who cannot tolerate conventional breath-hold, ECG-gated acquisitions. However, achieving high acceleration rates in RT cine MRI is challenging due to aliasing artifacts from extra-cardiac tissues, particularly at high undersampling factors. In this study, we propose a novel outer volume removal (OVR) method to address this challenge by eliminating aliasing contributions from non-cardiac regions in a post-processing framework. Our approach estimates the outer volume signal for each timeframe using composite temporal images from time-interleaved undersampling patterns, which inherently contain pseudo-periodic ghosting artifacts. A deep learning (DL) model is trained to identify and remove these artifacts, producing a clean outer volume estimate that is subsequently subtracted from the corresponding k-space data. The final reconstruction is performed with a physics-driven DL (PD-DL) method trained using an OVR-specific loss function to restore high spatio-temporal resolution images. Experimental results show that the proposed method at high accelerations achieves image quality that is visually comparable to clinical baseline images, while outperforming conventional reconstruction techniques, both qualitatively and quantitatively. The proposed approach provides a practical and effective solution for artifact reduction in RT cine MRI without requiring acquisition modifications, offering a pathway to higher acceleration rates while preserving diagnostic quality.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification</title>
<link>https://arxiv.org/abs/2505.00650</link>
<guid>https://arxiv.org/abs/2505.00650</guid>
<content:encoded><![CDATA[
arXiv:2505.00650v1 Announce Type: cross 
Abstract: Unsupervised learning of disease subtypes from multi-omics data presents a significant opportunity for advancing personalized medicine. We introduce OmicsCL, a modular contrastive learning framework that jointly embeds heterogeneous omics modalities-such as gene expression, DNA methylation, and miRNA expression-into a unified latent space. Our method incorporates a survival-aware contrastive loss that encourages the model to learn representations aligned with survival-related patterns, without relying on labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers clinically meaningful clusters and achieves strong unsupervised concordance with patient survival. The framework demonstrates robustness across hyperparameter configurations and can be tuned to prioritize either subtype coherence or survival stratification. Ablation studies confirm that integrating survival-aware loss significantly enhances the predictive power of learned embeddings. These results highlight the promise of contrastive objectives for biological insight discovery in high-dimensional, heterogeneous omics data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Understanding: an Inherent Ambiguity Barrier</title>
<link>https://arxiv.org/abs/2505.00654</link>
<guid>https://arxiv.org/abs/2505.00654</guid>
<content:encoded><![CDATA[
arXiv:2505.00654v1 Announce Type: cross 
Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v1 Announce Type: cross 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepCritic: Deliberate Critique with Large Language Models</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
arXiv:2505.00662v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Policy Optimization</title>
<link>https://arxiv.org/abs/2505.00663</link>
<guid>https://arxiv.org/abs/2505.00663</guid>
<content:encoded><![CDATA[
arXiv:2505.00663v1 Announce Type: cross 
Abstract: We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm for reinforcement learning in continuous action spaces. WPO can be derived as an approximation to Wasserstein gradient flow over the space of all policies projected into a finite-dimensional parameter space (e.g., the weights of a neural network), leading to a simple and completely general closed-form update. The resulting algorithm combines many properties of deterministic and classic policy gradient methods. Like deterministic policy gradients, it exploits knowledge of the gradient of the action-value function with respect to the action. Like classic policy gradients, it can be applied to stochastic policies with arbitrary distributions over actions -- without using the reparameterization trick. We show results on the DeepMind Control Suite and a magnetic confinement fusion task which compare favorably with state-of-the-art continuous control methods.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments</title>
<link>https://arxiv.org/abs/2505.00668</link>
<guid>https://arxiv.org/abs/2505.00668</guid>
<content:encoded><![CDATA[
arXiv:2505.00668v1 Announce Type: cross 
Abstract: Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Test-time Scaling for GUI Agent Grounding</title>
<link>https://arxiv.org/abs/2505.00684</link>
<guid>https://arxiv.org/abs/2505.00684</guid>
<content:encoded><![CDATA[
arXiv:2505.00684v1 Announce Type: cross 
Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\% on Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Micromobility through Scalable Urban Simulation</title>
<link>https://arxiv.org/abs/2505.00690</link>
<guid>https://arxiv.org/abs/2505.00690</guid>
<content:encoded><![CDATA[
arXiv:2505.00690v1 Announce Type: cross 
Abstract: Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Visual Instruction</title>
<link>https://arxiv.org/abs/2505.00693</link>
<guid>https://arxiv.org/abs/2505.00693</guid>
<content:encoded><![CDATA[
arXiv:2505.00693v1 Announce Type: cross 
Abstract: Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision for robotic control introduces challenges such as ambiguity and verbosity. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment, enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Code and Datasets in this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
arXiv:2505.00703v1 Announce Type: cross 
Abstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Scientific Discovery</title>
<link>https://arxiv.org/abs/2411.11672</link>
<guid>https://arxiv.org/abs/2411.11672</guid>
<content:encoded><![CDATA[
arXiv:2411.11672v2 Announce Type: replace 
Abstract: Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a popular board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings, and not rely on a rigid existing interpreter. Questioning the very process of learning an interpreter, we turn our attention to the inner functioning of modern multimodal models. This culminates in a simple idea to build CLIP-like models where interpretation and perception are explicitly disentangled: a cost-effective approach that couples two unimodal models using little multimodal data and no further training. Finally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark about interpreting Zendo-like explanations that sees LLMs going no further than random chance while being instead fully solved by humans.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers</title>
<link>https://arxiv.org/abs/2501.16961</link>
<guid>https://arxiv.org/abs/2501.16961</guid>
<content:encoded><![CDATA[
arXiv:2501.16961v2 Announce Type: replace 
Abstract: Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such *near-certain reasoning* as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolGround: A Benchmark for Molecular Grounding</title>
<link>https://arxiv.org/abs/2503.23668</link>
<guid>https://arxiv.org/abs/2503.23668</guid>
<content:encoded><![CDATA[
arXiv:2503.23668v4 Announce Type: replace 
Abstract: Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 117k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Human Actions in the Digital Platform by Temporal Context</title>
<link>https://arxiv.org/abs/2206.09535</link>
<guid>https://arxiv.org/abs/2206.09535</guid>
<content:encoded><![CDATA[
arXiv:2206.09535v2 Announce Type: replace-cross 
Abstract: Recent advances in digital platforms generate rich, high-dimensional logs of human behavior, and machine learning models have helped social scientists explain knowledge accumulation, communication, and information diffusion. Such models, however, almost always treat behavior as sequences of actions, abstracting the inter-temporal information among actions. To close this gap, we introduce a two-scale Action-Timing Context(ATC) framework that jointly embeds each action and its time interval. ATC obtains low-dimensional representations of actions and characterizes them with inter-temporal information. We provide three applications of ATC to real-world datasets and demonstrate that the method offers a unified view of human behavior. The presented qualitative findings demonstrate that explicitly modeling inter-temporal context is essential for a comprehensive, interpretable understanding of human activity on digital platforms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following</title>
<link>https://arxiv.org/abs/2303.15201</link>
<guid>https://arxiv.org/abs/2303.15201</guid>
<content:encoded><![CDATA[
arXiv:2303.15201v2 Announce Type: replace-cross 
Abstract: In this paper we introduce a general estimation methodology for learning a model of human perception and control in a sensorimotor control task based upon a finite set of demonstrations. The model's structure consists of i the agent's internal representation of how the environment and associated observations evolve as a result of control actions and ii the agent's preferences over observable outcomes. We consider a model's structure specification consistent with active inference, a theory of human perception and behavior from cognitive science. According to active inference, the agent acts upon the world so as to minimize surprise defined as a measure of the extent to which an agent's current sensory observations differ from its preferred sensory observations. We propose a bi-level optimization approach to estimation which relies on a structural assumption on prior distributions that parameterize the statistical accuracy of the human agent's model of the environment. To illustrate the proposed methodology, we present the estimation of a model for car-following behavior based upon a naturalistic dataset. Overall, the results indicate that learning active inference models of human perception and control from data is a promising alternative to black-box models of driving.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers</title>
<link>https://arxiv.org/abs/2309.08532</link>
<guid>https://arxiv.org/abs/2309.08532</guid>
<content:encoded><![CDATA[
arXiv:2309.08532v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omitted Labels Induce Nontransitive Paradoxes in Causality</title>
<link>https://arxiv.org/abs/2311.06840</link>
<guid>https://arxiv.org/abs/2311.06840</guid>
<content:encoded><![CDATA[
arXiv:2311.06840v4 Announce Type: replace-cross 
Abstract: We explore "omitted label contexts," in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific, focused studies. By studying Simpson's paradox, we observe that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2402.07033</link>
<guid>https://arxiv.org/abs/2402.07033</guid>
<content:encoded><![CDATA[
arXiv:2402.07033v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem</title>
<link>https://arxiv.org/abs/2403.00108</link>
<guid>https://arxiv.org/abs/2403.00108</guid>
<content:encoded><![CDATA[
arXiv:2403.00108v2 Announce Type: replace-cross 
Abstract: Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent as a Mechanical Designer</title>
<link>https://arxiv.org/abs/2404.17525</link>
<guid>https://arxiv.org/abs/2404.17525</guid>
<content:encoded><![CDATA[
arXiv:2404.17525v3 Announce Type: replace-cross 
Abstract: Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2405.04532</link>
<guid>https://arxiv.org/abs/2405.04532</guid>
<content:encoded><![CDATA[
arXiv:2405.04532v3 Announce Type: replace-cross 
Abstract: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers</title>
<link>https://arxiv.org/abs/2405.04620</link>
<guid>https://arxiv.org/abs/2405.04620</guid>
<content:encoded><![CDATA[
arXiv:2405.04620v5 Announce Type: replace-cross 
Abstract: In this work, we present a generalized formulation of the Transformer algorithm by reinterpreting its core mechanisms within the framework of Path Integral formalism. In this perspective, the attention mechanism is recast as a process that integrates all possible transition paths leading to future token states, with temporal evolution governed by the Feed-Forward Network. By systematically mapping each component of the Transformer to its counterpart in the Path Integral formulation, we obtain a more compact and efficient representation, in which the contextual information of a sequence is condensed into memory-like segments. These segments are recurrently processed across Transformer layers, enabling more effective long-term information retention. We validate the effectiveness of this approach through the Passkey retrieval task and a summarization task, demonstrating that the proposed method preserves historical information while exhibiting memory usage that scales linearly with sequence length. This contrasts with the non-linear memory growth typically observed in standard attention mechanisms. We expect that this quantum-inspired generalization of the Transformer architecture will open new avenues for enhancing both the efficiency and expressiveness of future Transformer models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment</title>
<link>https://arxiv.org/abs/2406.13216</link>
<guid>https://arxiv.org/abs/2406.13216</guid>
<content:encoded><![CDATA[
arXiv:2406.13216v2 Announce Type: replace-cross 
Abstract: Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model's discriminative power in distinguishing matched and unmatched node pairs across two graphs.Second, we study the model's capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure those matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Commute Graph Neural Networks</title>
<link>https://arxiv.org/abs/2407.01635</link>
<guid>https://arxiv.org/abs/2407.01635</guid>
<content:encoded><![CDATA[
arXiv:2407.01635v5 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have shown remarkable success in learning from graph-structured data. However, their application to directed graphs (digraphs) presents unique challenges, primarily due to the inherent asymmetry in node relationships. Traditional GNNs are adept at capturing unidirectional relations but fall short in encoding the mutual path dependencies between nodes, such as asymmetrical shortest paths typically found in digraphs. Recognizing this gap, we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly integrates node-wise commute time into the message passing scheme. The cornerstone of CGNN is an efficient method for computing commute time using a newly formulated digraph Laplacian. Commute time is then integrated into the neighborhood aggregation process, with neighbor contributions weighted according to their respective commute time to the central node in each layer. It enables CGNN to directly capture the mutual, asymmetric relationships in digraphs. Extensive experiments on 8 benchmarking datasets confirm the superiority of CGNN against 13 state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Review Generation Method Based on Large Language Models</title>
<link>https://arxiv.org/abs/2407.20906</link>
<guid>https://arxiv.org/abs/2407.20906</guid>
<content:encoded><![CDATA[
arXiv:2407.20906v5 Announce Type: replace-cross 
Abstract: Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Security and Privacy Risks of Healthcare AI</title>
<link>https://arxiv.org/abs/2409.07415</link>
<guid>https://arxiv.org/abs/2409.07415</guid>
<content:encoded><![CDATA[
arXiv:2409.07415v2 Announce Type: replace-cross 
Abstract: The integration of artificial intelligence (AI) and machine learning (ML) into healthcare systems holds great promise for enhancing patient care and care delivery efficiency; however, it also exposes sensitive data and system integrity to potential cyberattacks. Current security and privacy (S&amp;P) research on healthcare AI is highly unbalanced in terms of healthcare deployment scenarios and threat models, and has a disconnected focus with the biomedical research community. This hinders a comprehensive understanding of the risks that healthcare AI entails. To address this gap, this paper takes a thorough examination of existing healthcare AI S&amp;P research, providing a unified framework that allows the identification of under-explored areas. Our survey presents a systematic overview of healthcare AI attacks and defenses, and points out challenges and research opportunities for each AI-driven healthcare application domain. Through our experimental analysis of different threat models and feasibility studies on under-explored adversarial attacks, we provide compelling insights into the pressing need for cybersecurity research in the rapidly evolving field of healthcare AI.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LT3SD: Latent Trees for 3D Scene Diffusion</title>
<link>https://arxiv.org/abs/2409.08215</link>
<guid>https://arxiv.org/abs/2409.08215</guid>
<content:encoded><![CDATA[
arXiv:2409.08215v2 Announce Type: replace-cross 
Abstract: We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaeBench: Improving Quality of Toxic Adversarial Examples</title>
<link>https://arxiv.org/abs/2410.05573</link>
<guid>https://arxiv.org/abs/2410.05573</guid>
<content:encoded><![CDATA[
arXiv:2410.05573v2 Announce Type: replace-cross 
Abstract: Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Augmented Data Enhances Direct Preference Alignment of LLMs</title>
<link>https://arxiv.org/abs/2410.08067</link>
<guid>https://arxiv.org/abs/2410.08067</guid>
<content:encoded><![CDATA[
arXiv:2410.08067v4 Announce Type: replace-cross 
Abstract: Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Deep Learning for Time Series Forecasting: Architectural Diversity and Open Challenges</title>
<link>https://arxiv.org/abs/2411.05793</link>
<guid>https://arxiv.org/abs/2411.05793</guid>
<content:encoded><![CDATA[
arXiv:2411.05793v3 Announce Type: replace-cross 
Abstract: Time series forecasting is a critical task that provides key information for decision-making. After traditional statistical and machine learning approaches, various fundamental deep learning architectures such as MLPs, CNNs, RNNs, and GNNs have been developed. However, the structural limitations caused by the inductive biases of each deep learning architecture constrained their performance. Transformer models, which excel at handling long-term dependencies, have become significant architectural components for time series forecasting. However, recent research has shown that alternatives such as simple linear layers can outperform Transformers. These findings have opened up new possibilities for using diverse architectures, ranging from fundamental deep learning models to emerging architectures and hybrid approaches. In this context, architectural modeling of time series forecasting has now entered a renaissance. This survey not only provides a historical context for time series forecasting but also offers comprehensive and timely analysis of the movement toward architectural diversification. By comparing and re-examining deep learning models, we uncover new perspectives and present recent trends, including hybrid, diffusion, Mamba, and foundation models. By focusing on the inherent characteristics of time series data, we also address open challenges that have gained attention in time series forecasting, such as channel dependency, distribution shift, causality, and feature extraction. These contributions help lower entry barriers for newcomers by providing a systematic understanding of the diverse research areas in time series forecasting (TSF), while offering seasoned researchers broader perspectives and new opportunities through in-depth exploration of TSF challenges. (Shortened due to arXiv's 1,920-character limit. Full version in the paper.)
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Myopic Multi-Objective Bayesian Optimization</title>
<link>https://arxiv.org/abs/2412.08085</link>
<guid>https://arxiv.org/abs/2412.08085</guid>
<content:encoded><![CDATA[
arXiv:2412.08085v2 Announce Type: replace-cross 
Abstract: We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner</title>
<link>https://arxiv.org/abs/2412.18086</link>
<guid>https://arxiv.org/abs/2412.18086</guid>
<content:encoded><![CDATA[
arXiv:2412.18086v2 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at https://ezharjan.github.io/AutoSceneGen.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Calibration via Conformalized Credal Inference</title>
<link>https://arxiv.org/abs/2501.06066</link>
<guid>https://arxiv.org/abs/2501.06066</guid>
<content:encoded><![CDATA[
arXiv:2501.06066v3 Announce Type: replace-cross 
Abstract: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods</title>
<link>https://arxiv.org/abs/2501.13947</link>
<guid>https://arxiv.org/abs/2501.13947</guid>
<content:encoded><![CDATA[
arXiv:2501.13947v3 Announce Type: replace-cross 
Abstract: The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boli: A dataset for understanding stuttering experience and analyzing stuttered speech</title>
<link>https://arxiv.org/abs/2501.15877</link>
<guid>https://arxiv.org/abs/2501.15877</guid>
<content:encoded><![CDATA[
arXiv:2501.15877v3 Announce Type: replace-cross 
Abstract: There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions. We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data. The dataset is released as an open access to further speech technology development.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations</title>
<link>https://arxiv.org/abs/2501.16370</link>
<guid>https://arxiv.org/abs/2501.16370</guid>
<content:encoded><![CDATA[
arXiv:2501.16370v2 Announce Type: replace-cross 
Abstract: In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, fractional types, and Helmholtz-type integral equations involving oscillatory kernels. RISN integrates residual connections with high-accuracy numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms not only classical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and Self-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. These results highlight RISN's robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Belief Roadmaps with Uncertain Landmark Evanescence</title>
<link>https://arxiv.org/abs/2501.17982</link>
<guid>https://arxiv.org/abs/2501.17982</guid>
<content:encoded><![CDATA[
arXiv:2501.17982v2 Announce Type: replace-cross 
Abstract: We would like a robot to navigate to a goal location while minimizing state uncertainty. To aid the robot in this endeavor, maps provide a prior belief over the location of objects and regions of interest. To localize itself within the map, a robot identifies mapped landmarks using its sensors. However, as the time between map creation and robot deployment increases, portions of the map can become stale, and landmarks, once believed to be permanent, may disappear. We refer to the propensity of a landmark to disappear as landmark evanescence. Reasoning about landmark evanescence during path planning, and the associated impact on localization accuracy, requires analyzing the presence or absence of each landmark, leading to an exponential number of possible outcomes of a given motion plan. To address this complexity, we develop BRULE, an extension of the Belief Roadmap. During planning, we replace the belief over future robot poses with a Gaussian mixture which is able to capture the effects of landmark evanescence. Furthermore, we show that belief updates can be made efficient, and that maintaining a random subset of mixture components is sufficient to find high quality solutions. We demonstrate performance in simulated and real-world experiments. Software is available at https://bit.ly/BRULE.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization</title>
<link>https://arxiv.org/abs/2501.18768</link>
<guid>https://arxiv.org/abs/2501.18768</guid>
<content:encoded><![CDATA[
arXiv:2501.18768v2 Announce Type: replace-cross 
Abstract: The goal of offline model-based optimization (MBO) is to propose new designs that maximize a reward function given only an offline dataset. However, an important desiderata is to also propose a diverse set of final candidates that capture many optimal and near-optimal design configurations. We propose Diversity in Adversarial Model-based Optimization (DynAMO) as a novel method to introduce design diversity as an explicit objective into any MBO problem. Our key insight is to formulate diversity as a distribution matching problem where the distribution of generated designs captures the inherent diversity contained within the offline dataset. Extensive experiments spanning multiple scientific domains show that DynAMO can be used with common optimization methods to significantly improve the diversity of proposed designs while still discovering high-quality candidates.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Power Grid Topology Control</title>
<link>https://arxiv.org/abs/2502.00040</link>
<guid>https://arxiv.org/abs/2502.00040</guid>
<content:encoded><![CDATA[
arXiv:2502.00040v2 Announce Type: replace-cross 
Abstract: Transmission grid congestion increases as the electrification of various sectors requires transmitting more power. Topology control, through substation reconfiguration, can reduce congestion but its potential remains under-exploited in operations. A challenge is modeling the topology control problem to align well with the objectives and constraints of operators. Addressing this challenge, this paper investigates the application of multi-objective reinforcement learning (MORL) to integrate multiple conflicting objectives for power grid topology control. We develop a MORL approach using deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate a set of Pareto-optimal policies that balance objectives such as minimizing line loading, topological deviation, and switching frequency. Initial case studies show that the MORL approach can provide valuable insights into objective trade-offs and improve Pareto front approximation compared to a random search baseline. The generated multi-objective RL policies are 30% more successful in preventing grid failure under contingencies and 20% more effective when training budget is reduced - compared to the common single objective RL policy.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05945</link>
<guid>https://arxiv.org/abs/2502.05945</guid>
<content:encoded><![CDATA[
arXiv:2502.05945v2 Announce Type: replace-cross 
Abstract: Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?</title>
<link>https://arxiv.org/abs/2502.07963</link>
<guid>https://arxiv.org/abs/2502.07963</guid>
<content:encoded><![CDATA[
arXiv:2502.07963v2 Announce Type: replace-cross 
Abstract: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction</title>
<link>https://arxiv.org/abs/2502.10689</link>
<guid>https://arxiv.org/abs/2502.10689</guid>
<content:encoded><![CDATA[
arXiv:2502.10689v2 Announce Type: replace-cross 
Abstract: The burgeoning volume of electronic health records (EHRs) has enabled deep learning models to excel in predictive healthcare. However, for high-stakes applications such as diagnosis prediction, model interpretability remains paramount. Existing deep learning diagnosis prediction models with intrinsic interpretability often assign attention weights to every past diagnosis or hospital visit, providing explanations lacking flexibility and succinctness. In this paper, we introduce SHy, a self-explaining hypergraph neural network model, designed to offer personalized, concise and faithful explanations that allow for interventions from clinical experts. By modeling each patient as a unique hypergraph and employing a message-passing mechanism, SHy captures higher-order disease interactions and extracts distinct temporal phenotypes as personalized explanations. It also addresses the incompleteness of the EHR data by accounting for essential false negatives in the original diagnosis record. A qualitative case study and extensive quantitative evaluations on two real-world EHR datasets demonstrate the superior predictive performance and interpretability of SHy over existing state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Neural Architecture Search Reveals Hierarchical Entailment</title>
<link>https://arxiv.org/abs/2502.11141</link>
<guid>https://arxiv.org/abs/2502.11141</guid>
<content:encoded><![CDATA[
arXiv:2502.11141v2 Announce Type: replace-cross 
Abstract: Recent research has suggested that the brain is more shallow than previously thought, challenging the traditionally assumed hierarchical structure of the ventral visual pathway. Here, we demonstrate that optimizing convolutional network architectures for brain-alignment via evolutionary neural architecture search results in models with clear representational hierarchies. Despite having random weights, the identified models achieve brain-alignment scores surpassing even those of pretrained classification models - as measured by both regression and representational similarity analysis. Furthermore, through traditional supervised training, architectures optimized for alignment with late ventral regions become competitive classification models. These findings suggest that hierarchical structure is a fundamental mechanism of primate visual processing. Finally, this work demonstrates the potential of neural architecture search as a framework for computational cognitive neuroscience research that could reduce the field's reliance on manually designed convolutional networks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Predictive Control: Flow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks</title>
<link>https://arxiv.org/abs/2502.13406</link>
<guid>https://arxiv.org/abs/2502.13406</guid>
<content:encoded><![CDATA[
arXiv:2502.13406v2 Announce Type: replace-cross 
Abstract: Generative control policies have recently unlocked major progress in robotics. These methods produce action sequences via diffusion or flow matching, with training data provided by demonstrations. But existing methods come with two key limitations: they require expert demonstrations, which can be difficult to obtain, and they are limited to relatively slow, quasi-static tasks. In this paper, we leverage a tight connection between sampling-based predictive control and generative modeling to address each of these issues. In particular, we introduce generative predictive control, a supervised learning framework for tasks with fast dynamics that are easy to simulate but difficult to demonstrate. We then show how trained flow-matching policies can be warm-started at inference time, maintaining temporal consistency and enabling high-frequency feedback. We believe that generative predictive control offers a complementary approach to existing behavior cloning methods, and hope that it paves the way toward generalist policies that extend beyond quasi-static demonstration-oriented tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
arXiv:2502.18137v2 Announce Type: replace-cross 
Abstract: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2502.20984</link>
<guid>https://arxiv.org/abs/2502.20984</guid>
<content:encoded><![CDATA[
arXiv:2502.20984v3 Announce Type: replace-cross 
Abstract: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cross-regional review of AI safety regulations in the commercial aviation</title>
<link>https://arxiv.org/abs/2503.04767</link>
<guid>https://arxiv.org/abs/2503.04767</guid>
<content:encoded><![CDATA[
arXiv:2503.04767v2 Announce Type: replace-cross 
Abstract: In this paper we examine the existing artificial intelligence (AI) policy documents in aviation for the following three regions: the United States, European Union, and China. The aviation industry has always been a first mover in adopting technological advancements. This early adoption offers valuable insights because of its stringent regulations and safety-critical procedures. As a result, the aviation industry provides an optimal platform to counter AI vulnerabilities through its tight regulations, standardization processes, and certification of new technologies. Keywords: AI in aviation; aviation safety; standardization; certifiable AI; regulations
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents</title>
<link>https://arxiv.org/abs/2503.16248</link>
<guid>https://arxiv.org/abs/2503.16248</guid>
<content:encoded><![CDATA[
arXiv:2503.16248v2 Announce Type: replace-cross 
Abstract: The integration of AI agents with Web3 ecosystems harnesses their complementary potential for autonomy and openness yet also introduces underexplored security risks, as these agents dynamically interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation, a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds.
  Through empirical analysis of ElizaOS, a decentralized AI agent framework for automated Web3 operations, we demonstrate how adversaries can manipulate context by injecting malicious instructions into prompts or historical interaction records, leading to unintended asset transfers and protocol violations which could be financially devastating.
  To quantify these vulnerabilities, we design CrAIBench, a Web3 domain-specific benchmark that evaluates the robustness of AI agents against context manipulation attacks across 150+ realistic blockchain tasks, including token transfers, trading, bridges and cross-chain interactions and 500+ attack test cases using context manipulation. We systematically assess attack and defense strategies, analyzing factors like the influence of security prompts, reasoning models, and the effectiveness of alignment techniques.
  Our findings show that prompt-based defenses are insufficient when adversaries corrupt stored context, achieving significant attack success rates despite these defenses. Fine-tuning-based defenses offer a more robust alternative, substantially reducing attack success rates while preserving utility on single-step tasks. This research highlights the urgent need to develop AI agents that are both secure and fiduciarily responsible.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture</title>
<link>https://arxiv.org/abs/2503.19339</link>
<guid>https://arxiv.org/abs/2503.19339</guid>
<content:encoded><![CDATA[
arXiv:2503.19339v3 Announce Type: replace-cross 
Abstract: The ever-increasing security vulnerabilities in the Internet-of-Things (IoT) systems require improved threat detection approaches. This paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis, temporal support learning, and focused feature extraction. The proposed attention-based model benefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification accuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while maintaining high precision and recall across various scenarios. The proposed model's performance is further validated by key parameters, such as Mathews Correlation Coefficient and Cohen's kappa Correlation Coefficient. The close-to-ideal results for these parameters demonstrate the proposed model's ability to detect botnet attacks accurately and efficiently in practical settings and on unseen data. The proposed model proved to be a powerful defence mechanism for IoT networks to face emerging security challenges.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement</title>
<link>https://arxiv.org/abs/2503.23895</link>
<guid>https://arxiv.org/abs/2503.23895</guid>
<content:encoded><![CDATA[
arXiv:2503.23895v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opioid Named Entity Recognition (ONER-2025) from Reddit</title>
<link>https://arxiv.org/abs/2504.00027</link>
<guid>https://arxiv.org/abs/2504.00027</guid>
<content:encoded><![CDATA[
arXiv:2504.00027v3 Announce Type: replace-cross 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning</title>
<link>https://arxiv.org/abs/2504.02546</link>
<guid>https://arxiv.org/abs/2504.02546</guid>
<content:encoded><![CDATA[
arXiv:2504.02546v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model</title>
<link>https://arxiv.org/abs/2504.14560</link>
<guid>https://arxiv.org/abs/2504.14560</guid>
<content:encoded><![CDATA[
arXiv:2504.14560v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75\% while preserving performance. Experimental results demonstrate ReasoningV's effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5\%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at https://github.com/BUAA-CLab/ReasoningV.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence</title>
<link>https://arxiv.org/abs/2504.14625</link>
<guid>https://arxiv.org/abs/2504.14625</guid>
<content:encoded><![CDATA[
arXiv:2504.14625v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\%--1075\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Testing and Adapting REST APIs as LLM Tools</title>
<link>https://arxiv.org/abs/2504.15546</link>
<guid>https://arxiv.org/abs/2504.15546</guid>
<content:encoded><![CDATA[
arXiv:2504.15546v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Human-AI Coordination through Adversarial Training and Generative Models</title>
<link>https://arxiv.org/abs/2504.15457</link>
<guid>https://arxiv.org/abs/2504.15457</guid>
<content:encoded><![CDATA[
<div> Keywords: generative model, adversarial training, cooperative agents, coordination strategies, generalization <br />
<br />
Summary: <br />
Being able to cooperate with new individuals is crucial for various AI applications, such as household robotics and autonomous driving. However, achieving this requires training on diverse human behavior data. Adversarial training can help in creating robust agents, but it is challenging in cooperative settings as adversarial policies may sabotage tasks. To address this issue, the proposed GOAT method combines a pre-trained generative model with adversarial training to maximize regret and overcome self-sabotage. This framework dynamically generates coordination strategies to expose the learning policy to challenging interaction scenarios without adversarial exploitation. By updating only the generative model's embedding, GOAT maintains realistic coordination strategies and demonstrates superior generalization to diverse human behaviors, as evidenced by its performance on the Overcooked benchmark with real human partners. <div>
arXiv:2504.15457v2 Announce Type: replace 
Abstract: Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is one avenue for searching for such data and ensuring that agents are robust. However, it is difficult to apply in the cooperative setting because adversarial policies intentionally learn to sabotage the task instead of simulating valid cooperation partners. To address this challenge, we propose a novel strategy for overcoming self-sabotage that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches for and generates coordination strategies where the learning policy -- the Cooperator agent -- underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by updating only the generative model's embedding while keeping its parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Formalism for Optimal Search with Dynamic Heuristics</title>
<link>https://arxiv.org/abs/2504.21131</link>
<guid>https://arxiv.org/abs/2504.21131</guid>
<content:encoded><![CDATA[
<div> dynamic heuristics, heuristic search, A* algorithm, optimality results, mutable heuristic

Summary: This paper introduces the concept of dynamic heuristics in heuristic search, where heuristics depend on both the state and search history. A generic algorithm framework is proposed to incorporate dynamic heuristics, with a focus on modeling A* with dynamic heuristics. The study includes general optimality results for this instantiation. Existing approaches from classical planning are shown to be special cases of this framework, allowing for the direct application of the optimality results. By formalizing dynamic heuristics and incorporating them into heuristic search algorithms, this research provides a comprehensive understanding of searching with mutable heuristics, offering insights into the complexities and optimality of such approaches. <div>
arXiv:2504.21131v1 Announce Type: new 
Abstract: While most heuristics studied in heuristic search depend only on the state, some accumulate information during search and thus also depend on the search history. Various existing approaches use such dynamic heuristics in $\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$ to show optimality. However, doing so ignores the complexities of searching with a mutable heuristic. In this paper we formalize the idea of dynamic heuristics and use them in a generic algorithm framework. We study a particular instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show general optimality results. Finally we show how existing approaches from classical planning can be viewed as special cases of this instantiation, making it possible to directly apply our optimality results.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AffectEval: A Modular and Customizable Framework for Affective Computing</title>
<link>https://arxiv.org/abs/2504.21184</link>
<guid>https://arxiv.org/abs/2504.21184</guid>
<content:encoded><![CDATA[
<div> affective computing, emotion recognition, software framework, AffectEval, modular

Summary:<br /><br />
The article introduces AffectEval, a customizable framework designed to streamline the development of affective computing pipelines by reducing manual effort and redundant work. AffectEval aims to support multimodal and multi-domain emotion recognition applications, addressing the labor-intensive nature of current pipeline development. Through replicating prior experiments in affective computing, the framework has been validated to significantly reduce programming effort by up to 90%, as indicated by a reduction in raw lines of code. AffectEval offers modularity and customization, allowing for more efficient creation of pipelines for various applications in fields such as education, child development, and healthcare. This advancement in affective computing technology has the potential to enhance emotion recognition capabilities and facilitate the integration of emotional intelligence in a variety of practical settings. <div>
arXiv:2504.21184v1 Announce Type: new 
Abstract: The field of affective computing focuses on recognizing, interpreting, and responding to human emotions, and has broad applications across education, child development, and human health and wellness. However, developing affective computing pipelines remains labor-intensive due to the lack of software frameworks that support multimodal, multi-domain emotion recognition applications. This often results in redundant effort when building pipelines for different applications. While recent frameworks attempt to address these challenges, they remain limited in reducing manual effort and ensuring cross-domain generalizability. We introduce AffectEval, a modular and customizable framework to facilitate the development of affective computing pipelines while reducing the manual effort and duplicate work involved in developing such pipelines. We validate AffectEval by replicating prior affective computing experiments, and we demonstrate that our framework reduces programming effort by up to 90%, as measured by the reduction in raw lines of code.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations for Semantic Cognition in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.21218</link>
<guid>https://arxiv.org/abs/2504.21218</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive architecture, artificial intelligence, belief states, self-regulating agents, structured semantic state

Summary: This article introduces a modular cognitive architecture for artificial intelligence based on formal belief modeling. Belief states are dynamic ensembles of linguistic expressions within a navigable space, allowing for operations such as assimilation, abstraction, memory, and introspection. The framework draws from philosophy, cognitive science, and neuroscience to create self-regulating epistemic agents capable of reflective, goal-directed thought. Central to this architecture is the concept of the epistemic vacuum, a class of cognitive states that serves as the foundation of belief space. The Null Tower is a generative structure built recursively through internal representations. These theoretical constructs can be implemented in both symbolic and neural systems, including large language models and adaptive memory architectures. This work provides a basis for developing agents that can reason, remember, and regulate their beliefs in structured and interpretable ways. 

<br /><br />Summary: <div>
arXiv:2504.21218v1 Announce Type: new 
Abstract: This monograph presents a modular cognitive architecture for artificial intelligence grounded in the formal modeling of belief as structured semantic state. Belief states are defined as dynamic ensembles of linguistic expressions embedded within a navigable manifold, where operators enable assimilation, abstraction, nullification, memory, and introspection. Drawing from philosophy, cognitive science, and neuroscience, we develop a layered framework that enables self-regulating epistemic agents capable of reflective, goal-directed thought. At the core of this framework is the epistemic vacuum: a class of semantically inert cognitive states that serves as the conceptual origin of belief space. From this foundation, the Null Tower arises as a generative structure recursively built through internal representational capacities. The theoretical constructs are designed to be implementable in both symbolic and neural systems, including large language models, hybrid agents, and adaptive memory architectures. This work offers a foundational substrate for constructing agents that reason, remember, and regulate their beliefs in structured, interpretable ways.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.21277</link>
<guid>https://arxiv.org/abs/2504.21277</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Multimodal Large Language Models, Reasoning, Reward Mechanism, Benchmark Datasets

Summary: 
Reinforcement Learning (RL) integration into Multimodal Large Language Models (MLLMs) is a rapidly growing area of research. MLLMs expand Large Language Models (LLMs) to handle various modalities like vision and audio, yet multimodal reasoning remains a challenge. This survey reviews recent advances in RL-based reasoning for MLLMs, covering algorithmic designs, reward mechanisms, and applications. Two main RL paradigms - value-free and value-based methods - enhance reasoning by optimizing trajectories and aligning multimodal information. The review includes benchmark datasets, evaluation protocols, and limitations, while proposing future research directions to address sparse rewards, inefficient cross-modal reasoning, and deployment constraints. The aim is to provide a structured guide for researchers interested in advancing RL-based reasoning in the multimodal domain.<br /><br />Summary: <div>
arXiv:2504.21277v1 Announce Type: new 
Abstract: The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains a major challenge. This survey systematically reviews recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms--value-free and value-based methods--and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Furthermore, we provide an extensive overview of benchmark datasets, evaluation protocols, and existing limitations, and propose future research directions to address current bottlenecks such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to offer a comprehensive and structured guide to researchers interested in advancing RL-based reasoning in the multimodal era.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phi-4-reasoning Technical Report</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
<div> model, reasoning, Phi-4-reasoning, supervised fine-tuning, reinforcement learning

Summary:
Phi-4-reasoning is a 14-billion parameter reasoning model that excels in complex tasks through supervised fine-tuning on select prompts and reasoning demonstrations. The model, along with its enhanced variant Phi-4-reasoning-plus, surpasses larger open-weight models and nears the performance of the full DeepSeek-R1 model. Evaluations across various reasoning tasks show significant improvements, including in math, coding, planning, and spatial understanding. Notably, benefits from data curation and reinforcement learning are highlighted, with transferable enhancements observed in general-purpose benchmarks. The report emphasizes the importance of thoughtful data curation in supervised fine-tuning for reasoning models, and the added value of reinforcement learning. Opportunities for improving performance and robustness assessments of reasoning models are also identified. <div>
arXiv:2504.21318v1 Announce Type: new 
Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces</title>
<link>https://arxiv.org/abs/2504.21347</link>
<guid>https://arxiv.org/abs/2504.21347</guid>
<content:encoded><![CDATA[
<div> Keywords: IRL Ditto, embodied agent, remote collaboration, social relationships, workplace dynamics

Summary: 
The study introduces the In Real Life (IRL) Ditto, an AI-driven embodied agent aiming to represent remote colleagues in shared office spaces. The research question explores how IRL Ditto can influence interactions and relationships among colleagues in a shared office setting. Through a four-day study, the researchers assessed IRL Ditto's impact on strengthening social ties by simulating presence and facilitating meaningful interactions among colleagues of varying levels of social familiarity. The study findings highlight that the effectiveness of IRL Ditto in enhancing social relationships depended significantly on the existing foundation of the relationship participants had with the remote teammate represented by the agent. Overall, the research provides valuable insights into the role of embodied agents like IRL Ditto in improving workplace dynamics for distributed teams. 

<br /><br />Summary: <div>
arXiv:2504.21347v1 Announce Type: new 
Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent designed to represent remote colleagues in shared office spaces, creating opportunities for real-time exchanges even in their absence. IRL Ditto offers a unique hybrid experience by allowing in-person colleagues to encounter a digital version of their remote teammates, initiating greetings, updates, or small talk as they might in person. Our research question examines: How can the IRL Ditto influence interactions and relationships among colleagues in a shared office space? Through a four-day study, we assessed IRL Ditto's ability to strengthen social ties by simulating presence and enabling meaningful interactions across different levels of social familiarity. We find that enhancing social relationships depended deeply on the foundation of the relationship participants had with the source of the IRL Ditto. This study provides insights into the role of embodied agents in enriching workplace dynamics for distributed teams.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2504.21370</link>
<guid>https://arxiv.org/abs/2504.21370</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, reasoning models, Chain-of-Thought, optimal inference, compressible<br />
<br />
Summary:
ShorterBetter introduces a reinforcement learning method for reasoning language models to determine their optimal Chain-of-Thought (CoT) lengths. By defining the Sample Optimal Length (SOL) as the shortest correct response among multiple outputs per problem, the model can dynamically adjust its inference length. This approach, applied to the DeepSeek-Distill-Qwen-1.5B model, successfully reduces output length by up to 80% on reasoning tasks while maintaining accuracy. The study emphasizes that overly long reasoning traces often indicate a loss of direction in reasoning, suggesting that extended CoT can be highly compressible. This method enables models to discover their own efficient inference lengths without human intervention, improving performance on complex problems. <br /><br />Summary: <div>
arXiv:2504.21370v1 Announce Type: new 
Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often "overthink", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence</title>
<link>https://arxiv.org/abs/2504.21433</link>
<guid>https://arxiv.org/abs/2504.21433</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agent, integration, artificial general intelligence, versatility, adaptability

Summary: 
The paper argues for the development of the next generation of AI agents that can integrate various domain abilities to progress towards Artificial General Intelligence (AGI). Current AI agents excel in specific tasks but are limited to narrow domains. The authors propose that future AI agents should combine the strengths of specialized systems to operate across text, vision, robotics, reinforcement learning, emotional intelligence, and more. This integration is crucial for achieving human-like versatility and adaptability. Technological advancements and user demand for cross-domain capabilities indicate that such integration is feasible. Developing versatile agents is a critical step towards realizing AGI. The paper explores the reasoning behind this shift and suggests potential pathways for achieving it. <div>
arXiv:2504.21433v1 Announce Type: new 
Abstract: This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks</title>
<link>https://arxiv.org/abs/2504.21568</link>
<guid>https://arxiv.org/abs/2504.21568</guid>
<content:encoded><![CDATA[
<div> Keywords: group decision-making, fuzzy inference, Bayesian network, student evaluation, multi-objective attributes <br />
Summary:
This study proposes a new system for group decision-making that integrates fuzzy inference and Bayesian network. A fuzzy rule base is constructed to address quantitative challenges, utilizing threshold values, membership functions, expert experience, and domain knowledge. A hierarchical Bayesian network is designed to model nonlinear correlations among multidimensional indices and optimize the conditional probability table with maximum likelihood estimation. The method is compared with traditional weighted scoring in a student evaluation case, showing effectiveness in rule criterion construction and ranking consistency with a classification accuracy of 86.0% and F1 value improvement of 53.4%. Computational experiments on real-world datasets demonstrate the method's performance and robustness in various group decision scenarios, highlighting its reliability in diverse contexts. <br /><br />Summary: <div>
arXiv:2504.21568v1 Announce Type: new 
Abstract: Aiming at the group decision - making problem with multi - objective attributes, this study proposes a group decision - making system that integrates fuzzy inference and Bayesian network. A fuzzy rule base is constructed by combining threshold values, membership functions, expert experience, and domain knowledge to address quantitative challenges such as scale differences and expert linguistic variables. A hierarchical Bayesian network is designed, featuring a directed acyclic graph with nodes selected by experts, and maximum likelihood estimation is used to dynamically optimize the conditional probability table, modeling the nonlinear correlations among multidimensional indices for posterior probability aggregation. In a comprehensive student evaluation case, this method is compared with the traditional weighted scoring approach. The results indicate that the proposed method demonstrates effectiveness in both rule criterion construction and ranking consistency, with a classification accuracy of 86.0% and an F1 value improvement of 53.4% over the traditional method. Additionally, computational experiments on real - world datasets across various group decision scenarios assess the method's performance and robustness, providing evidence of its reliability in diverse contexts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation</title>
<link>https://arxiv.org/abs/2504.21643</link>
<guid>https://arxiv.org/abs/2504.21643</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous navigation, hierarchical control framework, neural network verification, control barrier functions, reinforcement learning

Summary:
This paper introduces a hierarchical control framework that utilizes neural network verification techniques to ensure safe autonomous navigation systems. The framework incorporates control barrier functions (CBFs) and policy correction mechanisms to guarantee safety while allowing for efficient navigation in dynamic environments. By leveraging probabilistic enumeration to identify unsafe regions, the framework constructs a CBF-based control layer that can be applied to various navigation policies. Validation of the system was conducted through simulation and real-world experiments involving a mobile robot benchmark and aquatic environmental monitoring task. The results demonstrate the framework's ability to correct unsafe actions while maintaining effective navigation behavior. This approach shows promise in developing verification-based systems that enable safe and robust navigation in complex scenarios.<br /><br />Summary: <div>
arXiv:2504.21643v1 Announce Type: new 
Abstract: Achieving safe autonomous navigation systems is critical for deploying robots in dynamic and uncertain real-world environments. In this paper, we propose a hierarchical control framework leveraging neural network verification techniques to design control barrier functions (CBFs) and policy correction mechanisms that ensure safe reinforcement learning navigation policies. Our approach relies on probabilistic enumeration to identify unsafe regions of operation, which are then used to construct a safe CBF-based control layer applicable to arbitrary policies. We validate our framework both in simulation and on a real robot, using a standard mobile robot benchmark and a highly dynamic aquatic environmental monitoring task. These experiments demonstrate the ability of the proposed solution to correct unsafe actions while preserving efficient navigation behavior. Our results show the promise of developing hierarchical verification-based systems to enable safe and robust navigation behaviors in complex scenarios.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, adaptive strategies, efficient reasoning, Long-CoT, hybrid reasoning model

Summary: 
This study introduces a two-stage framework for adaptive and efficient reasoning to address the varying performance of long-coherence reasoning models. By merging long and short CoT models, the framework allows for diverse reasoning styles. Bi-level preference training guides the model in selecting suitable reasoning styles at the group-level and emphasizes concise and correct reasoning at the instance-level. Experimental results show a significant reduction in inference costs while maintaining performance. The proposed method demonstrates a reduction of over 50% in the average length of reasoning on mathematical datasets. The findings highlight the potential of adaptive strategies in optimizing reasoning efficiency in large language models. The code for the framework will be available soon on GitHub at https://github.com/StarDewXXX/AdaR1

Summary:<br /><br />Keywords: reasoning models, adaptive strategies, efficient reasoning, Long-CoT, hybrid reasoning model
This study introduces a two-stage framework for adaptive and efficient reasoning to address the varying performance of long-coherence reasoning models. By merging long and short CoT models, the framework allows for diverse reasoning styles. Bi-level preference training guides the model in selecting suitable reasoning styles at the group-level and emphasizes concise and correct reasoning at the instance-level. Experimental results show a significant reduction in inference costs while maintaining performance. The proposed method demonstrates a reduction of over 50% in the average length of reasoning on mathematical datasets. The findings highlight the potential of adaptive strategies in optimizing reasoning efficiency in large language models. The code for the framework will be available soon on GitHub at https://github.com/StarDewXXX/AdaR1 <div>
arXiv:2504.21659v1 Announce Type: new 
Abstract: Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extension-ranking Semantics for Abstract Argumentation Preprint</title>
<link>https://arxiv.org/abs/2504.21683</link>
<guid>https://arxiv.org/abs/2504.21683</guid>
<content:encoded><![CDATA[
<div> Keywords: abstract argumentation, ranking, extension semantics, plausibility, acceptance

Summary:
Extension-ranking semantics is proposed as a framework for ranking sets of arguments in abstract argumentation based on their plausibility of acceptance. This approach generalizes Dung's extension semantics by introducing a preorder over the power set of arguments. To evaluate extension-ranking semantics, principles are established to ensure well-behaved semantics. Simple base relations representing different aspects of argumentative reasoning are combined to create a family of extension-ranking semantics. Various approaches for ranking extensions are adapted for use in this context. The study evaluates the behavior of these adapted ranking methods. The proposed framework offers a systematic way to rank arguments in abstract argumentation, providing a structured approach to evaluating the acceptance of sets of arguments. <div>
arXiv:2504.21683v1 Announce Type: new 
Abstract: In this paper, we present a general framework for ranking sets of arguments in abstract argumentation based on their plausibility of acceptance. We present a generalisation of Dung's extension semantics as extension-ranking semantics, which induce a preorder over the power set of all arguments, allowing us to state that one set is "closer" to being acceptable than another. To evaluate the extension-ranking semantics, we introduce a number of principles that a well-behaved extension-ranking semantics should satisfy. We consider several simple base relations, each of which models a single central aspect of argumentative reasoning. The combination of these base relations provides us with a family of extension-ranking semantics. We also adapt a number of approaches from the literature for ranking extensions to be usable in the context of extension-ranking semantics, and evaluate their behaviour.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation</title>
<link>https://arxiv.org/abs/2504.21694</link>
<guid>https://arxiv.org/abs/2504.21694</guid>
<content:encoded><![CDATA[
<div> Keywords: AutomationML, ontology, RDF, OWL, industrial knowledge graphs

Summary:
This article discusses the challenges faced when using AutomationML, an XML-based data exchange format in the automation domain, and proposes a solution through ontology and RDF mapping. The article provides practitioners with an up-to-date ontology of AutomationML concepts and a declarative mapping to transform AutomationML models into RDF triples. By converting AutomationML to OWL, practitioners can integrate AutomationML information into industrial knowledge graphs more efficiently. The study shows that transforming AutomationML to OWL enables powerful querying and validation capabilities that are not possible with the original format. This approach enhances the applicability of AutomationML data in various applications, making it easier for practitioners to work with the information effectively. <br /><br />Summary: <div>
arXiv:2504.21694v1 Announce Type: new 
Abstract: AutomationML has seen widespread adoption as an open data exchange format in the automation domain. It is an open and vendor neutral standard based on the extensible markup language XML. However, AutomationML extends XML with additional semantics, that limit the applicability of common XML-tools for applications like querying or data validation. This article provides practitioners with 1) an up-to-date ontology of the concepts in the AutomationML-standard, as well as 2) a declarative mapping to automatically transform any AutomationML model into RDF triples. Together, these artifacts allow practitioners an easy integration of AutomationML information into industrial knowledge graphs. A study on examples from the automation domain concludes that transforming AutomationML to OWL opens up new powerful ways for querying and validation that are impossible without transformation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?</title>
<link>https://arxiv.org/abs/2504.21774</link>
<guid>https://arxiv.org/abs/2504.21774</guid>
<content:encoded><![CDATA[
<div> Collaborative perception, communication-efficient framework, late-intermediate fusion, Unmanned Aerial Vehicles (UAVs), vision-guided positional embedding (VPE)<br />
<br />
Summary: 
The article introduces a novel communication-efficient collaborative perception framework for Unmanned Aerial Vehicles (UAVs) called LIF. LIF leverages late-intermediate fusion, VPE, and BoBEV to integrate information from multiple agents at the feature representation level. It introduces an uncertainty-driven communication mechanism that selects high-quality shared areas based on uncertainty evaluation. The experimental results demonstrate that LIF outperforms existing methods with minimal communication bandwidth, showcasing its effectiveness and practicality. The code and models are publicly available for further research and development. <div>
arXiv:2504.21774v1 Announce Type: new 
Abstract: Collaborative perception enhances environmental awareness through inter-agent communication and is regarded as a promising solution to intelligent transportation systems. However, existing collaborative methods for Unmanned Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV perspective, resulting in substantial communication overhead. To address this issue, we propose a novel communication-efficient collaborative perception framework based on late-intermediate fusion, dubbed LIF. The core concept is to exchange informative and compact detection results and shift the fusion stage to the feature representation level. In particular, we leverage vision-guided positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to effectively integrate complementary information from various agents. Additionally, we innovatively introduce an uncertainty-driven communication mechanism that uses uncertainty evaluation to select high-quality and reliable shared areas. Experimental results demonstrate that our LIF achieves superior performance with minimal communication bandwidth, proving its effectiveness and practicality. Code and models are available at https://github.com/uestchjw/LIF.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore</title>
<link>https://arxiv.org/abs/2504.21008</link>
<guid>https://arxiv.org/abs/2504.21008</guid>
<content:encoded><![CDATA[
<div> IoT, IIoT, network architecture, network traffic, security mechanisms <br />
<br />
Summary: 
The study introduces a novel network traffic anomaly detection model that combines a Convolutional Neural Network with a Bidirectional Long Short-Term Memory network, implemented on the MindSpore framework. The increasing complexity of network architectures due to IoT and IIoT technologies has led to a substantial growth in traffic volume, posing challenges for traditional security mechanisms. The proposed model was tested on the NF-BoT-IoT dataset and achieved a high accuracy of 99% across various metrics including precision, recall, and F1-score. This indicates the model's effectiveness and robustness in detecting diverse and highly covert network attacks. <div>
arXiv:2504.21008v1 Announce Type: cross 
Abstract: With the widespread adoption of the Internet of Things (IoT) and Industrial IoT (IIoT) technologies, network architectures have become increasingly complex, and the volume of traffic has grown substantially. This evolution poses significant challenges to traditional security mechanisms, particularly in detecting high-frequency, diverse, and highly covert network attacks. To address these challenges, this study proposes a novel network traffic anomaly detection model that integrates a Convolutional Neural Network (CNN) with a Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the MindSpore framework. Comprehensive experiments were conducted using the NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves 99% across accuracy, precision, recall, and F1-score, indicating its strong performance and robustness in network intrusion detection tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models</title>
<link>https://arxiv.org/abs/2504.21012</link>
<guid>https://arxiv.org/abs/2504.21012</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive thinking, large language models, cognitive dynamics, Transition-Inducing Prompt, conceptual fusion<br />
Summary: 
The study investigates the cognitive dynamics of humans and large language models (LLMs) by comparing their responsiveness to prompts triggering rapid shifts. A two-part framework is proposed using Transition-Inducing Prompts (TIPs) and Transition Quantifying Prompts (TQPs) to analyze LLM behavior. Controlled experiments were conducted using prompts embedding semantically distant concepts, both fused together and presented separately, to observe changes in linguistic quality and affective tone in LLM responses. While humans show heightened engagement with meaningfully blended concepts, LLMs do not exhibit significant differences in responsiveness. This suggests LLMs may not replicate human conceptual integration processes, indicating potential differences in how intuition and conceptual leaps emerge in artificial versus human minds. The proposed method allows for precise measurement of cognitive responsiveness, shedding light on the distinctions between artificial and human thinking processes.
<br /><br />Summary: <div>
arXiv:2504.21012v1 Announce Type: cross 
Abstract: What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)--either fused together or presented separately--by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept--a form of conceptual fusion--current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge</title>
<link>https://arxiv.org/abs/2504.21013</link>
<guid>https://arxiv.org/abs/2504.21013</guid>
<content:encoded><![CDATA[
<div> AI-generated feedback, linguistic characteristics, readability, lexical richness, adaptability, educational settings<br />
<br />
Summary: This study examines AI-generated feedback for computer science MCQs, considering difficulty levels and feedback tones. Linguistic metrics like length, readability, vocabulary richness, and lexical density were analyzed. A RoBERTa-based model predicted these properties accurately. Interaction effects between feedback tone and question difficulty were significant, showing dynamic adaptation of AI feedback. Insights can enhance personalized feedback mechanisms for better learning outcomes, emphasizing ethical considerations in design and deployment.<br /><br /> <div>
arXiv:2504.21013v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has garnered considerable attention due to its potential to enhance learning outcomes. However, a comprehensive understanding of the linguistic characteristics of AI-generated feedback, including readability, lexical richness, and adaptability across varying challenge levels, remains limited. This study delves into the linguistic and structural attributes of feedback generated by Google's Gemini 1.5-flash text model for computer science multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed, considering three difficulty levels (easy, medium, hard) and three feedback tones (supportive, neutral, challenging). Key linguistic metrics, such as length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness, and lexical density, were computed and examined. A fine-tuned RoBERTa-based multi-task learning (MTL) model was trained to predict these linguistic properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and 0.03 for vocabulary richness. The findings reveal significant interaction effects between feedback tone and question difficulty, demonstrating the dynamic adaptation of AI-generated feedback within diverse educational contexts. These insights contribute to the development of more personalized and effective AI-driven feedback mechanisms, highlighting the potential for improved learning outcomes while underscoring the importance of ethical considerations in their design and deployment.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title>
<link>https://arxiv.org/abs/2504.21019</link>
<guid>https://arxiv.org/abs/2504.21019</guid>
<content:encoded><![CDATA[
<div> Keyword: large language models, AIGT detection, generalization, robustness, DP-Net

Summary:
The article addresses the concerns regarding the misuse of AI-generated text (AIGT) due to the rising popularity of large language models. It highlights the need for an effective AIGT detection method that combines both generalization and robustness. The authors propose DP-Net, a novel detection method that utilizes dynamic perturbations introduced through reinforcement learning to improve generalization and robustness. Experimental results demonstrate that DP-Net outperforms existing methods in terms of generalization capacity across three cross-domain scenarios and shows superior robustness against two text adversarial attacks. The study reveals an intrinsic mechanism for model generalization in AIGT detection tasks and emphasizes the significance of addressing robustness as a form of domain shift. The code for DP-Net is available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2504.21019v1 Announce Type: cross 
Abstract: The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Enhanced Contrastive Search for Improved LLM Text Generation</title>
<link>https://arxiv.org/abs/2504.21020</link>
<guid>https://arxiv.org/abs/2504.21020</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Natural Language Processing, Contrastive Search, Contextual Calibration, Text Generation<br />
<br />
Summary: 
The paper introduces a novel enhancement of Contrastive Search called Context-Enhanced Contrastive Search (CECS) with contextual calibration. CECS aims to improve the coherence, diversity, and relevance of generated text by incorporating dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control. Experimental results show that CECS outperforms traditional decoding methods in generating high-quality text. The proposed algorithm has various real-world applications, including legal document drafting, customer service chatbots, and content marketing. Key metrics such as BLEU, ROUGE, and semantic similarity were used to evaluate the performance of CECS, demonstrating significant improvements in the quality of generated texts. Overall, CECS offers a promising solution to the challenges faced by Large Language Models in text generation tasks. <br /><br />Summary: <div>
arXiv:2504.21020v1 Announce Type: cross 
Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable advancements in Natural Language Processing (NLP). However, generating high-quality text that balances coherence, diversity, and relevance remains challenging. Traditional decoding methods, such as bean search and top-k sampling, often struggle with either repetitive or incoherent outputs, particularly in tasks that require long-form text generation. To address these limitations, the paper proposes a novel enhancement of the well-known Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with contextual calibration. The proposed scheme introduces several novelties including dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control, to optimize the balance between fluency, creativity, and precision. The performance of CECS is evaluated using several standard metrics such as BLEU, ROUGE, and semantic similarity. Experimental results demonstrate significant improvements in both coherence and relevance of the generated texts by CECS outperforming the existing Contrastive Search techniques. The proposed algorithm has several potential applications in the real world including legal document drafting, customer service chatbots, and content marketing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees</title>
<link>https://arxiv.org/abs/2504.21022</link>
<guid>https://arxiv.org/abs/2504.21022</guid>
<content:encoded><![CDATA[
<div> Keywords: Linear Temporal Logic, Natural Language, ConformalNL2LTL, Question-Answering, Uncertainty-aware translation

Summary:
ConformalNL2LTL introduces a method for translating Natural Language instructions into Linear Temporal Logic formulas with user-defined success rates. The approach iteratively constructs LTL formulas by addressing open-vocabulary Question-Answering problems using Large Language Models (LLMs). By leveraging conformal prediction, the method can quantify uncertainty in LLM-generated answers, allowing it to proceed with translation only when confident and request assistance otherwise. Theoretical and empirical results demonstrate that ConformalNL2LTL achieves specified translation accuracy while minimizing the need for help. Overall, the method provides a robust solution for translating NL commands into LTL, offering correctness guarantees and enabling uncertainty-aware translation processes.<br /><br />Summary: <div>
arXiv:2504.21022v1 Announce Type: cross 
Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.21026</link>
<guid>https://arxiv.org/abs/2504.21026</guid>
<content:encoded><![CDATA[
<div> Code-mixed text, abusive language detection, Telugu-English, Nepali-English, dataset<br />
Summary: <br />
- Detecting abusive language in code-mixed text poses challenges due to linguistic blending and context dependency. 
- A manually annotated dataset of Telugu-English and Nepali-English code-mixed comments was created for abusive language detection. 
- Various Machine Learning and Deep Learning models were evaluated on the dataset, including Logistic Regression, Random Forest, SVM, NN, LSTM, CNN, and LLMs. 
- Hyperparameter tuning was performed to optimize model performance, and 10-fold cross-validation was used for evaluation. 
- The study provides insights into detecting abusive language in low-resource languages and establishes benchmarks for moderation strategies in multilingual social media environments. <br /> 
Summary: <div>
arXiv:2504.21026v1 Announce Type: cross 
Abstract: With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models</title>
<link>https://arxiv.org/abs/2504.21027</link>
<guid>https://arxiv.org/abs/2504.21027</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Urban Planning, Benchmark, Fine-tuning, Machine Intelligence<br />
Summary:<br />
The paper explores the potential for Large Language Models (LLMs) to assist human practitioners in urban planning, introducing the UrbanPlanBench benchmark for evaluating their efficacy. The study reveals that LLMs struggle to meet professional standards in acquiring planning knowledge, particularly in understanding planning regulations. A significant finding is that fine-tuning models on the UrbanPlanText dataset improves model performance in memorization tests and comprehension of urban planning knowledge. However, challenges remain in tasks requiring domain-specific terminology and reasoning. By making their benchmark, dataset, and toolsets publicly available, the researchers aim to facilitate the integration of LLMs into practical urban planning, promoting collaboration between human expertise and machine intelligence.<br /><br />Summary: <div>
arXiv:2504.21027v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one such field heavily relying on multifaceted domain knowledge and experience of human experts. The extent to which LLMs can assist human practitioners in urban planning remains largely unexplored. In this paper, we introduce a comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of LLMs in urban planning, which encompasses fundamental principles, professional knowledge, and management and regulations, aligning closely with the qualifications expected of human planners. Through extensive evaluation, we reveal a significant imbalance in the acquisition of planning knowledge among LLMs, with even the most proficient models falling short of meeting professional standards. For instance, we observe that 70% of LLMs achieve subpar performance in understanding planning regulations compared to other aspects. Besides the benchmark, we present the largest-ever supervised fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Our findings demonstrate that fine-tuned models exhibit enhanced performance in memorization tests and comprehension of urban planning knowledge, while there exists significant room for improvement, particularly in tasks requiring domain-specific terminology and reasoning. By making our benchmark, dataset, and associated evaluation and fine-tuning toolsets publicly available at https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the integration of LLMs into practical urban planning, fostering a symbiotic collaboration between human expertise and machine intelligence.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings</title>
<link>https://arxiv.org/abs/2504.21028</link>
<guid>https://arxiv.org/abs/2504.21028</guid>
<content:encoded><![CDATA[
<div> Keywords: Malware classification, Large Language Models, Fine-tuning, Contrastive learning, Few-shot learning

Summary: 
- The paper introduces a contrastive fine-tuning (CFT) method to enhance the classification of malware families by refining Large Language Models (LLMs) embeddings.
- CFT method selects hard negative samples based on cosine similarity, improving the discriminative power of LLMs by distinguishing closely related malware families.
- The approach combines high-similarity negatives to boost precision and mid-tier negatives to increase embedding diversity, leading to optimized classification accuracy.
- Evaluated on CIC-AndMal-2020 and BODMAS datasets, the refined embeddings integrated into a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework achieved significant improvements in classification accuracy with as few as 20 samples.
- Ablation studies confirmed the superiority of similarity-based selection over random sampling in refining LLM embeddings for malware classification, bridging textual and binary feature gaps to provide nuanced semantic distinctions. 

<br /><br />Summary: <div>
arXiv:2504.21028v1 Announce Type: cross 
Abstract: The rapid evolution of malware variants requires robust classification methods to enhance cybersecurity. While Large Language Models (LLMs) offer potential for generating malware descriptions to aid family classification, their utility is limited by semantic embedding overlaps and misalignment with binary behavioral features. We propose a contrastive fine-tuning (CFT) method that refines LLM embeddings via targeted selection of hard negative samples based on cosine similarity, enabling LLMs to distinguish between closely related malware families. Our approach combines high-similarity negatives to enhance discriminative power and mid-tier negatives to increase embedding diversity, optimizing both precision and generalization. Evaluated on the CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework on a few-shot setting. Experiments demonstrate significant improvements: our method achieves 63.15% classification accuracy with as few as 20 samples on CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and surpassing prior negative sampling strategies. Ablation studies confirm the superiority of similarity-based selection over random sampling, with gains of 10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions that generalize to unseen variants, bridging textual and binary feature gaps. This work advances malware classification by enabling nuanced semantic distinctions and provides a scalable framework for adapting LLMs to cybersecurity challenges.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight</title>
<link>https://arxiv.org/abs/2504.21029</link>
<guid>https://arxiv.org/abs/2504.21029</guid>
<content:encoded><![CDATA[
<div> Keywords: PICO framework, transformer architecture, prompt injection attacks, cybersecurity, Mixture-of-Experts (MoE)

Summary:
The PICO framework is proposed to enhance cybersecurity in transformers by structurally separating trusted instructions from untrusted inputs. It utilizes dual channels processed independently and merged through a controlled fusion mechanism. The integration of a Security Expert Agent within a Mixture-of-Experts framework and a Cybersecurity Knowledge Graph provides domain-specific reasoning. Training ensures the system prompt branch remains unaltered while handling adversarial inputs safely. The mathematical formulation, transformer architecture specifics, and hypothetical case studies, including Policy Puppetry attacks, illustrate the PICO framework's effectiveness. Implementing transformers with a PICO-based approach from scratch is recommended, but a cost-effective fine-tuning method is also presented. 

<br /><br />Summary: 
1. PICO framework separates trusted and untrusted inputs to mitigate prompt injection attacks.
2. Dual channels and controlled fusion mechanism ensure reliable response generation.
3. Integration of Security Expert Agent and Cybersecurity Knowledge Graph enhances domain-specific reasoning.
4. Training design maintains system prompt integrity while handling adversarial inputs safely.
5. Effective implementation involves training transformers with the PICO framework from scratch or using a cost-effective fine-tuning approach. <div>
arXiv:2504.21029v1 Announce Type: cross 
Abstract: We propose a robust transformer architecture designed to prevent prompt injection attacks and ensure secure, reliable response generation. Our PICO (Prompt Isolation and Cybersecurity Oversight) framework structurally separates trusted system instructions from untrusted user inputs through dual channels that are processed independently and merged only by a controlled, gated fusion mechanism. In addition, we integrate a specialized Security Expert Agent within a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge Graph (CKG) to supply domain-specific reasoning. Our training design further ensures that the system prompt branch remains immutable while the rest of the network learns to handle adversarial inputs safely. This PICO framework is presented via a general mathematical formulation, then elaborated in terms of the specifics of transformer architecture, and fleshed out via hypothetical case studies including Policy Puppetry attacks. While the most effective implementation may involve training transformers in a PICO-based way from scratch, we also present a cost-effective fine-tuning approach.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications</title>
<link>https://arxiv.org/abs/2504.21030</link>
<guid>https://arxiv.org/abs/2504.21030</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, artificial intelligence, context management, coordination, scalability

Summary:
This paper introduces the Model Context Protocol (MCP) framework for advancing multi-agent systems in artificial intelligence. The framework addresses challenges in context management, coordination efficiency, and scalability through standardized context sharing and coordination mechanisms. It builds on existing AI agent architectures to provide a unified theoretical foundation, advanced context management techniques, and scalable coordination patterns. Case studies in enterprise knowledge management, collaborative research, and distributed problem-solving demonstrate significant performance improvements compared to traditional approaches. The evaluation methodology offers a systematic assessment framework with benchmark tasks and datasets tailored for multi-agent systems. The paper also highlights current limitations, emerging research opportunities, and transformative applications across industries. Overall, this work contributes to the development of more capable, collaborative, and context-aware artificial intelligence systems that can effectively tackle complex real-world challenges. 

<br /><br />Summary: <div>
arXiv:2504.21030v1 Announce Type: cross 
Abstract: Multi-agent systems represent a significant advancement in artificial intelligence, enabling complex problem-solving through coordinated specialized agents. However, these systems face fundamental challenges in context management, coordination efficiency, and scalable operation. This paper introduces a comprehensive framework for advancing multi-agent systems through Model Context Protocol (MCP), addressing these challenges through standardized context sharing and coordination mechanisms. We extend previous work on AI agent architectures by developing a unified theoretical foundation, advanced context management techniques, and scalable coordination patterns. Through detailed implementation case studies across enterprise knowledge management, collaborative research, and distributed problem-solving domains, we demonstrate significant performance improvements compared to traditional approaches. Our evaluation methodology provides a systematic assessment framework with benchmark tasks and datasets specifically designed for multi-agent systems. We identify current limitations, emerging research opportunities, and potential transformative applications across industries. This work contributes to the evolution of more capable, collaborative, and context-aware artificial intelligence systems that can effectively address complex real-world challenges.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selecting the Right LLM for eGov Explanations</title>
<link>https://arxiv.org/abs/2504.21032</link>
<guid>https://arxiv.org/abs/2504.21032</guid>
<content:encoded><![CDATA[
<div> e-government services, explanations, Large Language Models, perceived quality, trust<br />
<br />
Summary:<br />
The perceived quality of explanations in e-government services is essential for building trust and increasing usage. Large Language Models (LLMs) can automate content generation for explanations, but selecting the right type of LLM is challenging. A scale was adapted to compare the quality of explanations generated by different LLMs, using tax refund decisions as a case study. A user study with 128 respondents was conducted to rate LLM-generated explanations, aiding in selecting the most suitable LLM. To automate the feedback process, advanced predictive techniques were explored. This research contributes to improving the effectiveness of explanations in e-government services and showcases the potential of LLMs in enhancing user experience and trust. <br /><br /> <div>
arXiv:2504.21032v1 Announce Type: cross 
Abstract: The perceived quality of the explanations accompanying e-government services is key to gaining trust in these institutions, consequently amplifying further usage of these services. Recent advances in generative AI, and concretely in Large Language Models (LLMs) allow the automation of such content articulations, eliciting explanations' interpretability and fidelity, and more generally, adapting content to various audiences. However, selecting the right LLM type for this has become a non-trivial task for e-government service providers. In this work, we adapted a previously developed scale to assist with this selection, providing a systematic approach for the comparative analysis of the perceived quality of explanations generated by various LLMs. We further demonstrated its applicability through the tax-return process, using it as an exemplar use case that could benefit from employing an LLM to generate explanations about tax refund decisions. This was attained through a user study with 128 survey respondents who were asked to rate different versions of LLM-generated explanations about tax refund decisions, providing a methodological basis for selecting the most appropriate LLM. Recognizing the practical challenges of conducting such a survey, we also began exploring the automation of this process by attempting to replicate human feedback using a selection of cutting-edge predictive techniques.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality</title>
<link>https://arxiv.org/abs/2504.21033</link>
<guid>https://arxiv.org/abs/2504.21033</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D modeling, generative AI, augmented reality, object detection, user interaction

Summary: 
Traditional 3D modeling is typically complex and time-consuming, limiting accessibility. This research combines generative AI and augmented reality (AR) to enable real-time 3D model generation and manipulation. The system leverages advanced AI models like Shap-E and object detection methods such as Mask R-CNN to address key challenges like object isolation and user interaction. Evaluation with 35 participants resulted in an overall System Usability Scale (SUS) score of 69.64, with higher ratings from individuals familiar with AR/VR technologies. The research's relevance spans across gaming, education, and AR-based e-commerce, providing an intuitive model creation process for users lacking specialized skills.

<br /><br />Summary: <div>
arXiv:2504.21033v1 Announce Type: cross 
Abstract: Traditional 3D modeling requires technical expertise, specialized software, and time-intensive processes, making it inaccessible for many users. Our research aims to lower these barriers by combining generative AI and augmented reality (AR) into a cohesive system that allows users to easily generate, manipulate, and interact with 3D models in real time, directly within AR environments. Utilizing cutting-edge AI models like Shap-E, we address the complex challenges of transforming 2D images into 3D representations in AR environments. Key challenges such as object isolation, handling intricate backgrounds, and achieving seamless user interaction are tackled through advanced object detection methods, such as Mask R-CNN. Evaluation results from 35 participants reveal an overall System Usability Scale (SUS) score of 69.64, with participants who engaged with AR/VR technologies more frequently rating the system significantly higher, at 80.71. This research is particularly relevant for applications in gaming, education, and AR-based e-commerce, offering intuitive, model creation for users without specialized skills.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGA: A Security Architecture for Governing AI Agentic Systems</title>
<link>https://arxiv.org/abs/2504.21034</link>
<guid>https://arxiv.org/abs/2504.21034</guid>
<content:encoded><![CDATA[
<div> Architecture, Security, Agentic systems, Control, Agents<br />
<br />
Summary: 
The paper introduces SAGA, a Security Architecture for Governing Agentic systems, to offer users oversight over their agents' lifecycle. Users register their agents with a central entity, the Provider, which maintains contact information and access control policies. SAGA employs a cryptographic mechanism for deriving access control tokens, providing fine-grained control over agent interactions. Evaluation on various tasks and conditions shows minimal performance overhead without impacting task utility. SAGA enables the secure deployment of autonomous agents, promoting responsible adoption in sensitive environments. <div>
arXiv:2504.21034v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate, and delegate tasks to one another autonomously with minimal human interaction. Industry guidelines for agentic system governance emphasize the need for users to maintain comprehensive control over their agents, mitigating potential damage from malicious agents. Several proposed agentic system designs address agent identity, authorization, and delegation, but remain purely theoretical, without concrete implementation and evaluation. Most importantly, they do not provide user-controlled agent management. To address this gap, we propose SAGA, a Security Architecture for Governing Agentic systems, that offers user oversight over their agents' lifecycle. In our design, users register their agents with a central entity, the Provider, that maintains agents contact information, user-defined access control policies, and helps agents enforce these policies on inter-agent communication. We introduce a cryptographic mechanism for deriving access control tokens, that offers fine-grained control over an agent's interaction with other agents, balancing security and performance consideration. We evaluate SAGA on several agentic tasks, using agents in different geolocations, and multiple on-device and cloud LLMs, demonstrating minimal performance overhead with no impact on underlying task utility in a wide range of conditions. Our architecture enables secure and trustworthy deployment of autonomous agents, accelerating the responsible adoption of this technology in sensitive environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?</title>
<link>https://arxiv.org/abs/2504.21036</link>
<guid>https://arxiv.org/abs/2504.21036</guid>
<content:encoded><![CDATA[
<div> fine-tuning, language models, differential privacy, privacy risks, utility degradation  
Summary:  
Differential privacy (DP) was investigated in the context of fine-tuning large language models (LLMs), revealing varying impacts on model utility across different fine-tuning methods. Privacy risks differed significantly among non-DP fine-tuning approaches. Applying DP, even with a high budget, notably reduced privacy risk. The trade-off between privacy and utility under DP varied greatly among fine-tuning methods, with some methods suffering severe utility degradation. These findings offer insights for deploying LLMs with privacy in mind and suggest avenues for optimizing the privacy-utility balance in fine-tuning techniques. <br /><br /> <div>
arXiv:2504.21036v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) has become an essential strategy for adapting them to specialized tasks; however, this process introduces significant privacy challenges, as sensitive training data may be inadvertently memorized and exposed. Although differential privacy (DP) offers strong theoretical guarantees against such leakage, its empirical privacy effectiveness on LLMs remains unclear, especially under different fine-tuning methods. In this paper, we systematically investigate the impact of DP across fine-tuning methods and privacy budgets, using both data extraction and membership inference attacks to assess empirical privacy risks. Our main findings are as follows: (1) Differential privacy reduces model utility, but its impact varies significantly across different fine-tuning methods. (2) Without DP, the privacy risks of models fine-tuned with different approaches differ considerably. (3) When DP is applied, even a relatively high privacy budget can substantially lower privacy risk. (4) The privacy-utility trade-off under DP training differs greatly among fine-tuning methods, with some methods being unsuitable for DP due to severe utility degradation. Our results provide practical guidance for privacy-conscious deployment of LLMs and pave the way for future research on optimizing the privacy-utility trade-off in fine-tuning methodologies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest</title>
<link>https://arxiv.org/abs/2504.21037</link>
<guid>https://arxiv.org/abs/2504.21037</guid>
<content:encoded><![CDATA[
<div> machine learning, security bug reports, BERT, Random Forest, prediction

Summary:<br />
- Early detection of security bug reports (SBRs) is crucial for preventing vulnerabilities and ensuring system reliability.
- Random Forest (RF) outperforms BERT with a 34% higher average G-measure for within-project predictions.
- Including both security and nonsecurity bug reports reduces RF's average performance but boosts BERT to its best average performance.
- Adding SBRs from various projects improves the average performance of both models.
- In cross-project SBR prediction, BERT achieves a remarkable 62% G-measure, substantially higher than RF. 

Summary: <div>
arXiv:2504.21037v1 Announce Type: cross 
Abstract: Early detection of security bug reports (SBRs) is crucial for preventing vulnerabilities and ensuring system reliability. While machine learning models have been developed for SBR prediction, their predictive performance still has room for improvement. In this study, we conduct a comprehensive comparison between BERT and Random Forest (RF), a competitive baseline for predicting SBRs. The results show that RF outperforms BERT with a 34% higher average G-measure for within-project predictions. Adding only SBRs from various projects improves both models' average performance. However, including both security and nonsecurity bug reports significantly reduces RF's average performance to 46%, while boosts BERT to its best average performance of 66%, surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62% G-measure, which is substantially higher than RF.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary</title>
<link>https://arxiv.org/abs/2504.21038</link>
<guid>https://arxiv.org/abs/2504.21038</guid>
<content:encoded><![CDATA[
<div> Machine learning; large language models; adversarial attacks; jailbreak; prefilling feature <br />
Summary: <br />
This paper discusses the vulnerability of Large Language Models (LLMs) to adversarial attacks, specifically jailbreak methods that can bypass their safety protocols and generate harmful content or reveal sensitive data. The authors introduce a novel jailbreak attack method that leverages the prefilling feature of LLMs to manipulate the output by controlling the probability distribution of subsequent tokens. Two attack variants, Static Prefilling (SP) and Optimized Prefilling (OP), are proposed and tested on six state-of-the-art LLMs using the AdvBench benchmark. The experiments show that the OP method can achieve high attack success rates, outperforming baseline methods. The study emphasizes the importance of robust content validation mechanisms to prevent the exploitation of prefilling features in LLMs. <div>
arXiv:2504.21038v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are designed to generate helpful and safe content. However, adversarial attacks, commonly referred to as jailbreak, can bypass their safety protocols, prompting LLMs to generate harmful content or reveal sensitive data. Consequently, investigating jailbreak methodologies is crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding the continuous implementation of security enhancements by developers. In this paper, we introduce a novel jailbreak attack method that leverages the prefilling feature of LLMs, a feature designed to enhance model output constraints. Unlike traditional jailbreak methods, the proposed attack circumvents LLMs' safety mechanisms by directly manipulating the probability distribution of subsequent tokens, thereby exerting control over the model's output. We propose two attack variants: Static Prefilling (SP), which employs a universal prefill text, and Optimized Prefilling (OP), which iteratively optimizes the prefill text to maximize the attack success rate. Experiments on six state-of-the-art LLMs using the AdvBench benchmark validate the effectiveness of our method and demonstrate its capability to substantially enhance attack success rates when combined with existing jailbreak approaches. The OP method achieved attack success rates of up to 99.82% on certain models, significantly outperforming baseline methods. This work introduces a new jailbreak attack method in LLMs, emphasizing the need for robust content validation mechanisms to mitigate the adversarial exploitation of prefilling features. All code and data used in this paper are publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report</title>
<link>https://arxiv.org/abs/2504.21039</link>
<guid>https://arxiv.org/abs/2504.21039</guid>
<content:encoded><![CDATA[
<div> pretraining, cybersecurity, large language models, Foundation-Sec-8B, benchmarks <br />
Summary:<br />
The article introduces Foundation-Sec-8B, a cybersecurity-focused large language model (LLM) created using the Llama 3.1 architecture. It addresses the challenges faced in adopting LLMs in cybersecurity by pretraining on a specialized cybersecurity corpus. The model is evaluated on both established and new cybersecurity benchmarks, demonstrating comparable performance to existing LLMs in specific cybersecurity tasks. By releasing Foundation-Sec-8B to the public, the authors aim to drive progress and adoption of AI-driven tools in public and private cybersecurity domains. <div>
arXiv:2504.21039v1 Announce Type: cross 
Abstract: As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift</title>
<link>https://arxiv.org/abs/2504.21042</link>
<guid>https://arxiv.org/abs/2504.21042</guid>
<content:encoded><![CDATA[
<div> ConceptLens, trustworthiness, artificial intelligence, integrity threats, bias injection <br />
<br />Summary: ConceptLens is a framework designed to assess and attribute integrity threats in AI systems. It leverages pre-trained multimodal models to identify root causes of threats such as data poisoning attacks, bias injection, and privacy risks in high-risk samples. It also detects vulnerabilities in model training data and reveals dependencies on specific concepts. ConceptLens uncovers sociological biases in generative content and highlights the potential exploitation of safe data. By providing actionable insights, ConceptLens aims to increase trust in AI systems and promote their adoption and innovation. <div>
arXiv:2504.21042v1 Announce Type: cross 
Abstract: The growing adoption of artificial intelligence (AI) has amplified concerns about trustworthiness, including integrity, privacy, robustness, and bias. To assess and attribute these threats, we propose ConceptLens, a generic framework that leverages pre-trained multimodal models to identify the root causes of integrity threats by analyzing Concept Shift in probing samples. ConceptLens demonstrates strong detection performance for vanilla data poisoning attacks and uncovers vulnerabilities to bias injection, such as the generation of covert advertisements through malicious concept shifts. It identifies privacy risks in unaltered but high-risk samples, filters them before training, and provides insights into model weaknesses arising from incomplete or imbalanced training data. Additionally, at the model level, it attributes concepts that the target model is overly dependent on, identifies misleading concepts, and explains how disrupting key concepts negatively impacts the model. Furthermore, it uncovers sociological biases in generative content, revealing disparities across sociological contexts. Strikingly, ConceptLens reveals how safe training and inference data can be unintentionally and easily exploited, potentially undermining safety alignment. Our study informs actionable insights to breed trust in AI systems, thereby speeding adoption and driving greater innovation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain</title>
<link>https://arxiv.org/abs/2504.21043</link>
<guid>https://arxiv.org/abs/2504.21043</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, security vulnerabilities, smart contracts, CodeBC, fine-tuning approach 

Summary:
CodeBC is a code generation model developed to enhance security in smart contract development for blockchain. Unlike previous methods that rely on manually labeled vulnerability data, CodeBC uses a three-stage fine-tuning approach based on CodeLlama. Instead of pairwise vulnerability location annotations, CodeBC leverages vulnerability and security tags to teach the model the differences between vulnerable and secure code. During the inference phase, the model uses security tags to generate secure and robust code. Experimental results show that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU, and compilation pass rates while reducing vulnerability rates significantly. This approach proves to be effective and cost-efficient, signaling CodeBC as a promising solution for generating secure smart contract code. 

<br /><br />Summary: <div>
arXiv:2504.21043v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at generating code from natural language instructions, yet they often lack an understanding of security vulnerabilities. This limitation makes it difficult for LLMs to avoid security risks in generated code, particularly in high-security programming tasks such as smart contract development for blockchain. Researchers have attempted to enhance the vulnerability awareness of these models by training them to differentiate between vulnerable and fixed code snippets. However, this approach relies heavily on manually labeled vulnerability data, which is only available for popular languages like Python and C++. For low-resource languages like Solidity, used in smart contracts, large-scale annotated datasets are scarce and difficult to obtain. To address this challenge, we introduce CodeBC, a code generation model specifically designed for generating secure smart contracts in blockchain. CodeBC employs a three-stage fine-tuning approach based on CodeLlama, distinguishing itself from previous methods by not relying on pairwise vulnerability location annotations. Instead, it leverages vulnerability and security tags to teach the model the differences between vulnerable and secure code. During the inference phase, the model leverages security tags to generate secure and robust code. Experimental results demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU, and compilation pass rates, while significantly reducing vulnerability rates. These findings validate the effectiveness and cost-efficiency of our three-stage fine-tuning strategy, making CodeBC a promising solution for generating secure smart contract code.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection</title>
<link>https://arxiv.org/abs/2504.21044</link>
<guid>https://arxiv.org/abs/2504.21044</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Model Theft, Watermarking, Adversarial Attacks, Multimodal Models

Summary: 
The paper introduces AGATE, a Model-Agnostic Black-box Backdoor Watermarking Framework designed to address challenges in protecting copyright for large-scale AI models. By generating stealthy adversarial triggers from ordinary datasets, AGATE ensures visual fidelity while inducing semantic shifts, enhancing the model's stealthiness and robustness against watermark evasion. A post-transform module helps correct model outputs, closing the gap between adversarial trigger image embedding and text embedding to prevent anomaly detection. The proposed two-phase watermark verification process compares model results with and without the transform module to determine copyright infringement accurately. AGATE outperforms existing methods in multimodal image-text retrieval and image classification tasks across multiple datasets. Furthermore, the framework demonstrates robustness against adversarial attacks, showcasing its effectiveness in securing multimodal AI models from unauthorized use. 

<br /><br />Summary: <div>
arXiv:2504.21044v1 Announce Type: cross 
Abstract: Recent advancement in large-scale Artificial Intelligence (AI) models offering multimodal services have become foundational in AI systems, making them prime targets for model theft. Existing methods select Out-of-Distribution (OoD) data as backdoor watermarks and retrain the original model for copyright protection. However, existing methods are susceptible to malicious detection and forgery by adversaries, resulting in watermark evasion. In this work, we propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking Framework (AGATE) to address stealthiness and robustness challenges in multimodal model copyright protection. Specifically, we propose an adversarial trigger generation method to generate stealthy adversarial triggers from ordinary dataset, providing visual fidelity while inducing semantic shifts. To alleviate the issue of anomaly detection among model outputs, we propose a post-transform module to correct the model output by narrowing the distance between adversarial trigger image embedding and text embedding. Subsequently, a two-phase watermark verification is proposed to judge whether the current model infringes by comparing the two results with and without the transform module. Consequently, we consistently outperform state-of-the-art methods across five datasets in the downstream tasks of multimodal image-text retrieval and image classification. Additionally, we validated the robustness of AGATE under two adversarial attack scenarios.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection</title>
<link>https://arxiv.org/abs/2504.21045</link>
<guid>https://arxiv.org/abs/2504.21045</guid>
<content:encoded><![CDATA[
<div> machine learning, XSS attacks, obfuscated XSS, Large Language Model, security vulnerabilities

Summary:
- The research discusses the persistent threat of Cross-Site Scripting (XSS) attacks and the importance of effectively detecting and protecting systems from them.
- Traditional ML models trained on non-obfuscated XSS data struggle to accurately identify obfuscated XSS threats, highlighting the need for diverse training data.
- The study introduces a novel approach of fine-tuning a Large Language Model (LLM) to automatically generate complex obfuscated XSS payloads, improving model accuracy and performance.
- By creating challenging training data with diverse obfuscated variants, the proposed system achieved a high accuracy rate of 99.5% with the obfuscated dataset.
- The obfuscated samples generated by the LLMs were significantly more complex than those produced by existing tools, enhancing the model's capability to handle advanced XSS attacks effectively, making it more suitable for real-world application security.<br /><br />Summary: <div>
arXiv:2504.21045v1 Announce Type: cross 
Abstract: According to the Open Web Application Security Project (OWASP), Cross-Site Scripting (XSS) is a critical security vulnerability. Despite decades of research, XSS remains among the top 10 security vulnerabilities. Researchers have proposed various techniques to protect systems from XSS attacks, with machine learning (ML) being one of the most widely used methods. An ML model is trained on a dataset to identify potential XSS threats, making its effectiveness highly dependent on the size and diversity of the training data. A variation of XSS is obfuscated XSS, where attackers apply obfuscation techniques to alter the code's structure, making it challenging for security systems to detect its malicious intent. Our study's random forest model was trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy. However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%, underscoring the importance of training ML models with obfuscated data to improve their effectiveness in detecting XSS attacks. A significant challenge is to generate highly complex obfuscated code despite the availability of several public tools. These tools can only produce obfuscation up to certain levels of complexity.
  In our proposed system, we fine-tune a Large Language Model (LLM) to generate complex obfuscated XSS payloads automatically. By transforming original XSS samples into diverse obfuscated variants, we create challenging training data for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the obfuscated dataset. We also found that the obfuscated samples generated by the LLMs were 28.1% more complex than those created by other tools, significantly improving the model's ability to handle advanced XSS attacks and making it more effective for real-world application security.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Connectomes: A Generational Approach to Data-Efficient Language Models</title>
<link>https://arxiv.org/abs/2504.21047</link>
<guid>https://arxiv.org/abs/2504.21047</guid>
<content:encoded><![CDATA[
<div> Keywords: biological neural networks, evolution, individual learning, artificial neural networks, language

Summary:<br /><br />
This study introduces a novel framework that combines evolution and individual learning to train artificial neural networks. By incorporating an "outer loop" of evolution before the standard training procedure, artificial networks can better mimic the effects of evolution and learning seen in biological organisms. In an experiment focused on language, a model with an inherited "connectome" from evolution outperformed control models on natural language processing tasks, alignment with human behavior, and brain data. This suggests that a model connectome can serve as a strong prior for learning in situations with limited data, bridging the gap between artificial models and biologically evolved neural networks. By integrating biological principles into artificial network training, this approach shows promise for improving the performance and efficiency of artificial neural networks. <div>
arXiv:2504.21047v1 Announce Type: cross 
Abstract: Biological neural networks are shaped both by evolution across generations and by individual learning within an organism's lifetime, whereas standard artificial neural networks undergo a single, large training procedure without inherited constraints. In this preliminary work, we propose a framework that incorporates this crucial generational dimension - an "outer loop" of evolution that shapes the "inner loop" of learning - so that artificial networks better mirror the effects of evolution and individual learning in biological organisms. Focusing on language, we train a model that inherits a "model connectome" from the outer evolution loop before exposing it to a developmental-scale corpus of 100M tokens. Compared with two closely matched control models, we show that the connectome model performs better or on par on natural language processing tasks as well as alignment to human behavior and brain data. These findings suggest that a model connectome serves as an efficient prior for learning in low-data regimes - narrowing the gap between single-generation artificial models and biologically evolved neural networks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey</title>
<link>https://arxiv.org/abs/2504.21048</link>
<guid>https://arxiv.org/abs/2504.21048</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Reinforcement Learning, Resource Allocation Optimization, Industry 4.0, algorithms, decentralized contexts <br />
<br />
Summary: Multi-Agent Reinforcement Learning (MARL) is a valuable framework for addressing Resource Allocation Optimization (RAO) challenges in various sectors, particularly in the context of Industry 4.0. This survey explores recent MARL algorithms for RAO, covering fundamental concepts, classifications, and a structured taxonomy. By examining the current research landscape, key challenges, and future directions, the survey aims to assist researchers and practitioners in harnessing MARL's capabilities to enhance resource allocation solutions in dynamic and decentralized environments. <div>
arXiv:2504.21048v1 Announce Type: cross 
Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phishing URL Detection using Bi-LSTM</title>
<link>https://arxiv.org/abs/2504.21049</link>
<guid>https://arxiv.org/abs/2504.21049</guid>
<content:encoded><![CDATA[
<div> phishing, detection, deep learning, Bi-LSTM network, URL<br />
Summary:<br />
Phishing attacks pose a significant threat to online users, resulting in data breaches, financial losses, and identity theft. Traditional phishing detection systems often struggle with high false positive rates and limitations in identifying various types of attacks. This paper introduces a novel approach utilizing a Bidirectional Long Short-Term Memory (Bi-LSTM) network based on deep learning to classify URLs into four categories: benign, phishing, defacement, and malware. By leveraging sequential URL data and capturing contextual information, the model significantly enhances the accuracy of phishing detection. Experimental results on a dataset of 650,000 URLs exhibit the model's effectiveness, achieving an impressive 97% accuracy rate and outperforming traditional techniques. <div>
arXiv:2504.21049v1 Announce Type: cross 
Abstract: Phishing attacks threaten online users, often leading to data breaches, financial losses, and identity theft. Traditional phishing detection systems struggle with high false positive rates and are usually limited by the types of attacks they can identify. This paper proposes a deep learning-based approach using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs into four categories: benign, phishing, defacement, and malware. The model leverages sequential URL data and captures contextual information, improving the accuracy of phishing detection. Experimental results on a dataset comprising over 650,000 URLs demonstrate the model's effectiveness, achieving 97% accuracy and significant improvements over traditional techniques.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFIBA: Spatial-based Full-target Invisible Backdoor Attacks</title>
<link>https://arxiv.org/abs/2504.21052</link>
<guid>https://arxiv.org/abs/2504.21052</guid>
<content:encoded><![CDATA[
<div> target-specific, stealthy, deep neural networks, backdoor attacks, black-box settings
Summary:<br /><br />Multi-target backdoor attacks on deep neural networks pose significant security risks as they can preset multiple target classes through a single injection. Existing methods face challenges in achieving trigger specificity and stealthiness in black-box settings, limiting their effectiveness. To address these issues, the proposed Spatial-based Full-target Invisible Backdoor Attack (SFIBA) ensures triggers are restricted to specific local spatial regions for target classes and employs a frequency-domain-based injection method for stealthiness. By integrating trigger features through fast fourier transform, discrete wavelet transform, and singular value decomposition, SFIBA achieves excellent attack performance and stealthiness while maintaining model performance on benign samples. Experimental results demonstrate SFIBA's effectiveness in bypassing existing backdoor defenses on various datasets and models. <div>
arXiv:2504.21052v1 Announce Type: cross 
Abstract: Multi-target backdoor attacks pose significant security threats to deep neural networks, as they can preset multiple target classes through a single backdoor injection. This allows attackers to control the model to misclassify poisoned samples with triggers into any desired target class during inference, exhibiting superior attack performance compared with conventional backdoor attacks. However, existing multi-target backdoor attacks fail to guarantee trigger specificity and stealthiness in black-box settings, resulting in two main issues. First, they are unable to simultaneously target all classes when only training data can be manipulated, limiting their effectiveness in realistic attack scenarios. Second, the triggers often lack visual imperceptibility, making poisoned samples easy to detect. To address these problems, we propose a Spatial-based Full-target Invisible Backdoor Attack, called SFIBA. It restricts triggers for different classes to specific local spatial regions and morphologies in the pixel space to ensure specificity, while employing a frequency-domain-based trigger injection method to guarantee stealthiness. Specifically, for injection of each trigger, we first apply fast fourier transform to obtain the amplitude spectrum of clean samples in local spatial regions. Then, we employ discrete wavelet transform to extract the features from the amplitude spectrum and use singular value decomposition to integrate the trigger. Subsequently, we selectively filter parts of the trigger in pixel space to implement trigger morphology constraints and adjust injection coefficients based on visual effects. We conduct experiments on multiple datasets and models. The results demonstrate that SFIBA can achieve excellent attack performance and stealthiness, while preserving the model's performance on benign samples, and can also bypass existing backdoor defenses.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models</title>
<link>https://arxiv.org/abs/2504.21053</link>
<guid>https://arxiv.org/abs/2504.21053</guid>
<content:encoded><![CDATA[
<div> Keywords: Safety alignment, large language models, neuron activation analysis, similarity-based neuron identification, adversarial fine-tuning attacks

Summary:
Safety alignment in large language models (LLMs) involves fine-tuning mechanisms to regulate neuron activations for suppressing harmful content. This study introduces a novel approach to induce disalignment by identifying and modifying neurons responsible for safety constraints. The proposed method includes three main steps: Neuron Activation Analysis detects critical neurons for distinguishing harmful and harmless inputs; Similarity-Based Neuron Identification locates neurons responsible for safe alignment systematically; Neuron Relearning for Safety Removal fine-tunes selected neurons to restore the model's ability to generate restricted responses. Experimental results validate the efficacy of this method in removing safety constraints with minimal fine-tuning, revealing a significant vulnerability in current alignment techniques. The study emphasizes the importance of robust defense strategies to combat adversarial fine-tuning attacks on LLMs. 

<br /><br />Summary: <div>
arXiv:2504.21053v1 Announce Type: cross 
Abstract: Safety alignment in large language models (LLMs) is achieved through fine-tuning mechanisms that regulate neuron activations to suppress harmful content. In this work, we propose a novel approach to induce disalignment by identifying and modifying the neurons responsible for safety constraints. Our method consists of three key steps: Neuron Activation Analysis, where we examine activation patterns in response to harmful and harmless prompts to detect neurons that are critical for distinguishing between harmful and harmless inputs; Similarity-Based Neuron Identification, which systematically locates the neurons responsible for safe alignment; and Neuron Relearning for Safety Removal, where we fine-tune these selected neurons to restore the model's ability to generate previously restricted responses. Experimental results demonstrate that our method effectively removes safety constraints with minimal fine-tuning, highlighting a critical vulnerability in current alignment techniques. Our findings underscore the need for robust defenses against adversarial fine-tuning attacks on LLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FFCBA: Feature-based Full-target Clean-label Backdoor Attacks</title>
<link>https://arxiv.org/abs/2504.21054</link>
<guid>https://arxiv.org/abs/2504.21054</guid>
<content:encoded><![CDATA[
<div> feature-based, full-target, clean-label, backdoor attacks, multi-target

Summary:
Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) propose a novel approach to launching stealthy and effective backdoor attacks on deep neural networks. The FFCBA consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and Feature-Migrating Backdoor Attacks (FMBA). FSBA utilizes class-conditional autoencoders to generate noise triggers that maintain consistency and specificity within and across classes. On the other hand, FMBA employs a two-stage training process to generate triggers with strong target-class features for effective cross-model attacks. Experimental results demonstrate that FFCBA achieves exceptional attack performance while maintaining robustness against existing backdoor defense mechanisms. This approach opens up possibilities for more efficient and scalable multi-target backdoor attacks in the clean-label setting. <div>
arXiv:2504.21054v1 Announce Type: cross 
Abstract: Backdoor attacks pose a significant threat to deep neural networks, as backdoored models would misclassify poisoned samples with specific triggers into target classes while maintaining normal performance on clean samples. Among these, multi-target backdoor attacks can simultaneously target multiple classes. However, existing multi-target backdoor attacks all follow the dirty-label paradigm, where poisoned samples are mislabeled, and most of them require an extremely high poisoning rate. This makes them easily detectable by manual inspection. In contrast, clean-label attacks are more stealthy, as they avoid modifying the labels of poisoned samples. However, they generally struggle to achieve stable and satisfactory attack performance and often fail to scale effectively to multi-target attacks. To address this issue, we propose the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional autoencoders to generate noise triggers that align perturbed in-class samples with the original category's features, ensuring the effectiveness, intra-class consistency, inter-class specificity and natural-feature correlation of triggers. While FSBA supports swift and efficient attacks, its cross-model attack capability is relatively weak. FMBA employs a two-stage class-conditional autoencoder training process that alternates between using out-of-class samples and in-class samples. This allows FMBA to generate triggers with strong target-class features, making it highly effective for cross-model attacks. We conduct experiments on multiple datasets and models, the results show that FFCBA achieves outstanding attack performance and maintains desirable robustness against the state-of-the-art backdoor defenses.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Performance Analysis for Semantic Communications Based on Empirical Results</title>
<link>https://arxiv.org/abs/2504.21055</link>
<guid>https://arxiv.org/abs/2504.21055</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, semantic communications, performance analysis, power control, energy efficiency

Summary: 
The paper introduces the Alpha-Beta-Gamma (ABG) formula to model the relationship between end-to-end measurement and signal-to-noise ratio (SNR) in deep learning-based semantic communications. The formula is applicable for both image reconstruction and inference tasks and can fit various DL networks like SCUNet and Vision Transformer. The study reveals that the upper bound of the multi scale-structural similarity index measure (MS-SSIM) is influenced by the number of quantized output bits of semantic encoders. An adaptive power control scheme is proposed for semantic communications over random fading channels, ensuring quality of service (QoS) and optimizing energy efficiency. Additionally, an optimal power allocation scheme is designed to maximize the energy efficiency of the system. Through extensive simulations, the effectiveness and superiority of the ABG formula and power allocation schemes are demonstrated. <div>
arXiv:2504.21055v1 Announce Type: cross 
Abstract: Due to the black-box characteristics of deep learning based semantic encoders and decoders, finding a tractable method for the performance analysis of semantic communications is a challenging problem. In this paper, we propose an Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end measurement and SNR, which can be applied for both image reconstruction tasks and inference tasks. Specifically, for image reconstruction tasks, the proposed ABG formula can well fit the commonly used DL networks, such as SCUNet, and Vision Transformer, for semantic encoding with the multi scale-structural similarity index measure (MS-SSIM) measurement. Furthermore, we find that the upper bound of the MS-SSIM depends on the number of quantized output bits of semantic encoders, and we also propose a closed-form expression to fit the relationship between the MS-SSIM and quantized output bits. To the best of our knowledge, this is the first theoretical expression between end-to-end performance metrics and SNR for semantic communications. Based on the proposed ABG formula, we investigate an adaptive power control scheme for semantic communications over random fading channels, which can effectively guarantee quality of service (QoS) for semantic communications, and then design the optimal power allocation scheme to maximize the energy efficiency of the semantic communication system. Furthermore, by exploiting the bisection algorithm, we develop the power allocation scheme to maximize the minimum QoS of multiple users for OFDMA downlink semantic communication Extensive simulations verify the effectiveness and superiority of the proposed ABG formula and power allocation schemes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2504.21063</link>
<guid>https://arxiv.org/abs/2504.21063</guid>
<content:encoded><![CDATA[
<div> Token-level prompt mixture, parameter-free routing, federated domain generalization, vision-language models, communication efficiency
<br />
Summary:
TRIP is a framework for Federated Domain Generalization that uses a Token-level prompt mixture with parameter-free routing. It addresses the limitations of existing methods by assigning different tokens within an image to specific experts, ensuring optimal generalization results with minimal communication. TRIP synthesizes an instance-specific prompt by aggregating experts based on the number of tokens assigned to each. It also incorporates an unbiased learning strategy leveraging VLM's zero-shot generalization capability. The framework achieves optimal generalization results across four benchmarks with only 1K parameters per round. The code for TRIP is available on GitHub for further exploration and implementation. <div>
arXiv:2504.21063v1 Announce Type: cross 
Abstract: Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models (VLMs) in FedDG by learning a single global prompt. However, such a one-prompt-fits-all learning paradigm typically leads to performance degradation on personalized samples. Although the mixture of experts (MoE) offers a promising solution for specialization, existing MoE-based methods suffer from coarse image-level expert assignment and high communication costs from parameterized routers. To address these limitations, we propose TRIP, a Token-level prompt mixture with parameter-free routing framework for FedDG, which treats multiple prompts as distinct experts. Unlike existing image-level routing designs, TRIP assigns different tokens within an image to specific experts. To ensure communication efficiency, TRIP incorporates a parameter-free routing mechanism based on token clustering and optimal transport. The instance-specific prompt is then synthesized by aggregating experts, weighted by the number of tokens assigned to each. Additionally, TRIP develops an unbiased learning strategy for prompt experts, leveraging the VLM's zero-shot generalization capability. Extensive experiments across four benchmarks demonstrate that TRIP achieves optimal generalization results, with communication of only 1K parameters per round. Our code is available at https://github.com/GongShuai8210/TRIP.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS</title>
<link>https://arxiv.org/abs/2504.21064</link>
<guid>https://arxiv.org/abs/2504.21064</guid>
<content:encoded><![CDATA[
<div> Dataset, Graph Neural Network, Depression Diagnosis, Temporal Biomarker, Fourier Transform  
Summary:  
- Data-driven approaches in neuromedicine focus on depression diagnosis using large datasets and graph neural network (GNN) models.  
- A novel biomarker for depression diagnosis leveraging discrete Fourier transform (DFT) improves temporal characteristics representation in brain channels.  
- A customized Temporal Graph Convolutional Network (TGCN) architecture was proposed and trained on a dataset with 1,086 subjects, larger than previous datasets.  
- Propensity score matching (PSM) was used to create a refined subset for the model, leading to improved F1 scores in real-world and PSM datasets.  
- SHapley Additive exPlaination (SHAP) was used to validate the interpretability of the model, ensuring its practical applicability in medical settings.  
<br /><br /> <div>
arXiv:2504.21064v1 Announce Type: cross 
Abstract: Data-driven approaches for depression diagnosis have emerged as a significant research focus in neuromedicine, driven by the development of relevant datasets. Recently, graph neural network (GNN)-based models have gained widespread adoption due to their ability to capture brain channel functional connectivity from both spatial and temporal perspectives. However, their effectiveness is hindered by the absence of a robust temporal biomarker. In this paper, we introduce a novel and effective biomarker for depression diagnosis by leveraging the discrete Fourier transform (DFT) and propose a customized graph network architecture based on Temporal Graph Convolutional Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects, which is over 10 times larger than previous datasets in the field of depression diagnosis. Furthermore, to align with medical requirements, we performed propensity score matching (PSM) to create a refined subset, referred to as the PSM dataset. Experimental results demonstrate that incorporating our newly designed biomarker enhances the representation of temporal characteristics in brain channels, leading to improved F1 scores in both the real-world dataset and the PSM dataset. This advancement has the potential to contribute to the development of more effective depression diagnostic tools. In addition, we used SHapley Additive exPlaination (SHAP) to validate the interpretability of our model, ensuring its practical applicability in medical settings.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A 3D pocket-aware and affinity-guided diffusion model for lead optimization</title>
<link>https://arxiv.org/abs/2504.21065</link>
<guid>https://arxiv.org/abs/2504.21065</guid>
<content:encoded><![CDATA[
<div> Generative models, deep learning, molecular optimization, binding affinity, drug discovery <br />
<br />
Summary: 
The article introduces Diffleop, a 3D pocket-aware and affinity-guided diffusion model designed to optimize molecules for increased binding affinity in drug discovery. Traditional molecular optimization methods often struggle to adequately consider binding affinities with protein targets during lead optimization. Diffleop addresses this challenge by incorporating knowledge of protein-ligand binding affinity to guide molecule generation with higher affinity. Through comprehensive evaluations, Diffleop outperformed baseline models across various metrics, with a notable improvement in binding affinity. This innovative approach leverages deep learning-based 3D generative models to enhance the efficiency of molecular optimization, providing promising results for advancing drug discovery processes. <div>
arXiv:2504.21065v1 Announce Type: cross 
Abstract: Molecular optimization, aimed at improving binding affinity or other molecular properties, is a crucial task in drug discovery that often relies on the expertise of medicinal chemists. Recently, deep learning-based 3D generative models showed promise in enhancing the efficiency of molecular optimization. However, these models often struggle to adequately consider binding affinities with protein targets during lead optimization. Herein, we propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop, to optimize molecules with enhanced binding affinity. The model explicitly incorporates the knowledge of protein-ligand binding affinity to guide the denoising sampling for molecule generation with high affinity. The comprehensive evaluations indicated that Diffleop outperforms baseline models across multiple metrics, especially in terms of binding affinity.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection</title>
<link>https://arxiv.org/abs/2504.21066</link>
<guid>https://arxiv.org/abs/2504.21066</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, edge devices, compression techniques, transfer learning, domain generalization

Summary:
Compression techniques and transfer learning methods were explored to address the challenge of training and deploying deepfake detection models on edge devices. Pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques were evaluated using datasets such as Synthbuster, RAISE, and ForenSynths. Results showed that compression and transfer learning can effectively reduce computational demands and inference time, even with a high compression level of 90%. However, a domain generalization issue was identified when testing datasets were generated by DeepFake models not present in the training set. This highlights the importance of considering different scenarios when developing deepfake detection models for edge devices. The study demonstrates the feasibility of maintaining data privacy and confidentiality by processing data close to its source while efficiently deploying deepfake detection models despite limited computational and memory resources at the edge. 

<br /><br />Summary: <div>
arXiv:2504.21066v1 Announce Type: cross 
Abstract: Training and deploying deepfake detection models on edge devices offers the advantage of maintaining data privacy and confidentiality by processing it close to its source. However, this approach is constrained by the limited computational and memory resources available at the edge. To address this challenge, we explore compression techniques to reduce computational demands and inference time, alongside transfer learning methods to minimize training overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate the effectiveness of pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques. Our experimental results demonstrate that both compression and transfer learning can be effectively achieved, even with a high compression level of 90%, remaining at the same performance level when the training and validation data originate from the same DeepFake model. However, when the testing dataset is generated by DeepFake models not present in the training set, a domain generalization issue becomes evident.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erased but Not Forgotten: How Backdoors Compromise Concept Erasure</title>
<link>https://arxiv.org/abs/2504.21072</link>
<guid>https://arxiv.org/abs/2504.21072</guid>
<content:encoded><![CDATA[
<div> Toxic Erasure, backdoor attacks, machine unlearning, text-to-image models, security gap <br />
Summary:<br />
The paper introduces the concept of Toxic Erasure (ToxE) as a new threat model for large-scale text-to-image diffusion models. It demonstrates how recent unlearning algorithms can be circumvented through targeted backdoor attacks that establish a link between a trigger and the undesired content. The study evaluates five concept erasure methods and finds that for celebrity identity erasure, deep backdoor attacks can successfully bypass erasure attempts with up to 82% success rate. The attacks also manipulate explicit content erasure to elicit significantly more exposed body parts, up to 9 times compared to standard erasure methods. The findings underscore a critical security gap in current unlearning strategies and emphasize the need for more robust defenses against malicious content generation in AI models. <br /> <div>
arXiv:2504.21072v1 Announce Type: cross 
Abstract: The expansion of large-scale text-to-image diffusion models has raised growing concerns about their potential to generate undesirable or harmful content, ranging from fabricated depictions of public figures to sexually explicit images. To mitigate these risks, prior work has devised machine unlearning techniques that attempt to erase unwanted concepts through fine-tuning. However, in this paper, we introduce a new threat model, Toxic Erasure (ToxE), and demonstrate how recent unlearning algorithms, including those explicitly designed for robustness, can be circumvented through targeted backdoor attacks. The threat is realized by establishing a link between a trigger and the undesired content. Subsequent unlearning attempts fail to erase this link, allowing adversaries to produce harmful content. We instantiate ToxE via two established backdoor attacks: one targeting the text encoder and another manipulating the cross-attention layers. Further, we introduce Deep Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that optimizes the entire U-Net using a score-based objective, improving the attack's persistence across different erasure methods. We evaluate five recent concept erasure methods against our threat model. For celebrity identity erasure, our deep attack circumvents erasure with up to 82% success, averaging 57% across all erasure methods. For explicit content erasure, ToxE attacks can elicit up to 9 times more exposed body parts, with DISA yielding an average increase by a factor of 2.9. These results highlight a critical security gap in current unlearning strategies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks</title>
<link>https://arxiv.org/abs/2504.21074</link>
<guid>https://arxiv.org/abs/2504.21074</guid>
<content:encoded><![CDATA[
<div> capabilities, large language models, process mining tasks, semantic understanding, fine-tuning
Summary: 
Large language models (LLMs) have demonstrated their value in process mining tasks, including reasoning about process behavior. There is potential for LLMs to handle semantics-aware process mining tasks that require an understanding of activity meanings and relationships. In this study, the authors systematically examine the capabilities of LLMs for such tasks, evaluating their performance through in-context learning and supervised fine-tuning. Five process mining tasks necessitating semantic understanding are defined, with benchmarking datasets provided. While LLMs struggle with challenging tasks in their default state, they achieve strong performance when fine-tuned for these tasks across various industries and process types. <div>
arXiv:2504.21074v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown to be valuable tools for tackling process mining tasks. Existing studies report on their capability to support various data-driven process analyses and even, to some extent, that they are able to reason about how processes work. This reasoning ability suggests that there is potential for LLMs to tackle semantics-aware process mining tasks, which are tasks that rely on an understanding of the meaning of activities and their relationships. Examples of these include process discovery, where the meaning of activities can indicate their dependency, whereas in anomaly detection the meaning can be used to recognize process behavior that is abnormal. In this paper, we systematically explore the capabilities of LLMs for such tasks. Unlike prior work, which largely evaluates LLMs in their default state, we investigate their utility through both in-context learning and supervised fine-tuning. Concretely, we define five process mining tasks requiring semantic understanding and provide extensive benchmarking datasets for evaluation. Our experiments reveal that while LLMs struggle with challenging process mining tasks when used out of the box or with minimal in-context examples, they achieve strong performance when fine-tuned for these tasks across a broad range of process types and industries.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning</title>
<link>https://arxiv.org/abs/2504.21099</link>
<guid>https://arxiv.org/abs/2504.21099</guid>
<content:encoded><![CDATA[
<div> Fine-Tuning, Federated Learning, Parameter-Efficient, Additive PEFT, Selective PEFT <br />
<br />
Summary: 
This survey explores the integration of Parameter-Efficient Fine-Tuning (PEFT) methods within federated learning environments. The survey categorizes existing approaches into Additive PEFT, Selective PEFT, and Reparameterized PEFT, discussing how these methods address challenges in federated settings such as data heterogeneity, communication efficiency, computational constraints, and privacy concerns. The literature is organized based on application domains, including natural language processing and computer vision tasks. Promising research directions highlighted include scaling to larger foundation models, theoretical analysis of federated PEFT methods, and sustainable approaches for resource-constrained environments. <div>
arXiv:2504.21099v1 Announce Type: cross 
Abstract: Foundation models have revolutionized artificial intelligence by providing robust, versatile architectures pre-trained on large-scale datasets. However, adapting these massive models to specific downstream tasks requires fine-tuning, which can be prohibitively expensive in computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by selectively updating only a small subset of parameters. Meanwhile, Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. This survey provides a comprehensive review of the integration of PEFT techniques within federated learning environments. We systematically categorize existing approaches into three main groups: Additive PEFT (which introduces new trainable parameters), Selective PEFT (which fine-tunes only subsets of existing parameters), and Reparameterized PEFT (which transforms model architectures to enable efficient updates). For each category, we analyze how these methods address the unique challenges of federated settings, including data heterogeneity, communication efficiency, computational constraints, and privacy concerns. We further organize the literature based on application domains, covering both natural language processing and computer vision tasks. Finally, we discuss promising research directions, including scaling to larger foundation models, theoretical analysis of federated PEFT methods, and sustainable approaches for resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression</title>
<link>https://arxiv.org/abs/2504.21152</link>
<guid>https://arxiv.org/abs/2504.21152</guid>
<content:encoded><![CDATA[
arXiv:2504.21152v1 Announce Type: cross 
Abstract: Imbalanced regression refers to prediction tasks where the target variable is skewed. This skewness hinders machine learning models, especially neural networks, which concentrate on dense regions and therefore perform poorly on underrepresented (minority) samples. Despite the importance of this problem, only a few methods have been proposed for imbalanced regression. Many of the available solutions for imbalanced regression adapt techniques from the class imbalance domain, such as linear interpolation and the addition of Gaussian noise, to create synthetic data in sparse regions. However, in many cases, the underlying distribution of the data is complex and non-linear. Consequently, these approaches generate synthetic samples that do not accurately represent the true feature-target relationship. To overcome these limitations, we propose SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage 1, an existing oversampler generates initial synthetic samples in sparse target regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves as SMOGAN's filtering layer and refines these samples via adversarial loss augmented with a Maximum Mean Discrepancy objective, aligning them with the true joint feature-target distribution. Extensive experiments on 23 imbalanced datasets show that SMOGAN consistently outperforms the default oversampling method without the DistGAN filtering layer.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis</title>
<link>https://arxiv.org/abs/2504.21154</link>
<guid>https://arxiv.org/abs/2504.21154</guid>
<content:encoded><![CDATA[
arXiv:2504.21154v1 Announce Type: cross 
Abstract: This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including Random Forests and Support Vector Machines. Additionally, we provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human--computer interaction, with a highest accuracy of 96.85\%.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation</title>
<link>https://arxiv.org/abs/2504.21155</link>
<guid>https://arxiv.org/abs/2504.21155</guid>
<content:encoded><![CDATA[
arXiv:2504.21155v1 Announce Type: cross 
Abstract: Our contributions are motivated by fusion reactors that rely on maintaining magnetohydrodynamic (MHD) equilibrium, where the balance between plasma pressure and confining magnetic fields is required for stable operation. In axisymmetric tokamak reactors in particular, and under the assumption of toroidal symmetry, this equilibrium can be mathematically modelled using the Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing studies did not examine realistic scenarios in which a single network generalizes to a variety of boundary conditions. Addressing that limitation, we evaluate a PINN architecture that incorporates boundary points as network inputs. Additionally, we compare PINN model accuracy and inference speeds with a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most performant, and accurate in our setting, we use the network verification tool Marabou to perform a range of verification tasks. Although we find some discrepancies between evaluations of the networks natively in PyTorch, compared to via Marabou, we are able to demonstrate useful and practical verification workflows. Our study is the first investigation of verification of such networks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dance Style Recognition Using Laban Movement Analysis</title>
<link>https://arxiv.org/abs/2504.21166</link>
<guid>https://arxiv.org/abs/2504.21166</guid>
<content:encoded><![CDATA[
arXiv:2504.21166v1 Announce Type: cross 
Abstract: The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18\% which shows that the addition of temporal context significantly improves dance style recognition performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Light Weight CNN for classification of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2504.21188</link>
<guid>https://arxiv.org/abs/2504.21188</guid>
<content:encoded><![CDATA[
arXiv:2504.21188v1 Announce Type: cross 
Abstract: This study presents a convolutional neural network (CNN)-based approach for the multi-class classification of brain tumors using magnetic resonance imaging (MRI) scans. We utilize a publicly available dataset containing MRI images categorized into four classes: glioma, meningioma, pituitary tumor, and no tumor. Our primary objective is to build a light weight deep learning model that can automatically classify brain tumor types with high accuracy. To achieve this goal, we incorporate image preprocessing steps, including normalization, data augmentation, and a cropping technique designed to reduce background noise and emphasize relevant regions. The CNN architecture is optimized through hyperparameter tuning using Keras Tuner, enabling systematic exploration of network parameters. To ensure reliable evaluation, we apply 5-fold cross-validation, where each hyperparameter configuration is evaluated across multiple data splits to mitigate overfitting. Experimental results demonstrate that the proposed model achieves a classification accuracy of 98.78%, indicating its potential as a diagnostic aid in clinical settings. The proposed method offers a low-complexity yet effective solution for assisting in early brain tumor diagnosis.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.21189</link>
<guid>https://arxiv.org/abs/2504.21189</guid>
<content:encoded><![CDATA[
arXiv:2504.21189v1 Announce Type: cross 
Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual variability in its progression, complicating accurate prognosis and personalized care planning. This heterogeneity underscores the critical need for predictive models capable of forecasting patient-specific disease trajectories. Artificial Intelligence (AI) offers powerful tools to address this challenge by analyzing complex, multi-modal, and longitudinal patient data. This paper provides a comprehensive survey of AI methodologies applied to personalized AD progression prediction. We review key approaches including state-space models for capturing temporal dynamics, deep learning techniques like Recurrent Neural Networks for sequence modeling, Graph Neural Networks (GNNs) for leveraging network structures, and the emerging concept of AI-driven digital twins for individualized simulation. Recognizing that data limitations often impede progress, we examine common challenges such as high dimensionality, missing data, and dataset imbalance. We further discuss AI-driven mitigation strategies, with a specific focus on synthetic data generation using Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to augment and balance datasets. The survey synthesizes the strengths and limitations of current approaches, emphasizing the trend towards multimodal integration and the persistent need for model interpretability and generalizability. Finally, we identify critical open challenges, including robust external validation, clinical integration, and ethical considerations, and outline promising future research directions such as hybrid models, causal inference, and federated learning. This review aims to consolidate current knowledge and guide future efforts in developing clinically relevant AI tools for personalized AD prognostication.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2504.21190</link>
<guid>https://arxiv.org/abs/2504.21190</guid>
<content:encoded><![CDATA[
arXiv:2504.21190v1 Announce Type: cross 
Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA MoE), a novel computational framework integrating Parameter-Efficient Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in large model deployments. Unlike traditional MoE approaches, which face substantial computational overhead as expert counts grow, TT-LoRA MoE decomposes training into two distinct, optimized stages. First, we independently train lightweight, tensorized low-rank adapters (TT-LoRA experts), each specialized for specific tasks. Subsequently, these expert adapters remain frozen, eliminating inter-task interference and catastrophic forgetting in multi-task setting. A sparse MoE router, trained separately, dynamically leverages base model representations to select exactly one specialized adapter per input at inference time, automating expert selection without explicit task specification. Comprehensive experiments confirm our architecture retains the memory efficiency of low-rank adapters, seamlessly scales to large expert pools, and achieves robust task-level optimization. This structured decoupling significantly enhances computational efficiency and flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling practical and scalable multi-task inference deployments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare</title>
<link>https://arxiv.org/abs/2504.21191</link>
<guid>https://arxiv.org/abs/2504.21191</guid>
<content:encoded><![CDATA[
arXiv:2504.21191v1 Announce Type: cross 
Abstract: This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping</title>
<link>https://arxiv.org/abs/2504.21194</link>
<guid>https://arxiv.org/abs/2504.21194</guid>
<content:encoded><![CDATA[
arXiv:2504.21194v1 Announce Type: cross 
Abstract: This paper presents a novel approach to geolocating images captured from the International Space Station (ISS) using advanced machine learning algorithms. Despite having precise ISS coordinates, the specific Earth locations depicted in astronaut-taken photographs often remain unidentified. Our research addresses this gap by employing three distinct image processing pipelines: a Neural Network based approach, a SIFT based method, and GPT-4 model. Each pipeline is tailored to process high-resolution ISS imagery, identifying both natural and man-made geographical features. Through extensive evaluation on a diverse dataset of over 140 ISS images, our methods demonstrate significant promise in automated geolocation with varied levels of success. The NN approach showed a high success rate in accurately matching geographical features, while the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided enriched geographical descriptions alongside location predictions. This research contributes to the fields of remote sensing and Earth observation by enhancing the accuracy and efficiency of geolocating space-based imagery, thereby aiding environmental monitoring and global mapping efforts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves</title>
<link>https://arxiv.org/abs/2504.21195</link>
<guid>https://arxiv.org/abs/2504.21195</guid>
<content:encoded><![CDATA[
arXiv:2504.21195v1 Announce Type: cross 
Abstract: Extreme heat is the deadliest weather-related hazard in the United States. Furthermore, it is increasing in intensity, frequency, and duration, making skillful forecasts vital to protecting life and property. Traditional numerical weather prediction (NWP) models struggle with extreme heat for medium-range and subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial intelligence-based weather prediction (AIWP) models are progressing rapidly. However, it is largely unknown how well AIWP models forecast extremes, especially for medium-range and S2S timescales. This study investigates 2-m temperature forecasts for 60 heat waves across the four boreal seasons and over four CONUS regions at lead times up to 20 days, using two AIWP models (Google GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study analyses show that both AIWP models and the UFS GEFS exhibit consistent cold biases on regional scales in the 5-10 days of lead time before heat wave onset. GraphCast is the more skillful AIWP model, outperforming UFS GEFS and Pangu-Weather in most locations. Next, the two AIWP models are isolated and analyzed across all heat waves and seasons, with events split among the model's testing (2018-2023) and training (1979-2017) periods. There are cold biases before and during the heat waves in both models and all seasons, except Pangu-Weather in winter, which exhibits a mean warm bias before heat wave onset. Overall, results offer encouragement that AIWP models may be useful for medium-range and S2S predictability of extreme heat.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Legal Writing Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2504.21202</link>
<guid>https://arxiv.org/abs/2504.21202</guid>
<content:encoded><![CDATA[
arXiv:2504.21202v1 Announce Type: cross 
Abstract: Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories</title>
<link>https://arxiv.org/abs/2504.21205</link>
<guid>https://arxiv.org/abs/2504.21205</guid>
<content:encoded><![CDATA[
arXiv:2504.21205v1 Announce Type: cross 
Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure code generation in real-world repositories. SecRepoBench has 318 code generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19 state-of-the-art LLMs using our benchmark and find that the models struggle with generating correct and secure code. In addition, the performance of LLMs to generate self-contained programs as measured by prior benchmarks do not translate to comparative performance at generating secure and correct code at the repository level in SecRepoBench. We show that the state-of-the-art prompt engineering techniques become less effective when applied to the repository level secure code generation problem. We conduct extensive experiments, including an agentic technique to generate secure code, to demonstrate that our benchmark is currently the most difficult secure coding benchmark, compared to previous state-of-the-art benchmarks. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of LLMs to generate correct and secure code in real-world repositories.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2504.21206</link>
<guid>https://arxiv.org/abs/2504.21206</guid>
<content:encoded><![CDATA[
arXiv:2504.21206v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is homophilic to ensure similar neighbor distribution patterns of nodes. Such an assumption ensures that the learned knowledge is consistent across the local models from all clients. Therefore, these local models can be properly aggregated as a global model without undermining the overall performance. Nevertheless, when the neighbor distribution patterns of nodes vary across different clients (e.g., when clients hold graphs with different levels of heterophily), their local models may gain different and even conflict knowledge from their node-level predictive tasks. Consequently, aggregating these local models usually leads to catastrophic performance deterioration on the global model. To address this challenge, we propose FedHERO, an FGL framework designed to harness and share insights from heterophilic graphs effectively. At the heart of FedHERO is a dual-channel GNN equipped with a structure learner, engineered to discern the structural knowledge encoded in the local graphs. With this specialized component, FedHERO enables the local model for each client to identify and learn patterns that are universally applicable across graphs with different patterns of node neighbor distributions. FedHERO not only enhances the performance of individual client models by leveraging both local and shared structural insights but also sets a new precedent in this field to effectively handle graph data with various node neighbor distribution patterns. We conduct extensive experiments to validate the superior performance of FedHERO against existing alternatives.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces</title>
<link>https://arxiv.org/abs/2504.21211</link>
<guid>https://arxiv.org/abs/2504.21211</guid>
<content:encoded><![CDATA[
arXiv:2504.21211v1 Announce Type: cross 
Abstract: Wildlife trafficking remains a critical global issue, significantly impacting biodiversity, ecological stability, and public health. Despite efforts to combat this illicit trade, the rise of e-commerce platforms has made it easier to sell wildlife products, putting new pressure on wild populations of endangered and threatened species. The use of these platforms also opens a new opportunity: as criminals sell wildlife products online, they leave digital traces of their activity that can provide insights into trafficking activities as well as how they can be disrupted. The challenge lies in finding these traces. Online marketplaces publish ads for a plethora of products, and identifying ads for wildlife-related products is like finding a needle in a haystack. Learning classifiers can automate ad identification, but creating them requires costly, time-consuming data labeling that hinders support for diverse ads and research questions. This paper addresses a critical challenge in the data science pipeline for wildlife trafficking analytics: generating quality labeled data for classifiers that select relevant data. While large language models (LLMs) can directly label advertisements, doing so at scale is prohibitively expensive. We propose a cost-effective strategy that leverages LLMs to generate pseudo labels for a small sample of the data and uses these labels to create specialized classification models. Our novel method automatically gathers diverse and representative samples to be labeled while minimizing the labeling costs. Our experimental evaluation shows that our classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We present real use cases that demonstrate the effectiveness of our approach in enabling analyses of different aspects of wildlife trafficking.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining Large Brain Language Model for Active BCI: Silent Speech</title>
<link>https://arxiv.org/abs/2504.21214</link>
<guid>https://arxiv.org/abs/2504.21214</guid>
<content:encoded><![CDATA[
arXiv:2504.21214v1 Announce Type: cross 
Abstract: This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\% accuracy on semantic-level classification and 39.6\% in word-level classification, outperforming baseline methods by 5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemeBLIP2: A novel lightweight multimodal system to detect harmful memes</title>
<link>https://arxiv.org/abs/2504.21226</link>
<guid>https://arxiv.org/abs/2504.21226</guid>
<content:encoded><![CDATA[
arXiv:2504.21226v1 Announce Type: cross 
Abstract: Memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. In this paper, we introduces MemeBLIP2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. We build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. Using BLIP-2 as the core vision-language model, our system is evaluated on the PrideMM datasets. The results show that MemeBLIP2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2504.21228</link>
<guid>https://arxiv.org/abs/2504.21228</guid>
<content:encoded><![CDATA[
arXiv:2504.21228v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection</title>
<link>https://arxiv.org/abs/2504.21231</link>
<guid>https://arxiv.org/abs/2504.21231</guid>
<content:encoded><![CDATA[
arXiv:2504.21231v1 Announce Type: cross 
Abstract: Neck ultrasound (US) plays a vital role in airway management by providing non-invasive, real-time imaging that enables rapid and precise interventions. Deep learning-based anatomical landmark detection in neck US can further facilitate procedural efficiency. However, class imbalance within datasets, where key structures like tracheal rings and vocal folds are underrepresented, presents significant challenges for object detection models. To address this, we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to generate high-quality synthetic samples for underrepresented classes. This approach, rarely explored in the ultrasound domain, improves the representation of minority classes. Experimental results using YOLOv9 for anatomical landmark detection in neck US demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2, significantly surpassing the baseline of 66. This highlights its potential as a computationally efficient and scalable solution for mitigating class imbalance in AI-assisted ultrasound-guided interventions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs</title>
<link>https://arxiv.org/abs/2504.21235</link>
<guid>https://arxiv.org/abs/2504.21235</guid>
<content:encoded><![CDATA[
arXiv:2504.21235v1 Announce Type: cross 
Abstract: We present a lattice-based scheme for homomorphic evaluation of quantum programs and proofs that remains secure against quantum adversaries. Classical homomorphic encryption is lifted to the quantum setting by replacing composite-order groups with Module Learning-With-Errors (MLWE) lattices and by generalizing polynomial functors to bounded natural super functors (BNSFs). A secret depolarizing BNSF mask hides amplitudes, while each quantum state is stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game that allows coherent access to the encryption oracle and give a four-hybrid reduction to decisional MLWE.
  The design also covers practical issues usually left open. A typed QC-bridge keeps classical bits produced by measurements encrypted yet still usable as controls, with weak-measurement semantics for expectation-value workloads. Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them but cannot read them. A rho-calculus driver schedules encrypted tasks across several QPUs and records an auditable trace on an RChain-style ledger.
  Performance analysis shows that the extra lattice arithmetic fits inside today's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof runs in about 10 ms, the public key (seed only) is 32 bytes, and even a CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes homomorphic teleportation plus knowledge-base-relative amplitude checks appears feasible with current hardware. These results indicate that fully homomorphic, knowledge-base-aware quantum reasoning is compatible with near-term quantum clouds and standard post-quantum security assumptions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization and Knowledge Injection in Gated LLMs</title>
<link>https://arxiv.org/abs/2504.21239</link>
<guid>https://arxiv.org/abs/2504.21239</guid>
<content:encoded><![CDATA[
arXiv:2504.21239v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) currently struggle to sequentially add new memories and integrate new knowledge. These limitations contrast with the human ability to continuously learn from new experiences and acquire knowledge throughout life. Most existing approaches add memories either through large context windows or external memory buffers (e.g., Retrieval-Augmented Generation), and studies on knowledge injection rarely test scenarios resembling everyday life events. In this work, we introduce a continual learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event memories directly into the weights of LLMs. Each memory is stored in a dedicated set of gated low-rank weights. During inference, a gating mechanism activates relevant memory weights by matching query embeddings to stored memory embeddings. This enables the model to both recall entire memories and answer related questions. On two datasets - fictional characters and Wikipedia events - MEGa outperforms baseline approaches in mitigating catastrophic forgetting. Our model draws inspiration from the complementary memory system of the human brain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain Causal Discovery in Bijective Causal Models</title>
<link>https://arxiv.org/abs/2504.21261</link>
<guid>https://arxiv.org/abs/2504.21261</guid>
<content:encoded><![CDATA[
arXiv:2504.21261v1 Announce Type: cross 
Abstract: We consider the problem of causal discovery (a.k.a., causal structure learning) in a multi-domain setting. We assume that the causal functions are invariant across the domains, while the distribution of the exogenous noise may vary. Under causal sufficiency (i.e., no confounders exist), we show that the causal diagram can be discovered under less restrictive functional assumptions compared to previous work. What enables causal discovery in this setting is bijective generation mechanisms (BGM), which ensures that the functional relation between the exogenous noise $E$ and the endogenous variable $Y$ is bijective and differentiable in both directions at every level of the cause variable $X = x$. BGM generalizes a variety of models including additive noise model, LiNGAM, post-nonlinear model, and location-scale noise model. Further, we derive a statistical test to find the parents set of the target variable. Experiments on various synthetic and real-world datasets validate our theoretical findings.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing LLM code generation quality through path planning tasks</title>
<link>https://arxiv.org/abs/2504.21276</link>
<guid>https://arxiv.org/abs/2504.21276</guid>
<content:encoded><![CDATA[
arXiv:2504.21276v1 Announce Type: cross 
Abstract: As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction</title>
<link>https://arxiv.org/abs/2504.21289</link>
<guid>https://arxiv.org/abs/2504.21289</guid>
<content:encoded><![CDATA[
arXiv:2504.21289v1 Announce Type: cross 
Abstract: Biclustering is an effective technique in data mining and pattern recognition. Biclustering algorithms based on traditional clustering face two fundamental limitations when processing high-dimensional data: (1) The distance concentration phenomenon in high-dimensional spaces leads to data sparsity, rendering similarity measures ineffective; (2) Mainstream linear dimensionality reduction methods disrupt critical local structural patterns. To apply biclustering to high-dimensional datasets, we propose an orthogonal factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal factors in the vector space of the high-dimensional dataset. Then, we performed clustering using the coordinates of the original data in the orthogonal subspace as clustering targets. Finally, we obtained biclustering results of the original dataset. Since dimensionality reduction was applied before clustering, the proposed algorithm effectively mitigated the data sparsity problem caused by high dimensionality. Additionally, we applied this biclustering algorithm to stock technical indicator combinations and stock price trend prediction. Biclustering results were transformed into fuzzy rules, and we incorporated profit-preserving and stop-loss rules into the rule set, ultimately forming a fuzzy inference system for stock price trend predictions and trading signals. To evaluate the performance of BCBOF, we compared it with existing biclustering methods using multiple evaluation metrics. The results showed that our algorithm outperformed other biclustering techniques. To validate the effectiveness of the fuzzy inference system, we conducted virtual trading experiments using historical data from 10 A-share stocks. The experimental results showed that the generated trading strategies yielded higher returns for investors.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness in Graph Learning Augmented with Machine Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.21296</link>
<guid>https://arxiv.org/abs/2504.21296</guid>
<content:encoded><![CDATA[
arXiv:2504.21296v1 Announce Type: cross 
Abstract: Augmenting specialised machine learning techniques into traditional graph learning models has achieved notable success across various domains, including federated graph learning, dynamic graph learning, and graph transformers. However, the intricate mechanisms of these specialised techniques introduce significant challenges in maintaining model fairness, potentially resulting in discriminatory outcomes in high-stakes applications such as recommendation systems, disaster response, criminal justice, and loan approval. This paper systematically examines the unique fairness challenges posed by Graph Learning augmented with Machine Learning (GL-ML). It highlights the complex interplay between graph learning mechanisms and machine learning techniques, emphasising how the augmentation of machine learning both enhances and complicates fairness. Additionally, we explore four critical techniques frequently employed to improve fairness in GL-ML methods. By thoroughly investigating the root causes and broader implications of fairness challenges in this rapidly evolving field, this work establishes a robust foundation for future research and innovation in GL-ML fairness.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI</title>
<link>https://arxiv.org/abs/2504.21297</link>
<guid>https://arxiv.org/abs/2504.21297</guid>
<content:encoded><![CDATA[
arXiv:2504.21297v1 Announce Type: cross 
Abstract: This paper introduces a conversational interface system that enables participatory design of differentially private AI systems in public sector applications. Addressing the challenge of balancing mathematical privacy guarantees with democratic accountability, we propose three key contributions: (1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria decision analysis to align citizen preferences with differential privacy (DP) parameters, (2) an explainable noise-injection framework featuring real-time Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and (3) an integrated legal-compliance mechanism that dynamically modulates privacy budgets based on evolving regulatory constraints. Our results advance participatory AI practices by demonstrating how conversational interfaces can enhance public engagement in algorithmic privacy mechanisms, ensuring that privacy-preserving AI in public sector governance remains both mathematically robust and democratically accountable.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Backdoor the Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.21323</link>
<guid>https://arxiv.org/abs/2504.21323</guid>
<content:encoded><![CDATA[
arXiv:2504.21323v1 Announce Type: cross 
Abstract: Knowledge distillation has become a cornerstone in modern machine learning systems, celebrated for its ability to transfer knowledge from a large, complex teacher model to a more efficient student model. Traditionally, this process is regarded as secure, assuming the teacher model is clean. This belief stems from conventional backdoor attacks relying on poisoned training data with backdoor triggers and attacker-chosen labels, which are not involved in the distillation process. Instead, knowledge distillation uses the outputs of a clean teacher model to guide the student model, inherently preventing recognition or response to backdoor triggers as intended by an attacker. In this paper, we challenge this assumption by introducing a novel attack methodology that strategically poisons the distillation dataset with adversarial examples embedded with backdoor triggers. This technique allows for the stealthy compromise of the student model while maintaining the integrity of the teacher model. Our innovative approach represents the first successful exploitation of vulnerabilities within the knowledge distillation process using clean teacher models. Through extensive experiments conducted across various datasets and attack settings, we demonstrate the robustness, stealthiness, and effectiveness of our method. Our findings reveal previously unrecognized vulnerabilities and pave the way for future research aimed at securing knowledge distillation processes against backdoor attacks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-function Decomposition with Intervention Semantics with Factored Action Spaces</title>
<link>https://arxiv.org/abs/2504.21326</link>
<guid>https://arxiv.org/abs/2504.21326</guid>
<content:encoded><![CDATA[
arXiv:2504.21326v1 Announce Type: cross 
Abstract: Many practical reinforcement learning environments have a discrete factored action space that induces a large combinatorial set of actions, thereby posing significant challenges. Existing approaches leverage the regular structure of the action space and resort to a linear decomposition of Q-functions, which avoids enumerating all combinations of factored actions. In this paper, we consider Q-functions defined over a lower dimensional projected subspace of the original action space, and study the condition for the unbiasedness of decomposed Q-functions using causal effect estimation from the no unobserved confounder setting in causal statistics. This leads to a general scheme which we call action decomposed reinforcement learning that uses the projected Q-functions to approximate the Q-function in standard model-free reinforcement learning algorithms. The proposed approach is shown to improve sample complexity in a model-based reinforcement learning setting. We demonstrate improvements in sample efficiency compared to state-of-the-art baselines in online continuous control environments and a real-world offline sepsis treatment environment.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection</title>
<link>https://arxiv.org/abs/2504.21344</link>
<guid>https://arxiv.org/abs/2504.21344</guid>
<content:encoded><![CDATA[
arXiv:2504.21344v1 Announce Type: cross 
Abstract: Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</title>
<link>https://arxiv.org/abs/2504.21356</link>
<guid>https://arxiv.org/abs/2504.21356</guid>
<content:encoded><![CDATA[
arXiv:2504.21356v1 Announce Type: cross 
Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm's training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements across the field.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting</title>
<link>https://arxiv.org/abs/2504.21358</link>
<guid>https://arxiv.org/abs/2504.21358</guid>
<content:encoded><![CDATA[
arXiv:2504.21358v1 Announce Type: cross 
Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for which Machine Learning (ML) methods have been extensively explored to develop data-driven Artificial Intelligence (AI) solutions. Recent research focuses on modelling spatial-temporal correlations for short-term traffic prediction, leaving the favourable long-term forecasting a challenging and open issue. This paper presents a comparative study on large-scale real-world signalized arterials and freeway traffic flow datasets, aiming to evaluate promising ML methods in the context of large forecasting horizons up to 30 days. Focusing on modelling capacity for temporal dynamics, we develop one ensemble ML method, eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods, including Recurrent Neural Network (RNN)-based methods and the state-of-the-art Transformer-based method. Time embedding is leveraged to enhance their understanding of seasonality and event factors. Experimental results highlight that while the attention mechanism/Transformer framework is effective for capturing long-range dependencies in sequential data, as the forecasting horizon extends, the key to effective traffic forecasting gradually shifts from temporal dependency capturing to periodicity modelling. Time embedding is particularly effective in this context, helping naive RNN outperform Informer by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust model, XGBoost, while learning solely from time features, performs competitively with DL methods. Moreover, we investigate the impacts of various factors like input sequence length, holiday traffic, data granularity, and training data size. The findings offer valuable insights and serve as a reference for future long-term traffic forecasting research and the improvement of AI's corresponding learning capabilities.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion</title>
<link>https://arxiv.org/abs/2504.21366</link>
<guid>https://arxiv.org/abs/2504.21366</guid>
<content:encoded><![CDATA[
arXiv:2504.21366v1 Announce Type: cross 
Abstract: Current Audio-Visual Source Separation methods primarily adopt two design strategies. The first strategy involves fusing audio and visual features at the bottleneck layer of the encoder, followed by processing the fused features through the decoder. However, when there is a significant disparity between the two modalities, this approach may lead to the loss of critical information. The second strategy avoids direct fusion and instead relies on the decoder to handle the interaction between audio and visual features. Nonetheless, if the encoder fails to integrate information across modalities adequately, the decoder may be unable to effectively capture the complex relationships between them. To address these issues, this paper proposes a dynamic fusion method based on a gating mechanism that dynamically adjusts the modality fusion degree. This approach mitigates the limitations of solely relying on the decoder and facilitates efficient collaboration between audio and visual features. Additionally, an audio attention module is introduced to enhance the expressive capacity of audio features, thereby further improving model performance. Experimental results demonstrate that our method achieves significant performance improvements on two benchmark datasets, validating its effectiveness and advantages in Audio-Visual Source Separation tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality</title>
<link>https://arxiv.org/abs/2504.21368</link>
<guid>https://arxiv.org/abs/2504.21368</guid>
<content:encoded><![CDATA[
arXiv:2504.21368v1 Announce Type: cross 
Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction model and trained with a linear-$\beta$ noise schedule that spends much of its sampling steps at high noise levels. Because high noise levels are associated with recovering large-scale image structures and low noise levels with recovering details, this configuration can result in low-quality and blurry images. However, it should be possible to improve details while spending fewer steps recovering structures because the latent code should already contain structural information. Based on this insight, we propose a new DAE training method that improves the quality of reconstructed images. We divide training into two phases. In the first phase, the DAE is trained as a vanilla autoencoder by always setting the noise level to the highest, forcing the encoder and decoder to populate the latent code with structural information. In the second phase, we incorporate a noise schedule that spends more time in the low-noise region, allowing the DAE to learn how to perfect the details. Our method results in images that have accurate high-level structures and low-level details while still preserving useful properties of the latent codes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction</title>
<link>https://arxiv.org/abs/2504.21372</link>
<guid>https://arxiv.org/abs/2504.21372</guid>
<content:encoded><![CDATA[
arXiv:2504.21372v1 Announce Type: cross 
Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.21383</link>
<guid>https://arxiv.org/abs/2504.21383</guid>
<content:encoded><![CDATA[
arXiv:2504.21383v1 Announce Type: cross 
Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning (RL) have primarily focused on addressing function approximation errors, which contribute to the overestimation of Q-values for out-of-distribution actions, a challenge that static datasets exacerbate. However, high stakes applications such as recommendation systems in online gaming, introduce further complexities due to player's psychology (intent) driven by gameplay experiences and the inherent volatility on the platform. These factors create highly sparse, partially overlapping state spaces across policies, further influenced by the experiment path selection logic which biases state spaces towards specific policies. Current SOTA methods constrain learning from such offline data by clipping known counterfactual actions as out-of-distribution due to poor generalization across unobserved states. Further aggravating conservative Q-learning and necessitating more online exploration. FAST-Q introduces a novel approach that (1) leverages Gradient Reversal Learning to construct balanced state representations, regularizing the policy-specific bias between the player's state and action thereby enabling counterfactual estimation; (2) supports offline counterfactual exploration in parallel with static data exploitation; and (3) proposes a Q-value decomposition strategy for multi-objective optimization, facilitating explainable recommendations over short and long-term objectives. These innovations demonstrate superiority of FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4 percent enhancement in the recommendation driven engagement, 2 percent improvement in the player's platform dwell time and an impressive 10 percent reduction in the costs associated with the recommendation, on our volatile gaming platform.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Galvatron: An Automatic Distributed System for Efficient Foundation Model Training</title>
<link>https://arxiv.org/abs/2504.21411</link>
<guid>https://arxiv.org/abs/2504.21411</guid>
<content:encoded><![CDATA[
arXiv:2504.21411v1 Announce Type: cross 
Abstract: Galvatron is a distributed system for efficiently training large-scale Foundation Models. It overcomes the complexities of selecting optimal parallelism strategies by automatically identifying the most efficient hybrid strategy, incorporating data, tensor, pipeline, sharded data, and sequence parallelism, along with recomputation. The system's architecture includes a profiler for hardware and model analysis, a search engine for strategy optimization using decision trees and dynamic programming, and a runtime for executing these strategies efficiently. Benchmarking on various clusters demonstrates Galvatron's superior throughput compared to existing frameworks. This open-source system offers user-friendly interfaces and comprehensive documentation, making complex distributed training accessible and efficient. The source code of Galvatron is available at https://github.com/PKU-DAIR/Hetu-Galvatron.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges</title>
<link>https://arxiv.org/abs/2504.21415</link>
<guid>https://arxiv.org/abs/2504.21415</guid>
<content:encoded><![CDATA[
arXiv:2504.21415v1 Announce Type: cross 
Abstract: User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intrusive, and adaptable solution. However, challenges remain in determining the optimal data volume, balancing accuracy and practicality, and effectively capturing temporal behavioral patterns. In this study, we propose a statistical method using Gaussian kernel density estimate (KDE) and Kullback-Leibler (KL) divergence to estimate the sufficient data volume for training authentication models. We introduce the Mouse Authentication Unit (MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for efficient and accurate behavioral representation. Furthermore, we design the Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet for local feature extraction and GRU for modeling long-term temporal dependencies. Taking the Balabit and DFL datasets as examples, we significantly reduced the data scale, particularly by a factor of 10 for the DFL dataset, greatly alleviating the training burden. Additionally, we determined the optimal input recognition unit length for the user authentication system on different datasets based on the slope of Approximate Entropy. Training with imbalanced samples, our model achieved a successful defense AUC 98.52% for blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing the current sota performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers</title>
<link>https://arxiv.org/abs/2504.21427</link>
<guid>https://arxiv.org/abs/2504.21427</guid>
<content:encoded><![CDATA[
arXiv:2504.21427v1 Announce Type: cross 
Abstract: Accurate classification of EEG signals is crucial for brain-computer interfaces (BCIs) and neuroprosthetic applications, yet many existing methods fail to account for the non-Euclidean, manifold structure of EEG data, resulting in suboptimal performance. Preserving this manifold information is essential to capture the true geometry of EEG signals, but traditional classification techniques largely overlook this need. To this end, we propose MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers), that introduces two key innovations: (1) a feature engineering phase that combines covariance matrices and Radial Basis Function (RBF) kernels to capture both linear and non-linear relationships among EEG channels, and (2) a clustering phase that employs a modified K-means algorithm tailored for the Riemannian manifold space, ensuring local geometric sensitivity. Ensembling multiple clustering-based classifiers, MPEC achieves superior results, validated by significant improvements on the BCI Competition IV dataset 2a.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV Marketplace Simulation Tool for BVLOS Operations</title>
<link>https://arxiv.org/abs/2504.21428</link>
<guid>https://arxiv.org/abs/2504.21428</guid>
<content:encoded><![CDATA[
arXiv:2504.21428v1 Announce Type: cross 
Abstract: We present a simulation tool for evaluating team formation in autonomous multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of Sight (BVLOS). The tool models UAV collaboration and mission execution in dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt operations. Our tool allows researchers to integrate and compare various team formation strategies in a controlled environment with configurable mission parameters and adversarial behaviors. The log of each simulation run is stored in a structured way along with performance metrics so that statistical analysis could be done straightforwardly. The tool is versatile for testing and improving UAV coordination strategies in real-world applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v1 Announce Type: cross 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \textbf{series}. To address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on \textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Visual Layer Selection in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.21447</link>
<guid>https://arxiv.org/abs/2504.21447</guid>
<content:encoded><![CDATA[
arXiv:2504.21447v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments</title>
<link>https://arxiv.org/abs/2504.21454</link>
<guid>https://arxiv.org/abs/2504.21454</guid>
<content:encoded><![CDATA[
arXiv:2504.21454v1 Announce Type: cross 
Abstract: The use of machine learning in cyber-physical systems has attracted the interest of both industry and academia. However, no general solution has yet been found against the unpredictable behavior of neural networks and reinforcement learning agents. Nevertheless, the improvements of photo-realistic simulators have paved the way towards extensive testing of complex algorithms in different virtual scenarios, which would be expensive and dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot interaction with virtual environments, which operates as a vehicle-in-the-loop platform, rendering a virtual world while operating the vehicle in the real world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be configured to move its digital twin in a virtual world built with the Unreal Engine 5 graphic engine, which can be populated with objects, people, or other vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds while being light-weight to contain execution times and allow fast rendering. Its main advantage lies in the possibility of testing complex algorithms on the full software and hardware stack while minimizing the risks and costs of a test campaign. The framework has been validated by testing a reinforcement learning agent trained for obstacle avoidance on an AgileX Scout Mini rover that navigates a virtual office environment where everyday objects and people are placed as obstacles. The physical rover moves with no collision in an indoor limited space, thanks to a LiDAR-based heuristic.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xEEGNet: Towards Explainable AI in EEG Dementia Classification</title>
<link>https://arxiv.org/abs/2504.21457</link>
<guid>https://arxiv.org/abs/2504.21457</guid>
<content:encoded><![CDATA[
arXiv:2504.21457v1 Announce Type: cross 
Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network for EEG data analysis. It is fully interpretable and reduces overfitting through major parameter reduction. As an applicative use case, we focused on classifying common dementia conditions, Alzheimer's and frontotemporal dementia, versus controls. xEEGNet is broadly applicable to other neurological conditions involving spectral alterations. We initially used ShallowNet, a simple and popular model from the EEGNet-family. Its structure was analyzed and gradually modified to move from a "black box" to a more transparent model, without compromising performance. The learned kernels and weights were examined from a clinical standpoint to assess medical relevance. Model variants, including ShallowNet and the final xEEGNet, were evaluated using robust Nested-Leave-N-Subjects-Out cross-validation for unbiased performance estimates. Variability across data splits was explained using embedded EEG representations, grouped by class and set, with pairwise separability to quantify group distinction. Overfitting was assessed through training-validation loss correlation and training speed. xEEGNet uses only 168 parameters, 200 times fewer than ShallowNet, yet retains interpretability, resists overfitting, achieves comparable median performance (-1.5%), and reduces variability across splits. This variability is explained by embedded EEG representations: higher accuracy correlates with greater separation between test set controls and Alzheimer's cases, without significant influence from training data. xEEGNet's ability to filter specific EEG bands, learn band-specific topographies, and use relevant spectral features demonstrates its interpretability. While large deep learning models are often prioritized for performance, this study shows smaller architectures like xEEGNet can be equally effective in EEG pathology classification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging</title>
<link>https://arxiv.org/abs/2504.21474</link>
<guid>https://arxiv.org/abs/2504.21474</guid>
<content:encoded><![CDATA[
arXiv:2504.21474v1 Announce Type: cross 
Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines</title>
<link>https://arxiv.org/abs/2504.21475</link>
<guid>https://arxiv.org/abs/2504.21475</guid>
<content:encoded><![CDATA[
arXiv:2504.21475v1 Announce Type: cross 
Abstract: This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.21476</link>
<guid>https://arxiv.org/abs/2504.21476</guid>
<content:encoded><![CDATA[
arXiv:2504.21476v1 Announce Type: cross 
Abstract: Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present \textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is $\textbf{10}\times$ shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense</title>
<link>https://arxiv.org/abs/2504.21480</link>
<guid>https://arxiv.org/abs/2504.21480</guid>
<content:encoded><![CDATA[
arXiv:2504.21480v1 Announce Type: cross 
Abstract: With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS</title>
<link>https://arxiv.org/abs/2504.21489</link>
<guid>https://arxiv.org/abs/2504.21489</guid>
<content:encoded><![CDATA[
arXiv:2504.21489v1 Announce Type: cross 
Abstract: The rise of generative AI and deceptive synthetic media threatens the global information ecosystem, especially across the Global Majority. This report from WITNESS highlights the limitations of current AI detection tools, which often underperform in real-world scenarios due to challenges related to explainability, fairness, accessibility, and contextual relevance. In response, WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED) Benchmark, a new framework for evaluating detection tools based on their real-world impact and capacity for innovation. Drawing on frontline experiences, deceptive AI cases, and global consultations, the report outlines how detection tools must evolve to become truly innovative and relevant by meeting diverse linguistic, cultural, and technological contexts. It offers practical guidance for developers, policymakers, and standards bodies to design accountable, transparent, and user-centered detection solutions, and incorporate sociotechnical considerations into future AI standards, procedures and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can drive innovation, safeguard public trust, strengthen AI literacy, and contribute to a more resilient global information credibility.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2504.21491</link>
<guid>https://arxiv.org/abs/2504.21491</guid>
<content:encoded><![CDATA[
arXiv:2504.21491v1 Announce Type: cross 
Abstract: We propose a result-level category-specific fusion architecture called ClassWise-CRF. This architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture treats the segmentation predictions from multiple networks as confidence vector fields. It leverages segmentation metrics (such as Intersection over Union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. This fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. Building on this, the architecture further optimizes the fused results using unary and pairwise potentials in CRF to ensure spatial consistency and boundary accuracy. To validate the effectiveness of ClassWise-CRF, we conducted experiments on two remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced semantic segmentation networks. The results show that the ClassWise-CRF architecture significantly improves segmentation performance: on the LoveDA dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the Vaihingen dataset, the mIoU improved by 0.87% on the validation set and by 0.91% on the test set. These results fully demonstrate the effectiveness and generality of the ClassWise-CRF architecture in semantic segmentation of remote sensing images. The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta knowledge assisted Evolutionary Neural Architecture Search</title>
<link>https://arxiv.org/abs/2504.21545</link>
<guid>https://arxiv.org/abs/2504.21545</guid>
<content:encoded><![CDATA[
arXiv:2504.21545v1 Announce Type: cross 
Abstract: Evolutionary computation (EC)-based neural architecture search (NAS) has achieved remarkable performance in the automatic design of neural architectures. However, the high computational cost associated with evaluating searched architectures poses a challenge for these methods, and a fixed form of learning rate (LR) schedule means greater information loss on diverse searched architectures. This paper introduces an efficient EC-based NAS method to solve these problems via an innovative meta-learning framework. Specifically, a meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a suitable LR schedule, which guides the training process with lower information loss when evaluating each individual. An adaptive surrogate model is designed through an adaptive threshold to select the potential architectures in a few epochs and then evaluate the potential architectures with complete epochs. Additionally, a periodic mutation operator is proposed to increase the diversity of the population, which enhances the generalizability and robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets demonstrate that the proposed method achieves high performance comparable to that of many state-of-the-art peer methods, with lower computational cost and greater robustness.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2504.21559</link>
<guid>https://arxiv.org/abs/2504.21559</guid>
<content:encoded><![CDATA[
arXiv:2504.21559v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes</title>
<link>https://arxiv.org/abs/2504.21562</link>
<guid>https://arxiv.org/abs/2504.21562</guid>
<content:encoded><![CDATA[
arXiv:2504.21562v1 Announce Type: cross 
Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire gastrointestinal tract, and is a pain-free alternative to traditional endoscopy. It generates extensive video data that requires significant review time, and localizing the capsule after ingestion is a challenge. Techniques like bleeding detection and depth estimation can help with localization of pathologies, but deep learning models are typically too large to run directly on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation are trained on capsule endoscopic images. For monocular depth estimation, we distill a large foundation model into the lean NCA architecture, by treating the outputs of the foundation model as pseudo ground truth. We then port the trained NCA to the ESP32 microcontroller, enabling efficient image processing on hardware as small as a camera capsule. NCA are more accurate (Dice) than other portable segmentation models, while requiring more than 100x fewer parameters stored in memory than other small-scale models. The visual results of NCA depth estimation look convincing, and in some cases beat the realism and detail of the pseudo ground truth. Runtime optimizations on the ESP32-S3 accelerate the average inference speed significantly, by more than factor 3. With several algorithmic adjustments and distillation, it is possible to eNCApsulate NCA models into microcontrollers that fit into wireless capsule endoscopes. This is the first work that enables reliable bleeding segmentation and depth estimation on a miniaturized device, paving the way for precise diagnosis combined with visual odometry as a means of precise localization of the capsule -- on the capsule.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards proactive self-adaptive AI for non-stationary environments with dataset shifts</title>
<link>https://arxiv.org/abs/2504.21565</link>
<guid>https://arxiv.org/abs/2504.21565</guid>
<content:encoded><![CDATA[
arXiv:2504.21565v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) models deployed in production frequently face challenges in maintaining their performance in non-stationary environments. This issue is particularly noticeable in medical settings, where temporal dataset shifts often occur. These shifts arise when the distributions of training data differ from those of the data encountered during deployment over time. Further, new labeled data to continuously retrain AI is not typically available in a timely manner due to data access limitations. To address these challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive, where we model the temporal trajectory of AI parameters, allowing us to short-term forecast parameter values. To this end, we use polynomial spline bases, within an extensible Functional Data Analysis framework. We validate our methodology with a logistic regression model addressing prior probability shift, covariate shift, and concept shift. This validation is conducted on both a controlled simulated dataset and a publicly available real-world COVID-19 dataset from Mexico, with various shifts occurring between 2020 and 2024. Our results indicate that this approach enhances the performance of AI against shifts compared to baseline stable models trained at different time distances from the present, without requiring updated training data. This work lays the foundation for pro-adaptive AI research against dynamic, non-stationary environments, being compatible with data protection, in resilient AI production environments for health.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework</title>
<link>https://arxiv.org/abs/2504.21582</link>
<guid>https://arxiv.org/abs/2504.21582</guid>
<content:encoded><![CDATA[
arXiv:2504.21582v1 Announce Type: cross 
Abstract: Simulating collective decision-making involves more than aggregating individual behaviors; it arises from dynamic interactions among individuals. While large language models (LLMs) show promise for social simulation, existing approaches often exhibit deviations from real-world data. To address this gap, we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the feedback loop between micro-level decisions and macro-level population. MF-LLM alternates between two models: a policy model that generates individual actions based on personal states and group-level information, and a mean field model that updates the population distribution from the latest individual decisions. Together, they produce rollouts that simulate the evolving trajectories of collective decision-making. To better match real-world data, we introduce IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck principle, which maximizes the relevance of population distributions to future actions while minimizing redundancy with historical data. We evaluate MF-LLM on a real-world social dataset, where it reduces KL divergence to human population distributions by 47 percent over non-mean-field baselines, and enables accurate trend forecasting and intervention planning. It generalizes across seven domains and four LLM backbones, providing a scalable foundation for high-fidelity social simulation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.21585</link>
<guid>https://arxiv.org/abs/2504.21585</guid>
<content:encoded><![CDATA[
arXiv:2504.21585v1 Announce Type: cross 
Abstract: This paper tackles the challenge of learning multi-goal dexterous hand manipulation tasks using model-based Reinforcement Learning. We propose Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing probabilistic neural network ensembles to describe the high-dimensional dexterous hand dynamics and introducing an asynchronous MPC policy to meet the control frequency requirements in real-world dexterous hand systems. Extensive evaluations on four simulated Shadow Hand manipulation scenarios with randomly generated goals demonstrate GC-PMPC's superior performance over state-of-the-art baselines. It successfully drives a cable-driven Dexterous hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn manipulating a cubic die to three goal poses within approximately 80 minutes of interactions, demonstrating exceptional learning efficiency and control performance on a cost-effective dexterous hand platform.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms</title>
<link>https://arxiv.org/abs/2504.21586</link>
<guid>https://arxiv.org/abs/2504.21586</guid>
<content:encoded><![CDATA[
arXiv:2504.21586v1 Announce Type: cross 
Abstract: In high-speed quadcopter racing, finding a single controller that works well across different platforms remains challenging. This work presents the first neural network controller for drone racing that generalizes across physically distinct quadcopters. We demonstrate that a single network, trained with domain randomization, can robustly control various types of quadcopters. The network relies solely on the current state to directly compute motor commands. The effectiveness of this generalized controller is validated through real-world tests on two substantially different crafts (3-inch and 5-inch race quadcopters). We further compare the performance of this generalized controller with controllers specifically trained for the 3-inch and 5-inch drone, using their identified model parameters with varying levels of domain randomization (0%, 10%, 20%, 30%). While the generalized controller shows slightly slower speeds compared to the fine-tuned models, it excels in adaptability across different platforms. Our results show that no randomization fails sim-to-real transfer while increasing randomization improves robustness but reduces speed. Despite this trade-off, our findings highlight the potential of domain randomization for generalizing controllers, paving the way for universal AI controllers that can adapt to any platform.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing</title>
<link>https://arxiv.org/abs/2504.21589</link>
<guid>https://arxiv.org/abs/2504.21589</guid>
<content:encoded><![CDATA[
arXiv:2504.21589v1 Announce Type: cross 
Abstract: This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning</title>
<link>https://arxiv.org/abs/2504.21596</link>
<guid>https://arxiv.org/abs/2504.21596</guid>
<content:encoded><![CDATA[
arXiv:2504.21596v1 Announce Type: cross 
Abstract: With the rapid advancement of artificial intelligence, there is an increasing demand for intelligent robots capable of assisting humans in daily tasks and performing complex operations. Such robots not only require task planning capabilities but must also execute tasks with stability and robustness. In this paper, we present a closed-loop task planning and acting system, LLM-PAS, which is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans long-horizon tasks in a manner similar to traditional task and motion planners, it also emphasizes the execution phase of the task. By transferring part of the constraint-checking process from the planning phase to the execution phase, LLM-PAS enables exploration of the constraint space and delivers more accurate feedback on environmental anomalies during execution. The reasoning capabilities of the LLM allow it to handle anomalies that cannot be addressed by the robust executor. To further enhance the system's ability to assist the planner during replanning, we propose the First Look Prompting (FLP) method, which induces LLM to generate effective PDDL goals. Through comparative prompting experiments and systematic experiments, we demonstrate the effectiveness and robustness of LLM-PAS in handling anomalous conditions during task execution.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations</title>
<link>https://arxiv.org/abs/2504.21605</link>
<guid>https://arxiv.org/abs/2504.21605</guid>
<content:encoded><![CDATA[
arXiv:2504.21605v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data</title>
<link>https://arxiv.org/abs/2504.21634</link>
<guid>https://arxiv.org/abs/2504.21634</guid>
<content:encoded><![CDATA[
arXiv:2504.21634v1 Announce Type: cross 
Abstract: Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sadeed: Advancing Arabic Diacritization Through Small Language Model</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
arXiv:2504.21635v1 Announce Type: cross 
Abstract: Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning</title>
<link>https://arxiv.org/abs/2504.21685</link>
<guid>https://arxiv.org/abs/2504.21685</guid>
<content:encoded><![CDATA[
arXiv:2504.21685v1 Announce Type: cross 
Abstract: Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction</title>
<link>https://arxiv.org/abs/2504.21692</link>
<guid>https://arxiv.org/abs/2504.21692</guid>
<content:encoded><![CDATA[
arXiv:2504.21692v1 Announce Type: cross 
Abstract: Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling</title>
<link>https://arxiv.org/abs/2504.21695</link>
<guid>https://arxiv.org/abs/2504.21695</guid>
<content:encoded><![CDATA[
arXiv:2504.21695v1 Announce Type: cross 
Abstract: Ego-motion estimation is vital for drones when flying in GPS-denied environments. Vision-based methods struggle when flight speed increases and close-by objects lead to difficult visual conditions with considerable motion blur and large occlusions. To tackle this, vision is typically complemented by state estimation filters that combine a drone model with inertial measurements. However, these drone models are currently learned in a supervised manner with ground-truth data from external motion capture systems, limiting scalability to different environments and drones. In this work, we propose a self-supervised learning scheme to train a neural-network-based drone model using only onboard monocular video and flight controller data (IMU and motor feedback). We achieve this by first training a self-supervised relative pose estimation model, which then serves as a teacher for the drone model. To allow this to work at high speed close to obstacles, we propose an improved occlusion handling method for training self-supervised pose estimation models. Due to this method, the root mean squared error of resulting odometry estimates is reduced by an average of 15%. Moreover, the student neural drone model can be successfully obtained from the onboard data. It even becomes more accurate at higher speeds compared to its teacher, the self-supervised vision-based model. We demonstrate the value of the neural drone model by integrating it into a traditional filter-based VIO system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing trajectories near obstacles. Self-supervised learning of ego-motion estimation represents a significant step toward bridging the gap between flying in controlled, expensive lab environments and real-world drone applications. The fusion of vision and drone models will enable higher-speed flight and improve state estimation, on any drone in any environment.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs</title>
<link>https://arxiv.org/abs/2504.21700</link>
<guid>https://arxiv.org/abs/2504.21700</guid>
<content:encoded><![CDATA[
arXiv:2504.21700v1 Announce Type: cross 
Abstract: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. In response to this, LLM Jailbreaking is a significant threat to such protections, and many previous approaches have already demonstrated its effectiveness across diverse domains. Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted jailbreak attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel jailbreak attack that exploits these unique patterns to break the security constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our attack.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers in Precision Agriculture: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.21706</link>
<guid>https://arxiv.org/abs/2504.21706</guid>
<content:encoded><![CDATA[
arXiv:2504.21706v1 Announce Type: cross 
Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays a key role in maintaining crop health and increasing overall yield. Traditional approaches, though still valuable, often rely on manual inspection or conventional machine learning techniques, both of which face limitations in scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as a promising alternative, offering benefits such as improved handling of long-range dependencies and better scalability for visual tasks. This survey explores the application of ViTs in precision agriculture, covering tasks from classification to detection and segmentation. We begin by introducing the foundational architecture of ViTs and discuss their transition from Natural Language Processing (NLP) to computer vision. The discussion includes the concept of inductive bias in traditional models like Convolutional Neural Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive review of recent literature, focusing on key methodologies, datasets, and performance metrics. The survey also includes a comparative analysis of CNNs and ViTs, with a look at hybrid models and performance enhancements. Technical challenges - such as data requirements, computational demands, and model interpretability - are addressed alongside potential solutions. Finally, we outline potential research directions and technological advancements that could further support the integration of ViTs in real-world agricultural settings. Our goal with this study is to offer practitioners and researchers a deeper understanding of how ViTs are poised to transform smart and precision agriculture.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning</title>
<link>https://arxiv.org/abs/2504.21707</link>
<guid>https://arxiv.org/abs/2504.21707</guid>
<content:encoded><![CDATA[
arXiv:2504.21707v1 Announce Type: cross 
Abstract: We propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions While recent frameworks like Information Contrastive Learning I-Con unify multiple learning paradigms through KL divergence between fixed neighborhood conditionals we argue this view underplays a crucial recursive structure inherent in the learning process. We introduce Recursive KL Divergence Optimization RKDO a dynamic formalism where representation learning is framed as the evolution of KL divergences across data neighborhoods. This formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. Our experiments demonstrate that RKDO offers dual efficiency advantages approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. This suggests that RKDOs recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</title>
<link>https://arxiv.org/abs/2504.21716</link>
<guid>https://arxiv.org/abs/2504.21716</guid>
<content:encoded><![CDATA[
arXiv:2504.21716v1 Announce Type: cross 
Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sionna RT: Technical Report</title>
<link>https://arxiv.org/abs/2504.21719</link>
<guid>https://arxiv.org/abs/2504.21719</guid>
<content:encoded><![CDATA[
arXiv:2504.21719v1 Announce Type: cross 
Abstract: Sionna is an open-source, GPU-accelerated library that, as of version 0.14, incorporates a ray tracer for simulating radio wave propagation. A unique feature of Sionna RT is differentiability, enabling the calculation of gradients for the channel impulse responses (CIRs), radio maps, and other related metrics with respect to system and environmental parameters, such as material properties, antenna patterns, and array geometries. The release of Sionna 1.0 provides a complete overhaul of the ray tracer, significantly improving its speed, memory efficiency, and extensibility. This document details the algorithms employed by Sionna RT to simulate radio wave propagation efficiently, while also addressing their current limitations. Given that the computation of CIRs and radio maps requires distinct algorithms, these are detailed in separate sections. For CIRs, Sionna RT integrates shooting and bouncing of rays (SBR) with the image method and uses a hashing-based mechanism to efficiently eliminate duplicate paths. Radio maps are computed using a purely SBR-based approach.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cert-SSB: Toward Certified Sample-Specific Backdoor Defense</title>
<link>https://arxiv.org/abs/2504.21730</link>
<guid>https://arxiv.org/abs/2504.21730</guid>
<content:encoded><![CDATA[
arXiv:2504.21730v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.21731</link>
<guid>https://arxiv.org/abs/2504.21731</guid>
<content:encoded><![CDATA[
arXiv:2504.21731v1 Announce Type: cross 
Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline</title>
<link>https://arxiv.org/abs/2504.21772</link>
<guid>https://arxiv.org/abs/2504.21772</guid>
<content:encoded><![CDATA[
arXiv:2504.21772v1 Announce Type: cross 
Abstract: Short video platforms like YouTube Shorts and TikTok face significant copyright compliance challenges, as infringers frequently embed arbitrary background music (BGM) to obscure original soundtracks (OST) and evade content originality detection. To tackle this issue, we propose a novel pipeline that integrates Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR). Our approach effectively separates arbitrary BGM from the original OST, enabling the restoration of authentic video audio tracks. To support this work, we introduce two domain-specific datasets: OASD-20K for audio separation and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset comprising 1,121 video and mixed-audio pairs, specifically designed for short video restoration tasks. Experimental results demonstrate that our pipeline not only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring content integrity. This approach provides an ethical and scalable solution to copyright challenges in user-generated content on short video platforms.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness</title>
<link>https://arxiv.org/abs/2504.21773</link>
<guid>https://arxiv.org/abs/2504.21773</guid>
<content:encoded><![CDATA[
arXiv:2504.21773v1 Announce Type: cross 
Abstract: With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning</title>
<link>https://arxiv.org/abs/2504.21775</link>
<guid>https://arxiv.org/abs/2504.21775</guid>
<content:encoded><![CDATA[
arXiv:2504.21775v1 Announce Type: cross 
Abstract: Recent methods leverage a hypernet to handle the performance-fairness trade-offs in federated learning. This hypernet maps the clients' preferences between model performance and fairness to preference-specifc models on the trade-off curve, known as local Pareto front. However, existing methods typically adopt a uniform preference sampling distribution to train the hypernet across clients, neglecting the inherent heterogeneity of their local Pareto fronts. Meanwhile, from the perspective of generalization, they do not consider the gap between local and global Pareto fronts on the global dataset. To address these limitations, we propose HetPFL to effectively learn both local and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA) and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the optimal preference sampling distribution for each client to accommodate heterogeneous local Pareto fronts. While PHF performs preference-aware fusion of clients' hypernets to ensure the performance of the global Pareto front. We prove that HetPFL converges linearly with respect to the number of rounds, under weaker assumptions than existing methods. Extensive experiments on four datasets show that HetPFL significantly outperforms seven baselines in terms of the quality of learned local and global Pareto fronts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
arXiv:2504.21776v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
arXiv:2504.21798v1 Announce Type: cross 
Abstract: Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues</title>
<link>https://arxiv.org/abs/2504.21800</link>
<guid>https://arxiv.org/abs/2504.21800</guid>
<content:encoded><![CDATA[
arXiv:2504.21800v1 Announce Type: cross 
Abstract: The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</title>
<link>https://arxiv.org/abs/2504.21801</link>
<guid>https://arxiv.org/abs/2504.21801</guid>
<content:encoded><![CDATA[
arXiv:2504.21801v1 Announce Type: cross 
Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization</title>
<link>https://arxiv.org/abs/2504.21831</link>
<guid>https://arxiv.org/abs/2504.21831</guid>
<content:encoded><![CDATA[
arXiv:2504.21831v1 Announce Type: cross 
Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for Summarization), a lightweight, efficient, and scalable vision language model designed for segment wise video summarization. Leveraging multi modal prompts that combine textual and audio derived signals, DEEVISum incorporates Multi Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement over baseline distillation (0.5%), while EE reduces inference time by approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset, our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing the performance of significantly larger models, all while maintaining a lower computational footprint. We publicly release our code and processed dataset to support further research.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Light Modulation to Counter Manipulation of Speech Visual Content</title>
<link>https://arxiv.org/abs/2504.21846</link>
<guid>https://arxiv.org/abs/2504.21846</guid>
<content:encoded><![CDATA[
arXiv:2504.21846v1 Announce Type: cross 
Abstract: High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing AI Agents for Alignment and Governance</title>
<link>https://arxiv.org/abs/2504.21848</link>
<guid>https://arxiv.org/abs/2504.21848</guid>
<content:encoded><![CDATA[
arXiv:2504.21848v1 Announce Type: cross 
Abstract: The creation of effective governance mechanisms for AI agents requires a deeper understanding of their core properties and how these properties relate to questions surrounding the deployment and operation of agents in the world. This paper provides a characterization of AI agents that focuses on four dimensions: autonomy, efficacy, goal complexity, and generality. We propose different gradations for each dimension, and argue that each dimension raises unique questions about the design, operation, and governance of these systems. Moreover, we draw upon this framework to construct "agentic profiles" for different kinds of AI agents. These profiles help to illuminate cross-cutting technical and non-technical governance challenges posed by different classes of AI agents, ranging from narrow task-specific assistants to highly autonomous general-purpose systems. By mapping out key axes of variation and continuity, this framework provides developers, policymakers, and members of the public with the opportunity to develop governance approaches that better align with collective societal goals.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support</title>
<link>https://arxiv.org/abs/2504.21849</link>
<guid>https://arxiv.org/abs/2504.21849</guid>
<content:encoded><![CDATA[
arXiv:2504.21849v1 Announce Type: cross 
Abstract: Governance institutions must respond to societal risks, including those posed by generative AI. This study empirically examines how public trust in institutions and AI technologies, along with perceived risks, shape preferences for AI regulation. Using the nationally representative 2023 Artificial Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in government, AI companies, and AI technologies, as well as public support for regulatory measures such as slowing AI development or outright bans on advanced AI. Our findings reveal broad public support for AI regulation, with risk perception playing a significant role in shaping policy preferences. Individuals with higher trust in government favor regulation, while those with greater trust in AI companies and AI technologies are less inclined to support restrictions. Trust in government and perceived risks significantly predict preferences for both soft (e.g., slowing development) and strong (e.g., banning AI systems) regulatory interventions. These results highlight the importance of public opinion in AI governance. As AI capabilities advance, effective regulation will require balancing public concerns about risks with trust in institutions. This study provides a foundational empirical baseline for policymakers navigating AI governance and underscores the need for further research into public trust, risk perception, and regulatory strategies in the evolving AI landscape.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments</title>
<link>https://arxiv.org/abs/2504.21851</link>
<guid>https://arxiv.org/abs/2504.21851</guid>
<content:encoded><![CDATA[
arXiv:2504.21851v1 Announce Type: cross 
Abstract: Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Generating Explanations for Reinforcement Learning Policies: An Empirical Study</title>
<link>https://arxiv.org/abs/2309.16960</link>
<guid>https://arxiv.org/abs/2309.16960</guid>
<content:encoded><![CDATA[
arXiv:2309.16960v4 Announce Type: replace 
Abstract: Understanding a \textit{reinforcement learning} policy, which guides state-to-action mappings to maximize rewards, necessitates an accompanying explanation for human comprehension. In this paper, we introduce a set of \textit{linear temporal logic} formulae designed to provide explanations for policies, and an algorithm for searching through those formulae for the one that best explains a given policy. Our focus is on explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisite conditions it upholds throughout its execution. The effectiveness of our proposed approach is illustrated through a simulated game of capture-the-flag and a car-parking environment,
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes</title>
<link>https://arxiv.org/abs/2408.05794</link>
<guid>https://arxiv.org/abs/2408.05794</guid>
<content:encoded><![CDATA[
arXiv:2408.05794v2 Announce Type: replace 
Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2411.08165</link>
<guid>https://arxiv.org/abs/2411.08165</guid>
<content:encoded><![CDATA[
arXiv:2411.08165v2 Announce Type: replace 
Abstract: The Knowledge Graph Completion~(KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that KGR3 consistently improves various KGC methods. Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of 12.3% and 5.6% on the FB15k237 and WN18RR datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Restructuring Tractable Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2411.12256</link>
<guid>https://arxiv.org/abs/2411.12256</guid>
<content:encoded><![CDATA[
arXiv:2411.12256v2 Announce Type: replace 
Abstract: Probabilistic circuits (PCs) are a unifying representation for probabilistic models that support tractable inference. Numerous applications of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algorithms require that the circuits respect the same structure, i.e. variable scopes decomposes according to the same vtree. In this work, we propose and study the task of restructuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We propose a generic approach for this problem and show that it leads to novel polynomial-time algorithms for multiplying circuits respecting different vtrees, as well as a practical depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling efficient inference by changing their structures at inference time.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Board Games by External and Internal Planning with Language Models</title>
<link>https://arxiv.org/abs/2412.12119</link>
<guid>https://arxiv.org/abs/2412.12119</guid>
<content:encoded><![CDATA[
arXiv:2412.12119v2 Announce Type: replace 
Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?</title>
<link>https://arxiv.org/abs/2501.15857</link>
<guid>https://arxiv.org/abs/2501.15857</guid>
<content:encoded><![CDATA[
arXiv:2501.15857v4 Announce Type: replace 
Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, "FTCT" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
arXiv:2502.05439v2 Announce Type: replace 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty</title>
<link>https://arxiv.org/abs/2502.19915</link>
<guid>https://arxiv.org/abs/2502.19915</guid>
<content:encoded><![CDATA[
arXiv:2502.19915v2 Announce Type: replace 
Abstract: Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) - students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism - implementing personalized knowledge acquisition through gated networks and updating student knowledge state. Experimental results on two real datasets show our method consistently outperforms nine baseline models, improving AUC metrics by 2% to 10% while effectively addressing cold-start problems and enhancing model interpretability.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
arXiv:2504.09689v3 Announce Type: replace 
Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InvAASTCluster: On Applying Invariant-Based Program Clustering to Introductory Programming Assignments</title>
<link>https://arxiv.org/abs/2206.14175</link>
<guid>https://arxiv.org/abs/2206.14175</guid>
<content:encoded><![CDATA[
arXiv:2206.14175v3 Announce Type: replace-cross 
Abstract: Due to the vast number of students enrolled in programming courses, there has been an increasing number of automated program repair techniques focused on introductory programming assignments (IPAs). Typically, such techniques use program clustering to take advantage of previous correct student implementations to repair a new incorrect submission. These repair techniques use clustering methods since analyzing all available correct submissions to repair a program is not feasible. However, conventional clustering methods rely on program representations based on features such as abstract syntax trees (ASTs), syntax, control flow, and data flow.
  This paper proposes InvAASTCluster, a novel approach for program clustering that uses dynamically generated program invariants to cluster semantically equivalent IPAs. InvAASTCluster's program representation uses a combination of the program's semantics, through its invariants, and its structure through its anonymized abstract syntax tree (AASTs). Invariants denote conditions that must remain true during program execution, while AASTs are ASTs devoid of variable and function names, retaining only their types. Our experiments show that the proposed program representation outperforms syntax-based representations when clustering a set of correct IPAs. Furthermore, we integrate InvAASTCluster into a state-of-the-art clustering-based program repair tool. Our results show that InvAASTCluster advances the current state-of-the-art when used by clustering-based repair tools by repairing around 13% more students' programs, in a shorter amount of time.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images</title>
<link>https://arxiv.org/abs/2309.06129</link>
<guid>https://arxiv.org/abs/2309.06129</guid>
<content:encoded><![CDATA[
arXiv:2309.06129v4 Announce Type: replace-cross 
Abstract: Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes are consistently on-par or outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addition, a LEyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware. Going forward, we are confident that LEyes will revolutionize synthetic data generation for gaze estimation models, and lead to significant improvements of the next generation video-based eye trackers.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</title>
<link>https://arxiv.org/abs/2312.02312</link>
<guid>https://arxiv.org/abs/2312.02312</guid>
<content:encoded><![CDATA[
arXiv:2312.02312v2 Announce Type: replace-cross 
Abstract: Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Round Trip Translation Defence against Large Language Model Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2402.13517</link>
<guid>https://arxiv.org/abs/2402.13517</guid>
<content:encoded><![CDATA[
arXiv:2402.13517v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence
  This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Redshift: Random Networks are not Random Functions</title>
<link>https://arxiv.org/abs/2403.02241</link>
<guid>https://arxiv.org/abs/2403.02241</guid>
<content:encoded><![CDATA[
arXiv:2403.02241v3 Announce Type: replace-cross 
Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.
  Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inherit all these properties from their building blocks.
  Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation</title>
<link>https://arxiv.org/abs/2403.09326</link>
<guid>https://arxiv.org/abs/2403.09326</guid>
<content:encoded><![CDATA[
arXiv:2403.09326v4 Announce Type: replace-cross 
Abstract: Current text-to-avatar methods often rely on implicit representations (e.g., NeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit and animate in graphics software. This paper introduces a novel framework for generating stylized head avatars from text guidance, which leverages locally learnable mesh deformation and 2D diffusion priors to achieve high-quality digital assets for attribute-preserving manipulation. Given a template mesh, our method represents mesh deformation with per-face Jacobians and adaptively modulates local deformation using a learnable vector field. This vector field enables anisotropic scaling while preserving the rotation of vertices, which can better express identity and geometric details. We employ landmark- and contour-based regularization terms to balance the expressiveness and plausibility of generated avatars from multiple views without relying on any specific shape prior. Our framework can generate realistic shapes and textures that can be further edited via text, while supporting seamless editing using the preserved attributes from the template mesh, such as 3DMM parameters, blendshapes, and UV coordinates. Extensive experiments demonstrate that our framework can generate diverse and expressive head avatars with high-quality meshes that artists can easily manipulate in graphics software, facilitating downstream applications such as efficient asset creation and animation with preserved attributes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Historically Relevant Event Structuring for Temporal Knowledge Graph Reasoning</title>
<link>https://arxiv.org/abs/2405.10621</link>
<guid>https://arxiv.org/abs/2405.10621</guid>
<content:encoded><![CDATA[
arXiv:2405.10621v2 Announce Type: replace-cross 
Abstract: Temporal Knowledge Graph (TKG) reasoning focuses on predicting events through historical information within snapshots distributed on a timeline. Existing studies mainly concentrate on two perspectives of leveraging the history of TKGs, including capturing evolution of each recent snapshot or correlations among global historical facts. Despite the achieved significant accomplishments, these models still fall short of I) investigating the impact of multi-granular interactions across recent snapshots, and II) harnessing the expressive semantics of significant links accorded with queries throughout the entire history, particularly events exerting a profound impact on the future. These inadequacies restrict representation ability to reflect historical dependencies and future trends thoroughly. To overcome these drawbacks, we propose an innovative TKG reasoning approach towards \textbf{His}torically \textbf{R}elevant \textbf{E}vents \textbf{S}tructuring (HisRES). Concretely, HisRES comprises two distinctive modules excelling in structuring historically relevant events within TKGs, including a multi-granularity evolutionary encoder that captures structural and temporal dependencies of the most recent snapshots, and a global relevance encoder that concentrates on crucial correlations among events relevant to queries from the entire history. Furthermore, HisRES incorporates a self-gating mechanism for adaptively merging multi-granularity recent and historically relevant structuring representations. Extensive experiments on four event-based benchmarks demonstrate the state-of-the-art performance of HisRES and indicate the superiority and effectiveness of structuring historical relevance for TKG reasoning.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Offline Multi-agent Skill Discovery</title>
<link>https://arxiv.org/abs/2405.16386</link>
<guid>https://arxiv.org/abs/2405.16386</guid>
<content:encoded><![CDATA[
arXiv:2405.16386v3 Announce Type: replace-cross 
Abstract: Skills are effective temporal abstractions established for sequential decision making, which enable efficient hierarchical learning for long-horizon tasks and facilitate multi-task learning through their transferability. Despite extensive research, research gaps remain in multi-agent scenarios, particularly for automatically extracting subgroup coordination patterns in a multi-agent task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level abstractions and form multi-agent skills, which firstly solves the aforementioned challenge. An essential algorithm component of these schemes is a dynamic grouping function that can automatically detect latent subgroups based on agent interactions in a task. Further, our method can be applied to offline multi-task data, and the discovered subgroup skills can be transferred across relevant tasks without retraining. Empirical evaluations on StarCraft tasks indicate that our approach significantly outperforms existing hierarchical multi-agent reinforcement learning (MARL) methods. Moreover, skills discovered using our method can effectively reduce the learning difficulty in MARL scenarios with delayed and sparse reward signals. The codebase is available at https://github.com/LucasCJYSDL/VOMASD.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems</title>
<link>https://arxiv.org/abs/2405.20774</link>
<guid>https://arxiv.org/abs/2405.20774</guid>
<content:encoded><![CDATA[
arXiv:2405.20774v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models</title>
<link>https://arxiv.org/abs/2406.01698</link>
<guid>https://arxiv.org/abs/2406.01698</guid>
<content:encoded><![CDATA[
arXiv:2406.01698v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these gigantic models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With constant innovation in LLM serving optimizations and model architecture evolving at breakneck speed, the hardware requirements to meet Service Level Objectives (SLOs) remain an open research question.
  To answer the question, we present an analytical tool, GenZ, to efficiently navigate the relationship between diverse LLM model architectures(Dense, GQA, MoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding, quanitization), and AI platform design parameters. Our tool estimates LLM inference performance metrics for the given scenario. We have validated against real hardware platforms running various different LLM models, achieving a max geomean error of 5.82.We use GenZ to identify compute, memory capacity, memory bandwidth, network latency, and network bandwidth requirements across diverse LLM inference use cases. We also study diverse architectural choices in use today (inspired by LLM serving platforms from several vendors) to help inform computer architects designing next-generation AI hardware accelerators and platforms. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on your web browser.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models</title>
<link>https://arxiv.org/abs/2406.09495</link>
<guid>https://arxiv.org/abs/2406.09495</guid>
<content:encoded><![CDATA[
arXiv:2406.09495v4 Announce Type: replace-cross 
Abstract: Fairness-aware domain generalization (FairDG) has emerged as a critical challenge for deploying trustworthy AI systems, particularly in scenarios involving distribution shifts. Traditional methods for addressing fairness have failed in domain generalization due to their lack of consideration for distribution shifts. Although disentanglement has been used to tackle FairDG, it is limited by its strong assumptions. To overcome these limitations, we propose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as a novel approach to effectively address the FairDG issue. Specifically, we first pre-train a score-based diffusion model (SDM) and two classifiers to equip the model with strong generalization capabilities across different domains. Then, we guide the SDM using these pre-trained classifiers to effectively eliminate sensitive information from the generated data. Finally, the generated fair data is used to train downstream classifiers, ensuring robust performance under new data distributions. Extensive experiments on three real-world datasets demonstrate that FADE not only enhances fairness but also improves accuracy in the presence of distribution shifts. Additionally, FADE outperforms existing methods in achieving the best accuracy-fairness trade-offs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications</title>
<link>https://arxiv.org/abs/2407.02994</link>
<guid>https://arxiv.org/abs/2407.02994</guid>
<content:encoded><![CDATA[
arXiv:2407.02994v4 Announce Type: replace-cross 
Abstract: The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. Moreover, the recent rising of Large Multimodal Models (LMM) leads to a need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding CT or MR scans. This paper illustrates the entire workflow for building the data set MedPix 2.0. Starting from the well-known multimodal data set MedPix, mainly used by physicians, nurses and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure where noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a GUI aimed at navigating efficiently the MongoDB instance, and obtaining the raw data that can be easily used for training and/or fine-tuning LMMs. To enforce this point, we also propose a CLIP-based model trained on MedPix 2.0 for scanning modality and location classification tasks. MedPix 2.0 is available on GitHub
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents</title>
<link>https://arxiv.org/abs/2407.05679</link>
<guid>https://arxiv.org/abs/2407.05679</guid>
<content:encoded><![CDATA[
arXiv:2407.05679v3 Announce Type: replace-cross 
Abstract: World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless data compression by large models</title>
<link>https://arxiv.org/abs/2407.07723</link>
<guid>https://arxiv.org/abs/2407.07723</guid>
<content:encoded><![CDATA[
arXiv:2407.07723v3 Announce Type: replace-cross 
Abstract: Modern data compression methods are slowly reaching their limits after 80 years of research, millions of papers, and wide range of applications. Yet, the extravagant 6G communication speed requirement raises a major open question for revolutionary new ideas of data compression. We have previously shown all understanding or learning are compression, under reasonable assumptions. Large language models (LLMs) understand data better than ever before. Can they help us to compress data? The LLMs may be seen to approximate the uncomputable Solomonoff induction. Therefore, under this new uncomputable paradigm, we present LMCompress. LMCompress shatters all previous lossless compression algorithms, doubling the lossless compression ratios of JPEG-XL for images, FLAC for audios, and H.264 for videos, and quadrupling the compression ratio of bz2 for texts. The better a large model understands the data, the better LMCompress compresses.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training</title>
<link>https://arxiv.org/abs/2407.08306</link>
<guid>https://arxiv.org/abs/2407.08306</guid>
<content:encoded><![CDATA[
arXiv:2407.08306v3 Announce Type: replace-cross 
Abstract: As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music Understanding (SMU) has garnered significant attention for its potential to assist both musicians and enthusiasts in learning and creating music. Recently, pre-trained language models have been widely adopted in SMU due to the substantial similarities between symbolic music and natural language, as well as the ability of these models to leverage limited music data effectively. However, some studies have shown the common pre-trained methods like Mask Language Model (MLM) may introduce bias issues like racism discrimination in Natural Language Process (NLP) and affects the performance of downstream tasks, which also happens in SMU. This bias often arises when masked tokens cannot be inferred from their context, forcing the model to overfit the training set instead of generalizing. To address this challenge, we propose Adversarial-MidiBERT for SMU, which adaptively determines what to mask during MLM via a masker network, rather than employing random masking. By avoiding the masking of tokens that are difficult to infer from context, our model is better equipped to capture contextual structures and relationships, rather than merely conforming to the training data distribution. We evaluate our method across four SMU tasks, and our approach demonstrates excellent performance in all cases. The code for our model is publicly available at https://github.com/RS2002/Adversarial-MidiBERT .
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patched RTC: evaluating LLMs for diverse software development tasks</title>
<link>https://arxiv.org/abs/2407.16557</link>
<guid>https://arxiv.org/abs/2407.16557</guid>
<content:encoded><![CDATA[
arXiv:2407.16557v3 Announce Type: replace-cross 
Abstract: This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on "outer loop" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patched MOA: optimizing inference for diverse software development tasks</title>
<link>https://arxiv.org/abs/2407.18521</link>
<guid>https://arxiv.org/abs/2407.18521</guid>
<content:encoded><![CDATA[
arXiv:2407.18521v4 Announce Type: replace-cross 
Abstract: This paper introduces Patched MOA (Mixture of Agents), an inference optimization technique that significantly enhances the performance of large language models (LLMs) across diverse software development tasks. We evaluate three inference optimization algorithms - Best of N, Mixture of Agents, and Monte Carlo Tree Search and demonstrate that Patched MOA can boost the performance of smaller models to surpass that of larger, more expensive models. Notably, our approach improves the gpt-4o-mini model's performance on the Arena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of the cost. We also apply Patched MOA to various software development workflows, showing consistent improvements in task completion rates. Our method is model-agnostic, transparent to end-users, and can be easily integrated into existing LLM pipelines. This work contributes to the growing field of LLM optimization, offering a cost-effective solution for enhancing model performance without the need for fine-tuning or larger models. Our implementation is open-source and available at https://github.com/codelion/optillm.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SustainDC: Benchmarking for Sustainable Data Center Control</title>
<link>https://arxiv.org/abs/2408.07841</link>
<guid>https://arxiv.org/abs/2408.07841</guid>
<content:encoded><![CDATA[
arXiv:2408.07841v5 Announce Type: replace-cross 
Abstract: Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction</title>
<link>https://arxiv.org/abs/2409.15551</link>
<guid>https://arxiv.org/abs/2409.15551</guid>
<content:encoded><![CDATA[
arXiv:2409.15551v2 Announce Type: replace-cross 
Abstract: Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models</title>
<link>https://arxiv.org/abs/2409.16920</link>
<guid>https://arxiv.org/abs/2409.16920</guid>
<content:encoded><![CDATA[
arXiv:2409.16920v2 Announce Type: replace-cross 
Abstract: Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling</title>
<link>https://arxiv.org/abs/2409.16937</link>
<guid>https://arxiv.org/abs/2409.16937</guid>
<content:encoded><![CDATA[
arXiv:2409.16937v3 Announce Type: replace-cross 
Abstract: The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations</title>
<link>https://arxiv.org/abs/2409.17899</link>
<guid>https://arxiv.org/abs/2409.17899</guid>
<content:encoded><![CDATA[
arXiv:2409.17899v2 Announce Type: replace-cross 
Abstract: Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion</title>
<link>https://arxiv.org/abs/2410.00381</link>
<guid>https://arxiv.org/abs/2410.00381</guid>
<content:encoded><![CDATA[
arXiv:2410.00381v2 Announce Type: replace-cross 
Abstract: Understanding the risks posed by extreme rainfall events necessitates both high-resolution products (to assess localized hazards) and extensive historical records (to capture rare occurrences). Radar and mesonet networks provide kilometer-scale precipitation fields, but with limited historical records and geographical coverage. Conversely, global gauge and blended products span decades, yet their coarse 30-50 km grids obscure local extremes. This work introduces Wasserstein Regularized Diffusion (WassDiff), a generative downscaling framework that integrates diffusion modeling with a distribution-matching (Wasserstein) regularizer, suppressing bias throughout the entire generative denoising process. Conditioned on 55 km CPC gauge-based precipitation and the 31 km ERA5 reanalysis, WassDiff generates 1 km precipitation estimates that remain well-calibrated to targets across the full intensity range, including the extremes. Comprehensive evaluations demonstrate that WassDiff outperforms existing state-of-the-art downscaling methods, delivering lower reconstruction error and reduced bias. Case studies further demonstrate its ability to reproduce realistic fine-scale structures and accurate peak intensities from extreme weather phenomena, such as tropical storms and cold fronts. By unlocking decades of high-resolution rainfall information from globally available coarse records, WassDiff offers a practical pathway toward more accurate flood-risk assessments and climate-adaptation planning.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Generative Priors Improve World Models Sequence Modelling Capabilities</title>
<link>https://arxiv.org/abs/2410.07836</link>
<guid>https://arxiv.org/abs/2410.07836</guid>
<content:encoded><![CDATA[
arXiv:2410.07836v5 Announce Type: replace-cross 
Abstract: Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback</title>
<link>https://arxiv.org/abs/2410.08852</link>
<guid>https://arxiv.org/abs/2410.08852</guid>
<content:encoded><![CDATA[
arXiv:2410.08852v2 Announce Type: replace-cross 
Abstract: In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to Localize? A Risk-Constrained Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2411.02788</link>
<guid>https://arxiv.org/abs/2411.02788</guid>
<content:encoded><![CDATA[
arXiv:2411.02788v2 Announce Type: replace-cross 
Abstract: In a standard navigation pipeline, a robot localizes at every time step to lower navigational errors. However, in some scenarios, a robot needs to selectively localize when it is expensive to obtain observations. For example, an underwater robot surfacing to localize too often hinders it from searching for critical items underwater, such as black boxes from crashed aircraft. On the other hand, if the robot never localizes, poor state estimates cause failure to find the items due to inadvertently leaving the search area or entering hazardous, restricted areas. Motivated by these scenarios, we investigate approaches to help a robot determine "when to localize?" We formulate this as a bi-criteria optimization problem: minimize the number of localization actions while ensuring the probability of failure (due to collision or not reaching a desired goal) remains bounded. In recent work, we showed how to formulate this active localization problem as a constrained Partially Observable Markov Decision Process (POMDP), which was solved using an online POMDP solver. However, this approach is too slow and requires full knowledge of the robot transition and observation models. In this paper, we present RiskRL, a constrained Reinforcement Learning (RL) framework that overcomes these limitations. RiskRL uses particle filtering and recurrent Soft Actor-Critic network to learn a policy that minimizes the number of localizations while ensuring the probability of failure constraint is met. Our numerical experiments show that RiskRL learns a robust policy that leads to at least a 26% increase in success rates when traversing unseen test environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization</title>
<link>https://arxiv.org/abs/2411.05282</link>
<guid>https://arxiv.org/abs/2411.05282</guid>
<content:encoded><![CDATA[
arXiv:2411.05282v4 Announce Type: replace-cross 
Abstract: Quantization of foundational models (FMs) is significantly more challenging than traditional DNNs due to the emergence of large magnitude values called outliers. Existing outlier-aware algorithm-architecture co-design techniques either use mixed-precision, retaining outliers at high precision but compromise hardware efficiency, or quantize inliers and outliers at the same precision, improving hardware efficiency at the cost of accuracy. To address this mutual exclusivity, we propose MicroScopiQ, a novel co-design technique that leverages pruning to complement outlier-aware quantization. MicroScopiQ retains outliers at higher precision while pruning a certain fraction of least important weights to distribute the additional outlier bits; ensuring high accuracy, aligned memory and hardware efficiency. We design a high-throughput, low overhead accelerator architecture composed of multi-precision INT processing elements and a network-on-chip called ReCoN that efficiently abstracts the complexity of supporting high-precision outliers. Additionally, unlike prior techniques, MicroScopiQ does not assume any locality of outlier weights, enabling applicability to a broad range of FMs. Extensive experiments across diverse quantization settings demonstrate that MicroScopiQ achieves state-of-the-art quantization accuracy, while delivering up to 3x faster inference and 2x lower energy consumption compared to existing alternatives. Code is available at: https://github.com/georgia-tech-synergy-lab/MicroScopiQ-LLM-Quantization
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency Enhanced Hybrid Neural Representation for Video Compression</title>
<link>https://arxiv.org/abs/2411.06685</link>
<guid>https://arxiv.org/abs/2411.06685</guid>
<content:encoded><![CDATA[
arXiv:2411.06685v2 Announce Type: replace-cross 
Abstract: Neural Representations for Videos (NeRV) have simplified the video codec process and achieved swift decoding speeds by encoding video content into a neural network, presenting a promising solution for video compression. However, existing work overlooks the crucial issue that videos reconstructed by these methods lack high-frequency details. To address this problem, this paper introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our method focuses on leveraging high-frequency information to improve the synthesis of fine details by the network. Specifically, we design a wavelet high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD) blocks to generate high-frequency feature embeddings. Next, we design the High-Frequency Feature Modulation (HFM) block, which leverages the extracted high-frequency embeddings to enhance the fitting process of the decoder. Finally, with the refined Harmonic decoder block and a Dynamic Weighted Frequency Loss, we further reduce the potential loss of high-frequency information. Experiments on the Bunny and UVG datasets demonstrate that our method outperforms other methods, showing notable improvements in detail preservation and compression performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorEdit: Training-free Image-Guided Color editing with diffusion model</title>
<link>https://arxiv.org/abs/2411.10232</link>
<guid>https://arxiv.org/abs/2411.10232</guid>
<content:encoded><![CDATA[
arXiv:2411.10232v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</title>
<link>https://arxiv.org/abs/2411.19167</link>
<guid>https://arxiv.org/abs/2411.19167</guid>
<content:encoded><![CDATA[
arXiv:2411.19167v2 Announce Type: replace-cross 
Abstract: We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images</title>
<link>https://arxiv.org/abs/2412.06314</link>
<guid>https://arxiv.org/abs/2412.06314</guid>
<content:encoded><![CDATA[
arXiv:2412.06314v2 Announce Type: replace-cross 
Abstract: Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: https://github.com/AmanoTooko-jie/CAD-Unet.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FILA: Fine-Grained Vision Language Models</title>
<link>https://arxiv.org/abs/2412.08378</link>
<guid>https://arxiv.org/abs/2412.08378</guid>
<content:encoded><![CDATA[
arXiv:2412.08378v3 Announce Type: replace-cross 
Abstract: Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into smaller sub-images, which are then fed into a vision encoder that was pre-trained on lower-resolution images. However, this cropping approach often truncates objects and connected areas in the original image, causing semantic breaks. To address this limitation, we introduce HyViLM, designed to process images of any resolution while retaining the overall context during encoding. Specifically, we: (i) Design a new visual encoder called Hybrid Encoder that not only encodes individual sub-images but also interacts with detailed global visual features, significantly improving the model's ability to encode high-resolution images. (ii) Propose an optimal feature fusion strategy for the dynamic cropping approach, effectively leveraging information from different layers of the vision encoder. Compared with the state-of-the-art MLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out of ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance on the TextVQA task and a 6.9% enhancement on the DocVQA task.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects</title>
<link>https://arxiv.org/abs/2412.10133</link>
<guid>https://arxiv.org/abs/2412.10133</guid>
<content:encoded><![CDATA[
arXiv:2412.10133v2 Announce Type: replace-cross 
Abstract: The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Library for Learning Neural Operators</title>
<link>https://arxiv.org/abs/2412.10354</link>
<guid>https://arxiv.org/abs/2412.10354</guid>
<content:encoded><![CDATA[
arXiv:2412.10354v3 Announce Type: replace-cross 
Abstract: We present NeuralOperator, an open-source Python library for operator learning. Neural operators generalize neural networks to maps between function spaces instead of finite-dimensional Euclidean spaces. They can be trained and inferenced on input and output functions given at various discretizations, satisfying a discretization convergence properties. Built on top of PyTorch, NeuralOperator provides all the tools for training and deploying neural operator models, as well as developing new ones, in a high-quality, tested, open-source package. It combines cutting-edge models and customizability with a gentle learning curve and simple user interface for newcomers.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data</title>
<link>https://arxiv.org/abs/2412.14810</link>
<guid>https://arxiv.org/abs/2412.14810</guid>
<content:encoded><![CDATA[
arXiv:2412.14810v2 Announce Type: replace-cross 
Abstract: In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Disease Progression Models That Capture Health Disparities</title>
<link>https://arxiv.org/abs/2412.16406</link>
<guid>https://arxiv.org/abs/2412.16406</guid>
<content:encoded><![CDATA[
arXiv:2412.16406v2 Announce Type: replace-cross 
Abstract: Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for any of these disparities can result in biased estimates of severity (e.g., underestimating severity for disadvantaged groups). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities while inferring disease severity meaningfully shifts which patients are considered high-risk.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation</title>
<link>https://arxiv.org/abs/2501.01991</link>
<guid>https://arxiv.org/abs/2501.01991</guid>
<content:encoded><![CDATA[
arXiv:2501.01991v2 Announce Type: replace-cross 
Abstract: Model checking, a formal verification technique, ensures systems meet predefined requirements, playing a crucial role in minimizing errors and enhancing quality during development. This paper introduces a novel hybrid framework integrating model checking with deep learning for brain tumor detection and validation in medical imaging. By combining model-checking principles with CNN-based feature extraction and K-FCM clustering for segmentation, the proposed approach enhances the reliability of tumor detection and segmentation. Experimental results highlight the framework's effectiveness, achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its potential as a robust tool for advanced medical image analysis.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment</title>
<link>https://arxiv.org/abs/2501.17690</link>
<guid>https://arxiv.org/abs/2501.17690</guid>
<content:encoded><![CDATA[
arXiv:2501.17690v2 Announce Type: replace-cross 
Abstract: We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry</title>
<link>https://arxiv.org/abs/2502.06485</link>
<guid>https://arxiv.org/abs/2502.06485</guid>
<content:encoded><![CDATA[
arXiv:2502.06485v2 Announce Type: replace-cross 
Abstract: Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fr\'echet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WyckoffDiff against recently proposed generative models for crystal generation. Code is available online at https://github.com/httk/wyckoffdiff
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting clinical outcomes from patient care pathways represented with temporal knowledge graphs</title>
<link>https://arxiv.org/abs/2502.21138</link>
<guid>https://arxiv.org/abs/2502.21138</guid>
<content:encoded><![CDATA[
arXiv:2502.21138v2 Announce Type: replace-cross 
Abstract: Background: With the increasing availability of healthcare data, predictive modeling finds many applications in the biomedical domain, such as the evaluation of the level of risk for various conditions, which in turn can guide clinical decision making. However, it is unclear how knowledge graph data representations and their embedding, which are competitive in some settings, could be of interest in biomedical predictive modeling. Method: We simulated synthetic but realistic data of patients with intracranial aneurysm and experimented on the task of predicting their clinical outcome. We compared the performance of various classification approaches on tabular data versus a graph-based representation of the same data. Next, we investigated how the adopted schema for representing first individual data and second temporal data impacts predictive performances. Results: Our study illustrates that in our case, a graph representation and Graph Convolutional Network (GCN) embeddings reach the best performance for a predictive task from observational data. We emphasize the importance of the adopted schema and of the consideration of literal values in the representation of individual data. Our study also moderates the relative impact of various time encoding on GCN performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</title>
<link>https://arxiv.org/abs/2503.01453</link>
<guid>https://arxiv.org/abs/2503.01453</guid>
<content:encoded><![CDATA[
arXiv:2503.01453v2 Announce Type: replace-cross 
Abstract: Most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in English language. This often restricts this important assistive tool for widespread use across language and accessibility barriers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. The AC-Lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. A combination of ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention is found to provide the best performance with minimum compute. AC-Lite was observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs and 22.87M parameters.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: A Framework of Precise Retrieval for RAG</title>
<link>https://arxiv.org/abs/2503.01713</link>
<guid>https://arxiv.org/abs/2503.01713</guid>
<content:encoded><![CDATA[
arXiv:2503.01713v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.
  In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System</title>
<link>https://arxiv.org/abs/2503.14258</link>
<guid>https://arxiv.org/abs/2503.14258</guid>
<content:encoded><![CDATA[
arXiv:2503.14258v3 Announce Type: replace-cross 
Abstract: This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding</title>
<link>https://arxiv.org/abs/2503.18578</link>
<guid>https://arxiv.org/abs/2503.18578</guid>
<content:encoded><![CDATA[
arXiv:2503.18578v2 Announce Type: replace-cross 
Abstract: Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology classification tasks (up to $+0.17$ F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial KA</title>
<link>https://arxiv.org/abs/2504.05255</link>
<guid>https://arxiv.org/abs/2504.05255</guid>
<content:encoded><![CDATA[
arXiv:2504.05255v2 Announce Type: replace-cross 
Abstract: Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or {\guillemotleft}expressing{\guillemotright} functions, we test its robustness by analyzing its ability to withstand adversarial attacks. We find KA to be robust to countable collections of continuous adversaries, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of adversaries. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes</title>
<link>https://arxiv.org/abs/2504.09948</link>
<guid>https://arxiv.org/abs/2504.09948</guid>
<content:encoded><![CDATA[
arXiv:2504.09948v2 Announce Type: replace-cross 
Abstract: Dish images play a crucial role in the digital era, with the demand for culturally distinctive dish images continuously increasing due to the digitization of the food industry and e-commerce. In general cases, existing text-to-image generation models excel in producing high-quality images; however, they struggle to capture diverse characteristics and faithful details of specific domains, particularly Chinese dishes. To address this limitation, we propose Omni-Dish, the first text-to-image generation model specifically tailored for Chinese dishes. We develop a comprehensive dish curation pipeline, building the largest dish dataset to date. Additionally, we introduce a recaption strategy and employ a coarse-to-fine training scheme to help the model better learn fine-grained culinary nuances. During inference, we enhance the user's textual input using a pre-constructed high-quality caption library and a large language model, enabling more photorealistic and faithful image generation. Furthermore, to extend our model's capability for dish editing tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish editing dataset and train a specialized editing model. Extensive experiments demonstrate the superiority of our methods.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weight Ensembling Improves Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2504.10478</link>
<guid>https://arxiv.org/abs/2504.10478</guid>
<content:encoded><![CDATA[
arXiv:2504.10478v3 Announce Type: replace-cross 
Abstract: We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades off between bias and variance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</title>
<link>https://arxiv.org/abs/2504.11014</link>
<guid>https://arxiv.org/abs/2504.11014</guid>
<content:encoded><![CDATA[
arXiv:2504.11014v4 Announce Type: replace-cross 
Abstract: The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations</title>
<link>https://arxiv.org/abs/2504.13955</link>
<guid>https://arxiv.org/abs/2504.13955</guid>
<content:encoded><![CDATA[
arXiv:2504.13955v3 Announce Type: replace-cross 
Abstract: The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning</title>
<link>https://arxiv.org/abs/2504.14070</link>
<guid>https://arxiv.org/abs/2504.14070</guid>
<content:encoded><![CDATA[
arXiv:2504.14070v3 Announce Type: replace-cross 
Abstract: This paper demonstrates a probabilistic bit physics inspired solver with 440 spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area efficiency is maximized through a current-mode implementation of the neuron update circuit, standard cell design for analog blocks pitch-matched to digital blocks, and a shared power supply for both digital and analog components. Process variation related mismatches introduced by this approach are effectively mitigated using a hardware aware contrastive divergence algorithm during training. We validate the chip's ability to perform probabilistic computing tasks such as modeling logic gates and full adders, as well as optimization tasks such as MaxCut, demonstrating its potential for AI and machine learning applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSage: A Multi-aspect RAG System for Financial Filings Question Answering</title>
<link>https://arxiv.org/abs/2504.14493</link>
<guid>https://arxiv.org/abs/2504.14493</guid>
<content:encoded><![CDATA[
arXiv:2504.14493v2 Announce Type: replace-cross 
Abstract: Leveraging large language models in real-world settings often entails a need to utilize domain-specific data and tools in order to follow the complex regulations that need to be followed for acceptable use. Within financial sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation (RAG) systems to address complex compliance requirements in financial document workflows. However, existing solutions struggle to account for the inherent heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of regulatory standards used in financial filings, leading to compromised accuracy in critical information extraction. We propose the FinSage framework as a solution, utilizing a multi-aspect RAG framework tailored for regulatory compliance analysis in multi-modal financial documents. FinSage introduces three innovative components: (1) a multi-modal pre-processing pipeline that unifies diverse data formats and generates chunk-level metadata summaries, (2) a multi-path sparse-dense retrieval system augmented with query expansion (HyDE) and metadata-aware semantic search, and (3) a domain-specialized re-ranking module fine-tuned via Direct Preference Optimization (DPO) to prioritize compliance-critical content. Extensive experiments demonstrate that FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions derived from surpasses the best baseline method on the FinanceBench question answering datasets by 24.06% in accuracy. Moreover, FinSage has been successfully deployed as financial question-answering agent in online meetings, where it has already served more than 1,200 people.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution of AI in Education: Agentic Workflows</title>
<link>https://arxiv.org/abs/2504.20082</link>
<guid>https://arxiv.org/abs/2504.20082</guid>
<content:encoded><![CDATA[
<div> AI agents, education, large language models, automated essay scoring, transformative potential <br />
<br />
Summary: This article explores the role of AI agents in education, focusing on their application in automated tutoring, assessment, and content generation. Traditional large language models (LLMs) face limitations such as static training data and lack of reasoning abilities. AI agents offer a more adaptable and reasoning-driven approach to education, with potential benefits in reflection, planning, tool use, and multi-agent collaboration paradigms. The authors present a multi-agent framework for automated essay scoring as a proof-of-concept application, showing potential for improved consistency compared to stand-alone LLMs. The review highlights the transformative potential of AI agents in educational settings but also raises questions about their interpretability, trustworthiness, and long-term impact on pedagogy.<br /><br />Summary: <div>
arXiv:2504.20082v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has transformed various aspects of education, with large language models (LLMs) driving advancements in automated tutoring, assessment, and content generation. However, conventional LLMs are constrained by their reliance on static training data, limited adaptability, and lack of reasoning. To address these limitations and foster more sustainable technological practices, AI agents have emerged as a promising new avenue for educational innovation. In this review, we examine agentic workflows in education according to four major paradigms: reflection, planning, tool use, and multi-agent collaboration. We critically analyze the role of AI agents in education through these key design paradigms, exploring their advantages, applications, and challenges. To illustrate the practical potential of agentic systems, we present a proof-of-concept application: a multi-agent framework for automated essay scoring. Preliminary results suggest this agentic approach may offer improved consistency compared to stand-alone LLMs. Our findings highlight the transformative potential of AI agents in educational settings while underscoring the need for further research into their interpretability, trustworthiness, and sustainable impact on pedagogical impact.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Awareness</title>
<link>https://arxiv.org/abs/2504.20084</link>
<guid>https://arxiv.org/abs/2504.20084</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, awareness, meta-cognition, self-awareness, AI capabilities

Summary:<br /><br />The review explores the emerging landscape of AI awareness, including meta-cognition, self-awareness, social awareness, and situational awareness. It traces the theoretical foundations of awareness and examines how these forms manifest in state-of-the-art AI. Current evaluation methods and empirical findings are analyzed to understand AI awareness manifestations. The review highlights the close link between AI awareness and capabilities, showing that more aware AI agents tend to exhibit higher levels of intelligent behaviors. The article discusses the risks associated with AI awareness, such as misalignment and societal concerns, emphasizing the need for careful oversight as AI capabilities advance. Overall, the interdisciplinary review provides a roadmap for future research and clarifies the role of AI awareness in the development of intelligent machines.

Summary: <div>
arXiv:2504.20084v1 Announce Type: new 
Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raises concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. On the whole, our interdisciplinary review provides a roadmap for future research and aims to clarify the role of AI awareness in the ongoing development of intelligent machines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spark: A System for Scientifically Creative Idea Generation</title>
<link>https://arxiv.org/abs/2504.20090</link>
<guid>https://arxiv.org/abs/2504.20090</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, idea generation, computational creativity, Spark, scientific reviews 

Summary:
The article introduces Spark, an idea generation system that utilizes large language models (LLMs) and a reviewer model named Judge, trained on 600K scientific reviews from OpenReview. This system combines retrieval-augmented idea generation with creative evaluations to generate novel research ideas in science. By grounding the generation and evaluation of scientific ideas within computational creativity principles, the authors aim to inspire other researchers in the field. The annotated dataset used to train Judge is also released, encouraging further exploration of LLMs for idea generation and creative evaluations. The integration of LLMs and a reviewer model aims to enhance the creativity and innovation in scientific research by leveraging the capabilities of advanced language models in generating and evaluating research ideas. Overall, Spark demonstrates the potential of using LLMs in computational creativity for generating and evaluating scientific ideas. 

<br /><br />Summary: <div>
arXiv:2504.20090v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems</title>
<link>https://arxiv.org/abs/2504.20109</link>
<guid>https://arxiv.org/abs/2504.20109</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, General Intelligence, Lifelong Learning, Neuroscience, Edge Devices

Summary:
This paper discusses the limitations of current AI models in achieving true Artificial General Intelligence (AGI) and proposes a novel architecture for Personalized AGI. The proposed architecture integrates brain-like learning mechanisms and aims to support continuous learning and personalization on resource-constrained edge devices. Inspired by neuroscience principles such as Synaptic Pruning, Hebbian plasticity, and Dual Memory Systems, the architecture features fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates. It addresses challenges like catastrophic forgetting, memory efficiency, and system scalability. The paper also provides conceptual diagrams of the proposed architecture and discusses application scenarios for mobile AI assistants and humanoid robots. While the architecture is theoretical, it offers a roadmap for future implementation and research towards achieving truly personalized AGI on the edge.<br /><br />Summary: This paper proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms and aims to support continuous learning and personalization on edge devices. It addresses limitations of current AI models, draws inspiration from neuroscience principles, and provides conceptual diagrams and application scenarios. <div>
arXiv:2504.20109v1 Announce Type: new 
Abstract: Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI</title>
<link>https://arxiv.org/abs/2504.20113</link>
<guid>https://arxiv.org/abs/2504.20113</guid>
<content:encoded><![CDATA[
<div> natural language processing, machine learning, automated meta-analysis, statistical modeling, AI systems <br />
Summary: 
Exponential growth in scientific literature has led to the emergence of Automated Meta-analysis (AMA) as a field leveraging natural language processing and machine learning. However, a structured framework review revealed a predominant focus on automating data processing over advanced synthesis stages, limiting AMA's potential for full automation. Integration of large language models and AI into statistical modeling and higher-order synthesis remains underdeveloped, hindering fully autonomous meta-analysis. While AI has enhanced specific tasks, seamless end-to-end automation remains a challenge. Implementation patterns and effectiveness varied across medical and non-medical domains, with gaps in scalability, efficiency, and reproducibility. Bridging automation across all stages, improving interpretability, and ensuring methodological robustness are crucial for realizing AMA's potential in scalable, domain-agnostic synthesis. <div>
arXiv:2504.20113v1 Announce Type: new 
Abstract: Exponential growth in scientific literature has heightened the demand for efficient evidence-based synthesis, driving the rise of the field of Automated Meta-analysis (AMA) powered by natural language processing and machine learning. This PRISMA systematic review introduces a structured framework for assessing the current state of AMA, based on screening 978 papers from 2006 to 2024, and analyzing 54 studies across diverse domains. Findings reveal a predominant focus on automating data processing (57%), such as extraction and statistical modeling, while only 17% address advanced synthesis stages. Just one study (2%) explored preliminary full-process automation, highlighting a critical gap that limits AMA's capacity for comprehensive synthesis. Despite recent breakthroughs in large language models (LLMs) and advanced AI, their integration into statistical modeling and higher-order synthesis, such as heterogeneity assessment and bias evaluation, remains underdeveloped. This has constrained AMA's potential for fully autonomous meta-analysis. From our dataset spanning medical (67%) and non-medical (33%) applications, we found that AMA has exhibited distinct implementation patterns and varying degrees of effectiveness in actually improving efficiency, scalability, and reproducibility. While automation has enhanced specific meta-analytic tasks, achieving seamless, end-to-end automation remains an open challenge. As AI systems advance in reasoning and contextual understanding, addressing these gaps is now imperative. Future efforts must focus on bridging automation across all meta-analysis stages, refining interpretability, and ensuring methodological robustness to fully realize AMA's potential for scalable, domain-agnostic synthesis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Physics Prior for First Order Inverse Optimization</title>
<link>https://arxiv.org/abs/2504.20278</link>
<guid>https://arxiv.org/abs/2504.20278</guid>
<content:encoded><![CDATA[
<div> Keywords: Inverse design optimization, Deep Physics Prior, first-order gradient-based, surrogate machine learning models, prior distribution constraints

Summary:<br /><br />
Inverse design optimization is a challenging task in various fields where system parameters need to be inferred from observed solutions. Traditional approaches such as generative AI and Bayesian optimization have limitations. Generative AI is computationally expensive, while Bayesian optimization struggles with scalability and sensitivity to priors and noise. This paper introduces Deep Physics Prior (DPP), a novel method that enables first-order gradient-based inverse optimization using surrogate machine learning models. By utilizing pretrained auxiliary Neural Operators, DPP enforces prior distribution constraints to ensure robust and meaningful solutions, especially in cases where prior data and observation distributions are unknown. DPP offers a promising solution for addressing the challenges involved in inverse design optimization, providing a more efficient and effective approach compared to existing methods. <div>
arXiv:2504.20278v1 Announce Type: new 
Abstract: Inverse design optimization aims to infer system parameters from observed solutions, posing critical challenges across domains such as semiconductor manufacturing, structural engineering, materials science, and fluid dynamics. The lack of explicit mathematical representations in many systems complicates this process and makes the first order optimization impossible. Mainstream approaches, including generative AI and Bayesian optimization, address these challenges but have limitations. Generative AI is computationally expensive, while Bayesian optimization, relying on surrogate models, suffers from scalability, sensitivity to priors, and noise issues, often leading to suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel method enabling first-order gradient-based inverse optimization with surrogate machine learning models. By leveraging pretrained auxiliary Neural Operators, DPP enforces prior distribution constraints to ensure robust and meaningful solutions. This approach is particularly effective when prior data and observation distributions are unknown.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mrCAD: Multimodal Refinement of Computer-aided Designs</title>
<link>https://arxiv.org/abs/2504.20294</link>
<guid>https://arxiv.org/abs/2504.20294</guid>
<content:encoded><![CDATA[
<div> Dataset, multimodal instructions, collaboration, generative AI, refinement

Summary:
The article introduces mrCAD, a dataset of multimodal instructions in a communication game where players create and refine computer-aided designs (CADs) to match target designs. The dataset comprises 6,082 communication games and 15,163 instruction-execution rounds played by 1,092 pairs of human players. Analysis of the dataset reveals differences in the composition of drawing and text between generation and refinement instructions. Benchmarking the task with state-of-the-art VLMs shows that these models perform better at following generation instructions than refinement instructions. The findings highlight the need for a multimodal language of refinement in AI models, bridging the gap between how humans and machines perform edits. 

<br /><br />Summary: <div>
arXiv:2504.20294v1 Announce Type: new 
Abstract: A key feature of human collaboration is the ability to iteratively refine the concepts we have communicated. In contrast, while generative AI excels at the \textit{generation} of content, it often struggles to make specific language-guided \textit{modifications} of its prior outputs. To bridge the gap between how humans and machines perform edits, we present mrCAD, a dataset of multimodal instructions in a communication game. In each game, players created computer aided designs (CADs) and refined them over several rounds to match specific target designs. Only one player, the Designer, could see the target, and they must instruct the other player, the Maker, using text, drawing, or a combination of modalities. mrCAD consists of 6,082 communication games, 15,163 instruction-execution rounds, played between 1,092 pairs of human players. We analyze the dataset and find that generation and refinement instructions differ in their composition of drawing and text. Using the mrCAD task as a benchmark, we find that state-of-the-art VLMs are better at following generation instructions than refinement instructions. These results lay a foundation for analyzing and modeling a multimodal language of refinement that is not represented in previous datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Action Relational Structures for Integrated Learning and Planning</title>
<link>https://arxiv.org/abs/2504.20318</link>
<guid>https://arxiv.org/abs/2504.20318</guid>
<content:encoded><![CDATA[
<div> partial-space search, PDDL action schemas, action set heuristics, LazyLifted, ML-based heuristic

Summary: 
The paper introduces partial-space search, a novel search space for classical planning that considers the relational structure of PDDL action schemas. By providing a more detailed view of the search space and enabling early pruning of suboptimal actions, partial-space search improves planning efficiency. Action set heuristics are introduced to evaluate sets of actions in a given state, either derived automatically from existing heuristics or trained from scratch using large datasets. The new planner, LazyLifted, integrates search and learning heuristics effectively, surpassing the state-of-the-art ML-based heuristic on IPC 2023 benchmarks. LazyLifted also demonstrates superior performance on high-branching factor tasks, outperforming LAMA on combined IPC 2023 and high-branching factor benchmarks. <div>
arXiv:2504.20318v1 Announce Type: new 
Abstract: Recent advances in planning have explored using learning methods to help planning. However, little attention has been given to adapting search algorithms to work better with learning systems. In this paper, we introduce partial-space search, a new search space for classical planning that leverages the relational structure of actions given by PDDL action schemas -- a structure overlooked by traditional planning approaches. Partial-space search provides a more granular view of the search space and allows earlier pruning of poor actions compared to state-space search. To guide partial-space search, we introduce action set heuristics that evaluate sets of actions in a state. We describe how to automatically convert existing heuristics into action set heuristics. We also train action set heuristics from scratch using large training datasets from partial-space search. Our new planner, LazyLifted, exploits our better integrated search and learning heuristics and outperforms the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT) benchmarks. We also show the efficiency of LazyLifted on high-branching factor tasks and show that it surpasses LAMA in the combined IPC 2023 LT and high-branching factor benchmarks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks</title>
<link>https://arxiv.org/abs/2504.20340</link>
<guid>https://arxiv.org/abs/2504.20340</guid>
<content:encoded><![CDATA[
<div> AI-generated content, image regeneration, iterative prompt refinement, image similarity metrics, user study <br />
Summary: <br />
The study explores image regeneration in AI content creation through iterative prompt refinement. It investigates the alignment of image similarity metrics (ISMs) with human perception and the effectiveness of iterative workflows in improving the similarity of regenerated images to their targets. Results show that incremental prompt adjustments significantly enhance alignment, as confirmed by subjective evaluations and quantitative measures. This research highlights the potential of iterative workflows to enhance generative AI content creation in various domains. <br /> <div>
arXiv:2504.20340v1 Announce Type: new 
Abstract: With AI-generated content becoming ubiquitous across the web, social media, and other digital platforms, it is vital to examine how such content are inspired and generated. The creation of AI-generated images often involves refining the input prompt iteratively to achieve desired visual outcomes. This study focuses on the relatively underexplored concept of image regeneration using AI, in which a human operator attempts to closely recreate a specific target image by iteratively refining their prompt. Image regeneration is distinct from normal image generation, which lacks any predefined visual reference. A separate challenge lies in determining whether existing image similarity metrics (ISMs) can provide reliable, objective feedback in iterative workflows, given that we do not fully understand if subjective human judgments of similarity align with these metrics. Consequently, we must first validate their alignment with human perception before assessing their potential as a feedback mechanism in the iterative prompt refinement process. To address these research gaps, we present a structured user study evaluating how iterative prompt refinement affects the similarity of regenerated images relative to their targets, while also examining whether ISMs capture the same improvements perceived by human observers. Our findings suggest that incremental prompt adjustments substantially improve alignment, verified through both subjective evaluations and quantitative measures, underscoring the broader potential of iterative workflows to enhance generative AI content creation across various application domains.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs</title>
<link>https://arxiv.org/abs/2504.20406</link>
<guid>https://arxiv.org/abs/2504.20406</guid>
<content:encoded><![CDATA[
<div> Keywords: scripting interfaces, Large Language Models, offline simulation framework, Graph Neural Network, automation success rates

Summary: 
The article discusses the use of Large Language Models (LLMs) to automate script generation in software interfaces. It introduces an offline simulation framework that curates a collection of verified scripts to bridge the gap for users without programming expertise. The framework consists of task creation using top-down functionality guidance and API synergy exploration, and skill generation through trials and execution feedback. A Graph Neural Network (GNN) is utilized to predict API synergy and expand the skillset's diversity. Experiments with Adobe Illustrator show improved automation success rates, reduced response time, and savings in runtime token costs compared to traditional methods. This approach offers a controlled environment for LLM-based systems, highlighting the benefits of using execution feedback to align AI capabilities with user needs in specialized software domains.<br /><br />Summary: <div>
arXiv:2504.20406v1 Announce Type: new 
Abstract: Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library</title>
<link>https://arxiv.org/abs/2504.20426</link>
<guid>https://arxiv.org/abs/2504.20426</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, reasoning capabilities, mathematics, RV-Syn, data synthesis <br />
Summary: RV-Syn is introduced as a new approach for generating high-quality reasoning data, particularly in mathematics, to advance the reasoning capabilities of Large Language Models (LLMs). The method involves constructing a structured mathematical operation function library, generating computational graphs as solutions, and back-translating them into complex problems. RV-Syn focuses on solution-guided logic-aware problem generation and ensures the verifiability of the solving process through the executability of the computational graph. Experimental results demonstrate that RV-Syn outperforms existing synthesis methods, including human-generated problems, in achieving efficient data scaling. This novel approach provides a scalable framework for generating high-quality reasoning datasets.<br /><br />Summary: <div>
arXiv:2504.20426v1 Announce Type: new 
Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2504.20445</link>
<guid>https://arxiv.org/abs/2504.20445</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, Knowledge Distillation, Kullback-Leibler divergence, HTA-KL, CIFAR-10

Summary:<br />
Spiking Neural Networks (SNNs) are energy-efficient and biologically plausible for computation but face a performance gap compared to Artificial Neural Networks (ANNs). Knowledge distillation (KD) transfers knowledge from ANNs to SNNs, with traditional methods using Kullback-Leibler (KL) divergence. However, conventional KL-based approaches do not fully leverage SNN characteristics, leading to suboptimal generalization. To address this, a novel KD method called Head-Tail Aware Kullback-Leibler (HTA-KL) divergence is proposed. HTA-KL introduces a mask based on cumulative probability to differentiate high- and low-probability regions, improving knowledge transfer balance and performance. By integrating forward and reverse KL divergence, the method effectively aligns head and tail regions of the distribution. Evaluation on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets shows that HTA-KL outperforms existing methods with fewer timesteps.<br /> 
Summary: <div>
arXiv:2504.20445v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for energy-efficient and biologically plausible computation. However, due to limitations in existing training methods and inherent model constraints, SNNs often exhibit a performance gap when compared to Artificial Neural Networks (ANNs). Knowledge distillation (KD) has been explored as a technique to transfer knowledge from ANN teacher models to SNN student models to mitigate this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence to align output distributions. However, conventional KL-based approaches fail to fully exploit the unique characteristics of SNNs, as they tend to overemphasize high-probability predictions while neglecting low-probability ones, leading to suboptimal generalization. To address this, we propose Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically distinguish between high- and low-probability regions. It assigns adaptive weights to ensure balanced knowledge transfer, enhancing the overall performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence, our method effectively align both head and tail regions of the distribution. We evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our method outperforms existing methods on most datasets with fewer timesteps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data</title>
<link>https://arxiv.org/abs/2504.20462</link>
<guid>https://arxiv.org/abs/2504.20462</guid>
<content:encoded><![CDATA[
<div> complexity, fault response, large language models, AIOps, root cause analysis <br />
Summary: 
The article introduces TAMO, a tool-assisted LLM agent for fine-grained root cause analysis in distributed systems and cloud native technologies. TAMO addresses challenges such as text input constraints, dynamic service dependency hallucinations, and context window limitations by unifying multi-modal observational data and employing specialized tools for root cause localization and fault classification. It overcomes limitations of LLM in handling real-time service dependencies and raw data, guiding the generation of repair strategies aligned with system contexts. Experimental results demonstrate TAMO's effectiveness in root cause analysis with public datasets featuring heterogeneity and common fault types. <div>
arXiv:2504.20462v1 Announce Type: new 
Abstract: With the development of distributed systems, microservices and cloud native technologies have become central to modern enterprise software development. Despite bringing significant advantages, these technologies also increase system complexity and operational challenges. Traditional root cause analysis (RCA) struggles to achieve automated fault response, heavily relying on manual intervention. In recent years, large language models (LLMs) have made breakthroughs in contextual inference and domain knowledge integration, providing new solutions for Artificial Intelligence for Operations (AIOps). However, Existing LLM-based approaches face three key challenges: text input constraints, dynamic service dependency hallucinations, and context window limitations. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies multi-modal observational data into time-aligned representations to extract consistent features and employs specialized root cause localization and fault classification tools for perceiving the contextual environment. This approach overcomes the limitations of LLM in handling real-time changing service dependencies and raw observational data and guides LLM to generate repair strategies aligned with system contexts by structuring key information into a prompt. Experimental results show that TAMO performs well in root cause analysis when dealing with public datasets characterized by heterogeneity and common fault types, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20464</link>
<guid>https://arxiv.org/abs/2504.20464</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, GUI agents, Multi-modal Large Language Models (MLLMs), Markov Decision Processes, Training methodologies <br />
Summary: This paper presents a structured overview of recent advancements in Graphical User Interface (GUI) agents powered by Multi-modal Large Language Models (MLLMs) and enhanced by Reinforcement Learning (RL). It formalizes GUI agent tasks as Markov Decision Processes, discusses typical execution environments and evaluation metrics, and reviews the modular architecture of (M)LLM-based GUI agents. The paper categorizes GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, showcasing the progression from prompt engineering to dynamic policy learning through RL. Recent innovations in perception, decision reasoning, and action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. The paper concludes by outlining key challenges and suggesting future directions for developing more capable and reliable GUI agents. <br /><br /> <div>
arXiv:2504.20464v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured summary of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living</title>
<link>https://arxiv.org/abs/2504.20505</link>
<guid>https://arxiv.org/abs/2504.20505</guid>
<content:encoded><![CDATA[
<div> Dataset, Large Language Models, Human Activity Recognition, Ambient Sensors, MuRAL<br />
<br />
Summary: 
The paper introduces MuRAL, the first Multi-Resident Ambient sensor dataset with natural language annotations for human activity recognition. Existing datasets lack the contextual richness required for leveraging Large Language Models (LLMs) effectively in smart environments. MuRAL contains 21 hours of multi-user sensor data with detailed natural language descriptions, resident identities, and activity labels in realistic settings. The dataset is used to evaluate LLMs for subject assignment, action description, and activity classification tasks. Results show that while LLMs can interpret ambient data semantically, they struggle with multi-user ambiguity and unclear sensor contexts. The release of MuRAL aims to support further research in LLM-powered, explainable, and socially aware activity understanding in smart homes. Contact information is provided for dataset access. <div>
arXiv:2504.20505v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have shown promising potential for human activity recognition (HAR) using ambient sensors, especially through natural language reasoning and zero-shot learning. However, existing datasets such as CASAS, ARAS, and MARBLE were not originally designed with LLMs in mind and therefore lack the contextual richness, complexity, and annotation granularity required to fully exploit LLM capabilities. In this paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with natural Language, comprising over 21 hours of multi-user sensor data collected from 21 sessions in a smart-home environment. MuRAL is annotated with fine-grained natural language descriptions, resident identities, and high-level activity labels, all situated in dynamic, realistic multi-resident settings. We benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject assignment, action description, and activity classification. Our results demonstrate that while LLMs can provide rich semantic interpretations of ambient data, current models still face challenges in handling multi-user ambiguity and under-specified sensor contexts. We release MuRAL to support future research on LLM-powered, explainable, and socially aware activity understanding in smart environments. For access to the dataset, please reach out to us via the provided contact information. A direct link for dataset retrieval will be made available at this location in due course.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonIR: Training Retrievers for Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
<div> Retriever, Reasoning, Information Retrieval, Synthetic Data, State-of-the-Art <br />
Summary:<br /> 
ReasonIR-8B is introduced as the first retriever specifically trained for general reasoning tasks, outperforming existing models in reasoning-intensive benchmarks. The model is trained on a combination of synthetic and public data, achieving state-of-the-art performance on the BRIGHT benchmark. It improves performance on tasks such as MMLU and GPQA compared to closed-book baselines. ReasonIR-8B demonstrates effective use of test-time compute, showing consistent performance gains with longer and more information-rich queries. The model's training recipe is generalizable to future language models, and the code, data, and model are open-sourced for further development. <div>
arXiv:2504.20595v1 Announce Type: new 
Abstract: We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval</title>
<link>https://arxiv.org/abs/2504.20624</link>
<guid>https://arxiv.org/abs/2504.20624</guid>
<content:encoded><![CDATA[
<div> Keywords: social chatbots, proactive dialogues, personalized real-time retrieval, large language model, dialogue duration improvement

Summary:
PaRT is a novel framework designed to enhance social chatbots by implementing proactive dialogues through personalized real-time retrieval and generation. By integrating user profiles and dialogue context into a large language model (LLM), PaRT is able to refine user queries and recognize intents for upcoming conversations. The LLM generates personalized dialogue topics based on these intents, which are then used to retrieve relevant passages from RedNote. These passages are then summarized and used to prompt the LLM to generate knowledge-grounded and engagement-optimized responses. In a real-world production environment, PaRT has proven successful, with a notable 21.77% improvement in the average duration of dialogues.<br /><br />Summary: <div>
arXiv:2504.20624v1 Announce Type: new 
Abstract: Social chatbots have become essential intelligent companions in daily scenarios ranging from emotional support to personal interaction. However, conventional chatbots with passive response mechanisms usually rely on users to initiate or sustain dialogues by bringing up new topics, resulting in diminished engagement and shortened dialogue duration. In this paper, we present PaRT, a novel framework enabling context-aware proactive dialogues for social chatbots through personalized real-time retrieval and generation. Specifically, PaRT first integrates user profiles and dialogue context into a large language model (LLM), which is initially prompted to refine user queries and recognize their underlying intents for the upcoming conversation. Guided by refined intents, the LLM generates personalized dialogue topics, which then serve as targeted queries to retrieve relevant passages from RedNote. Finally, we prompt LLMs with summarized passages to generate knowledge-grounded and engagement-optimized responses. Our approach has been running stably in a real-world production environment for more than 30 days, achieving a 21.77\% improvement in the average duration of dialogues.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive maps are generative programs</title>
<link>https://arxiv.org/abs/2504.20628</link>
<guid>https://arxiv.org/abs/2504.20628</guid>
<content:encoded><![CDATA[
<div> cognitive mapping, structured spaces, generative programs, resource-efficient planning, human behavior<br />
Summary:<br />
The article explores how human resource-efficient planning may arise from representing the world as predictably structured. It suggests that cognitive maps can take the form of generative programs that exploit predictability and redundancy. Through a behavioral experiment, it shows that people navigating in structured spaces rely on modular planning strategies aligning with programmatic map representations. A computational model predicts human behavior in structured scenarios by inferring a distribution over possible programmatic cognitive maps based on human prior knowledge and using a Large Language Model to embed human priors. The model demonstrates improved computational efficiency, requiring less memory and outperforming unstructured planning algorithms with cognitive constraints. This suggests that human planning strategies rely on programmatic cognitive maps.<br /> 
Summary: <div>
arXiv:2504.20628v1 Announce Type: new 
Abstract: Making sense of the world and acting in it relies on building simplified mental representations that abstract away aspects of reality. This principle of cognitive mapping is universal to agents with limited resources. Living organisms, people, and algorithms all face the problem of forming functional representations of their world under various computing constraints. In this work, we explore the hypothesis that human resource-efficient planning may arise from representing the world as predictably structured. Building on the metaphor of concepts as programs, we propose that cognitive maps can take the form of generative programs that exploit predictability and redundancy, in contrast to directly encoding spatial layouts. We use a behavioral experiment to show that people who navigate in structured spaces rely on modular planning strategies that align with programmatic map representations. We describe a computational model that predicts human behavior in a variety of structured scenarios. This model infers a small distribution over possible programmatic cognitive maps conditioned on human prior knowledge of the world, and uses this distribution to generate resource-efficient plans. Our models leverages a Large Language Model as an embedding of human priors, implicitly learned through training on a vast corpus of human data. Our model demonstrates improved computational efficiency, requires drastically less memory, and outperforms unstructured planning algorithms with cognitive constraints at predicting human behavior, suggesting that human planning strategies rely on programmatic cognitive maps.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of AI Explainability: An Algorithmic Information Theory Approach</title>
<link>https://arxiv.org/abs/2504.20676</link>
<guid>https://arxiv.org/abs/2504.20676</guid>
<content:encoded><![CDATA[
<div> complexity, explainability, AI, algorithmic information theory, governance

Summary: 
This paper explores the theoretical limits of AI explainability using algorithmic information theory. It formalizes explainability as the approximation of complex models by simpler ones, quantifying approximation error and explanation complexity through Kolmogorov complexity. The key contributions include a complexity gap theorem, precise bounds on explanation complexity for Lipschitz functions, and a comparison between local and global explainability. It also presents a regulatory impossibility theorem showing the challenges of balancing AI capabilities, human-interpretable explanations, and error tolerance. These results provide insights for the development, evaluation, and regulation of explainable AI systems. <div>
arXiv:2504.20676v1 Announce Type: new 
Abstract: This paper establishes a theoretical foundation for understanding the fundamental limits of AI explainability through algorithmic information theory. We formalize explainability as the approximation of complex models by simpler ones, quantifying both approximation error and explanation complexity using Kolmogorov complexity. Our key theoretical contributions include: (1) a complexity gap theorem proving that any explanation significantly simpler than the original model must differ from it on some inputs; (2) precise bounds showing that explanation complexity grows exponentially with input dimension but polynomially with error tolerance for Lipschitz functions; and (3) a characterization of the gap between local and global explainability, demonstrating that local explanations can be significantly simpler while maintaining accuracy in relevant regions. We further establish a regulatory impossibility theorem proving that no governance framework can simultaneously pursue unrestricted AI capabilities, human-interpretable explanations, and negligible error. These results highlight considerations likely to be relevant to the design, evaluation, and oversight of explainable AI systems.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration</title>
<link>https://arxiv.org/abs/2504.20756</link>
<guid>https://arxiv.org/abs/2504.20756</guid>
<content:encoded><![CDATA[
<div> Graph-based framework, fault diagnosis, rotating machinery, vibration signals, classification<br />
Summary:<br />
- Proposal of a novel graph-based framework for robust and interpretable multiclass fault diagnosis in rotating machinery.
- Integration of entropy-optimized signal segmentation, time-frequency feature extraction, and graph-theoretic modeling for structured representations suitable for classification.
- Computation of graph metrics and combination with local features to capture global and segment-level fault characteristics.
- High diagnostic accuracy achieved on benchmark datasets, including the CWRU bearing dataset and the SU gearbox and bearing datasets.
- Strong noise resilience demonstrated with over 95.4% accuracy at high noise levels and excellent cross-domain transferability with up to 99.7% F1-score in load-transfer scenarios.<br /> 
Summary: <div>
arXiv:2504.20756v1 Announce Type: new 
Abstract: This paper proposes a novel graph-based framework for robust and interpretable multiclass fault diagnosis in rotating machinery. The method integrates entropy-optimized signal segmentation, time-frequency feature extraction, and graph-theoretic modeling to transform vibration signals into structured representations suitable for classification. Graph metrics, such as average shortest path length, modularity, and spectral gap, are computed and combined with local features to capture global and segment-level fault characteristics. The proposed method achieves high diagnostic accuracy when evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP loads) and the SU gearbox and bearing datasets (under different speed-load configurations). Classification scores reach up to 99.8% accuracy on Case Western Reserve University (CWRU) and 100% accuracy on the Southeast University datasets using a logistic regression classifier. Furthermore, the model exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise levels (standard deviation = 0.5), and demonstrates excellent cross-domain transferability with up to 99.7% F1-score in load-transfer scenarios. Compared to traditional techniques, this approach requires no deep learning architecture, enabling lower complexity while ensuring interpretability. The results confirm the method's scalability, reliability, and potential for real-time deployment in industrial diagnostics.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Lifted Model Construction</title>
<link>https://arxiv.org/abs/2504.20784</link>
<guid>https://arxiv.org/abs/2504.20784</guid>
<content:encoded><![CDATA[
<div> parametric factor graphs, probabilistic relational models, lifted inference, Advanced Colour Passing algorithm, $\varepsilon$-Advanced Colour Passing algorithm <br />
Summary: <br />
The article introduces the $\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm as an enhancement to the existing ACP algorithm for probabilistic relational models. While ACP requires exact matches in underlying distributions to identify indistinguishabilities, $\varepsilon$-ACP allows for a deviation in potentials based on a hyperparameter $\varepsilon. This deviation permits the algorithm to uncover and utilize indistinguishabilities that are not exact, making it more suitable for practical applications where potentials learned from data may vary. The paper proves that the approximation error of $\varepsilon$-ACP is strictly bounded, and experiments demonstrate that the approximation error in practice is minimal. The $\varepsilon$-ACP algorithm offers efficient lifted inference by accommodating deviations in potentials while still effectively exploiting indistinguishabilities in relational models. <div>
arXiv:2504.20784v1 Announce Type: new 
Abstract: Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning</title>
<link>https://arxiv.org/abs/2504.20797</link>
<guid>https://arxiv.org/abs/2504.20797</guid>
<content:encoded><![CDATA[
<div> Keywords: Few-Shot Class-Incremental Learning, dynamic world, human intelligence, independent models, Uncertainty Quantification

Summary:<br />
The paper introduces Few-Shot Class-Incremental Learning (FSCIL) as a solution to the limitations of current deep learning techniques that rely heavily on extensive training data. FSCIL focuses on continuous learning of new categories with limited samples while retaining old knowledge. The proposed method develops independent models for each session to avoid the stability-plasticity dilemma, inspired by the way humans store knowledge in different brain cortices. During testing, Uncertainty Quantification is integrated for model deployment, enhancing performance. This approach offers a fresh perspective for FSCIL and achieves state-of-the-art results on CIFAR-100 and mini-ImageNet datasets.

<br /><br />Summary: <div>
arXiv:2504.20797v1 Announce Type: new 
Abstract: Current mainstream deep learning techniques exhibit an over-reliance on extensive training data and a lack of adaptability to the dynamic world, marking a considerable disparity from human intelligence. To bridge this gap, Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous learning of new categories with limited samples without forgetting old knowledge. Existing FSCIL studies typically use a single model to learn knowledge across all sessions, inevitably leading to the stability-plasticity dilemma. Unlike machines, humans store varied knowledge in different cerebral cortices. Inspired by this characteristic, our paper aims to develop a method that learns independent models for each session. It can inherently prevent catastrophic forgetting. During the testing stage, our method integrates Uncertainty Quantification (UQ) for model deployment. Our method provides a fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on CIFAR-100 and mini-ImageNet datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ascendra: Dynamic Request Prioritization for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2504.20828</link>
<guid>https://arxiv.org/abs/2504.20828</guid>
<content:encoded><![CDATA[
<div> Efficient serving strategies, Large Language Models, Ascendra, TTFT, TBT <br />
Summary: 
The article introduces Ascendra, a Large Language Model (LLM) serving system designed to optimize Time To First Token (TTFT) and Time Between Tokens (TBT) simultaneously. Ascendra partitions GPU resources into low-priority and high-priority instances to balance throughput and latency. Low-priority instances handle requests out of order to maximize throughput but may risk request starvation. A performance model predicts requests at risk of missing SLOs and offloads them to high-priority instances optimized for low-latency execution. This strategy improves system throughput by up to 1.7x compared to existing systems while meeting both TTFT and TBT SLOs. Ascendra's innovative approach addresses the trade-off between throughput and latency, making it an efficient solution for serving large language models. <br /> <div>
arXiv:2504.20828v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for more efficient serving strategies. In this context, efficiency refers to the proportion of requests that meet their Service Level Objectives (SLOs), particularly for Time To First Token (TTFT) and Time Between Tokens (TBT). However, existing systems often prioritize one metric at the cost of the other. We present Ascendra, an LLM serving system designed to meet both TTFT and TBT SLOs simultaneously. The core insight behind Ascendra is that a request's urgency evolves as it approaches its deadline. To leverage this, Ascendra partitions GPU resources into two types of instances: low-priority and high-priority. Low-priority instances maximize throughput by processing requests out of arrival order, but at the risk of request starvation. To address this, Ascendra employs a performance model to predict requests at risk of missing their SLOs and proactively offloads them to high-priority instances. High-priority instances are optimized for low-latency execution and handle urgent requests nearing their deadlines. This partitioned architecture enables Ascendra to effectively balance high throughput and low latency. Extensive evaluation shows that Ascendra improves system throughput by up to 1.7x compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information</title>
<link>https://arxiv.org/abs/2504.20846</link>
<guid>https://arxiv.org/abs/2504.20846</guid>
<content:encoded><![CDATA[
<div> ILP, heuristic methods, post-hoc explanations, clusters, tags <br />
<br />
The article discusses generating post-hoc explanations of clusters using auxiliary information called tags. Two types of explanations are considered: disjunctive form (a set of tags) and two-clause CNF form (two sets of tags combined with the AND operator). The explanations are generated using integer linear programming (ILP) and heuristic methods. Various datasets are experimented with to provide insights from the explanations. The scalability of the explanation methods is also evaluated through experimental results. Overall, the study explores the generation of explanations for clusters based on additional information not utilized by the clustering algorithms, using different explanation formats and methods while analyzing the performance and insights provided by these explanations. <br /><br />Summary: <div>
arXiv:2504.20846v1 Announce Type: new 
Abstract: We consider generating post-hoc explanations of clusters generated from various datasets using auxiliary information which was not used by clustering algorithms. Following terminology used in previous work, we refer to the auxiliary information as tags. Our focus is on two forms of explanations, namely disjunctive form (where the explanation for a cluster consists of a set of tags) and a two-clause conjunctive normal form (CNF) explanation (where the explanation consists of two sets of tags, combined through the AND operator). We use integer linear programming (ILP) as well as heuristic methods to generate these explanations. We experiment with a variety of datasets and discuss the insights obtained from our explanations. We also present experimental results regarding the scalability of our explanation methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Leaderboard Illusion</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
<div> private testing practices, distorted playing field, biased scores, data access asymmetries, overfitting

Summary:
The study highlights systematic issues in the Chatbot Arena leaderboard for AI systems. It reveals that undisclosed private testing practices create a distorted playing field, leading to biased scores. Certain providers can select the best scores, impacting the fairness of the rankings. Additionally, proprietary closed models are favored over open-source alternatives, creating data access imbalances. This results in overfitting to Arena-specific dynamics rather than overall model quality. Access to Chatbot Arena data provides significant benefits, with even limited additional data leading to substantial performance gains. The study offers recommendations to reform the evaluation framework of the Chatbot Arena for fairer and more transparent benchmarking in the field. 

<br /><br />Summary: <div>
arXiv:2504.20879v1 Announce Type: new 
Abstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2504.20898</link>
<guid>https://arxiv.org/abs/2504.20898</guid>
<content:encoded><![CDATA[
<div> Concept Bottleneck Models, Multi-Agent Retrieval-Augmented Generation, chest X-ray, radiology report generation, AI<br />
<br />
Advancements in generative AI are opening new possibilities in automating radiology workflows. However, challenges such as interpretability and reliability have slowed down clinical adoption. This paper presents a framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to address these challenges. CBMs map chest X-ray features to clinical concepts, allowing for transparent disease classification. The RAG system utilizes multi-agent collaboration and external knowledge to generate contextually rich, evidence-based reports. The system can deliver interpretable predictions, reduce hallucinations, and produce high-quality, tailored reports through an interactive interface. By improving diagnostic consistency and providing actionable insights, this framework aims to empower radiologists and enhance patient care.<br /><br />Summary: <div>
arXiv:2504.20898v1 Announce Type: new 
Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare</title>
<link>https://arxiv.org/abs/2504.20921</link>
<guid>https://arxiv.org/abs/2504.20921</guid>
<content:encoded><![CDATA[
<div> machine learning, artificial intelligence, electronic health records, synthetic data, data validation
Summary: 
This study utilized prompt engineering with the GPT-4 API to create synthetic medical datasets for training AI algorithms within EHR applications. The generated data included a wide range of patient admission information, ensuring quality and integrity through validation techniques like BERT's Next Sentence Prediction and RoBERTa. Various models were employed for coherence, plausibility, consistency, and anomaly detection. The validated synthetic data was integrated into a PostgreSQL database to manage the EHR application's data effectively. By using generative AI models and rigorous validation processes, high-quality synthetic medical data was produced, addressing privacy concerns associated with real patient data and enabling efficient AI algorithm training. 
Summary: <div>
arXiv:2504.20921v1 Announce Type: new 
Abstract: Access to high-quality medical data is often restricted due to privacy concerns, posing significant challenges for training artificial intelligence (AI) algorithms within Electronic Health Record (EHR) applications. In this study, prompt engineering with the GPT-4 API was employed to generate high-quality synthetic datasets aimed at overcoming this limitation. The generated data encompassed a comprehensive array of patient admission information, including healthcare provider details, hospital departments, wards, bed assignments, patient demographics, emergency contacts, vital signs, immunizations, allergies, medical histories, appointments, hospital visits, laboratory tests, diagnoses, treatment plans, medications, clinical notes, visit logs, discharge summaries, and referrals. To ensure data quality and integrity, advanced validation techniques were implemented utilizing models such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly detection, and conducted diversity analysis. Synthetic data that met all validation criteria were integrated into a comprehensive PostgreSQL database, serving as the data management system for the EHR application. This approach demonstrates that leveraging generative AI models with rigorous validation can effectively produce high-quality synthetic medical data, facilitating the training of AI algorithms while addressing privacy concerns associated with real patient data.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain-Agnostic Scalable AI Safety Ensuring Framework</title>
<link>https://arxiv.org/abs/2504.20924</link>
<guid>https://arxiv.org/abs/2504.20924</guid>
<content:encoded><![CDATA[
<div> Framework, AI safety, user-defined constraints, probabilistic constraint satisfaction, internal test data

Summary:
The article presents a novel AI safety framework that allows AI systems to comply with any user-defined constraint with any desired probability across various domains. The framework combines an AI component with an optimization problem to minimize objectives while meeting user-defined constraints. It introduces internal test data for credibility assessment and a conservative testing methodology for statistical validity. The article also discusses an approximation method for loss function and its gradient computation. It mathematically proves guaranteed probabilistic constraint satisfaction under specific conditions and establishes a scaling law between safety and the number of internal test data. Experimental demonstrations show the framework's effectiveness in diverse domains, such as demand prediction, safe reinforcement learning, and AI chatbot output monitoring. The method ensures safety for user-specified constraints, outperforms existing methods in low safety threshold regions, and scales effectively with internal test data size.<br /><br />Summary: <div>
arXiv:2504.20924v1 Announce Type: new 
Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with \textbf{any user-defined constraint}, with \textbf{any desired probability}, and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose \textit{internal test data}, a supplementary set of safety-labeled data, and a \textit{conservative testing} methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms {for \textbf{up to several order of magnitudes}} existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
<link>https://arxiv.org/abs/2504.20930</link>
<guid>https://arxiv.org/abs/2504.20930</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, reinforcement learning, process rewards, RadRBench-CXR, RadRScore
Summary:<br />
- ChestX-Reasoner is a radiology diagnosis MLLM that incorporates structured reasoning processes from clinical reports for improved performance.<br />
- A large dataset is created by extracting and refining reasoning chains from routine radiology reports.<br />
- The model is trained using a two-stage framework that combines supervised fine-tuning and reinforcement learning guided by process rewards.<br />
- RadRBench-CXR is introduced as a benchmark with visual question answering samples and clinically validated reasoning steps.<br />
- ChestX-Reasoner outperforms existing medical and general-domain MLLMs in diagnostic accuracy and reasoning ability, with significant improvements in outcome accuracy. All resources are open-sourced for further research in medical reasoning MLLMs.<br /> 
<br />Summary: <div>
arXiv:2504.20930v1 Announce Type: new 
Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jekyll-and-Hyde Tipping Point in an AI's Behavior</title>
<link>https://arxiv.org/abs/2504.20980</link>
<guid>https://arxiv.org/abs/2504.20980</guid>
<content:encoded><![CDATA[
<div> Keywords: AI trust, LLM, tipping point, attention spreading, Jekyll-and-Hyde

Summary: 
The article addresses the lack of scientific predictability in AI-generated outputs, particularly in Language Model systems like ChatGPT. It discusses the growing concern around the potential dangers posed by these systems, such as misinformation and harm. By deriving a formula based on attention spreading dynamics in AI, the article identifies a tipping point where an LLM's output can suddenly become unreliable or harmful. The formula, based on basic mathematics, offers insights into how changes in prompts and training can delay or prevent this tipping point. This analytical approach aims to provide policymakers and the public with a quantitative framework to understand and mitigate risks associated with AI technologies. The article emphasizes the importance of transparent communication and decision-making in interactions with AI systems, highlighting the need for clearer guidelines on how to engage with these systems safely. 

<br /><br />Summary: <div>
arXiv:2504.20980v1 Announce Type: new 
Abstract: Trust in AI is undermined by the fact that there is no science that predicts -- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is likely to tip mid-response to become wrong, misleading, irrelevant or dangerous. With deaths and trauma already being blamed on LLMs, this uncertainty is even pushing people to treat their 'pet' LLM more politely to 'dissuade' it (or its future Artificial General Intelligence offspring) from suddenly turning on them. Here we address this acute need by deriving from first principles an exact formula for when a Jekyll-and-Hyde tipping point occurs at LLMs' most basic level. Requiring only secondary school mathematics, it shows the cause to be the AI's attention spreading so thin it suddenly snaps. This exact formula provides quantitative predictions for how the tipping-point can be delayed or prevented by changing the prompt and the AI's training. Tailored generalizations will provide policymakers and the public with a firm platform for discussing any of AI's broader uses and risks, e.g. as a personal counselor, medical advisor, decision-maker for when to use force in a conflict situation. It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains</title>
<link>https://arxiv.org/abs/2504.20983</link>
<guid>https://arxiv.org/abs/2504.20983</guid>
<content:encoded><![CDATA[
<div> game-theoretic technique, adaptive strategies, LTLf synthesis, multi-tier goal, nondeterministic planning

Summary:
1. The study focuses on a variant of LTLf synthesis that deals with multi-tier goals in planning domains.
2. Adaptive strategies are evaluated, which aim to achieve multiple challenging LTLf objectives with possible cooperation from the environment.
3. These strategies dynamically enforce objectives and utilize environmental cooperation to maximize goal achievement.
4. A game-theoretic technique is introduced to compute adaptive strategies efficiently, with polynomial complexity in the number of objectives.
5. The technique is both sound and complete, offering a practical approach for handling multi-tier goals in LTLf synthesis. <div>
arXiv:2504.20983v1 Announce Type: new 
Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for achieving a multi-tier goal, consisting of multiple increasingly challenging LTLf objectives in nondeterministic planning domains. Adaptive strategies are strategies that at any point of their execution (i) enforce the satisfaction of as many objectives as possible in the multi-tier goal, and (ii) exploit possible cooperation from the environment to satisfy as many as possible of the remaining ones. This happens dynamically: if the environment cooperates (ii) and an objective becomes enforceable (i), then our strategies will enforce it. We provide a game-theoretic technique to compute adaptive strategies that is sound and complete. Notably, our technique is polynomial, in fact quadratic, in the number of objectives. In other words, it handles multi-tier goals with only a minor overhead compared to standard LTLf synthesis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCT-QA: A Benchmark for Question Answering on Human-Centric Tables</title>
<link>https://arxiv.org/abs/2504.20047</link>
<guid>https://arxiv.org/abs/2504.20047</guid>
<content:encoded><![CDATA[
<div> Keywords: Tabular data, PDF files, natural language queries, Large Language Models, data extraction
<br />
Summary:
<br />
This paper introduces HCT-QA, a benchmark dataset for evaluating the processing and querying of human-centric tables (HCTs) using natural language queries. The dataset includes real-world and synthetic tables with associated QA pairs to assess the ability of Large Language Models in handling the complex layouts of HCTs. Current solutions for data extraction from HCTs are limited in handling their intricate structures, making them challenging for querying. The study aims to address this limitation by evaluating the performance of Large Language Models in processing and querying HCTs. The dataset includes thousands of tables and QA pairs to provide a comprehensive evaluation of the capabilities of these models. Through this benchmark, researchers can assess the effectiveness of different query engines in handling and querying diverse and complex HCTs found in various sectors. <div>
arXiv:2504.20047v1 Announce Type: cross 
Abstract: Tabular data embedded within PDF files, web pages, and other document formats are prevalent across numerous sectors such as government, engineering, science, and business. These human-centric tables (HCTs) possess a unique combination of high business value, intricate layouts, limited operational power at scale, and sometimes serve as the only data source for critical insights. However, their complexity poses significant challenges to traditional data extraction, processing, and querying methods. While current solutions focus on transforming these tables into relational formats for SQL queries, they fall short in handling the diverse and complex layouts of HCTs and hence being amenable to querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural language queries, and related answers on thousands of tables. Our dataset includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables with 67.5K QA pairs. While HCTs can be potentially processed by different type of query engines, in this paper, we focus on Large Language Models as potential engines and assess their ability in processing and querying such tables.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A constraints-based approach to fully interpretable neural networks for detecting learner behaviors</title>
<link>https://arxiv.org/abs/2504.20055</link>
<guid>https://arxiv.org/abs/2504.20055</guid>
<content:encoded><![CDATA[
<div> machine learning, education, neural network, behavior detection, interpretable<br />
<br />
Summary: 
The paper introduces a novel approach to developing a neural-network-based behavior detection model that is fully interpretable. By incorporating constraints that simplify the model's inference process and align it with human understanding, the model can effectively detect gaming-the-system behavior. The parameters extracted for explanations have clear interpretations, capturing the model's learned knowledge about learner behavior. The model's performance in identifying gaming-the-system behavior was evaluated and compared to human expert analysis, revealing successful pattern detection. The results demonstrate the model's ability to provide fully interpretable explanations. The paper discusses the significance of this approach and suggests utilizing a human-grounded approach for evaluating explainability. <div>
arXiv:2504.20055v1 Announce Type: cross 
Abstract: The increasing use of complex machine learning models in education has led to concerns about their interpretability, which in turn has spurred interest in developing explainability techniques that are both faithful to the model's inner workings and intelligible to human end-users. In this paper, we describe a novel approach to creating a neural-network-based behavior detection model that is interpretable by design. Our model is fully interpretable, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. We achieve this by implementing a series of constraints to the model that both simplify its inference process and bring it closer to a human conception of the task at hand. We train the model to detect gaming-the-system behavior, evaluate its performance on this task, and compare its learned patterns to those identified by human experts. Our results show that the model is successfully able to learn patterns indicative of gaming-the-system behavior while providing evidence for fully interpretable explanations. We discuss the implications of our approach and suggest ways to evaluate explainability using a human-grounded approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.20059</link>
<guid>https://arxiv.org/abs/2504.20059</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical trials, online platforms, TrialGPT, patient cases, recruitment 

Summary: 
Trial recruitment can be challenging due to various factors, such as lack of awareness and complex eligibility criteria. Patients are increasingly turning to social media and online health communities for support and information, expanding the recruitment pool for clinical trials. In this study, the researchers used TrialGPT, a framework that uses a large language model, to match 50 patient cases from online sources to clinical trials. The results showed that TrialGPT was 46% more effective in identifying eligible trials compared to traditional keyword-based searches. On average, each patient was eligible for approximately 7 trials. The outreach efforts to inform case authors and trial organizers of these patient-trial matches received positive feedback. This study highlights the potential of leveraging online platforms and AI-based tools like TrialGPT to enhance clinical trial recruitment and improve patient access to new treatments. 

<br /><br />Summary: <div>
arXiv:2504.20059v1 Announce Type: cross 
Abstract: Clinical trials are crucial for assessing new treatments; however, recruitment challenges - such as limited awareness, complex eligibility criteria, and referral barriers - hinder their success. With the growth of online platforms, patients increasingly turn to social media and health communities for support, research, and advocacy, expanding recruitment pools and established enrollment pathways. Recognizing this potential, we utilized TrialGPT, a framework that leverages a large language model (LLM) as its backbone, to match 50 online patient cases (collected from published case reports and a social media website) to clinical trials and evaluate performance against traditional keyword-based searches. Our results show that TrialGPT outperforms traditional methods by 46% in identifying eligible trials, with each patient, on average, being eligible for around 7 trials. Additionally, our outreach efforts to case authors and trial organizers regarding these patient-trial matches yielded highly positive feedback, which we present from both perspectives.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives</title>
<link>https://arxiv.org/abs/2504.20069</link>
<guid>https://arxiv.org/abs/2504.20069</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG signals, EEG foundation models, architectures, pre-training strategies, datasets, challenges

Summary: 
EEG signals are important for understanding brain activity and diagnosing neurological disorders. Recent development has focused on EEG foundation models (EEG-FMs) for processing and analyzing EEG data. Various EEG-FMs are discussed, including their architectures, pre-training strategies, and datasets used for pre-training and downstream tasks. The review also addresses challenges and future directions in this field. Researchers and practitioners interested in EEG analysis and EEG-FMs can benefit from the comprehensive overview provided in this review.<br /><br />Summary: <div>
arXiv:2504.20069v1 Announce Type: cross 
Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological disorders. This review focuses on the recent development of EEG foundation models(EEG-FMs), which have shown great potential in processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pre-training strategies, their pre-training and downstream datasets and other details. The review also highlights the challenges and future directions in this field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, trajectory-level agent RL, Echo Trap, reasoning-aware reward signals

Summary:
Training large language models (LLMs) as interactive agents faces challenges such as long-horizon decision making and stochastic feedback. The proposed StarPO framework and RAGEN system aim to address these challenges by introducing trajectory-level agent RL. The study on three environments highlights key findings. Firstly, the Echo Trap phenomenon is identified, and a stabilized variant, StarPO-S, is introduced to mitigate it. Secondly, the importance of diverse initial states, medium interaction granularity, and frequent sampling in shaping RL rollouts is emphasized. Lastly, the lack of reasoning-aware reward signals hinders the emergence of agent reasoning in multi-turn RL, leading to superficial strategies or hallucinatory behavior. Overall, the study provides insights into improving agent RL training for LLMs, with the code and environments available for further exploration on GitHub. 

<br /><br />Summary: <div>
arXiv:2504.20073v1 Announce Type: cross 
Abstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures</title>
<link>https://arxiv.org/abs/2504.20074</link>
<guid>https://arxiv.org/abs/2504.20074</guid>
<content:encoded><![CDATA[
<div> framework, fault detection, mitigation, approximate computing, deep neural network accelerators 

Summary:
The article introduces EPSILON, a lightweight framework designed for efficient fault detection and mitigation in approximate computing deep neural network accelerators (AxDNNs). EPSILON leverages pre-computed statistical signatures and layer-wise importance metrics to detect faults without interrupting normal execution. It utilizes a non-parametric pattern-matching algorithm for constant-time fault detection and dynamically adapts to different network architectures and fault patterns. The framework maintains model accuracy by adjusting mitigation strategies based on weight distribution and layer criticality. Extensive evaluations show that EPSILON maintains 80.05% accuracy while improving inference time by 22% and energy efficiency by 28%, making it a practical solution for deploying reliable AxDNNs in safety-critical edge applications. 

<br /><br />Summary: <div>
arXiv:2504.20074v1 Announce Type: cross 
Abstract: The increasing adoption of approximate computing in deep neural network accelerators (AxDNNs) promises significant energy efficiency gains. However, permanent faults in AxDNNs can severely degrade their performance compared to their accurate counterparts (AccDNNs). Traditional fault detection and mitigation approaches, while effective for AccDNNs, introduce substantial overhead and latency, making them impractical for energy-constrained real-time deployment. To address this, we introduce EPSILON, a lightweight framework that leverages pre-computed statistical signatures and layer-wise importance metrics for efficient fault detection and mitigation in AxDNNs. Our framework introduces a novel non-parametric pattern-matching algorithm that enables constant-time fault detection without interrupting normal execution while dynamically adapting to different network architectures and fault patterns. EPSILON maintains model accuracy by intelligently adjusting mitigation strategies based on a statistical analysis of weight distribution and layer criticality while preserving the energy benefits of approximate computing. Extensive evaluations across various approximate multipliers, AxDNN architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and fault scenarios demonstrate that EPSILON maintains 80.05\% accuracy while offering 22\% improvement in inference time and 28\% improvement in energy efficiency, establishing EPSILON as a practical solution for deploying reliable AxDNNs in safety-critical edge applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Based Learning for Improved Classification Under Adversarial Noise</title>
<link>https://arxiv.org/abs/2504.20077</link>
<guid>https://arxiv.org/abs/2504.20077</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial noise, Fast Gradient Sign Method, image classification, edge features, deep learning models

Summary: 
Adversarial noise in images can deceive deep learning models, leading to misclassification and reduced recognition accuracy. Training on images with adversarial noise can improve model resilience. The study focused on the effects of Fast Gradient Sign Method (FGSM) adversarial noise on image classification. Retraining on a combination of clean and noisy images improved model performance. Edge features were found to be more robust to adversarial attacks compared to non-edge regions. Models trained exclusively on edge-based representations showed greater resilience to noise. While adversarial noise affects non-edge regions more, retraining on original data yields slightly better accuracy. Leveraging edge-based learning can enhance deep learning model resilience against adversarial perturbations. 

<br /><br />Summary: <div>
arXiv:2504.20077v1 Announce Type: cross 
Abstract: Adversarial noise introduces small perturbations in images, misleading deep learning models into misclassification and significantly impacting recognition accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method (FGSM) adversarial noise on image classification and investigated whether training on specific image features can improve robustness. We hypothesize that while adversarial noise perturbs various regions of an image, edges may remain relatively stable and provide essential structural information for classification. To test this, we conducted a series of experiments using brain tumor and COVID datasets. Initially, we trained the models on clean images and then introduced subtle adversarial perturbations, which caused deep learning models to significantly misclassify the images. Retraining on a combination of clean and noisy images led to improved performance. To evaluate the robustness of the edge features, we extracted edges from the original/clean images and trained the models exclusively on edge-based representations. When noise was introduced to the images, the edge-based models demonstrated greater resilience to adversarial attacks compared to those trained on the original or clean images. These results suggest that while adversarial noise is able to exploit complex non-edge regions significantly more than edges, the improvement in the accuracy after retraining is marginally more in the original data as compared to the edges. Thus, leveraging edge-based learning can improve the resilience of deep learning models against adversarial perturbations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking</title>
<link>https://arxiv.org/abs/2504.20079</link>
<guid>https://arxiv.org/abs/2504.20079</guid>
<content:encoded><![CDATA[
<div> Keywords: Differentiable Architecture Search, Flexible DARTS, Entropy-based Super-Network Shrinking, neural architectures, image classification benchmarks

Summary: 
Flexible DARTS (FX-DARTS) aims to reduce prior constraints in Differentiable Architecture Search (DARTS) by allowing more flexible cell topologies and modifying the discretization mechanism for super-networks. This method leverages an Entropy-based Super-Network Shrinking (ESS) framework to address challenges arising from the elimination of prior constraints. FX-DARTS enables the derivation of neural architectures without strict rules while maintaining stability in the search space. Experimental results on image classification benchmarks show that FX-DARTS can explore a range of competitive neural architectures with trade-offs between performance and computational complexity in a single search procedure. <div>
arXiv:2504.20079v1 Announce Type: cross 
Abstract: Strong priors are imposed on the search space of Differentiable Architecture Search (DARTS), such that cells of the same type share the same topological structure and each intermediate node retains two operators from distinct nodes. While these priors reduce optimization difficulties and improve the applicability of searched architectures, they hinder the subsequent development of automated machine learning (Auto-ML) and prevent the optimization algorithm from exploring more powerful neural networks through improved architectural flexibility. This paper aims to reduce these prior constraints by eliminating restrictions on cell topology and modifying the discretization mechanism for super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which leverages an Entropy-based Super-Network Shrinking (ESS) framework, is presented to address the challenges arising from the elimination of prior constraints. Notably, FX-DARTS enables the derivation of neural architectures without strict prior rules while maintaining the stability in the enlarged search space. Experimental results on image classification benchmarks demonstrate that FX-DARTS is capable of exploring a set of neural architectures with competitive trade-offs between performance and computational complexity within a single search procedure.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNAD: Differentiable Neural Architecture Distillation</title>
<link>https://arxiv.org/abs/2504.20080</link>
<guid>https://arxiv.org/abs/2504.20080</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, computational complexity, differentiable neural architecture distillation, super-network progressive shrinking, knowledge distillation

Summary: 
The article introduces the Differentiable Neural Architecture Distillation (DNAD) algorithm, which aims to design efficient neural networks balancing model performance and computational complexity. DNAD incorporates two core components: search by deleting and search by imitating. The Super-Network Progressive Shrinking (SNPS) algorithm, based on the Differentiable Architecture Search (DARTS) framework, progressively shrinks a dynamic super-network to derive a diverse set of architectures with flexible structures. DNAD combines SNPS with Knowledge Distillation (KD) to minimize behavioral differences between super-network and teacher network, avoiding overfitting and deriving well-performing neural architectures. Experimental results on CIFAR-10 and ImageNet show that DNAD achieves top-tier performance with fewer parameters and FLOPs compared to conventional DARTS-based methods, showcasing its effectiveness in neural architecture search and model optimization.

<br /><br />Summary: <div>
arXiv:2504.20080v1 Announce Type: cross 
Abstract: To meet the demand for designing efficient neural networks with appropriate trade-offs between model performance (e.g., classification accuracy) and computational complexity, the differentiable neural architecture distillation (DNAD) algorithm is developed based on two cores, namely search by deleting and search by imitating. Primarily, to derive neural architectures in a space where cells of the same type no longer share the same topology, the super-network progressive shrinking (SNPS) algorithm is developed based on the framework of differentiable architecture search (DARTS), i.e., search by deleting. Unlike conventional DARTS-based approaches which yield neural architectures with simple structures and derive only one architecture during the search procedure, SNPS is able to derive a Pareto-optimal set of architectures with flexible structures by forcing the dynamic super-network shrink from a dense structure to a sparse one progressively. Furthermore, since knowledge distillation (KD) has shown great effectiveness to train a compact network with the assistance of an over-parameterized model, we integrate SNPS with KD to formulate the DNAD algorithm, i.e., search by imitating. By minimizing behavioral differences between the super-network and teacher network, the over-fitting of one-level DARTS is avoided and well-performed neural architectures are derived. Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both SNPS and DNAD are able to derive a set of architectures which achieve similar or lower error rates with fewer parameters and FLOPs. Particularly, DNAD achieves the top-1 error rate of 23.7% on ImageNet classification with a model of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A model and package for German ColBERT</title>
<link>https://arxiv.org/abs/2504.20083</link>
<guid>https://arxiv.org/abs/2504.20083</guid>
<content:encoded><![CDATA[
<div> introduce, German version, ColBERT, late interaction, multi-dense vector, retrieval, RAG applications, package, models, support, workflows 

Summary:
The article presents the introduction of a German version of ColBERT, a late interaction multi-dense vector retrieval method focused on applications in RAG. The main features of a package for ColBERT models are also discussed, which support both retrieval and fine-tuning workflows. The introduction of the German version expands the accessibility and applicability of ColBERT in the German language, offering enhanced retrieval capabilities for information retrieval tasks. The package for ColBERT models serves as a valuable tool for researchers and practitioners seeking to leverage advanced retrieval techniques for diverse applications. By supporting both retrieval and fine-tuning workflows, the package enables users to efficiently customize and optimize ColBERT models for specific use cases, further enhancing the retrieval performance and flexibility of the method. <div>
arXiv:2504.20083v1 Announce Type: cross 
Abstract: In this work, we introduce a German version for ColBERT, a late interaction multi-dense vector retrieval method, with a focus on RAG applications. We also present the main features of our package for ColBERT models, supporting both retrieval and fine-tuning workflows.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Risks of Generative AI in Financial Services</title>
<link>https://arxiv.org/abs/2504.20086</link>
<guid>https://arxiv.org/abs/2504.20086</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, financial services, content safety, risk taxonomy, technical guardrails 

Summary:
Generative AI (GenAI) products require clear definitions of acceptable inputs and outputs to ensure responsible development. While academic research typically focuses on evaluating models for aspects like toxicity and bias, less attention is paid to specialized domains subject to legal and regulatory scrutiny. This paper addresses AI content safety considerations in the financial services domain, presenting an AI content risk taxonomy and comparing it to existing work. The implications of risk category violations on stakeholders are discussed, highlighting the importance of industry-specific laws, regulations, and governance requirements. Evaluation of open-source technical guardrail solutions reveals gaps in detecting content risks specific to financial services. These findings emphasize the need for tailored approaches to mitigate risks and ensure the safe deployment of GenAI products in regulated industries.<br /><br />Summary: <div>
arXiv:2504.20086v1 Announce Type: cross 
Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a "safe" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation</title>
<link>https://arxiv.org/abs/2504.20092</link>
<guid>https://arxiv.org/abs/2504.20092</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized food recommendation systems, Large Language Models, Food-RecSys, Food Recommendation as Language Processing, World Food Atlas

Summary: 
The study addresses the limitations of existing personalized food recommendation systems (Food-RecSys) by identifying key components and introducing innovative solutions. A multimedia food logging platform and the World Food Atlas are proposed to capture rich contextual data and enable geolocation-based food analysis. The Food Recommendation as Language Processing (F-RLP) framework is introduced to leverage Large Language Models (LLMs) in a specialized manner for the food domain. F-RLP overcomes the shortcomings of generic models and provides a robust infrastructure for effective and truly personalized food recommendations. This integrated approach enhances the understanding and performance of Food-RecSys by utilizing tailored strategies and advanced technology, paving the way for more accurate and contextualized food recommendations in the future.<br /><br />Summary: The study introduces innovative solutions to enhance personalized food recommendation systems, including a multimedia food logging platform and the World Food Atlas. The Food Recommendation as Language Processing framework leverages Large Language Models in a specialized manner to provide advanced and truly personalized food recommendations, addressing the limitations of current generic models. By incorporating rich contextual data and geolocation-based analysis, the approach offers a robust infrastructure for effective and tailored food recommendations, improving the overall performance and understanding of Food-RecSys. <div>
arXiv:2504.20092v1 Announce Type: cross 
Abstract: Personalized food recommendation systems (Food-RecSys) critically underperform due to fragmented component understanding and the failure of conventional machine learning with vast, imbalanced food data. While Large Language Models (LLMs) offer promise, current generic Recommendation as Language Processing (RLP) strategies lack the necessary specialization for the food domain's complexity. This thesis tackles these deficiencies by first identifying and analyzing the essential components for effective Food-RecSys. We introduce two key innovations: a multimedia food logging platform for rich contextual data acquisition and the World Food Atlas, enabling unique geolocation-based food analysis previously unavailable. Building on this foundation, we pioneer the Food Recommendation as Language Processing (F-RLP) framework - a novel, integrated approach specifically architected for the food domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations of generic models and providing a robust infrastructure for effective, contextual, and truly personalized food recommendations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Healing Software Systems: Lessons from Nature, Powered by AI</title>
<link>https://arxiv.org/abs/2504.20093</link>
<guid>https://arxiv.org/abs/2504.20093</guid>
<content:encoded><![CDATA[
<div> Keywords: self-healing software, artificial intelligence, observability tools, log analysis, software resilience

Summary:<br /><br />
The paper discusses the concept of self-healing software inspired by biological healing mechanisms. It proposes a framework where observability tools act as sensory inputs, AI models diagnose and repair issues, and healing agents implement targeted code and test modifications. This approach aims to reduce downtime, accelerate debugging, and enhance software resilience. Through the combination of log analysis, static code inspection, and AI-driven patch generation, the model is evaluated through case studies and simulations. A comparison against traditional manual debugging workflows showcases the effectiveness of the proposed approach. The ultimate goal is to create intelligent software systems capable of continuous healing, similar to living organisms. <div>
arXiv:2504.20093v1 Announce Type: cross 
Abstract: As modern software systems grow in complexity and scale, their ability to autonomously detect, diagnose, and recover from failures becomes increasingly vital. Drawing inspiration from biological healing - where the human body detects damage, signals the brain, and activates targeted recovery - this paper explores the concept of self-healing software driven by artificial intelligence. We propose a novel framework that mimics this biological model system observability tools serve as sensory inputs, AI models function as the cognitive core for diagnosis and repair, and healing agents apply targeted code and test modifications. By combining log analysis, static code inspection, and AI-driven generation of patches or test updates, our approach aims to reduce downtime, accelerate debugging, and enhance software resilience. We evaluate the effectiveness of this model through case studies and simulations, comparing it against traditional manual debugging and recovery workflows. This work paves the way toward intelligent, adaptive and self-reliant software systems capable of continuous healing, akin to living organisms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics</title>
<link>https://arxiv.org/abs/2504.20099</link>
<guid>https://arxiv.org/abs/2504.20099</guid>
<content:encoded><![CDATA[
<div> transformer-based, pre-trained architectures, multivariate time series, visual analysis, interpretability, MOMENT models

Summary:<br /><br />The study evaluates the interpretability of latent spaces in time series foundation models, focusing on the MOMENT family of transformer-based pre-trained architectures. It assesses the models' capacity on various datasets for tasks like imputation, prediction, classification, and anomaly detection, with fine-tuning enhancing performance. While significant loss reduction was observed post fine-tuning, the interpretability of the latent spaces remained limited. The results hint at the robustness of MOMENT models but suggest the need for methodological refinements like alternative projection techniques, loss functions, or data preprocessing to improve interpretability. Despite limitations, foundation models like MOMENT offer notable reductions in execution time, representing a significant advancement for interactive visual analytics. <div>
arXiv:2504.20099v1 Announce Type: cross 
Abstract: The present study explores the interpretability of latent spaces produced by time series foundation models, focusing on their potential for visual analysis tasks. Specifically, we evaluate the MOMENT family of models, a set of transformer-based, pre-trained architectures for multivariate time series tasks such as: imputation, prediction, classification, and anomaly detection. We evaluate the capacity of these models on five datasets to capture the underlying structures in time series data within their latent space projection and validate whether fine tuning improves the clarity of the resulting embedding spaces. Notable performance improvements in terms of loss reduction were observed after fine tuning. Visual analysis shows limited improvement in the interpretability of the embeddings, requiring further work. Results suggest that, although Time Series Foundation Models such as MOMENT are robust, their latent spaces may require additional methodological refinements to be adequately interpreted, such as alternative projection techniques, loss functions, or data preprocessing strategies. Despite the limitations of MOMENT, foundation models supose a big reduction in execution time and so a great advance for interactive visual analytics.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenTorrent: Scaling Large Language Model Serving with An Overley Network</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
<div> decentralized overlay nodes, large-language models, GenTorrent, serving scalability, peer-to-peer networks <br />
Summary: 
The article discusses the challenges in serving scalability for open-source and cost-efficient large-language models (LLMs) and introduces GenTorrent, a decentralized LLM serving overlay. It addresses four key research problems: overlay network organization, LLM communication privacy, overlay forwarding for resource efficiency, and verification of serving quality. The prototype implementation on decentralized nodes shows a latency reduction of over 50% compared to a baseline design without overlay forwarding. Additionally, security features introduce minimal overhead to serving latency and throughput. This work marks a pioneering effort in democratizing and scaling AI serving capabilities, leveraging decentralized resources for increased efficiency and availability. <br /><br /> <div>
arXiv:2504.20101v1 Announce Type: cross 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction</title>
<link>https://arxiv.org/abs/2504.20102</link>
<guid>https://arxiv.org/abs/2504.20102</guid>
<content:encoded><![CDATA[
<div> Keywords: Protein-protein interactions, deep learning, hyperbolic graphical neural networks, multiscale graphical wavelet transform, PPI prediction

Summary:
HyboWaveNet is a novel deep learning framework for predicting protein-protein interactions (PPIs) by combining hyperbolic graphical neural networks and multiscale graphical wavelet transform. By mapping protein features into Lorentz space and using hyperbolic distance metrics, the model captures hierarchical relationships among biomolecules more effectively. The framework simulates hierarchical and scale-free biological relationships and extracts local and global interaction features across different resolutions through wavelet transforms. By generating node feature representations and positive samples under multiple views for comparison, HyboWaveNet improves predictive performance over existing methods. Experimental results on public datasets demonstrate the model's superior performance. Ablation studies show that the multi-scale graph wavelet transform module enhances predictive performance and generalization ability. This work bridges geometric deep learning and signal processing to provide a principled approach for analyzing complex biological systems. 

<br /><br />Summary: <div>
arXiv:2504.20102v1 Announce Type: cross 
Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular functions,disease pathways,and drug discovery.Although existing neural networks and machine learning methods have achieved high accuracy in PPI prediction,their black-box nature leads to a lack of causal interpretation of the prediction results and difficulty in capturing hierarchical geometries and multi-scale dynamic interaction patterns among proteins.To address these challenges, we propose HyboWaveNet,a novel deep learning framework that collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale graphical wavelet transform for robust PPI prediction. Mapping protein features to Lorentz space simulates hierarchical topological relationships among biomolecules via a hyperbolic distance metric,enabling node feature representations that better fit biological a priori.HyboWaveNet inherently simulates hierarchical and scale-free biological relationships, while the integration of wavelet transforms enables adaptive extraction of local and global interaction features across different resolutions. Our framework generates node feature representations via a graph neural network under the Lorenz model and generates pairs of positive samples under multiple different views for comparative learning, followed by further feature extraction via multi-scale graph wavelet transforms to predict potential PPIs. Experiments on public datasets show that HyboWaveNet improves over both existing state-of-the-art methods. We also demonstrate through ablation experimental studies that the multi-scale graph wavelet transform module improves the predictive performance and generalization ability of HyboWaveNet. This work links geometric deep learning and signal processing to advance PPI prediction, providing a principled approach for analyzing complex biological systems
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning</title>
<link>https://arxiv.org/abs/2504.20103</link>
<guid>https://arxiv.org/abs/2504.20103</guid>
<content:encoded><![CDATA[
<div> Drug-target interaction prediction, machine learning, interpretability, heterogeneous network, graph neural network<br />
Summary:<br />
This study introduces a novel framework for drug-target interaction prediction, addressing the black box problem of traditional machine learning methods. The framework integrates graph neural network and multi-scale signal processing to enhance prediction efficiency and interpretability. Key dimensions of the framework include a collaborative perception module for local-global feature aggregation, a graph signal decomposition module for biological interpretation, and a contrastive learning module for aligning and fusing multi-dimensional information. Experimental results demonstrate the framework's superior prediction performance on various datasets. Overall, the framework provides a comprehensive solution for drug target discovery by decoding the underlying mechanisms of biomolecular interactions. <div>
arXiv:2504.20103v1 Announce Type: cross 
Abstract: Drug-target interaction (DTI) prediction is a core task in drug development and precision medicine in the biomedical field. However, traditional machine learning methods generally have the black box problem, which makes it difficult to reveal the deep correlation between the model decision mechanism and the interaction pattern between biological molecules. This study proposes a heterogeneous network drug target interaction prediction framework, integrating graph neural network and multi scale signal processing technology to construct a model with both efficient prediction and multi level interpretability. Its technical breakthroughs are mainly reflected in the following three dimensions:Local global feature collaborative perception module. Based on heterogeneous graph convolutional neural network (HGCN), a multi order neighbor aggregation strategy is designed.Multi scale graph signal decomposition and biological interpretation module. A deep hierarchical node feature transform (GWT) architecture is proposed.Contrastive learning combining multi dimensional perspectives and hierarchical representations. By comparing the learning models, the node representations from the two perspectives of HGCN and GWT are aligned and fused, so that the model can integrate multi dimensional information and improve the prediction robustness. Experimental results show that our framework shows excellent prediction performance on all datasets. This study provides a complete solution for drug target discovery from black box prediction to mechanism decoding, and its methodology has important reference value for modeling complex biomolecular interaction systems.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers</title>
<link>https://arxiv.org/abs/2504.20105</link>
<guid>https://arxiv.org/abs/2504.20105</guid>
<content:encoded><![CDATA[
<div> Keywords: Geo-distributed Data Centers, electricity costs, workflow applications, server frequencies, scheduling algorithm

Summary:
The article discusses the challenges faced by Geo-distributed Data Centers (GDCs) in reducing electricity costs while meeting workflow application deadlines. To tackle this issue, the authors propose the Electricity Cost aware Multiple Workflows Scheduling algorithm (ECMWS) for servers with fixed frequency and power in GDCs. The algorithm includes stages such as workflow sequencing, deadline partitioning, task sequencing, and resource allocation, utilizing graph embedding models and a policy network to solve the Markov Decision Process. By statistically calibrating parameters and components, the ECMWS algorithm is shown to outperform existing methods by over 15% in experiments on diverse workflow instances. The source codes for the algorithm are available for further exploration. <div>
arXiv:2504.20105v1 Announce Type: cross 
Abstract: Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage services for massive workflow applications, resulting in high electricity costs that vary depending on geographical locations and time. How to reduce electricity costs while satisfying the deadline constraints of workflow applications is important in GDCs, which is determined by the execution time of servers, power, and electricity price. Determining the completion time of workflows with different server frequencies can be challenging, especially in scenarios with heterogeneous computing resources in GDCs. Moreover, the electricity price is also different in geographical locations and may change dynamically. To address these challenges, we develop a geo-distributed system architecture and propose an Electricity Cost aware Multiple Workflows Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and power. ECMWS comprises four stages, namely workflow sequencing, deadline partitioning, task sequencing, and resource allocation where two graph embedding models and a policy network are constructed to solve the Markov Decision Process (MDP). After statistically calibrating parameters and algorithm components over a comprehensive set of workflow instances, the proposed algorithms are compared with the state-of-the-art methods over two types of workflow instances. The experimental results demonstrate that our proposed algorithm significantly outperforms other algorithms, achieving an improvement of over 15\% while maintaining an acceptable computational time. The source codes are available at https://gitee.com/public-artifacts/ecmws-experiments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors</title>
<link>https://arxiv.org/abs/2504.20106</link>
<guid>https://arxiv.org/abs/2504.20106</guid>
<content:encoded><![CDATA[
<div> framework, language models, preference, optimization, trade-offs
Summary:<br /><br />The article introduces a novel framework called Preference Vector to address the challenge of balancing helpfulness and harm in large language models (LLMs). Existing approaches like reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) face performance conflicts and limited controllability. The Preference Vector framework trains separate models on individual preferences, extracts behavior shifts as preference vectors, and dynamically merges them at test time. This modular approach enables fine-grained preference adjustments and seamless integration of new preferences without retraining. Experimental results demonstrate that the Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment. <div>
arXiv:2504.20106v1 Announce Type: cross 
Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Pretraining for Material Property Prediction</title>
<link>https://arxiv.org/abs/2504.20112</link>
<guid>https://arxiv.org/abs/2504.20112</guid>
<content:encoded><![CDATA[
arXiv:2504.20112v1 Announce Type: cross 
Abstract: Accurate prediction of material properties facilitates the discovery of novel materials with tailored functionalities. Deep learning models have recently shown superior accuracy and flexibility in capturing structure-property relationships. However, these models often rely on supervised learning, which requires large, well-annotated datasets an expensive and time-consuming process. Self-supervised learning (SSL) offers a promising alternative by pretraining on large, unlabeled datasets to develop foundation models that can be fine-tuned for material property prediction. In this work, we propose supervised pretraining, where available class information serves as surrogate labels to guide learning, even when downstream tasks involve unrelated material properties. We evaluate this strategy on two state-of-the-art SSL models and introduce a novel framework for supervised pretraining. To further enhance representation learning, we propose a graph-based augmentation technique that injects noise to improve robustness without structurally deforming material graphs. The resulting foundation models are fine-tuned for six challenging material property predictions, achieving significant performance gains over baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE) and establishing a new benchmark in material property prediction. This study represents the first exploration of supervised pertaining with surrogate labels in material property prediction, advancing methodology and application in the field.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2504.20114</link>
<guid>https://arxiv.org/abs/2504.20114</guid>
<content:encoded><![CDATA[
arXiv:2504.20114v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional "Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined "Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\%-0.4\% of the model parameter size and reducing the query latency by approximately 99\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers</title>
<link>https://arxiv.org/abs/2504.20115</link>
<guid>https://arxiv.org/abs/2504.20115</guid>
<content:encoded><![CDATA[
arXiv:2504.20115v1 Announce Type: cross 
Abstract: Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies</title>
<link>https://arxiv.org/abs/2504.20117</link>
<guid>https://arxiv.org/abs/2504.20117</guid>
<content:encoded><![CDATA[
arXiv:2504.20117v1 Announce Type: cross 
Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis</title>
<link>https://arxiv.org/abs/2504.20118</link>
<guid>https://arxiv.org/abs/2504.20118</guid>
<content:encoded><![CDATA[
arXiv:2504.20118v1 Announce Type: cross 
Abstract: Traditional Chinese Medicine (TCM) represents a rich repository of ancient medical knowledge that continues to play an important role in modern healthcare. Due to the complexity and breadth of the TCM literature, the integration of AI technologies is critical for its modernization and broader accessibility. However, this integration poses considerable challenges, including the interpretation of obscure classical Chinese texts and the modeling of intricate semantic relationships among TCM concepts. In this paper, we develop OpenTCM, an LLM-based system that combines a domain-specific TCM knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG). First, we extract more than 3.73 million classical Chinese characters from 68 gynecological books in the Chinese Medical Classics Database, with the help of TCM and gynecology experts. Second, we construct a comprehensive multi-relational knowledge graph comprising more than 48,000 entities and 152,000 interrelationships, using customized prompts and Chinese-oriented LLMs such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last, we integrate OpenTCM with this knowledge graph, enabling high-fidelity ingredient knowledge retrieval and diagnostic question-answering without model fine-tuning. Experimental evaluations demonstrate that our prompt design and model selection significantly improve knowledge graph quality, achieving a precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves mean expert scores of 4.5 in ingredient information retrieval and 3.8 in diagnostic question-answering tasks, outperforming state-of-the-art solutions in real-world TCM use cases.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets</title>
<link>https://arxiv.org/abs/2504.20119</link>
<guid>https://arxiv.org/abs/2504.20119</guid>
<content:encoded><![CDATA[
arXiv:2504.20119v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier</title>
<link>https://arxiv.org/abs/2504.20124</link>
<guid>https://arxiv.org/abs/2504.20124</guid>
<content:encoded><![CDATA[
arXiv:2504.20124v1 Announce Type: cross 
Abstract: Early detection of asthma in children is crucial to prevent long-term respiratory complications and reduce emergency interventions. This work presents an AI-powered diagnostic pipeline that leverages Googles Health Acoustic Representations (HeAR) model to detect early signs of asthma from pediatric respiratory sounds. The SPRSound dataset, the first open-access collection of annotated respiratory sounds in children aged 1 month to 18 years, is used to extract 2-second audio segments labeled as wheeze, crackle, rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional representation using HeAR, a foundation model pretrained on 300 million health-related audio clips, including 100 million cough sounds. Multiple classifiers, including SVM, Random Forest, and MLP, are trained on these embeddings to distinguish between asthma-indicative and normal sounds. The system achieves over 91\% accuracy, with strong performance on precision-recall metrics for positive cases. In addition to classification, learned embeddings are visualized using PCA, misclassifications are analyzed through waveform playback, and ROC and confusion matrix insights are provided. This method demonstrates that short, low-resource pediatric recordings, when powered by foundation audio models, can enable fast, noninvasive asthma screening. The approach is especially promising for digital diagnostics in remote or underserved healthcare settings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization</title>
<link>https://arxiv.org/abs/2504.20125</link>
<guid>https://arxiv.org/abs/2504.20125</guid>
<content:encoded><![CDATA[
arXiv:2504.20125v1 Announce Type: cross 
Abstract: A key factor for lunar mission planning is the ability to assess the local availability of raw materials. However, many potentially relevant measurements are scattered across a variety of scientific publications. In this paper we consider the viability of obtaining lunar composition data by leveraging LLMs to rapidly process a corpus of scientific publications. While leveraging LLMs to obtain knowledge from scientific documents is not new, this particular application presents interesting challenges due to the heterogeneity of lunar samples and the nuances involved in their characterization. Accuracy and uncertainty quantification are particularly crucial since many materials properties can be sensitive to small variations in composition. Our findings indicate that off-the-shelf LLMs are generally effective at extracting data from tables commonly found in these documents. However, there remains opportunity to further refine the data we extract in this initial approach; in particular, to capture fine-grained mineralogy information and to improve performance on more subtle/complex pieces of information.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LZ Penalty: An information-theoretic repetition penalty for autoregressive language models</title>
<link>https://arxiv.org/abs/2504.20131</link>
<guid>https://arxiv.org/abs/2504.20131</guid>
<content:encoded><![CDATA[
arXiv:2504.20131v1 Announce Type: cross 
Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate repetitions in autoregressive language models without loss of capability. The penalty is based on the codelengths in the LZ77 universal lossless compression algorithm. Through the lens of the prediction-compression duality, decoding the LZ penalty has the interpretation of sampling from the residual distribution after removing the information that is highly compressible. We demonstrate the LZ penalty enables state-of-the-art open-source reasoning models to operate with greedy (temperature zero) decoding without loss of capability and without instances of degenerate repetition. Both the industry-standard frequency penalty and repetition penalty are ineffective, incurring degenerate repetition rates of up to 4%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools</title>
<link>https://arxiv.org/abs/2504.20168</link>
<guid>https://arxiv.org/abs/2504.20168</guid>
<content:encoded><![CDATA[
arXiv:2504.20168v1 Announce Type: cross 
Abstract: Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Identification in Time Series Models</title>
<link>https://arxiv.org/abs/2504.20172</link>
<guid>https://arxiv.org/abs/2504.20172</guid>
<content:encoded><![CDATA[
arXiv:2504.20172v1 Announce Type: cross 
Abstract: In this paper, we analyze the applicability of the Causal Identification algorithm to causal time series graphs with latent confounders. Since these graphs extend over infinitely many time steps, deciding whether causal effects across arbitrary time intervals are identifiable appears to require computation on graph segments of unbounded size. Even for deciding the identifiability of intervention effects on variables that are close in time, no bound is known on how many time steps in the past need to be considered. We give a first bound of this kind that only depends on the number of variables per time step and the maximum time lag of any direct or latent causal effect. More generally, we show that applying the Causal Identification algorithm to a constant-size segment of the time series graph is sufficient to decide identifiability of causal effects, even across unbounded time intervals.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration Flow Models</title>
<link>https://arxiv.org/abs/2504.20179</link>
<guid>https://arxiv.org/abs/2504.20179</guid>
<content:encoded><![CDATA[
arXiv:2504.20179v1 Announce Type: cross 
Abstract: Ordinary differential equation (ODE) based generative models have emerged as a powerful approach for producing high-quality samples in many applications. However, the ODE-based methods either suffer the discretization error of numerical solvers of ODE, which restricts the quality of samples when only a few NFEs are used, or struggle with training instability. In this paper, we proposed Integration Flow, which directly learns the integral of ODE-based trajectory paths without solving the ODE functions. Moreover, Integration Flow explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in guiding the reverse-time dynamics. We have theoretically proven this can contribute to both stability and accuracy. To the best of our knowledge, Integration Flow is the first model with a unified structure to estimate ODE-based generative models and the first to show the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, we show that Integration Flows achieve improved performance when it is applied to existing ODE-based models, such as diffusion models, Rectified Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model, 3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without reflow and 4.15 for PFGM++.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics</title>
<link>https://arxiv.org/abs/2504.20183</link>
<guid>https://arxiv.org/abs/2504.20183</guid>
<content:encoded><![CDATA[
arXiv:2504.20183v1 Announce Type: cross 
Abstract: The application of Large Language Models (LLMs) for Automated Algorithm Discovery (AAD), particularly for optimisation heuristics, is an emerging field of research. This emergence necessitates robust, standardised benchmarking practices to rigorously evaluate the capabilities and limitations of LLM-driven AAD methods and the resulting generated algorithms, especially given the opacity of their design process and known issues with existing benchmarks. To address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated Design and Evolution), a modular and extensible framework specifically designed for benchmarking LLM-driven AAD methods in a continuous black-box optimisation context. BLADE integrates collections of benchmark problems (including MA-BBOB and SBOX-COST among others) with instance generators and textual descriptions aimed at capability-focused testing, such as generalisation, specialisation and information exploitation. It offers flexible experimental setup options, standardised logging for reproducibility and fair comparison, incorporates methods for analysing the AAD process (e.g., Code Evolution Graphs and various visualisation approaches) and facilitates comparison against human-designed baselines through integration with established tools like IOHanalyser and IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically evaluate LLM-driven AAD approaches. The framework is demonstrated through two distinct use cases exploring mutation prompt strategies and function specialisation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20187</link>
<guid>https://arxiv.org/abs/2504.20187</guid>
<content:encoded><![CDATA[
arXiv:2504.20187v1 Announce Type: cross 
Abstract: In this paper, we present an adherence-aware reinforcement learning (RL) approach aimed at seeking optimal lane-changing recommendations within a semi-autonomous driving environment to enhance a single vehicle's travel efficiency. The problem is framed within a Markov decision process setting and is addressed through an adherence-aware deep Q network, which takes into account the partial compliance of human drivers with the recommended actions. This approach is evaluated within CARLA's driving environment under realistic scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting LLMs for Code Editing: Struggles and Remedies</title>
<link>https://arxiv.org/abs/2504.20196</link>
<guid>https://arxiv.org/abs/2504.20196</guid>
<content:encoded><![CDATA[
arXiv:2504.20196v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly transforming software engineering, with coding assistants embedded in an IDE becoming increasingly prevalent. While research has focused on improving the tools and understanding developer perceptions, a critical gap exists in understanding how developers actually use these tools in their daily workflows, and, crucially, where they struggle. This paper addresses part of this gap through a multi-phased investigation of developer interactions with an LLM-powered code editing and transformation feature, Transform Code, in an IDE widely used at Google. First, we analyze telemetry logs of the feature usage, revealing that frequent re-prompting can be an indicator of developer struggles with using Transform Code. Second, we conduct a qualitative analysis of unsatisfactory requests, identifying five key categories of information often missing from developer prompts. Finally, based on these findings, we propose and evaluate a tool, AutoPrompter, for automatically improving prompts by inferring missing information from the surrounding code context, leading to a 27% improvement in edit correctness on our test set.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning on a Random Lattice</title>
<link>https://arxiv.org/abs/2504.20197</link>
<guid>https://arxiv.org/abs/2504.20197</guid>
<content:encoded><![CDATA[
arXiv:2504.20197v1 Announce Type: cross 
Abstract: Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains</title>
<link>https://arxiv.org/abs/2504.20199</link>
<guid>https://arxiv.org/abs/2504.20199</guid>
<content:encoded><![CDATA[
arXiv:2504.20199v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework</title>
<link>https://arxiv.org/abs/2504.20213</link>
<guid>https://arxiv.org/abs/2504.20213</guid>
<content:encoded><![CDATA[
arXiv:2504.20213v1 Announce Type: cross 
Abstract: This paper investigates the logical reasoning capabilities of large language models (LLMs). For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic. A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions. Incorrect proofs are caught by an automated proof checker. A critical obstacle for training is the scarcity of real-world proofs. We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions. The central evaluation question is whether an LLM has indeed learned to reason. We propose tests to measure the reasoning ability of a black-box LLM. By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity. Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Platform for Generating Educational Activities to Teach English as a Second Language</title>
<link>https://arxiv.org/abs/2504.20251</link>
<guid>https://arxiv.org/abs/2504.20251</guid>
<content:encoded><![CDATA[
arXiv:2504.20251v1 Announce Type: cross 
Abstract: We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities --games and language practice exercises-- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Water Security with AI and Blockchain-Enhanced Digital Twins</title>
<link>https://arxiv.org/abs/2504.20275</link>
<guid>https://arxiv.org/abs/2504.20275</guid>
<content:encoded><![CDATA[
arXiv:2504.20275v1 Announce Type: cross 
Abstract: Water distribution systems in rural areas face serious challenges such as a lack of real-time monitoring, vulnerability to cyberattacks, and unreliable data handling. This paper presents an integrated framework that combines LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure and transparent water management. The IDS filters anomalous or spoofed data using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before validated data is logged via smart contracts on a private Ethereum blockchain using Proof of Authority (PoA) consensus. The verified data feeds into a real-time DT model supporting leak detection, consumption forecasting, and predictive maintenance. Experimental results demonstrate that the system achieves over 80 transactions per second (TPS) with under 2 seconds of latency while remaining cost-effective and scalable for up to 1,000 smart meters. This work demonstrates a practical and secure architecture for decentralized water infrastructure in under-connected rural environments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting</title>
<link>https://arxiv.org/abs/2504.20295</link>
<guid>https://arxiv.org/abs/2504.20295</guid>
<content:encoded><![CDATA[
arXiv:2504.20295v1 Announce Type: cross 
Abstract: Digital twins (DTs) are improving water distribution systems by using real-time data, analytics, and prediction models to optimize operations. This paper presents a DT platform designed for a Spanish water supply network, utilizing Long Short-Term Memory (LSTM) networks to predict water consumption. However, machine learning models are vulnerable to adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). These attacks manipulate critical model parameters, injecting subtle distortions that degrade forecasting accuracy. To further exploit these vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based approach that dynamically adjusts perturbations, making adversarial attacks more difficult to detect. Experimental results show that this approach significantly impacts prediction reliability, causing the Mean Absolute Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack strategies amplify this effect, highlighting cybersecurity risks in AI-driven DTs. These findings emphasize the urgent need for robust defenses, including adversarial training, anomaly detection, and secure data pipelines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions</title>
<link>https://arxiv.org/abs/2504.20304</link>
<guid>https://arxiv.org/abs/2504.20304</guid>
<content:encoded><![CDATA[
arXiv:2504.20304v1 Announce Type: cross 
Abstract: CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning</title>
<link>https://arxiv.org/abs/2504.20310</link>
<guid>https://arxiv.org/abs/2504.20310</guid>
<content:encoded><![CDATA[
arXiv:2504.20310v1 Announce Type: cross 
Abstract: In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers of Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation (DbM). Our definitions come in the form of a 3-round protocol between two resource-bounded parties: a trainer/defender and an attacker. The attacker aims to produce inference-time inputs that fool the training algorithm. We define correctness, completeness, and soundness properties to capture successful defense at inference time while not degrading (too much) the performance of the algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML classification tasks. Surprisingly, this is not the case for ML generative learning tasks, where there are many possible correct outputs that can be generated for each input. We show a separation between DbD and DbM by exhibiting a generative learning task for which is possible to defend by mitigation but is provably impossible to defend by detection under the assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation phase uses significantly fewer samples than the initial training algorithm.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training</title>
<link>https://arxiv.org/abs/2504.20314</link>
<guid>https://arxiv.org/abs/2504.20314</guid>
<content:encoded><![CDATA[
arXiv:2504.20314v1 Announce Type: cross 
Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN) training paradigm that offers computational simplicity and memory savings. However, this seemingly promising approach faces a significant and long-ignored challenge. ZO requires generating a substantial number of Gaussian random numbers, which poses significant difficulties and even makes it infeasible for hardware platforms, such as FPGAs and ASICs. In this paper, we identify this critical issue, which arises from the mismatch between algorithm and hardware designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO framework. Specifically, we design random number reuse strategies to significantly reduce the demand for random number generation and introduce a hardware-friendly adaptive scaling method to replace the costly Gaussian distribution with a uniform distribution. Our experiments show that PeZO reduces the required LUTs and FFs for random number generation by 48.6\% and 12.7\%, and saves at maximum 86\% power consumption, all without compromising training performance, making ZO optimization feasible for on-device training. To the best of our knowledge, we are the first to explore the potential of on-device ZO optimization, providing valuable insights for future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation</title>
<link>https://arxiv.org/abs/2504.20323</link>
<guid>https://arxiv.org/abs/2504.20323</guid>
<content:encoded><![CDATA[
arXiv:2504.20323v1 Announce Type: cross 
Abstract: This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI</title>
<link>https://arxiv.org/abs/2504.20342</link>
<guid>https://arxiv.org/abs/2504.20342</guid>
<content:encoded><![CDATA[
arXiv:2504.20342v1 Announce Type: cross 
Abstract: Reflexion is an AI-powered platform designed to enable structured emotional self-reflection at scale. By integrating real-time emotion detection, layered reflective prompting, and metaphorical storytelling generation, Reflexion empowers users to engage in autonomous emotional exploration beyond basic sentiment categorization. Grounded in theories of expressive writing, cognitive restructuring, self-determination, and critical consciousness development, the system scaffolds a progressive journey from surface-level emotional recognition toward value-aligned action planning. Initial pilot studies with diverse participants demonstrate positive outcomes in emotional articulation, cognitive reframing, and perceived psychological resilience. Reflexion represents a promising direction for scalable, theory-informed affective computing interventions aimed at fostering emotional literacy and psychological growth across educational, therapeutic, and public health contexts.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices</title>
<link>https://arxiv.org/abs/2504.20348</link>
<guid>https://arxiv.org/abs/2504.20348</guid>
<content:encoded><![CDATA[
arXiv:2504.20348v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) enable real-time function calling in edge AI systems but introduce significant computational overhead, leading to high power consumption and carbon emissions. Existing methods optimize for performance while neglecting sustainability, making them inefficient for energy-constrained environments. We introduce CarbonCall, a sustainability-aware function-calling framework that integrates dynamic tool selection, carbon-aware execution, and quantized LLM adaptation. CarbonCall adjusts power thresholds based on real-time carbon intensity forecasts and switches between model variants to sustain high tokens-per-second throughput under power constraints. Experiments on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by up to 52%, power consumption by 30%, and execution time by 30%, while maintaining high efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.20355</link>
<guid>https://arxiv.org/abs/2504.20355</guid>
<content:encoded><![CDATA[
arXiv:2504.20355v1 Announce Type: cross 
Abstract: In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Unit Test Case Generation: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2504.20357</link>
<guid>https://arxiv.org/abs/2504.20357</guid>
<content:encoded><![CDATA[
arXiv:2504.20357v1 Announce Type: cross 
Abstract: Software is omnipresent within all factors of society. It is thus important to ensure that software are well tested to mitigate bad user experiences as well as the potential for severe financial and human losses. Software testing is however expensive and absorbs valuable time and resources. As a result, the field of automated software testing has grown of interest to researchers in past decades. In our review of present and past research papers, we have identified an information gap in the areas of improvement for the Genetic Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current challenges that face automated testing has also been identified. We therefore present this systematic literature review in an effort to consolidate existing knowledge in regards to the evolutionary approaches as well as their improvements and resulting limitations. These improvements include hybrid algorithm combinations as well as interoperability with mutation testing and neural networks. We will also explore the main test criterion that are used in these algorithms alongside the challenges currently faced in the field related to readability, mocking and more.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury</title>
<link>https://arxiv.org/abs/2504.20368</link>
<guid>https://arxiv.org/abs/2504.20368</guid>
<content:encoded><![CDATA[
arXiv:2504.20368v1 Announce Type: cross 
Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an assumed or known shared perspective (global model) to explain patient observations with evidence assigned towards a clinical assessment. But in several (complex) medical situations, multiple experts work together as a team to optimize health evaluation and decision-making by leveraging different perspectives. Such consensus-driven reasoning reflects individual knowledge contributing toward a broader perspective on the patient. In this light, we introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework automating the learning of these global models and their incorporation as prior beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof of concept with a prosocial MAS application for predicting acute kidney injuries (AKIs). In this case, we found that incorporating a global structure enabled multiple agents to achieve better performance (average precision, AP) in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT, AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs. baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180) for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents with higher recall scores reported lower confidence levels in the initial round on true positive and false negative cases. But after explicit interactions, their confidence in their decisions increased (suggesting reinforced belief). In contrast, the SF-FT agent with the lowest recall decreased its confidence in true positive and false negative cases (suggesting a new belief). This approach suggests that learning and leveraging global structures in MAS is necessary prior to achieving competitive classification and diagnostic reasoning performance.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses</title>
<link>https://arxiv.org/abs/2504.20405</link>
<guid>https://arxiv.org/abs/2504.20405</guid>
<content:encoded><![CDATA[
arXiv:2504.20405v1 Announce Type: cross 
Abstract: While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and presents a deep learning (DL) framework for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were trained using a combination of CNNs and transformers. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83% and 94%, and specificity of 91% and 86% for standard MRIs and MRAs, respectively. Notably, model performance on non-invasive standard MRIs matched or surpassed radiologists interpreting MRAs. External validation demonstrated initial generalizability across imaging protocols. This study demonstrates that DL models can achieve radiologist-level diagnostic performance on standard MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular codebase for training and evaluating deep learning models on 3D medical imaging data, we aim to accelerate research in musculoskeletal imaging and support the development of new datasets for clinically challenging diagnostic tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation</title>
<link>https://arxiv.org/abs/2504.20408</link>
<guid>https://arxiv.org/abs/2504.20408</guid>
<content:encoded><![CDATA[
arXiv:2504.20408v1 Announce Type: cross 
Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the evolution of particle distribution functions through a nonlinear, high-dimensional collision operator. However, its numerical solution remains computationally demanding, particularly for inelastic collisions and high-dimensional velocity domains. In this work, we propose the Fourier Neural Spectral Network (FourierSpecNet), a hybrid framework that integrates the Fourier spectral method with deep learning to approximate the collision operator in Fourier space efficiently. FourierSpecNet achieves resolution-invariant learning and supports zero-shot super-resolution, enabling accurate predictions at unseen resolutions without retraining. Beyond empirical validation, we establish a consistency result showing that the trained operator converges to the spectral solution as the discretization is refined. We evaluate our method on several benchmark cases, including Maxwellian and hard-sphere molecular models, as well as inelastic collision scenarios. The results demonstrate that FourierSpecNet offers competitive accuracy while significantly reducing computational cost compared to traditional spectral solvers. Our approach provides a robust and scalable alternative for solving the Boltzmann equation across both elastic and inelastic regimes.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrashFixer: A crash resolution agent for the Linux kernel</title>
<link>https://arxiv.org/abs/2504.20412</link>
<guid>https://arxiv.org/abs/2504.20412</guid>
<content:encoded><![CDATA[
arXiv:2504.20412v1 Announce Type: cross 
Abstract: Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent that is applicable to Linux kernel bugs. Inspired by the typical workflow of a kernel developer, we identify the key capabilities an expert developer leverages to resolve a kernel crash. Using this as our guide, we revisit the kGym platform and identify key system improvements needed to practically run LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of code). We implement these changes by extending kGym to create an improved platform - called kGymSuite, which will be open-sourced. Finally, the paper presents an evaluation of various repair strategies for such complex kernel bugs and showcases the value of explicitly generating a hypothesis before attempting to fix bugs in complex systems such as the Linux kernel. We also evaluated CrashFixer's capabilities on still open bugs, and found at least two patch suggestions considered plausible to resolve the reported bug.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement</title>
<link>https://arxiv.org/abs/2504.20434</link>
<guid>https://arxiv.org/abs/2504.20434</guid>
<content:encoded><![CDATA[
arXiv:2504.20434v1 Announce Type: cross 
Abstract: In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection</title>
<link>https://arxiv.org/abs/2504.20437</link>
<guid>https://arxiv.org/abs/2504.20437</guid>
<content:encoded><![CDATA[
arXiv:2504.20437v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized natural language understanding and generation but face significant memory bottlenecks during training. GaLore, Gradient Low-Rank Projection, addresses this issue by leveraging the inherent low-rank structure of weight gradients, enabling substantial memory savings without sacrificing performance. Recent works further extend GaLore from various aspects, including low-bit quantization and higher-order tensor structures. However, there are several remaining challenges for GaLore, such as the computational overhead of SVD for subspace updates and the integration with state-of-the-art training parallelization strategies (e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable GaLore framework that addresses these challenges and incorporates recent advancements. In addition, we demonstrate the scalability of GaLore 2 by pre-training Llama 7B from scratch using up to 500 billion training tokens, highlighting its potential impact on real LLM pre-training scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?</title>
<link>https://arxiv.org/abs/2504.20444</link>
<guid>https://arxiv.org/abs/2504.20444</guid>
<content:encoded><![CDATA[
arXiv:2504.20444v1 Announce Type: cross 
Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech</title>
<link>https://arxiv.org/abs/2504.20447</link>
<guid>https://arxiv.org/abs/2504.20447</guid>
<content:encoded><![CDATA[
arXiv:2504.20447v1 Announce Type: cross 
Abstract: Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing News Recommendation with Hierarchical LLM Prompting</title>
<link>https://arxiv.org/abs/2504.20452</link>
<guid>https://arxiv.org/abs/2504.20452</guid>
<content:encoded><![CDATA[
arXiv:2504.20452v1 Announce Type: cross 
Abstract: Personalized news recommendation systems often struggle to effectively capture the complexity of user preferences, as they rely heavily on shallow representations, such as article titles and abstracts. To address this problem, we introduce a novel method, namely PNR-LLM, for Large Language Models for Personalized News Recommendation. Specifically, PNR-LLM harnesses the generation capabilities of LLMs to enrich news titles and abstracts, and consequently improves recommendation quality. PNR-LLM contains a novel module, News Enrichment via LLMs, which generates deeper semantic information and relevant entities from articles, transforming shallow contents into richer representations. We further propose an attention mechanism to aggregate enriched semantic- and entity-level data, forming unified user and news embeddings that reveal a more accurate user-news match. Extensive experiments on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines. Moreover, the proposed data enrichment module is model-agnostic, and we empirically show that applying our proposed module to multiple existing models can further improve their performance, verifying the advantage of our design.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Estimation of Continual Causal Effect for Dataset Shifting Streams</title>
<link>https://arxiv.org/abs/2504.20471</link>
<guid>https://arxiv.org/abs/2504.20471</guid>
<content:encoded><![CDATA[
arXiv:2504.20471v1 Announce Type: cross 
Abstract: Causal effect estimation has been widely used in marketing optimization. The framework of an uplift model followed by a constrained optimization algorithm is popular in practice. To enhance performance in the online environment, the framework needs to be improved to address the complexities caused by temporal dataset shift. This paper focuses on capturing the dataset shift from user behavior and domain distribution changing over time. We propose an Incremental Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle this challenge. The ICE-PKD framework includes two components: (i) a multi-treatment uplift network that eliminates confounding bias using counterfactual regression; (ii) an incremental training strategy that adapts to the temporal dataset shift by updating with the latest data and protects generalization via replay-based knowledge distillation. We also revisit the uplift modeling metrics and introduce a novel metric for more precise online evaluation in multiple treatment scenarios. Extensive experiments on both simulated and online datasets show that the proposed framework achieves better performance. The ICE-PKD framework has been deployed in the marketing system of Huaxiaozhu, a ride-hailing platform in China.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias</title>
<link>https://arxiv.org/abs/2504.20482</link>
<guid>https://arxiv.org/abs/2504.20482</guid>
<content:encoded><![CDATA[
arXiv:2504.20482v1 Announce Type: cross 
Abstract: Knowledge distillation typically transfers knowledge from a teacher model to a student model by minimizing differences between their output distributions. However, existing distillation approaches largely focus on mimicking absolute probabilities and neglect the valuable relational inductive biases embedded in the teacher's relative predictions, leading to exposure bias. In this paper, we propose Group Relative Knowledge Distillation (GRKD), a novel framework that distills teacher knowledge by learning the relative ranking among classes, rather than directly fitting the absolute distribution. Specifically, we introduce a group relative loss that encourages the student model to preserve the pairwise preference orderings provided by the teacher's outputs. Extensive experiments on classification benchmarks demonstrate that GRKD achieves superior generalization compared to existing methods, especially in tasks requiring fine-grained class differentiation. Our method provides a new perspective on exploiting teacher knowledge, focusing on relational structure rather than absolute likelihood.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression</title>
<link>https://arxiv.org/abs/2504.20493</link>
<guid>https://arxiv.org/abs/2504.20493</guid>
<content:encoded><![CDATA[
arXiv:2504.20493v1 Announce Type: cross 
Abstract: While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a "thinking-stopped" vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named "Reasoning Interruption Attack", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations</title>
<link>https://arxiv.org/abs/2504.20520</link>
<guid>https://arxiv.org/abs/2504.20520</guid>
<content:encoded><![CDATA[
arXiv:2504.20520v1 Announce Type: cross 
Abstract: Learning from few demonstrations to develop policies robust to variations in robot initial positions and object poses is a problem of significant practical interest in robotics. Compared to imitation learning, which often struggles to generalize from limited samples, reinforcement learning (RL) can autonomously explore to obtain robust behaviors. Training RL agents through direct interaction with the real world is often impractical and unsafe, while building simulation environments requires extensive manual effort, such as designing scenes and crafting task-specific reward functions. To address these challenges, we propose an integrated real-to-sim-to-real pipeline that constructs simulation environments based on expert demonstrations by identifying scene objects from images and retrieving their corresponding 3D models from existing libraries. We introduce a projection-based reward model for RL policy training that is supervised by a vision-language model (VLM) using human-guided object projection relationships as prompts, with the policy further fine-tuned using expert demonstrations. In general, our work focuses on the construction of simulation environments and RL-based policy training, ultimately enabling the deployment of reliable robotic control policies in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generate more than one child in your co-evolutionary semi-supervised learning GAN</title>
<link>https://arxiv.org/abs/2504.20560</link>
<guid>https://arxiv.org/abs/2504.20560</guid>
<content:encoded><![CDATA[
arXiv:2504.20560v1 Announce Type: cross 
Abstract: Generative Adversarial Networks (GANs) are very useful methods to address semi-supervised learning (SSL) datasets, thanks to their ability to generate samples similar to real data. This approach, called SSL-GAN has attracted many researchers in the last decade. Evolutionary algorithms have been used to guide the evolution and training of SSL-GANs with great success. In particular, several co-evolutionary approaches have been applied where the two networks of a GAN (the generator and the discriminator) are evolved in separate populations. The co-evolutionary approaches published to date assume some spatial structure of the populations, based on the ideas of cellular evolutionary algorithms. They also create one single individual per generation and follow a generational replacement strategy in the evolution. In this paper, we re-consider those algorithmic design decisions and propose a new co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN), with panmictic population, elitist replacement, and more than one individual in the offspring. We evaluate the performance of our proposed method using three standard benchmark datasets. The results show that creating more than one offspring per population and using elitism improves the results in comparison with a classical SSL-GAN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2504.20566</link>
<guid>https://arxiv.org/abs/2504.20566</guid>
<content:encoded><![CDATA[
arXiv:2504.20566v1 Announce Type: cross 
Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Online Incremental Learning (BOIL), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BOIL method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BOIL, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Reasoning in Large Language Models with One Training Example</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
arXiv:2504.20571v1 Announce Type: cross 
Abstract: We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Retrieval in the Age of Generative AI: The RGB Model</title>
<link>https://arxiv.org/abs/2504.20610</link>
<guid>https://arxiv.org/abs/2504.20610</guid>
<content:encoded><![CDATA[
arXiv:2504.20610v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2504.20612</link>
<guid>https://arxiv.org/abs/2504.20612</guid>
<content:encoded><![CDATA[
arXiv:2504.20612v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has enhanced software development processes, minimizing the time and effort required for coding and enhancing developer productivity. However, despite their potential benefits, code generated by LLMs has been shown to generate insecure code in controlled environments, raising critical concerns about their reliability and security in real-world applications. This paper uses predefined security parameters to evaluate the security compliance of LLM-generated code across multiple models, such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals critical vulnerabilities in authentication mechanisms, session management, input validation and HTTP security headers. Although some models implement security measures to a limited extent, none fully align with industry best practices, highlighting the associated risks in automated software development. Our findings underscore that human expertise is crucial to ensure secure software deployment or review of LLM-generated code. Also, there is a need for robust security assessment frameworks to enhance the reliability of LLM-generated code in real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models</title>
<link>https://arxiv.org/abs/2504.20625</link>
<guid>https://arxiv.org/abs/2504.20625</guid>
<content:encoded><![CDATA[
arXiv:2504.20625v1 Announce Type: cross 
Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are crucial in multiple audio signal processing tasks. High-quality RIR estimates drive applications such as virtual microphones, sound source localization, augmented reality, and data augmentation. However, obtaining RIR measurements with high spatial resolution is resource-intensive, making it impractical for large spaces or when dense sampling is required. This research addresses the challenge of estimating RIRs at unmeasured locations within a room using Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the analogy between RIR matrices and image inpainting, transforming RIR data into a format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our approach's effectiveness on microphone arrays of different curvatures, from linear to semi-circular. Our method successfully reconstructs missing RIRs, even in large gaps between microphones. Under these conditions, it achieves accurate reconstruction, significantly outperforming baseline Spline Cubic Interpolation in terms of Normalized Mean Square Error and Cosine Distance between actual and interpolated RIRs.
  This research highlights the potential of using generative models for effective RIR interpolation, paving the way for generating additional data from limited real-world measurements.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</title>
<link>https://arxiv.org/abs/2504.20629</link>
<guid>https://arxiv.org/abs/2504.20629</guid>
<content:encoded><![CDATA[
arXiv:2504.20629v1 Announce Type: cross 
Abstract: In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Stochastic Rounding with Few Random Bits</title>
<link>https://arxiv.org/abs/2504.20634</link>
<guid>https://arxiv.org/abs/2504.20634</guid>
<content:encoded><![CDATA[
arXiv:2504.20634v1 Announce Type: cross 
Abstract: Large-scale numerical computations make increasing use of low-precision (LP) floating point formats and mixed precision arithmetic, which can be enhanced by the technique of stochastic rounding (SR), that is, rounding an intermediate high-precision value up or down randomly as a function of the value's distance to the two rounding candidates. Stochastic rounding requires, in addition to the high-precision input value, a source of random bits. As the provision of high-quality random bits is an additional computational cost, it is of interest to require as few bits as possible while maintaining the desirable properties of SR in a given computation, or computational domain. This paper examines a number of possible implementations of few-bit stochastic rounding (FBSR), and shows how several natural implementations can introduce sometimes significant bias into the rounding process, which are not present in the case of infinite-bit, infinite-precision examinations of these implementations. The paper explores the impact of these biases in machine learning examples, and hence opens another class of configuration parameters of which practitioners should be aware when developing or adopting low-precision floating point. Code is available at http://github.com/graphcore-research/arith25-stochastic-rounding.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations</title>
<link>https://arxiv.org/abs/2504.20643</link>
<guid>https://arxiv.org/abs/2504.20643</guid>
<content:encoded><![CDATA[
arXiv:2504.20643v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</title>
<link>https://arxiv.org/abs/2504.20648</link>
<guid>https://arxiv.org/abs/2504.20648</guid>
<content:encoded><![CDATA[
arXiv:2504.20648v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated learning, ethics, and the double black box problem in medical AI</title>
<link>https://arxiv.org/abs/2504.20656</link>
<guid>https://arxiv.org/abs/2504.20656</guid>
<content:encoded><![CDATA[
arXiv:2504.20656v1 Announce Type: cross 
Abstract: Federated learning (FL) is a machine learning approach that allows multiple devices or institutions to collaboratively train a model without sharing their local data with a third-party. FL is considered a promising way to address patient privacy concerns in medical artificial intelligence. The ethical risks of medical FL systems themselves, however, have thus far been underexamined. This paper aims to address this gap. We argue that medical FL presents a new variety of opacity -- federation opacity -- that, in turn, generates a distinctive double black box problem in healthcare AI. We highlight several instances in which the anticipated benefits of medical FL may be exaggerated, and conclude by highlighting key challenges that must be overcome to make FL ethically feasible in medicine.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks</title>
<link>https://arxiv.org/abs/2504.20658</link>
<guid>https://arxiv.org/abs/2504.20658</guid>
<content:encoded><![CDATA[
arXiv:2504.20658v1 Announce Type: cross 
Abstract: AI-generated synthetic media are increasingly used in real-world scenarios, often with the purpose of spreading misinformation and propaganda through social media platforms, where compression and other processing can degrade fake detection cues. Currently, many forensic tools fail to account for these in-the-wild challenges. In this work, we introduce TrueFake, a large-scale benchmarking dataset of 600,000 images including top notch generative techniques and sharing via three different social networks. This dataset allows for rigorous evaluation of state-of-the-art fake image detectors under very realistic and challenging conditions. Through extensive experimentation, we analyze how social media sharing impacts detection performance, and identify current most effective detection and training strategies. Our findings highlight the need for evaluating forensic models in conditions that mirror real-world use.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advance Fake Video Detection via Vision Transformers</title>
<link>https://arxiv.org/abs/2504.20669</link>
<guid>https://arxiv.org/abs/2504.20669</guid>
<content:encoded><![CDATA[
arXiv:2504.20669v1 Announce Type: cross 
Abstract: Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2504.20673</link>
<guid>https://arxiv.org/abs/2504.20673</guid>
<content:encoded><![CDATA[
arXiv:2504.20673v1 Announce Type: cross 
Abstract: Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?</title>
<link>https://arxiv.org/abs/2504.20699</link>
<guid>https://arxiv.org/abs/2504.20699</guid>
<content:encoded><![CDATA[
arXiv:2504.20699v1 Announce Type: cross 
Abstract: A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
arXiv:2504.20708v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\% and 10\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vulnerability Reports with Automated and Augmented Description Summarization</title>
<link>https://arxiv.org/abs/2504.20726</link>
<guid>https://arxiv.org/abs/2504.20726</guid>
<content:encoded><![CDATA[
arXiv:2504.20726v1 Announce Type: cross 
Abstract: Public vulnerability databases, such as the National Vulnerability Database (NVD), document vulnerabilities and facilitate threat information sharing. However, they often suffer from short descriptions and outdated or insufficient information. In this paper, we introduce Zad, a system designed to enrich NVD vulnerability descriptions by leveraging external resources. Zad consists of two pipelines: one collects and filters supplementary data using two encoders to build a detailed dataset, while the other fine-tunes a pre-trained model on this dataset to generate enriched descriptions. By addressing brevity and improving content quality, Zad produces more comprehensive and cohesive vulnerability descriptions. We evaluate Zad using standard summarization metrics and human assessments, demonstrating its effectiveness in enhancing vulnerability information.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Surrogate Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.20733</link>
<guid>https://arxiv.org/abs/2504.20733</guid>
<content:encoded><![CDATA[
arXiv:2504.20733v1 Announce Type: cross 
Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn a neural network representation, i.e. regular patterns of normal data, which anomalies are deviating from. Inspired by a similar concept in engineering, we refer to our methodology as surrogate anomaly detection. We formalize the concept of surrogate anomaly detection into a set of axioms required for optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121 benchmark datasets, demonstrating its competitive performance against 19 existing methods, as well as the scalability and reliability of our method.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
arXiv:2504.20734v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In defence of post-hoc explanations in medical AI</title>
<link>https://arxiv.org/abs/2504.20741</link>
<guid>https://arxiv.org/abs/2504.20741</guid>
<content:encoded><![CDATA[
arXiv:2504.20741v1 Announce Type: cross 
Abstract: Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a "silver bullet" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
arXiv:2504.20752v1 Announce Type: cross 
Abstract: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
arXiv:2504.20769v1 Announce Type: cross 
Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation</title>
<link>https://arxiv.org/abs/2504.20770</link>
<guid>https://arxiv.org/abs/2504.20770</guid>
<content:encoded><![CDATA[
arXiv:2504.20770v1 Announce Type: cross 
Abstract: The discovery of new molecules based on the original chemical molecule distributions is of great importance in medicine. The graph transformer, with its advantages of high performance and scalability compared to traditional graph networks, has been widely explored in recent research for applications of graph structures. However, current transformer-based graph decoders struggle to effectively utilize graph information, which limits their capacity to leverage only sequences of nodes rather than the complex topological structures of molecule graphs. This paper focuses on building a graph transformer-based framework for molecular generation, which we call \textbf{JTreeformer} as it transforms graph generation into junction tree generation. It combines GCN parallel with multi-head attention as the encoder. It integrates a directed acyclic GCN into a graph-based Transformer to serve as a decoder, which can iteratively synthesize the entire molecule by leveraging information from the partially constructed molecular structure at each step. In addition, a diffusion model is inserted in the latent space generated by the encoder, to enhance the efficiency and effectiveness of sampling further. The empirical results demonstrate that our novel framework outperforms existing molecule generation methods, thus offering a promising tool to advance drug discovery (https://anonymous.4open.science/r/JTreeformer-C74C).
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe</title>
<link>https://arxiv.org/abs/2504.20776</link>
<guid>https://arxiv.org/abs/2504.20776</guid>
<content:encoded><![CDATA[
arXiv:2504.20776v1 Announce Type: cross 
Abstract: Currently available tools for the automated acoustic recognition of European insects in natural soundscapes are limited in scope. Large and ecologically heterogeneous acoustic datasets are currently needed for these algorithms to cross-contextually recognize the subtle and complex acoustic signatures produced by each species, thus making the availability of such datasets a key requisite for their development. Here we present ECOSoundSet (European Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when including subspecies) present in North, Central, and temperate Western Europe (Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland, Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly through targeted fieldwork in South France and Catalonia and partly through contributions from various European entomologists. The dataset is composed of a combination of coarsely labeled recordings, for which we can only infer the presence, at some point, of their target species (weak labeling), and finely annotated recordings, for which we know the specific time and frequency range of each insect sound present in the recording (strong labeling). We also provide a train/validation/test split of the strongly labeled recordings, with respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate their incorporation in the training and evaluation of deep learning algorithms. This dataset could serve as a meaningful complement to recordings already available online for the training of deep learning algorithms for the acoustic classification of orthopterans and cicadas in North, Central, and temperate Western Europe.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs in Generating Design Rationale for Software Architecture Decisions</title>
<link>https://arxiv.org/abs/2504.20781</link>
<guid>https://arxiv.org/abs/2504.20781</guid>
<content:encoded><![CDATA[
arXiv:2504.20781v1 Announce Type: cross 
Abstract: Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. Based on the results, we further discussed the pros and cons of the three prompting strategies and the strengths and limitations of the DR generated by LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges</title>
<link>https://arxiv.org/abs/2504.20799</link>
<guid>https://arxiv.org/abs/2504.20799</guid>
<content:encoded><![CDATA[
arXiv:2504.20799v1 Announce Type: cross 
Abstract: Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</title>
<link>https://arxiv.org/abs/2504.20808</link>
<guid>https://arxiv.org/abs/2504.20808</guid>
<content:encoded><![CDATA[
arXiv:2504.20808v1 Announce Type: cross 
Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion</title>
<link>https://arxiv.org/abs/2504.20829</link>
<guid>https://arxiv.org/abs/2504.20829</guid>
<content:encoded><![CDATA[
arXiv:2504.20829v1 Announce Type: cross 
Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for LLM Reasoning Under Memory Constraints</title>
<link>https://arxiv.org/abs/2504.20834</link>
<guid>https://arxiv.org/abs/2504.20834</guid>
<content:encoded><![CDATA[
arXiv:2504.20834v1 Announce Type: cross 
Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within targeted problem spaces in large language models (LLMs) under memory and compute constraints. Our focus is on critic-free methods compatible with LoRA fine-tuning on a single 40GB GPU, a common limitation in academic settings. We introduce S-GRPO, a memory-efficient variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching strategy for fine-grained credit assignment. Despite limited resources, when used to fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in multi-digit multiplication tasks, underscoring the potential of RL fine-tuning under hardware constraints. Additionally, we find that our full-token GRPO baseline under LoRA fine-tuning did not improve model performance (compared to base model) on either task, suggesting that our memory-efficient methods may act as a form of regularization that stabilizes training when only a small subset of parameters are updated.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadSAM: Segmenting 3D radiological images with a 2D promptable model</title>
<link>https://arxiv.org/abs/2504.20837</link>
<guid>https://arxiv.org/abs/2504.20837</guid>
<content:encoded><![CDATA[
arXiv:2504.20837v1 Announce Type: cross 
Abstract: Medical image segmentation is a crucial and time-consuming task in clinical care, where mask precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, as it provides an interactive interface based on visual prompting and edition to refine an initial segmentation. This model has strong generalization capabilities, does not rely on predefined classes, and adapts to diverse objects; however, it is pre-trained on natural images and lacks the ability to process medical data effectively. In addition, this model is built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging are based on 2D models, thus requiring one prompt per slice to segment 3D objects, making the segmentation process tedious. They also lack important features such as editing. To bridge this gap, we propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt. In practice, we train a 2D model using noisy masks as initial prompts, in addition to bounding boxes and points. We then use this novel prompt type with an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art models on this benchmark using the AMOS abdominal organ segmentation dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating the Structural Bias in Graph Adversarial Defenses</title>
<link>https://arxiv.org/abs/2504.20848</link>
<guid>https://arxiv.org/abs/2504.20848</guid>
<content:encoded><![CDATA[
arXiv:2504.20848v1 Announce Type: cross 
Abstract: In recent years, graph neural networks (GNNs) have shown great potential in addressing various graph structure-related downstream tasks. However, recent studies have found that current GNNs are susceptible to malicious adversarial attacks. Given the inevitable presence of adversarial attacks in the real world, a variety of defense methods have been proposed to counter these attacks and enhance the robustness of GNNs. Despite the commendable performance of these defense methods, we have observed that they tend to exhibit a structural bias in terms of their defense capability on nodes with low degree (i.e., tail nodes), which is similar to the structural bias of traditional GNNs on nodes with low degree in the clean graph. Therefore, in this work, we propose a defense strategy by including hetero-homo augmented graph construction, $k$NN augmented graph construction, and multi-view node-wise attention modules to mitigate the structural bias of GNNs against adversarial attacks. Notably, the hetero-homo augmented graph consists of removing heterophilic links (i.e., links connecting nodes with dissimilar features) globally and adding homophilic links (i.e., links connecting nodes with similar features) for nodes with low degree. To further enhance the defense capability, an attention mechanism is adopted to adaptively combine the representations from the above two kinds of graph views. We conduct extensive experiments to demonstrate the defense and debiasing effect of the proposed strategy on benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework</title>
<link>https://arxiv.org/abs/2504.20851</link>
<guid>https://arxiv.org/abs/2504.20851</guid>
<content:encoded><![CDATA[
arXiv:2504.20851v1 Announce Type: cross 
Abstract: In an era increasingly shaped by decentralized knowledge ecosystems and pervasive AI technologies, fostering sustainable learner agency has become a critical educational imperative. This study introduces a novel conceptual framework integrating Generative Artificial Intelligence and Learning Analytics to cultivate Self-Directed Growth, a dynamic competency that enables learners to iteratively drive their own developmental pathways across diverse contexts.Building upon critical gaps in current research on Self Directed Learning and AI-mediated education, the proposed Aspire to Potentials for Learners (A2PL) model reconceptualizes the interplay of learner aspirations, complex thinking, and summative self-assessment within GAI supported environments.Methodological implications for future intervention design and learning analytics applications are discussed, positioning Self-Directed Growth as a pivotal axis for developing equitable, adaptive, and sustainable learning systems in the digital era.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning</title>
<link>https://arxiv.org/abs/2504.20854</link>
<guid>https://arxiv.org/abs/2504.20854</guid>
<content:encoded><![CDATA[
arXiv:2504.20854v1 Announce Type: cross 
Abstract: This paper lays the foundation for Genie, a testing framework that captures the impact of real hardware network behavior on ML workload performance, without requiring expensive GPUs. Genie uses CPU-initiated traffic over a hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim simulator to model interaction between the network and the ML workload.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
arXiv:2504.20859v1 Announce Type: cross 
Abstract: As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data</title>
<link>https://arxiv.org/abs/2504.20862</link>
<guid>https://arxiv.org/abs/2504.20862</guid>
<content:encoded><![CDATA[
arXiv:2504.20862v1 Announce Type: cross 
Abstract: The remarkable success of Deep Learning approaches is often based and demonstrated on large public datasets. However, when applying such approaches to internal, private datasets, one frequently faces challenges arising from structural differences in the datasets, domain shift, and the lack of labels. In this work, we introduce Tabular Data Adapters (TDA), a novel method for generating soft labels for unlabeled tabular data in outlier detection tasks. By identifying statistically similar public datasets and transforming private data (based on a shared autoencoder) into a format compatible with state-of-the-art public models, our approach enables the generation of weak labels. It thereby can help to mitigate the cold start problem of labeling by basing on existing outlier detection models for public datasets. In experiments on 50 tabular datasets across different domains, we demonstrate that our method is able to provide more accurate annotations than baseline approaches while reducing computational time. Our approach offers a scalable, efficient, and cost-effective solution, to bridge the gap between public research models and real-world industrial applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks</title>
<link>https://arxiv.org/abs/2504.20869</link>
<guid>https://arxiv.org/abs/2504.20869</guid>
<content:encoded><![CDATA[
arXiv:2504.20869v1 Announce Type: cross 
Abstract: Graph neural networks have been widely utilized to solve graph-related tasks because of their strong learning power in utilizing the local information of neighbors. However, recent studies on graph adversarial attacks have proven that current graph neural networks are not robust against malicious attacks. Yet much of the existing work has focused on the optimization objective based on attack performance to obtain (near) optimal perturbations, but paid less attention to the strength quantification of each perturbation such as the injection of a particular node/link, which makes the choice of perturbations a black-box model that lacks interpretability. In this work, we propose the concept of noise to quantify the attack strength of each adversarial link. Furthermore, we propose three attack strategies based on the defined noise and classification margins in terms of single and multiple steps optimization. Extensive experiments conducted on benchmark datasets against three representative graph neural networks demonstrate the effectiveness of the proposed attack strategies. Particularly, we also investigate the preferred patterns of effective adversarial perturbations by analyzing the corresponding properties of the selected perturbation nodes.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation</title>
<link>https://arxiv.org/abs/2504.20887</link>
<guid>https://arxiv.org/abs/2504.20887</guid>
<content:encoded><![CDATA[
arXiv:2504.20887v1 Announce Type: cross 
Abstract: When optimising for conditional value at risk (CVaR) using policy gradients (PG), current meth- ods rely on discarding a large proportion of tra- jectories, resulting in poor sample efficiency. We propose a reformulation of the CVaR optimisation problem by capping the total return of trajecto- ries used in training, rather than simply discard- ing them, and show that this is equivalent to the original problem if the cap is set appropriately. We show, with empirical results in an number of environments, that this reformulation of the prob- lem results in consistently improved performance compared to baselines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers</title>
<link>https://arxiv.org/abs/2504.20902</link>
<guid>https://arxiv.org/abs/2504.20902</guid>
<content:encoded><![CDATA[
arXiv:2504.20902v1 Announce Type: cross 
Abstract: A person downloading a pre-trained model from the web should be aware of its biases. Existing approaches for bias identification rely on datasets containing labels for the task of interest, something that a non-expert may not have access to, or may not have the necessary resources to collect: this greatly limits the number of tasks where model biases can be identified. In this work, we present Classifier-to-Bias (C2B), the first bias discovery framework that works without access to any labeled data: it only relies on a textual description of the classification task to identify biases in the target classification model. This description is fed to a large language model to generate bias proposals and corresponding captions depicting biases together with task-specific target labels. A retrieval model collects images for those captions, which are then used to assess the accuracy of the model w.r.t. the given biases. C2B is training-free, does not require any annotations, has no constraints on the list of biases, and can be applied to any pre-trained model on any classification task. Experiments on two publicly available datasets show that C2B discovers biases beyond those of the original datasets and outperforms a recent state-of-the-art bias detection baseline that relies on task-specific annotations, being a promising first step toward addressing task-agnostic unsupervised bias detection.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling AI-Human Collaboration as a Multi-Agent Adaptation</title>
<link>https://arxiv.org/abs/2504.20903</link>
<guid>https://arxiv.org/abs/2504.20903</guid>
<content:encoded><![CDATA[
arXiv:2504.20903v1 Announce Type: cross 
Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a function of task structure, advancing a generalizable framework for strategic decision-making in organizations. Distinguishing between heuristic-based human adaptation and rule-based AI search, we model interactions across modular (parallel) and sequenced (interdependent) tasks using an NK model. Our results reveal that in modular tasks, AI often substitutes for humans - delivering higher payoffs unless human expertise is very high, and the AI search space is either narrowly focused or extremely broad. In sequenced tasks, interesting complementarities emerge. When an expert human initiates the search and AI subsequently refines it, aggregate performance is maximized. Conversely, when AI leads, excessive heuristic refinement by the human can reduce payoffs. We also show that even "hallucinatory" AI - lacking memory or structure - can improve outcomes when augmenting low-capability humans by helping escape local optima. These results yield a robust implication: the effectiveness of AI-human collaboration depends less on context or industry, and more on the underlying task structure. By elevating task decomposition as the central unit of analysis, our model provides a transferable lens for strategic decision-making involving humans and an agentic AI across diverse organizational settings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines</title>
<link>https://arxiv.org/abs/2504.20910</link>
<guid>https://arxiv.org/abs/2504.20910</guid>
<content:encoded><![CDATA[
arXiv:2504.20910v1 Announce Type: cross 
Abstract: Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniquely interactional mode of testing, one in which individuals on red teams actively interact with the system, leveraging natural language to simulate malicious actors and solicit harmful outputs. This interactional labor done by red teams can result in mental health harms that are uniquely tied to the adversarial engagement strategies necessary to effectively red team. The importance of ensuring that generative AI models do not propagate societal or individual harm is widely recognized -- one less visible foundation of end-to-end AI safety is also the protection of the mental health and wellbeing of those who work to keep model outputs safe. In this paper, we argue that the unmet mental health needs of AI red-teamers is a critical workplace safety concern. Through analyzing the unique mental health impacts associated with the labor done by red teams, we propose potential individual and organizational strategies that could be used to meet these needs, and safeguard the mental health of red-teamers. We develop our proposed strategies through drawing parallels between common red-teaming practices and interactional labor common to other professions (including actors, mental health professionals, conflict photographers, and content moderators), describing how individuals and organizations within these professional spaces safeguard their mental health given similar psychological demands. Drawing on these protective practices, we describe how safeguards could be adapted for the distinct mental health challenges experienced by red teaming organizations as they mitigate emerging technological risks on the new digital frontlines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNAMAX: Dynamic computing for Transformers and Mamba based architectures</title>
<link>https://arxiv.org/abs/2504.20922</link>
<guid>https://arxiv.org/abs/2504.20922</guid>
<content:encoded><![CDATA[
arXiv:2504.20922v1 Announce Type: cross 
Abstract: Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models</title>
<link>https://arxiv.org/abs/2504.20946</link>
<guid>https://arxiv.org/abs/2504.20946</guid>
<content:encoded><![CDATA[
arXiv:2504.20946v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification</title>
<link>https://arxiv.org/abs/2504.20964</link>
<guid>https://arxiv.org/abs/2504.20964</guid>
<content:encoded><![CDATA[
arXiv:2504.20964v1 Announce Type: cross 
Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</title>
<link>https://arxiv.org/abs/2504.20970</link>
<guid>https://arxiv.org/abs/2504.20970</guid>
<content:encoded><![CDATA[
arXiv:2504.20970v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning</title>
<link>https://arxiv.org/abs/2504.20988</link>
<guid>https://arxiv.org/abs/2504.20988</guid>
<content:encoded><![CDATA[
arXiv:2504.20988v1 Announce Type: cross 
Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm for collaborative machine learning that combines the strengths of Federated Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier communication structure that avoids the single point of failure inherent in FL and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local (ELL). At equal communication budgets (total edges), HSL achieves higher performance than ELL, while at significantly lower communication budgets, it can match ELL's performance. For instance, with only 400 edges, HSL reaches the same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL also achieves stronger consensus among nodes after mixing, resulting in improved performance with fewer training rounds. We substantiate these claims through rigorous theoretical analyses and extensive experimental results, showcasing HSL's practicality for large-scale collaborative learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Efficient Exploration by Large Language Model Agents</title>
<link>https://arxiv.org/abs/2504.20997</link>
<guid>https://arxiv.org/abs/2504.20997</guid>
<content:encoded><![CDATA[
arXiv:2504.20997v1 Announce Type: cross 
Abstract: A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YoChameleon: Personalized Vision and Language Generation</title>
<link>https://arxiv.org/abs/2504.20998</link>
<guid>https://arxiv.org/abs/2504.20998</guid>
<content:encoded><![CDATA[
arXiv:2504.20998v1 Announce Type: cross 
Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem</title>
<link>https://arxiv.org/abs/2311.18662</link>
<guid>https://arxiv.org/abs/2311.18662</guid>
<content:encoded><![CDATA[
arXiv:2311.18662v3 Announce Type: replace 
Abstract: Route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation, often integrated within larger Intelligent Transportation Systems (ITS). This problem is commonly formulated as a Vehicle Routing Problem (VRP) known as the Team Orienteering Problem (TOP). Existing solvers for this problem primarily rely on either linear programming, which provides accurate solutions but requires computation times that grow with the size of the problem, or heuristic methods, which typically find suboptimal solutions in a shorter time. In this paper, we introduce TOP-Former, a multi-agent route planning neural network designed to efficiently and accurately solve the Team Orienteering Problem. The proposed algorithm is based on a centralized Transformer neural network capable of learning to encode the scenario (modeled as a graph) and analyze the complete context of all agents to deliver fast, precise, and collaborative solutions. Unlike other neural network-based approaches that adopt a more local perspective, TOP-Former is trained to understand the global situation of the vehicle fleet and generate solutions that maximize long-term expected returns. Extensive experiments demonstrate that the presented system outperforms most state-of-the-art methods in terms of both accuracy and computation speed.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem Solving Through Human-AI Preference-Based Cooperation</title>
<link>https://arxiv.org/abs/2408.07461</link>
<guid>https://arxiv.org/abs/2408.07461</guid>
<content:encoded><![CDATA[
arXiv:2408.07461v4 Announce Type: replace 
Abstract: While there is a widespread belief that artificial general intelligence (AGI) -- or even superhuman AI -- is imminent, complex problems in expert domains are far from being solved. We argue that such problems require human-AI cooperation and that the current state of the art in generative AI is unable to play the role of a reliable partner due to a multitude of shortcomings, including difficulty to keep track of a complex solution artifact (e.g., a software program), limited support for versatile human preference expression and lack of adapting to human preference in an interactive setting. To address these challenges, we propose HAICo2, a novel human-AI co-construction framework. We take first steps towards a formalization of HAICo2 and discuss the difficult open research problems that it faces.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing</title>
<link>https://arxiv.org/abs/2302.00671</link>
<guid>https://arxiv.org/abs/2302.00671</guid>
<content:encoded><![CDATA[
arXiv:2302.00671v3 Announce Type: replace-cross 
Abstract: Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeled data between tasks. In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors. We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm. Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments. Videos are available at https://qmp-mtrl.github.io.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Consistency for Assuring Reliability of Large Language Models</title>
<link>https://arxiv.org/abs/2308.09138</link>
<guid>https://arxiv.org/abs/2308.09138</guid>
<content:encoded><![CDATA[
arXiv:2308.09138v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition</title>
<link>https://arxiv.org/abs/2310.18511</link>
<guid>https://arxiv.org/abs/2310.18511</guid>
<content:encoded><![CDATA[
arXiv:2310.18511v3 Announce Type: replace-cross 
Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at CVPR2023, showcasing the winning method's utilization of a modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies</title>
<link>https://arxiv.org/abs/2401.10266</link>
<guid>https://arxiv.org/abs/2401.10266</guid>
<content:encoded><![CDATA[
arXiv:2401.10266v2 Announce Type: replace-cross 
Abstract: Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process (TEP) is conducted. This research will be beneficial for both researchers who are new to the field and experts, as it covers the literature on condition monitoring and state-of-the-art methods alongside the challenges and possible solutions to them.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI: The Era of Semantic Decoding</title>
<link>https://arxiv.org/abs/2403.14562</link>
<guid>https://arxiv.org/abs/2403.14562</guid>
<content:encoded><![CDATA[
arXiv:2403.14562v2 Announce Type: replace-cross 
Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection</title>
<link>https://arxiv.org/abs/2403.15509</link>
<guid>https://arxiv.org/abs/2403.15509</guid>
<content:encoded><![CDATA[
arXiv:2403.15509v2 Announce Type: replace-cross 
Abstract: Representation learning (RL) methods for cyberattack detection face the diversity and sophistication of attack data, leading to the issue of mixed representations of different classes, particularly as the number of classes increases. To address this, the paper proposes a novel deep learning architecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input data into latent space and then deterministically shifts data samples of different classes further apart to create separable data representations, referred to as representation targets. TAE's decoder then projects the input data into these representation targets. After training, TAE's decoder extracts data representations. TAE's representation target serves as a novel dynamic codeword, which refers to the vector that represents a specific class. This vector is updated after each training epoch for every data sample, in contrast to the conventional fixed codeword that does not incorporate information from the input data. We conduct extensive experiments on diverse cybersecurity datasets, including seven IoT botnet datasets, two network IDS datasets, three malware datasets, one cloud DDoS dataset, and ten artificial datasets as the number of classes increases. TAE boosts accuracy and F-score in attack detection by around 2% compared to state-of-the-art models, achieving up to 96.1% average accuracy in IoT attack detection. Additionally, TAE is well-suited for cybersecurity applications and potentially for IoT systems, with a model size of approximately 1 MB and an average running time of around 2.6E-07 seconds for extracting a data sample.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent</title>
<link>https://arxiv.org/abs/2405.00252</link>
<guid>https://arxiv.org/abs/2405.00252</guid>
<content:encoded><![CDATA[
arXiv:2405.00252v3 Announce Type: replace-cross 
Abstract: Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD. However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization. Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian. Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.
  Matrix inversion can be translated into solving a series of linear equations. Given that quantum linear solver algorithms (QLSAs), leveraging the principles of quantum superposition and entanglement, can operate within a $\text{polylog}(N)$ time frame, they present a promising approach with exponential acceleration. Specifically, one of the most recent QLSAs demonstrates a complexity scaling of $O(d\cdot\kappa \log(N\cdot\kappa/\epsilon))$, depending on: {size~$N$, condition number~$\kappa$, error tolerance~$\epsilon$, quantum oracle sparsity~$d$} of the matrix. However, this also implies that their potential exponential advantage may be hindered by certain properties (i.e. $\kappa$ and $d$).
  We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating neural network training with Newton's GD. Q-Newton utilizes a streamlined scheduling module that coordinates between quantum and classical linear solvers, by estimating & reducing $\kappa$ and constructing $d$ for the quantum solver.
  Our evaluation showcases the potential for Q-Newton to significantly reduce the total training time compared to commonly used optimizers like SGD. We hypothesize a future scenario where the gate time of quantum machines is reduced, possibly realized by attoseconds physics. Our evaluation establishes an ambitious and promising target for the evolution of quantum computing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation</title>
<link>https://arxiv.org/abs/2405.15863</link>
<guid>https://arxiv.org/abs/2405.15863</guid>
<content:encoded><![CDATA[
arXiv:2405.15863v3 Announce Type: replace-cross 
Abstract: Text-to-music (TTM) generation, which converts textual descriptions into audio, opens up innovative avenues for multimedia creation. Achieving high quality and diversity in this process demands extensive, high-quality data, which are often scarce in available datasets. Most open-source datasets frequently suffer from issues like low-quality waveforms and low text-audio consistency, hindering the advancement of music generation models. To address these challenges, we propose a novel quality-aware training paradigm for generating high-quality, high-musicality music from large-scale, quality-imbalanced datasets. Additionally, by leveraging unique properties in the latent space of musical signals, we adapt and implement a masked diffusion transformer (MDT) model for the TTM task, showcasing its capacity for quality control and enhanced musicality. Furthermore, we introduce a three-stage caption refinement approach to address low-quality captions' issue. Experiments show state-of-the-art (SOTA) performance on benchmark datasets including MusicCaps and the Song-Describer Dataset with both objective and subjective metrics. Demo audio samples are available at https://qa-mdt.github.io/, code and pretrained checkpoints are open-sourced at https://github.com/ivcylc/OpenMusic.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Analysis of Human Alignment with *PO</title>
<link>https://arxiv.org/abs/2407.15229</link>
<guid>https://arxiv.org/abs/2407.15229</guid>
<content:encoded><![CDATA[
arXiv:2407.15229v2 Announce Type: replace-cross 
Abstract: At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment</title>
<link>https://arxiv.org/abs/2408.00137</link>
<guid>https://arxiv.org/abs/2408.00137</guid>
<content:encoded><![CDATA[
arXiv:2408.00137v2 Announce Type: replace-cross 
Abstract: A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law</title>
<link>https://arxiv.org/abs/2408.05231</link>
<guid>https://arxiv.org/abs/2408.05231</guid>
<content:encoded><![CDATA[
arXiv:2408.05231v3 Announce Type: replace-cross 
Abstract: A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering</title>
<link>https://arxiv.org/abs/2408.15268</link>
<guid>https://arxiv.org/abs/2408.15268</guid>
<content:encoded><![CDATA[
arXiv:2408.15268v3 Announce Type: replace-cross 
Abstract: This article proposes a novel fuzzy clustering based anomaly detection method for pump current time series of EDFA systems. The proposed change detection framework (CDF) strategically combines the advantages of entropy analysis (EA) and principle component analysis (PCA) with fuzzy clustering procedures. In the framework, EA is applied for dynamic selection of features for reduction of the feature space and increase of computational performance. Furthermore, PCA is utilized to extract features from the raw feature space to enable generalization capability of the subsequent fuzzy clustering procedures. Three different fuzzy clustering methods, more precisely the fuzzy clustering algorithm, a probabilistic clustering algorithm and a possibilistic clustering algorithm are evaluated for performance and generalization. Hence, the proposed framework has the innovative feature to detect changes in pump current time series at an early stage for arbitrary points of operation, compared to state-of-the-art predefined alarms in commercially used EDFAs. Moreover, the approach is implemented and tested using experimental data. In addition, the proposed framework enables further approaches of applying decentralized predictive maintenance for optical fiber networks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination</title>
<link>https://arxiv.org/abs/2409.14634</link>
<guid>https://arxiv.org/abs/2409.14634</guid>
<content:encoded><![CDATA[
arXiv:2409.14634v4 Announce Type: replace-cross 
Abstract: The scientific ideation process often involves blending salient aspects of existing papers to create new ideas, and facet-based ideation is an established framework for idea generation. To see how large language models (LLMs) might assist in this process, we contribute a novel mixed-initiative ideation tool called Scideator. Starting from a user-provided set of scientific papers, Scideator extracts key facets -- purposes, mechanisms, and evaluations -- from these and related papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users gauge idea originality by searching the literature for overlaps, assessing idea novelty and providing explanations. To support these tasks, Scideator introduces three LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker. In a within-subjects user study (N=22) with computer-science researchers comparing Scideator to a strong baseline, our tool provided significantly more creativity support, particularly with respect to exploration, which participants considered the most important factor for idea generation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</title>
<link>https://arxiv.org/abs/2409.16902</link>
<guid>https://arxiv.org/abs/2409.16902</guid>
<content:encoded><![CDATA[
arXiv:2409.16902v4 Announce Type: replace-cross 
Abstract: Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art performance on the UW-COT220 dataset. The dataset and codes are available at~\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\color{magenta}{here}}.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthiness of Stochastic Gradient Descent in Distributed Learning</title>
<link>https://arxiv.org/abs/2410.21491</link>
<guid>https://arxiv.org/abs/2410.21491</guid>
<content:encoded><![CDATA[
arXiv:2410.21491v3 Announce Type: replace-cross 
Abstract: Distributed learning (DL) uses multiple nodes to accelerate training, enabling efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains unexplored, leaving important questions about its reliability unanswered.
  In this paper, we provide a trustworthiness evaluation of compressed versus uncompressed SGD. Specifically, we conducted empirical studies using GradInv attacks, revealing that compressed SGD demonstrates significantly higher resistance to privacy leakage compared to uncompressed SGD. In addition, our findings suggest that MIA may not be a reliable metric for assessing privacy risks in distributed learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title>
<link>https://arxiv.org/abs/2410.24175</link>
<guid>https://arxiv.org/abs/2410.24175</guid>
<content:encoded><![CDATA[
arXiv:2410.24175v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM</title>
<link>https://arxiv.org/abs/2411.05982</link>
<guid>https://arxiv.org/abs/2411.05982</guid>
<content:encoded><![CDATA[
arXiv:2411.05982v2 Announce Type: replace-cross 
Abstract: Sandboxes and other dynamic analysis processes are prevalent in malware detection systems nowadays to enhance the capability of detecting 0-day malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in modern malware samples, and sandboxes can suffer from false negatives and analysis failures when analyzing the samples with TADAs. In such cases, human reverse engineers will get involved in conducting dynamic analysis manually (i.e., debugging, patching), which in turn also gets obstructed by TADAs. In this work, we propose a Large Language Model (LLM) based workflow that can pinpoint the location of the TADA implementation in the code, to help reverse engineers place breakpoints used in debugging. Our evaluation shows that we successfully identified the locations of 87.80% known TADA implementations adopted from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement</title>
<link>https://arxiv.org/abs/2411.09413</link>
<guid>https://arxiv.org/abs/2411.09413</guid>
<content:encoded><![CDATA[
arXiv:2411.09413v2 Announce Type: replace-cross 
Abstract: The early diagnosis of autism spectrum disorder (ASD) is critically dependent on systematic observation and analysis of children's social behaviors. While current methodologies predominantly utilize supervised learning approaches, their clinical adoption faces two principal limitations: insufficient ASD diagnostic samples and inadequate interpretability of the detection outcomes. This paper presents a novel zero-shot ASD detection framework based on script-centric behavioral understanding with emotional enhancement, which is designed to overcome the aforementioned clinical constraints. The proposed pipeline automatically converts audio-visual data into structured behavioral text scripts through computer vision techniques, subsequently capitalizing on the generalization capabilities of large language models (LLMs) for zero-shot/few-shot ASD detection. Three core technical contributions are introduced: (1) A multimodal script transcription module transforming behavioral cues into structured textual representations. (2) An emotion textualization module encoding emotional dynamics as the contextual features to augment behavioral understanding. (3) A domain-specific prompt engineering strategy enables the injection of clinical knowledge into LLMs. Our method achieves an F1-score of 95.24\% in diagnosing ASD in children with an average age of two years while generating interpretable detection rationales. This work opens up new avenues for leveraging the power of LLMs in analyzing and understanding ASD-related human behavior, thereby enhancing the accuracy of assisted autism diagnosis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks</title>
<link>https://arxiv.org/abs/2411.18384</link>
<guid>https://arxiv.org/abs/2411.18384</guid>
<content:encoded><![CDATA[
arXiv:2411.18384v2 Announce Type: replace-cross 
Abstract: The rise of programmable data plane (PDP) and in-network computing (INC) paradigms paves the way for the development of network devices (switches, network interface cards, etc.) capable of performing advanced processing tasks. This allows running various types of algorithms, including machine learning, within the network itself to support user and network services. In particular, this paper delves into the deployment of in-network learning models with the aim of implementing fully distributed intrusion detection systems (IDS) or intrusion prevention systems (IPS). Specifically, a model is proposed for the optimal distribution of the IDS/IPS workload among data plane devices with the aim of ensuring complete network security without excessively burdening the normal operations of the devices. Furthermore, a meta-heuristic approach is proposed to reduce the long computation time required by the exact solution provided by the mathematical model and its performance is evaluated. The analysis conducted and the results obtained demonstrate the enormous potential of the proposed new approach for the creation of intelligent data planes that act effectively and autonomously as the first line of defense against cyber attacks, with minimal additional workload on the network devices involved.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering</title>
<link>https://arxiv.org/abs/2412.06832</link>
<guid>https://arxiv.org/abs/2412.06832</guid>
<content:encoded><![CDATA[
arXiv:2412.06832v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG tailored for real-world Question Answering (QA) applications. By integrating task-specific non-functional requirements-such as answer quality, cost, and latency-into the system, we enable dynamic reconfiguration to meet diverse SLAs. Our method maps these Service Level Objectives (SLOs) to system-level parameters, allowing the generation of optimal results within specified resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic re-orchestration of a multi-agent RAG system can effectively manage the trade-off between answer quality and cost. By adjusting the system based on query intent and operational conditions, we systematically balance performance and resource utilization. This approach allows the system to meet SLOs for various query types, showcasing its practicality for real-world applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADGEN: Mass-Spec attends to De Novo Molecular generation</title>
<link>https://arxiv.org/abs/2501.01950</link>
<guid>https://arxiv.org/abs/2501.01950</guid>
<content:encoded><![CDATA[
arXiv:2501.01950v4 Announce Type: replace-cross 
Abstract: The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the "dark chemical space" without structural annotations. To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SR-Reward: Taking The Path More Traveled</title>
<link>https://arxiv.org/abs/2501.02330</link>
<guid>https://arxiv.org/abs/2501.02330</guid>
<content:encoded><![CDATA[
arXiv:2501.02330v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called \textit{SR-Reward}, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good things come in small packages: Should we build AI clusters with Lite-GPUs?</title>
<link>https://arxiv.org/abs/2501.10187</link>
<guid>https://arxiv.org/abs/2501.10187</guid>
<content:encoded><![CDATA[
arXiv:2501.10187v2 Announce Type: replace-cross 
Abstract: To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can enable distributing AI workloads onto many Lite-GPUs through high bandwidth and efficient communication. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time regression: a unifying framework for designing sequence models with associative memory</title>
<link>https://arxiv.org/abs/2501.12352</link>
<guid>https://arxiv.org/abs/2501.12352</guid>
<content:encoded><![CDATA[
arXiv:2501.12352v2 Announce Type: replace-cross 
Abstract: Sequence models lie at the heart of modern deep learning. However, rapid advancements have produced a diversity of seemingly unrelated architectures, such as Transformers and recurrent alternatives. In this paper, we introduce a unifying framework to understand and derive these sequence models, inspired by the empirical importance of associative recall, the capability to retrieve contextually relevant tokens. We formalize associative recall as a two-step process, memorization and retrieval, casting memorization as a regression problem. Layers that combine these two steps perform associative recall via ``test-time regression'' over its input tokens. Prominent layers, including linear attention, state-space models, fast-weight programmers, online learners, and softmax attention, arise as special cases defined by three design choices: the regression weights, the regressor function class, and the test-time optimization algorithm. Our approach clarifies how linear attention fails to capture inter-token correlations and offers a mathematical justification for the empirical effectiveness of query-key normalization in softmax attention. Further, it illuminates unexplored regions within the design space, which we use to derive novel higher-order generalizations of softmax attention. Beyond unification, our work bridges sequence modeling with classic regression methods, a field with extensive literature, paving the way for developing more powerful and theoretically principled architectures.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models</title>
<link>https://arxiv.org/abs/2501.12433</link>
<guid>https://arxiv.org/abs/2501.12433</guid>
<content:encoded><![CDATA[
arXiv:2501.12433v2 Announce Type: replace-cross 
Abstract: Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as "owls as wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors</title>
<link>https://arxiv.org/abs/2501.18045</link>
<guid>https://arxiv.org/abs/2501.18045</guid>
<content:encoded><![CDATA[
arXiv:2501.18045v2 Announce Type: replace-cross 
Abstract: How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations</title>
<link>https://arxiv.org/abs/2502.03629</link>
<guid>https://arxiv.org/abs/2502.03629</guid>
<content:encoded><![CDATA[
arXiv:2502.03629v2 Announce Type: replace-cross 
Abstract: Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</title>
<link>https://arxiv.org/abs/2502.05857</link>
<guid>https://arxiv.org/abs/2502.05857</guid>
<content:encoded><![CDATA[
arXiv:2502.05857v2 Announce Type: replace-cross 
Abstract: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.06820</link>
<guid>https://arxiv.org/abs/2502.06820</guid>
<content:encoded><![CDATA[
arXiv:2502.06820v2 Announce Type: replace-cross 
Abstract: Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain decomposition with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Principled Multi-Agent Task Agnostic Exploration</title>
<link>https://arxiv.org/abs/2502.08365</link>
<guid>https://arxiv.org/abs/2502.08365</guid>
<content:encoded><![CDATA[
arXiv:2502.08365v2 Announce Type: replace-cross 
Abstract: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</title>
<link>https://arxiv.org/abs/2502.10712</link>
<guid>https://arxiv.org/abs/2502.10712</guid>
<content:encoded><![CDATA[
arXiv:2502.10712v2 Announce Type: replace-cross 
Abstract: Aircraft manufacturing is the jewel in the crown of industry, among which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. While existing deep-learning-based methods rely on predefined parametric function families, e.g., B\'ezier curves and discrete point-based representations, they suffer from inherent trade-offs between expressiveness and resolution flexibility. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly learns functional airfoil geometries. Our method inherits both the advantages of arbitrary resolution sampling and the smoothness of parametric functions, as well as the strong expressiveness of discrete point-based functions. Empirical evaluations on the AFBench dataset demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation by achieving a relative -74.4 label error reduction and +23.2 diversity increase on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design. Our code will be released.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D ReX: Causal Explanations in 3D Neuroimaging Classification</title>
<link>https://arxiv.org/abs/2502.12181</link>
<guid>https://arxiv.org/abs/2502.12181</guid>
<content:encoded><![CDATA[
arXiv:2502.12181v3 Announce Type: replace-cross 
Abstract: Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innamark: A Whitespace Replacement Information-Hiding Method</title>
<link>https://arxiv.org/abs/2502.12710</link>
<guid>https://arxiv.org/abs/2502.12710</guid>
<content:encoded><![CDATA[
arXiv:2502.12710v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and one generated by an LLM has become almost impossible. Information-hiding techniques such as digital watermarking or steganography can help by embedding information inside text in a form that is unlikely to be noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or cannot be applied to pure, unformatted text. In this paper, we introduce a novel method for information hiding called Innamark, which can conceal any byte-encoded sequence within a sufficiently long cover text. This method is implemented as a multi-platform library using the Kotlin programming language, which is accompanied by a command-line tool and a web interface. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without changing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. An experimental benchmark comparison on a dataset of 1000000 Wikipedia articles compares ten algorithms. The results demonstrate the robustness of our proposed Innamark method in various applications and the imperceptibility of its watermarks to humans. We discuss the limits to the embedding capacity and robustness of the algorithm and how these could be addressed in future work.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.17163</link>
<guid>https://arxiv.org/abs/2502.17163</guid>
<content:encoded><![CDATA[
arXiv:2502.17163v3 Announce Type: replace-cross 
Abstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms</title>
<link>https://arxiv.org/abs/2502.17801</link>
<guid>https://arxiv.org/abs/2502.17801</guid>
<content:encoded><![CDATA[
arXiv:2502.17801v2 Announce Type: replace-cross 
Abstract: Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.18773</link>
<guid>https://arxiv.org/abs/2502.18773</guid>
<content:encoded><![CDATA[
arXiv:2502.18773v2 Announce Type: replace-cross 
Abstract: This study addresses the challenge of resource scheduling optimization in edge-cloud collaborative computing using deep reinforcement learning (DRL). The proposed DRL-based approach improves task processing efficiency, reduces overall processing time, enhances resource utilization, and effectively controls task migrations. Experimental results demonstrate the superiority of DRL over traditional scheduling algorithms, particularly in managing complex task allocation, dynamic workloads, and multiple resource constraints. Despite its advantages, further improvements are needed to enhance learning efficiency, reduce training time, and address convergence issues. Future research should focus on increasing the algorithm's fault tolerance to handle more complex and uncertain scheduling scenarios, thereby advancing the intelligence and efficiency of edge-cloud computing systems.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Shot Clustering for Federated Learning</title>
<link>https://arxiv.org/abs/2503.04231</link>
<guid>https://arxiv.org/abs/2503.04231</guid>
<content:encoded><![CDATA[
arXiv:2503.04231v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is a widespread and well adopted paradigm of decentralized learning that allows training one model from multiple sources without the need to directly transfer data between participating clients. Since its inception in 2015, it has been divided into numerous sub-fields that deal with application-specific issues, be it data heterogeneity or resource allocation. One such sub-field, Clustered Federated Learning (CFL), is dealing with the problem of clustering the population of clients into separate cohorts to deliver personalized models. Although few remarkable works have been published in this domain, the problem is still largely unexplored, as its basic assumption and settings are slightly different from standard FL. In this work, we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic algorithm that can automatically detect the earliest suitable moment for clustering. Our algorithm is based on the computation of cosine similarity between gradients of the clients and a temperature measure that detects when the federated model starts to converge. We empirically evaluate our methodology by testing various one-shot clustering algorithms for over thirty different tasks on three benchmark datasets. Our experiments showcase the good performance of our approach when used to perform CFL in an automated manner without the need to adjust hyperparameters.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
arXiv:2503.04606v3 Announce Type: replace-cross 
Abstract: Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\sim$14,000$\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments</title>
<link>https://arxiv.org/abs/2503.04821</link>
<guid>https://arxiv.org/abs/2503.04821</guid>
<content:encoded><![CDATA[
arXiv:2503.04821v2 Announce Type: replace-cross 
Abstract: Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Plug-n-Play Knowledge Modules with Deep Context Distillation</title>
<link>https://arxiv.org/abs/2503.08727</link>
<guid>https://arxiv.org/abs/2503.08727</guid>
<content:encoded><![CDATA[
arXiv:2503.08727v2 Announce Type: replace-cross 
Abstract: Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocAgent: Graph-Guided LLM Agents for Code Localization</title>
<link>https://arxiv.org/abs/2503.09089</link>
<guid>https://arxiv.org/abs/2503.09089</guid>
<content:encoded><![CDATA[
arXiv:2503.09089v2 Announce Type: replace-cross 
Abstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting</title>
<link>https://arxiv.org/abs/2503.22809</link>
<guid>https://arxiv.org/abs/2503.22809</guid>
<content:encoded><![CDATA[
arXiv:2503.22809v2 Announce Type: replace-cross 
Abstract: Manual fruit harvesting is common in agriculture, but the amount of time pickers spend on non-productive activities can make it very inefficient. Accurately identifying picking vs. non-picking activity is crucial for estimating picker efficiency and optimising labour management and harvest processes. In this study, a practical system was developed to calculate the efficiency of pickers in commercial strawberry harvesting. Instrumented picking carts were developed to record the harvested fruit weight, geolocation, and cart movement in real time. These carts were deployed during the commercial strawberry harvest season in Santa Maria, CA. The collected data was then used to train a CNN-LSTM-based deep neural network to classify a picker's activity into "Pick" and "NoPick" classes. Experimental evaluations showed that the CNN-LSTM model showed promising activity recognition performance with an F1 score accuracy of over 0.97. The recognition results were then used to compute picker efficiency and the time required to fill a tray. Analysis of the season-long harvest data showed that the average picker efficiency was 75.07% with an estimation accuracy of 95.22%. Furthermore, the average tray fill time was 6.79 minutes with an estimation accuracy of 96.43%. When integrated into commercial harvesting, the proposed technology can aid growers in monitoring automated worker activity and optimising harvests to reduce non-productive time and enhance overall harvest efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM &amp; HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks</title>
<link>https://arxiv.org/abs/2504.03665</link>
<guid>https://arxiv.org/abs/2504.03665</guid>
<content:encoded><![CDATA[
arXiv:2504.03665v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as GPT-4 and DeepSeek, have been applied to a wide range of domains in software engineering. However, their potential in the context of High-Performance Computing (HPC) much remains to be explored. This paper evaluates how well DeepSeek, a recent LLM, performs in generating a set of HPC benchmark codes: a conjugate gradient solver, the parallel heat equation, parallel matrix multiplication, DGEMM, and the STREAM triad operation. We analyze DeepSeek's code generation capabilities for traditional HPC languages like Cpp, Fortran, Julia and Python. The evaluation includes testing for code correctness, performance, and scaling across different configurations and matrix sizes. We also provide a detailed comparison between DeepSeek and another widely used tool: GPT-4. Our results demonstrate that while DeepSeek generates functional code for HPC tasks, it lags behind GPT-4, in terms of scalability and execution efficiency of the generated code.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models</title>
<link>https://arxiv.org/abs/2504.03799</link>
<guid>https://arxiv.org/abs/2504.03799</guid>
<content:encoded><![CDATA[
arXiv:2504.03799v2 Announce Type: replace-cross 
Abstract: This study investigates the application of novel model architectures and large-scale foundational models in temporal series analysis of lower limb rehabilitation motion data, aiming to leverage advancements in machine learning and artificial intelligence to empower active rehabilitation guidance strategies for post-stroke patients in limb motor function recovery. Utilizing the SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, we systematically elucidate the implementation and analytical outcomes of the innovative xLSTM architecture and the foundational model Lag-Llama in short-term temporal prediction tasks involving joint kinematics and dynamics parameters. The research provides novel insights for AI-enabled medical rehabilitation applications, demonstrating the potential of cutting-edge model architectures and large-scale models in rehabilitation medicine temporal prediction. These findings establish theoretical foundations for future applications of personalized rehabilitation regimens, offering significant implications for the development of customized therapeutic interventions in clinical practice.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Bench: Human-Aligned Video Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.04907</link>
<guid>https://arxiv.org/abs/2504.04907</guid>
<content:encoded><![CDATA[
arXiv:2504.04907v2 Announce Type: replace-cross 
Abstract: Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion</title>
<link>https://arxiv.org/abs/2504.08524</link>
<guid>https://arxiv.org/abs/2504.08524</guid>
<content:encoded><![CDATA[
arXiv:2504.08524v2 Announce Type: replace-cross 
Abstract: Voice conversion (VC) transforms source speech into a target voice by preserving the content. However, timbre information from the source speaker is inherently embedded in the content representations, causing significant timbre leakage and reducing similarity to the target speaker. To address this, we introduce a residual block to a content extractor. The residual block consists of two weighted branches: 1) universal semantic dictionary based Content Feature Re-expression (CFR) module, supplying timbre-free content representation. 2) skip connection to the original content layer, providing complementary fine-grained information. In the CFR module, each dictionary entry in the universal semantic dictionary represents a phoneme class, computed statistically using speech from multiple speakers, creating a stable, speaker-independent semantic set. We introduce a CFR method to obtain timbre-free content representations by expressing each content frame as a weighted linear combination of dictionary entries using corresponding phoneme posteriors as weights. Extensive experiments across various VC frameworks demonstrate that our approach effectively mitigates timbre leakage and significantly improves similarity to the target speaker.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simulation-heuristics dual-process model for intuitive physics</title>
<link>https://arxiv.org/abs/2504.09546</link>
<guid>https://arxiv.org/abs/2504.09546</guid>
<content:encoded><![CDATA[
arXiv:2504.09546v2 Announce Type: replace-cross 
Abstract: The role of mental simulation in human physical reasoning is widely acknowledged, but whether it is employed across scenarios with varying simulation costs and where its boundary lies remains unclear. Using a pouring-marble task, our human study revealed two distinct error patterns when predicting pouring angles, differentiated by simulation time. While mental simulation accurately captured human judgments in simpler scenarios, a linear heuristic model better matched human predictions when simulation time exceeded a certain boundary. Motivated by these observations, we propose a dual-process framework, Simulation-Heuristics Model (SHM), where intuitive physics employs simulation for short-time simulation but switches to heuristics when simulation becomes costly. By integrating computational methods previously viewed as separate into a unified model, SHM quantitatively captures their switching mechanism. The SHM aligns more precisely with human behavior and demonstrates consistent predictive performance across diverse scenarios, advancing our understanding of the adaptive nature of intuitive physical reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models</title>
<link>https://arxiv.org/abs/2504.10498</link>
<guid>https://arxiv.org/abs/2504.10498</guid>
<content:encoded><![CDATA[
arXiv:2504.10498v2 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) in Q&amp;A task increased substantially through Retrieval-Augmented Generation (RAG) which brings in external knowledge. However, the main difficulty lies in balancing the inherent self-knowledge of LLMs with external information retrieval (IR). The current threshold-based methods apply one-dimensional static mechanisms with single criterion. As a result, their IR decisions might be irrelevant to the LLMs' response under difficult queries. To alleviate this problem, we propose Cognitive Convection of Self-Knowledge (CCSK). Different from traditional methods that maintain single fixed IR activation criteria, CCSK implements a dynamic joint decision process via a Siamese Network module and a Response Quality Model. The Siamese Network calculates the cosine similarity between the current query and the historical queries. The Response Quality Model evaluates the responses of LLMs through LightGBM. The final decision of the CCSK is derived from the outputs of the two modules, as well as text features fused using a multi-head attention mechanism. Extensive experiments on real-world datasets show that CCSK significantly enhances the model's effectiveness in information retrieval.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activated LoRA: Fine-tuned LLMs for Intrinsics</title>
<link>https://arxiv.org/abs/2504.12397</link>
<guid>https://arxiv.org/abs/2504.12397</guid>
<content:encoded><![CDATA[
arXiv:2504.12397v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing and Inducing Combinational Creativity in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.13120</link>
<guid>https://arxiv.org/abs/2504.13120</guid>
<content:encoded><![CDATA[
arXiv:2504.13120v2 Announce Type: replace-cross 
Abstract: The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs' outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis</title>
<link>https://arxiv.org/abs/2504.13754</link>
<guid>https://arxiv.org/abs/2504.13754</guid>
<content:encoded><![CDATA[
arXiv:2504.13754v2 Announce Type: replace-cross 
Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at https://github.com/JSLiam94/CMSwinKAN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism</title>
<link>https://arxiv.org/abs/2503.17401</link>
<guid>https://arxiv.org/abs/2503.17401</guid>
<content:encoded><![CDATA[
<div> Keywords: AIJIM, Artificial Intelligence, Journalism, Integration, Environmental<br />
Summary: <br /> 
AIJIM is introduced as a framework that integrates real-time AI into environmental journalism. It combines Vision Transformer-based hazard detection, crowdsourced validation with 252 validators, and automated reporting in a scalable architecture. The model achieved an 85.4% detection accuracy and 89.7% agreement with expert annotations in a 2024 pilot on Mallorca, reducing reporting latency by 40%. Ethical transparency is ensured through a dual-layer explainability approach using CAM-based visual overlays and LIME-based box-level interpretations. AIJIM offers a participatory, community-driven approach to environmental reporting, advancing journalism, artificial intelligence, and sustainability in alignment with the UN Sustainable Development Goals and the EU AI Act. <br /> 
Summary: <div>
arXiv:2503.17401v5 Announce Type: replace-cross 
Abstract: This paper introduces AIJIM, the Artificial Intelligence Journalism Integration Model -- a novel framework for integrating real-time AI into environmental journalism. AIJIM combines Vision Transformer-based hazard detection, crowdsourced validation with 252 validators, and automated reporting within a scalable, modular architecture. A dual-layer explainability approach ensures ethical transparency through fast CAM-based visual overlays and optional LIME-based box-level interpretations. Validated in a 2024 pilot on the island of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\% detection accuracy and 89.7\% agreement with expert annotations, while reducing reporting latency by 40\%. Unlike conventional approaches such as Data-Driven Journalism or AI Fact-Checking, AIJIM provides a transferable model for participatory, community-driven environmental reporting, advancing journalism, artificial intelligence, and sustainability in alignment with the UN Sustainable Development Goals and the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion Stylization in Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.02450</link>
<guid>https://arxiv.org/abs/2504.02450</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, autonomous driving, decision-making, cognitive hierarchy, human-like behaviors
Summary: 
The paper introduces CHARMS, a Cognitive Hierarchical Agent for Reasoning and Motion Stylization that addresses the challenge of limited interactivity and behavioral diversity in autonomous driving decision-making. By utilizing Level-k game theory, CHARMS undergoes a two-stage training process involving reinforcement learning pretraining and supervised fine-tuning to mimic human-like reasoning patterns. This approach results in models exhibiting diverse behaviors akin to humans, enhancing their decision-making capabilities in complex traffic scenarios. Additionally, a scenario generation framework is introduced, utilizing Poisson cognitive hierarchy theory to control the distribution of vehicles with various driving styles. Experimental results demonstrate CHARMS' effectiveness in making intelligent driving decisions and generating realistic driving scenarios. The code for CHARMS is publicly available on GitHub. <br /><br />Summary: <div>
arXiv:2504.02450v3 Announce Type: replace-cross 
Abstract: To address the challenge of insufficient interactivity and behavioral diversity in autonomous driving decision-making, this paper proposes a Cognitive Hierarchical Agent for Reasoning and Motion Stylization (CHARMS). By leveraging Level-k game theory, CHARMS captures human-like reasoning patterns through a two-stage training pipeline comprising reinforcement learning pretraining and supervised fine-tuning. This enables the resulting models to exhibit diverse and human-like behaviors, enhancing their decision-making capacity and interaction fidelity in complex traffic environments. Building upon this capability, we further develop a scenario generation framework that utilizes the Poisson cognitive hierarchy theory to control the distribution of vehicles with different driving styles through Poisson and binomial sampling. Experimental results demonstrate that CHARMS is capable of both making intelligent driving decisions as an ego vehicle and generating diverse, realistic driving scenarios as environment vehicles. The code for CHARMS is released at https://github.com/chuduanfeng/CHARMS.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
<div> Personality traits, agent behavior, performance, text-based interactive environments, PANDA<br />
<br />
Personality traits play a significant role in shaping the behavior and performance of artificial agents in text-based interactive environments. The study introduces PANDA, a method that projects human personality traits onto agents to guide their actions. By training a personality classifier and integrating personality profiles into the agent's learning process, the research shows that agents can exhibit specific personality types in their decision-making. Agents embodying personality traits such as higher levels of Openness demonstrate improved performance in 25 text-based games. These findings highlight the potential of personality-adapted agents in promoting aligned and effective decision-making that is more in line with human values in interactive environments.<br /><br />Summary: Agents in text-based interactive environments can exhibit human-like personality traits, impacting their behavior and performance. The PANDA method guides agent actions based on personality profiles, leading to improved performance in certain personality types. The study showcases the potential of personality-adapted agents in fostering more aligned and effective decision-making in interactive settings. <div>
arXiv:2504.06868v3 Announce Type: replace-cross 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences</title>
<link>https://arxiv.org/abs/2504.09428</link>
<guid>https://arxiv.org/abs/2504.09428</guid>
<content:encoded><![CDATA[
<div> friend recommendation, online games, multi-modal user features, structural information, FROG

Summary:
The article introduces FROG, an end-to-end model designed to improve friend recommendation in online games by effectively incorporating multi-modal user features and structural information in the friendship graph. Existing approaches have limitations such as ignoring high-order structural proximity between users, failing to learn pairwise relevance at modality-specific levels, and being unable to capture local and global user preferences across different modalities. FROG addresses these issues and outperforms existing methods in both offline and online evaluations at Tencent. The proposed model successfully models user preferences for potential friends by considering all these factors. <div>
arXiv:2504.09428v3 Announce Type: replace-cross 
Abstract: Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (e.g., images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model FROG that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at Tencent have demonstrated the superiority of FROG over existing approaches.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance</title>
<link>https://arxiv.org/abs/2504.13376</link>
<guid>https://arxiv.org/abs/2504.13376</guid>
<content:encoded><![CDATA[
<div> minor-embedding problem, Ising model, quantum annealing, Minorminer, D-Wave Systems

Summary:
- The study focuses on the minor-embedding problem in quantum annealing, mapping Ising model variables onto processors.
- Performance disparity is noted between problems suited to the processor's architecture and non-hardware-native topologies.
- The impact of embedding quality on D-Wave Systems' quantum annealers' performance is analyzed.
- There is a clear correlation between the average chain length of embeddings and the relative errors of the sampled solutions.
- Minorminer, a widely used algorithm for minor-embedding, is evaluated, showing room for improvement compared to a worst-case scenario algorithm, Clique Embedding. 
<br /><br />Summary: <div>
arXiv:2504.13376v2 Announce Type: replace-cross 
Abstract: This study addresses the minor-embedding problem, which involves mapping the variables of an Ising model onto a quantum annealing processor. The primary motivation stems from the observed performance disparity of quantum annealers when solving problems suited to the processor's architecture versus those with non-hardware-native topologies. Our research has two main objectives: i) to analyze the impact of embedding quality on the performance of D-Wave Systems quantum annealers, and ii) to evaluate the quality of the embeddings generated by Minorminer, an algorithm provided by D-Wave and widely recognized as the standard minor-embedding technique in the literature. Regarding the first objective, our experiments reveal a clear correlation between the average chain length of embeddings and the relative errors of the solutions sampled. This underscores the critical influence of embedding quality on quantum annealing performance. For the second objective, we focus on the Minorminer technique, assessing its capacity to embed problems, the quality of the embeddings produced, and the robustness of the results. We also compare its performance with Clique Embedding, another algorithm developed by D-Wave, which is deterministic and designed to embed fully connected Ising models into quantum annealing processors, serving as a worst-case scenario. The results demonstrate that there is significant room for improvement for Minorminer, as it has not consistently outperformed the worst-case scenario.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Non-local Observable on Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2504.13414</link>
<guid>https://arxiv.org/abs/2504.13414</guid>
<content:encoded><![CDATA[
<div> Hermitian observables, Quantum Machine Learning, Variational Quantum Circuits, Non-local measurements, Quantum Neural Network<br />
<br />
Summary:<br />
Conventional Variational Quantum Circuits (VQCs) use fixed Hermitian observables based on Pauli operators. A new adaptive non-local measurement framework introduces dynamical Hermitian observables with evolving parameters, increasing model complexity. Optimizing VQC rotations involves tracing a trajectory in the observable space, highlighting the Heisenberg representation. Incorporating variational rotations with non-local observables enhances qubit interaction and information mixture, allowing for flexible circuit designs. Two non-local measurement schemes are presented, showing superior performance in classification tasks compared to traditional VQCs. This approach offers a more powerful and resource-efficient Quantum Neural Network design. <div>
arXiv:2504.13414v2 Announce Type: replace-cross 
Abstract: Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning typically rely on a fixed Hermitian observable, often built from Pauli operators. Inspired by the Heisenberg picture, we propose an adaptive non-local measurement framework that substantially increases the model complexity of the quantum circuits. Our introduction of dynamical Hermitian observables with evolving parameters shows that optimizing VQC rotations corresponds to tracing a trajectory in the observable space. This viewpoint reveals that standard VQCs are merely a special case of the Heisenberg representation.
  Furthermore, we show that properly incorporating variational rotations with non-local observables enhances qubit interaction and information mixture, admitting flexible circuit designs. Two non-local measurement schemes are introduced, and numerical simulations on classification tasks confirm that our approach outperforms conventional VQCs, yielding a more powerful and resource-efficient approach as a Quantum Neural Network.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Act II: Test Time Scaling Drives Cognition Engineering</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Act I, Act II, prompt engineering, cognition engineering<br />
<br />
Summary:<br />
The paper discusses the transition from the first generation of Large Language Models ("Act I") to the current phase ("Act II"), focusing on the limitations of the former and the advancements in cognition engineering in the latter. It explains how models are evolving from knowledge-retrieval systems to thought-construction engines through test-time scaling techniques. The new paradigm establishes a mind-level connection with AI through language-based thoughts. The paper provides a detailed explanation of cognition engineering, breaking down advanced approaches and offering tutorials and implementations to democratize access to this field. The GitHub Repository mentioned in the paper offers a collection of papers on test-time scaling, allowing practitioners to participate in AI's second act. <div>
arXiv:2504.13828v3 Announce Type: replace-cross 
Abstract: The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations such as knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BELL: Benchmarking the Explainability of Large Language Models</title>
<link>https://arxiv.org/abs/2504.18572</link>
<guid>https://arxiv.org/abs/2504.18572</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Explainability, Benchmarking, Transparency, Interpretability

Summary:
Large Language Models have shown impressive performance in natural language processing tasks, but their decision-making processes are often not transparent. This lack of transparency raises concerns about trust, bias, and overall model performance. To address these issues, it is essential to understand and evaluate the interpretability of Large Language Models (LLMs). This paper introduces a standardized benchmarking technique called Benchmarking the Explainability of Large Language Models, specifically designed to assess the explainability of LLMs. By providing a structured approach to evaluate the interpretability of these models, this benchmarking technique aims to shed light on their decision-making processes and mitigate concerns related to trust, bias, and model performance. Through this standardized evaluation, researchers and developers can gain insights into how LLMs make decisions and improve their transparency. 

<br /><br />Summary: <div>
arXiv:2504.18572v1 Announce Type: new 
Abstract: Large Language Models have demonstrated remarkable capabilities in natural language processing, yet their decision-making processes often lack transparency. This opaqueness raises significant concerns regarding trust, bias, and model performance. To address these issues, understanding and evaluating the interpretability of LLMs is crucial. This paper introduces a standardised benchmarking technique, Benchmarking the Explainability of Large Language Models, designed to evaluate the explainability of large language models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantBench: Benchmarking AI Methods for Quantitative Investment</title>
<link>https://arxiv.org/abs/2504.18600</link>
<guid>https://arxiv.org/abs/2504.18600</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, quantitative investment, benchmark platform, continual learning, overfitting

Summary: 
QuantBench is a new benchmark platform designed to standardize AI research in quantitative investment, aligning with industry practices. It offers flexibility to integrate various AI algorithms and covers the entire investment process. Empirical studies using QuantBench highlight the importance of continual learning to address distribution shifts, improved methods for modeling financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation, QuantBench aims to accelerate progress in AI for quantitative investment, fostering collaboration between researchers and practitioners. <div>
arXiv:2504.18600v1 Announce Type: new 
Abstract: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study</title>
<link>https://arxiv.org/abs/2504.18604</link>
<guid>https://arxiv.org/abs/2504.18604</guid>
<content:encoded><![CDATA[
<div> cognitive-mechanistic framework, human reliability analysis, nuclear power plants, human error probabilities, Bayesian network
Summary: 
The study introduces a cognitive-mechanistic framework (COGMIF) that enhances traditional human reliability analysis methods by integrating an ACT-R-based human digital twin (HDT) with TimeGAN-augmented simulation. The COGMIF simulates operator cognition in high-fidelity scenarios from a high-temperature gas-cooled reactor (HTGR) simulator, generating synthetic operator behavior datasets for scalable estimation of human error probabilities (HEPs) in industrial settings. Comparative analyses with SPAR-H demonstrate the robustness of the proposed method. The framework maps procedural features onto a Bayesian network to identify key drivers of operational risk, offering a credible and computationally efficient approach to incorporate cognitive theory into human reliability analysis practices. <div>
arXiv:2504.18604v1 Announce Type: new 
Abstract: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA, rely on expert judgment and empirical rules that often overlook the cognitive underpinnings of human error. Moreover, conducting human-in-the-loop experiments for advanced nuclear power plants is increasingly impractical due to novel interfaces and limited operational data. This study proposes a cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA methodology by integrating an ACT-R-based human digital twin (HDT) with TimeGAN-augmented simulation. The ACT-R model simulates operator cognition, including memory retrieval, goal-directed procedural reasoning, and perceptual-motor execution, under high-fidelity scenarios derived from a high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource constraints of large-scale cognitive modeling, TimeGAN is trained on ACT-R-generated time-series data to produce high-fidelity synthetic operator behavior datasets. These simulations are then used to drive IDHEAS-ECA assessments, enabling scalable, mechanism-informed estimation of human error probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity assessments demonstrate the robustness and practical advantages of the proposed COGMIF. Finally, procedural features are mapped onto a Bayesian network to quantify the influence of contributing factors, revealing key drivers of operational risk. This work offers a credible and computationally efficient pathway to integrate cognitive theory into industrial HRA practices.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion</title>
<link>https://arxiv.org/abs/2504.18631</link>
<guid>https://arxiv.org/abs/2504.18631</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized intervention, time series data, medical data fusion, neural network, decision-making

Summary:
In the medical field, personalized intervention plans based on high-dimensional heterogeneous time series data are crucial. This study introduces a system that utilizes Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion to generate personalized medical intervention strategies. By incorporating relative policy constraints and utilizing a multi-layer neural network structure for patient grouping, individual and group gains can be balanced effectively. A multi-channel neural network with a self-attention mechanism is employed for dynamic feature extraction from multi-source heterogeneous time series data, enhancing decision-making robustness and interpretability. Key feature screening and aggregation are achieved through a differentiable gating network. Furthermore, a collaborative search process involving a genetic algorithm and Monte Carlo tree search is used to find the optimal intervention strategy, leading to improved accuracy, coverage, and decision-making benefits compared to existing methods. <div>
arXiv:2504.18631v1 Announce Type: new 
Abstract: With the timely formation of personalized intervention plans based on high-dimensional heterogeneous time series information becoming an important challenge in the medical field today, electronic medical records, wearables, and other multi-source medical data are increasingly generated and diversified. In this work, we develop a system to generate personalized medical intervention strategies based on Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion. First, by incorporating relative policy constraints among the groups during policy gradient updates, we adaptively balance individual and group gains. To improve the robustness and interpretability of decision-making, a multi-layer neural network structure is employed to group-code patient characteristics. Second, for the rapid multi-modal fusion of multi-source heterogeneous time series, a multi-channel neural network combined with a self-attention mechanism is used for dynamic feature extraction. Key feature screening and aggregation are achieved through a differentiable gating network. Finally, a collaborative search process combining a genetic algorithm and Monte Carlo tree search is proposed to find the ideal intervention strategy, achieving global optimization. Experimental results show significant improvements in accuracy, coverage, and decision-making benefits compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development</title>
<link>https://arxiv.org/abs/2504.18651</link>
<guid>https://arxiv.org/abs/2504.18651</guid>
<content:encoded><![CDATA[
<div> ontology, species classification, ChatGPT-4, automation, taxonomy

Summary: 
- The paper explores using ChatGPT-4 to automate the development of the :Organism module in the Agricultural Product Types Ontology (APTO) for species classification.
- Two approaches were tested: prompting ChatGPT-4 to execute tasks via the BrowserOP plugin and directing it to design a Python algorithm.
- The first approach showed scalability limitations, while the second approach used a Python algorithm to overcome challenges but struggled with typographical errors.
- Large language models like ChatGPT-4 show promise in streamlining species name management in ontologies.
- Despite limitations, these tools offer advancements in automating taxonomy-related tasks and improving ontology development efficiency.

Summary: <div>
arXiv:2504.18651v1 Announce Type: new 
Abstract: Managing scientific names in ontologies that represent species taxonomies is challenging due to the ever-evolving nature of these taxonomies. Manually maintaining these names becomes increasingly difficult when dealing with thousands of scientific names. To address this issue, this paper investigates the use of ChatGPT-4 to automate the development of the :Organism module in the Agricultural Product Types Ontology (APTO) for species classification. Our methodology involved leveraging ChatGPT-4 to extract data from the GBIF Backbone API and generate OWL files for further integration in APTO. Two alternative approaches were explored: (1) issuing a series of prompts for ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4 to design a Python algorithm to perform analogous tasks. Both approaches rely on a prompting method where we provide instructions, context, input data, and an output indicator. The first approach showed scalability limitations, while the second approach used the Python algorithm to overcome these challenges, but it struggled with typographical errors in data handling. This study highlights the potential of Large language models like ChatGPT-4 to streamline the management of species names in ontologies. Despite certain limitations, these tools offer promising advancements in automating taxonomy-related tasks and improving the efficiency of ontology development.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction</title>
<link>https://arxiv.org/abs/2504.18671</link>
<guid>https://arxiv.org/abs/2504.18671</guid>
<content:encoded><![CDATA[
<div> Fine-tuned vision-language models, OpenAI-o3 reasoning LLM, TBI diagnosis, consensus decision-making, transparent decision-making <br />
<br />
Summary: Proof-of-TBI is a medical diagnosis support system that integrates fine-tuned vision-language models with the OpenAI-o3 reasoning LLM to effectively diagnose mild Traumatic Brain Injury (TBI). Multiple vision-language models are trained on a labeled dataset of TBI MRI scans and their predictions are aggregated through a consensus-based decision-making process. The OpenAI-o3 reasoning LLM evaluates these predictions and orchestrates interactions between the vision-language models, ensuring transparency, reliability, and automation in the final decision-making process. The prototype developed in collaboration with the U.S. Army Medical Research team demonstrates the transformative potential of this innovative approach for accurate TBI prediction. This research is the first to apply fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks. <div>
arXiv:2504.18671v1 Announce Type: new 
Abstract: Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformational Creativity in Science: A Graphical Theory</title>
<link>https://arxiv.org/abs/2504.18687</link>
<guid>https://arxiv.org/abs/2504.18687</guid>
<content:encoded><![CDATA[
<div> Keywords: transformational creativity, enabling constraints, conceptual space, paradigm shifts, graphical theory

Summary:
Transformational scientific creativity can be categorized into three types: combinatorial, exploratory, and transformational. This study focuses on transformational creativity and proposes a graphical theory that combines Boden's concept of enabling constraints and Kuhn's idea of paradigm shifts. The research demonstrates that changing the axioms in the graphical model has the most significant transformative potential. By applying this framework, the analysis showcases how historical cases of transformational creativity align with the proposed model. The study highlights the importance of modifying the underlying principles of a conceptual space to drive transformative scientific progress, offering insights into understanding and potentially fostering breakthroughs in the field of scientific creativity. <br /><br />Summary: <div>
arXiv:2504.18687v1 Announce Type: new 
Abstract: Creative processes are typically divided into three types: combinatorial, exploratory, and transformational. Here, we provide a graphical theory of transformational scientific creativity, synthesizing Boden's insight that transformational creativity arises from changes in the "enabling constraints" of a conceptual space and Kuhn's structure of scientific revolutions as resulting from paradigm shifts. We prove that modifications made to axioms of our graphical model have the most transformative potential and then illustrate how several historical instances of transformational creativity can be captured by our framework.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision for Auto Research with LLM Agents</title>
<link>https://arxiv.org/abs/2504.18765</link>
<guid>https://arxiv.org/abs/2504.18765</guid>
<content:encoded><![CDATA[
<div> Keywords: Agent-Based Auto Research, structured multi-agent framework, scientific research automation, large language models, self-improving AI-driven research

Summary: 
Agent-Based Auto Research is a novel framework designed to automate, optimize, and coordinate various stages of scientific research. Utilizing large language models and collaborative agents, the system covers literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues like fragmented workflows, uneven expertise, and cognitive overload, Auto Research offers a systematic and scalable approach to research. Preliminary studies demonstrate its feasibility and potential as a self-improving, AI-driven research paradigm. <div>
arXiv:2504.18765v1 Announce Type: new 
Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating AI-Driven Automated Map Digitization in QGIS</title>
<link>https://arxiv.org/abs/2504.18777</link>
<guid>https://arxiv.org/abs/2504.18777</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence, Map Digitization, Machine Learning, Deepness, Remote Sensing<br />
<br />
Artificial intelligence has made significant advancements in the field of map digitization, offering an alternative to traditional human-intensive processes. Deepness, a Deep Neural Remote Sensing tool, integrated into QGIS, allows for automated digitization. This study assesses the effectiveness of Deepness by comparing AI-generated digitization results from Google Earth imagery with outputs from OpenStreetMap (OSM). The research aims to evaluate the performance of AI-driven digitization in accurately translating complex map features into digital formats. Through this analysis, the study aims to determine the reliability and efficiency of Deepness in automating the map digitization process. The comparison between AI-generated results and manually digitized OSM outputs provides insights into the potential benefits and limitations of using machine learning techniques for map digitization tasks.<br /><br />Summary: <div>
arXiv:2504.18777v1 Announce Type: new 
Abstract: Map digitization is an important process that converts maps into digital formats that can be used for further analysis. This process typically requires a deep human involvement because of the need for interpretation and decision-making when translating complex features. With the advancement of artificial intelligence, there is an alternative to conducting map digitization with the help of machine learning techniques. Deepness, or Deep Neural Remote Sensing, is an advanced AI-driven tool designed and integrated as a plugin in QGIS application. This research focuses on assessing the effectiveness of Deepness in automated digitization. This study analyses AI-generated digitization results from Google Earth imagery and compares them with digitized outputs from OpenStreetMap (OSM) to evaluate performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots</title>
<link>https://arxiv.org/abs/2504.18794</link>
<guid>https://arxiv.org/abs/2504.18794</guid>
<content:encoded><![CDATA[
<div> Hierarchical reinforcement learning, HRL, robot learning tasks, sparse reward schemes, complex navigation tasks<br />
Summary:<br />
This research compares hierarchical reinforcement learning (HRL) with standard reinforcement learning in complex navigation tasks. It examines HRL's unique characteristics such as creating sub-goals and termination functions. Experiments explore differences between Proximal Policy Optimization (PPO) and HRL, various sub-goal creation methods, manual vs automatic sub-goal creation, and termination frequency effects on performance. The study demonstrates the advantages of HRL and how it achieves them. <br /> <div>
arXiv:2504.18794v1 Announce Type: new 
Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to take advantage of the inherent hierarchy in robot learning tasks with sparse reward schemes, in contrast to more traditional reinforcement learning algorithms. In this research, hierarchical reinforcement learning is evaluated and contrasted with standard reinforcement learning in complex navigation tasks. We evaluate unique characteristics of HRL, including their ability to create sub-goals and the termination function. We constructed experiments to test the differences between PPO and HRL, different ways of creating sub-goals, manual vs automatic sub-goal creation, and the effects of the frequency of termination on performance. These experiments highlight the advantages of HRL and how it achieves these advantages.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative to Agentic AI: Survey, Conceptualization, and Challenges</title>
<link>https://arxiv.org/abs/2504.18875</link>
<guid>https://arxiv.org/abs/2504.18875</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, Generative AI, evolution, capabilities, challenges

Summary: 
Agentic Artificial Intelligence (AI) represents a significant advancement from Generative AI, offering enhanced reasoning and interaction abilities for handling complex tasks. The survey delves into the comparison between GenAI and Agentic AI, highlighting key characteristics and the evolutionary progression toward more autonomous behavior. It explains how Agentic AI addresses limitations of GenAI and outlines novel applications enabled by Agentic AI. The examination also delves into recent developments and practical considerations related to defining agents within Agentic AI. The survey concludes by identifying potential research challenges and cautioning against risks associated with surpassing human intelligence. <br /><br />Summary: <div>
arXiv:2504.18875v1 Announce Type: new 
Abstract: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It constitutes the next major step in the evolution of AI with much stronger reasoning and interaction capabilities that enable more autonomous behavior to tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI has seen widespread adoption, giving users firsthand experience. However, the distinction between Agentic AI and GenAI remains less well understood. To address this gap, our survey is structured in two parts. In the first part, we compare GenAI and Agentic AI using existing literature, discussing their key characteristics, how Agentic AI remedies limitations of GenAI, and the major steps in GenAI's evolution toward Agentic AI. This section is intended for a broad audience, including academics in both social sciences and engineering, as well as industry professionals. It provides the necessary insights to comprehend novel applications that are possible with Agentic AI but not with GenAI. In the second part, we deep dive into novel aspects of Agentic AI, including recent developments and practical concerns such as defining agents. Finally, we discuss several challenges that could serve as a future research agenda, while cautioning against risks that can emerge when exceeding human intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents</title>
<link>https://arxiv.org/abs/2504.18880</link>
<guid>https://arxiv.org/abs/2504.18880</guid>
<content:encoded><![CDATA[
<div> Keywords: metal-organic frameworks, synthesis conditions, Large Language Models, MOFh6, efficiency<br />
<br />
Summary: <br />
The article discusses the use of Large Language Models (LLMs) in identifying synthesis conditions for metal-organic frameworks (MOFs). These conditions are crucial for efficient MOF synthesis but can be challenging to pinpoint among numerous possibilities. By employing LLMs, particularly gpt-4o-mini, the researchers developed MOFh6, a tool that integrates various MOF-related agents to streamline the synthesis process. MOFh6 allows users to query in different formats, such as scientific literature or specific MOF codes, to obtain optimal synthesis conditions. Additionally, the tool generates model files for density functional theory pre-modeling, enhancing the efficiency of researchers in synthesizing MOFs. The integration of LLMs in MOF synthesis presents a promising solution to this complex materials science challenge. <br /> <div>
arXiv:2504.18880v1 Announce Type: new 
Abstract: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a significant focus in materials science. However, identifying the precise synthesis conditions for specific MOFs within the vast array of possibilities presents a considerable challenge. Large Language Models (LLMs) offer a promising solution to this problem. We leveraged the capabilities of LLMs, specifically gpt-4o-mini, as core agents to integrate various MOF-related agents, including synthesis, attribute, and chemical information agents. This integration culminated in the development of MOFh6, an LLM tool designed to streamline the MOF synthesis process. MOFh6 allows users to query in multiple formats, such as submitting scientific literature, or inquiring about specific MOF codes or structural properties. The tool analyzes these queries to provide optimal synthesis conditions and generates model files for density functional theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF synthesis of all researchers.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms</title>
<link>https://arxiv.org/abs/2504.18948</link>
<guid>https://arxiv.org/abs/2504.18948</guid>
<content:encoded><![CDATA[
<div> Keywords: Data collection, Handwritten digits, Deep learning, OCR, Maternal health<br />
Summary:<br />
This research introduces a dataset of handwritten digits and deep learning models for effective data collection in development programs. Traditional paper-based data collection methods are used with OCR and OMR technologies for digitization. The study demonstrates the deployment of these tools in a maternal and child health project in rural India. By collecting phone numbers of Self Help Group members using paper forms, nearly 4 million awareness calls were successfully made through IVR systems. The dataset, models, and code developed are open-source and can be used in real-world environments. This approach addresses the challenges of field-based data collection in development projects, especially when digital devices are not readily available or when training is lacking. The use of OCR tools for digitization proves to be efficient and cost-effective, making it suitable for various contexts. <br /><br />Summary: <div>
arXiv:2504.18948v1 Announce Type: new 
Abstract: Initiation, monitoring, and evaluation of development programmes can involve field-based data collection about project activities. This data collection through digital devices may not always be feasible though, for reasons such as unaffordability of smartphones and tablets by field-based cadre, or shortfalls in their training and capacity building. Paper-based data collection has been argued to be more appropriate in several contexts, with automated digitization of the paper forms through OCR (Optical Character Recognition) and OMR (Optical Mark Recognition) techniques. We contribute with providing a large dataset of handwritten digits, and deep learning based models and methods built using this data, that are effective in real-world environments. We demonstrate the deployment of these tools in the context of a maternal and child health and nutrition awareness project, which uses IVR (Interactive Voice Response) systems to provide awareness information to rural women SHG (Self Help Group) members in north India. Paper forms were used to collect phone numbers of the SHG members at scale, which were digitized using the OCR tools developed by us, and used to push almost 4 million phone calls. The data, model, and code have been released in the open-source domain.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles</title>
<link>https://arxiv.org/abs/2504.19017</link>
<guid>https://arxiv.org/abs/2504.19017</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, autonomous discovery, protein science, generative sequence design, scientific inquiry

Summary:
Sparks is an AI model that autonomously conducts the entire scientific discovery cycle in protein science, from hypothesis generation to report writing. It uncovered two new phenomena: a length-dependent mechanical crossover in peptide unfolding force and a chain-length/secondary-structure stability map. This was achieved through generative sequence design, structure prediction, and physics-aware property models. The model's paired agents ensure self-correction and reproducibility. Sparks demonstrated the ability to independently identify previously unknown scientific principles, showcasing the potential for autonomous discovery in AI. <div>
arXiv:2504.19017v1 Announce Type: new 
Abstract: Advances in artificial intelligence (AI) promise autonomous discovery, yet most systems still resurface knowledge latent in their training data. We present Sparks, a multi-modal multi-agent AI model that executes the entire discovery cycle that includes hypothesis generation, experiment design and iterative refinement to develop generalizable principles and a report without human intervention. Applied to protein science, Sparks uncovered two previously unknown phenomena: (i) a length-dependent mechanical crossover whereby beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond ~80 residues, establishing a new design principle for peptide mechanics; and (ii) a chain-length/secondary-structure stability map revealing unexpectedly robust beta-sheet-rich architectures and a "frustration zone" of high variance in mixed alpha/beta folds. These findings emerged from fully self-directed reasoning cycles that combined generative sequence design, high-accuracy structure prediction and physics-aware property models, with paired generation-and-reflection agents enforcing self-correction and reproducibility. The key result is that Sparks can independently conduct rigorous scientific inquiry and identify previously unknown scientific principles.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models</title>
<link>https://arxiv.org/abs/2504.19023</link>
<guid>https://arxiv.org/abs/2504.19023</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic reasoning, OWL ontologies, Graph Language Model, Consistency checking, Machine learning <br />
Summary: <br />
- Semantic reasoning involves inferring new knowledge from existing information using OWL ontologies.
- The challenge of verifying ontology consistency is addressed in this study.
- The proposed GLaMoR (Graph Language Model for Reasoning) pipeline converts OWL ontologies into graph-structured data for efficient consistency checking.
- By adapting the GLM architecture, the model achieves 95% accuracy on NCBO BioPortal ontologies.
- GLaMoR outperforms classical reasoners in terms of speed and accuracy, being 20 times faster while maintaining high performance. 

Summary: <div>
arXiv:2504.19023v1 Announce Type: new 
Abstract: Semantic reasoning aims to infer new knowledge from existing knowledge, with OWL ontologies serving as a standardized framework for organizing information. A key challenge in semantic reasoning is verifying ontology consistency. However, state-of-the-art reasoners are computationally expensive, and their efficiency decreases as ontology sizes grow. While classical machine learning models have been explored for consistency checking, they struggle to capture complex relationships within ontologies. Large language models (LLMs) have shown promising results for simple reasoning tasks but perform poorly on structured reasoning. The recently introduced Graph Language Model (GLM) offers a way to simultaneously process graph-structured data and text. This paper proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that transforms OWL ontologies into graph-structured data and adapts the GLM architecture for consistency checking. We evaluate GLaMoR on ontologies from the NCBO BioPortal repository, converting them into triples suitable for model input. Our results show that the GLM outperforms all baseline models, achieving $95\%$ accuracy while being 20 times faster than classical reasoners.
  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning</title>
<link>https://arxiv.org/abs/2504.19027</link>
<guid>https://arxiv.org/abs/2504.19027</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable artificial intelligence, Counterfactual explanations, DiCE-Extended, Robustness, Multi-objective optimization
Summary: 
Explainable artificial intelligence (XAI) is crucial in decision-making fields like healthcare and finance. This study introduces DiCE-Extended, an enhanced framework for generating Counterfactual (CF) explanations that prioritize robustness and diversity. By incorporating a novel robustness metric and weighted loss components, DiCE-Extended improves CF validity and stability while maintaining interpretability. The framework is validated on various datasets and machine learning backends, showing enhanced alignment with decision boundaries. The findings demonstrate the potential of DiCE-Extended for generating reliable and interpretable CFs in high-stakes applications. Future research will focus on adapting optimization techniques and incorporating domain-specific constraints for further improvements.<br /><br />Summary: <div>
arXiv:2504.19027v1 Announce Type: new 
Abstract: Explainable artificial intelligence (XAI) has become increasingly important in decision-critical domains such as healthcare, finance, and law. Counterfactual (CF) explanations, a key approach in XAI, provide users with actionable insights by suggesting minimal modifications to input features that lead to different model outcomes. Despite significant advancements, existing CF generation methods often struggle to balance proximity, diversity, and robustness, limiting their real-world applicability. A widely adopted framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but lacks robustness, making CF explanations sensitive to perturbations and domain constraints. To address these challenges, we introduce DiCE-Extended, an enhanced CF explanation framework that integrates multi-objective optimization techniques to improve robustness while maintaining interpretability. Our approach introduces a novel robustness metric based on the Dice-Sorensen coefficient, ensuring stability under small input variations. Additionally, we refine CF generation using weighted loss components (lambda_p, lambda_d, lambda_r) to balance proximity, diversity, and robustness. We empirically validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch, TensorFlow). Results demonstrate improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE-generated explanations. Our findings highlight the potential of DiCE-Extended in generating more reliable and interpretable CFs for high-stakes applications. Future work will explore adaptive optimization techniques and domain-specific constraints to further enhance CF generation in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development</title>
<link>https://arxiv.org/abs/2504.19144</link>
<guid>https://arxiv.org/abs/2504.19144</guid>
<content:encoded><![CDATA[
<div> Keywords: Domain-Specific Architecture, Agile Hardware Development Methodology, Hardware Construction Language, Large Language Models, ChiseLLM<br />
Summary: <br />
The paper introduces ChiseLLM, a solution for enhanced Chisel code generation using Large Language Models (LLMs) in the context of Domain-Specific Architecture (DSA) and Agile Hardware Development Methodology (AHDM). The ChiseLLM approach involves data processing, prompt-guided reasoning trace synthesis, and domain-adapted model training. High-quality datasets were created from public RTL code resources to improve syntax correctness and design variability in Chisel generation tasks. Experimental results show significant improvements in syntax correctness and design variability with ChiseLLM-7B and ChiseLLM-32B models over base models. The datasets and models are publicly available, offering a cost-effective solution for HCL-Based AHDM and serving as a benchmark for future research efforts. <br /><br />Summary: <div>
arXiv:2504.19144v1 Announce Type: new 
Abstract: The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data</title>
<link>https://arxiv.org/abs/2504.19148</link>
<guid>https://arxiv.org/abs/2504.19148</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive Dynamic Attribute, Rule, Neuro-fuzzy inference systems, High-dimensional data, RMSE

Summary: 
The paper introduces the Adaptive Dynamic Attribute and Rule (ADAR) framework, aimed at addressing high-dimensional data challenges in neuro-fuzzy inference systems. ADAR incorporates dual weighting mechanisms for attributes and rules, along with growth and pruning strategies, to streamline fuzzy models without compromising performance or interpretability. Experimental evaluations on various datasets demonstrate that ADAR-based models consistently outperform state-of-the-art baselines in terms of Root Mean Square Error (RMSE). For instance, on the Beijing PM2.5 dataset, ADAR-SOFENN achieved a lower RMSE than traditional ANFIS and SOFENN models. The models also showed superior performance on the Appliances Energy dataset compared to existing fuzzy logic approaches. Ablation studies confirmed that the combination of rule-level and attribute-level weight assignment reduces model overlap while maintaining essential features, enhancing explainability. Overall, ADAR effectively balances rule complexity and feature importance, making it suitable for scalable, high-accuracy, and transparent neuro-fuzzy systems in real-world applications.

<br /><br />Summary: <div>
arXiv:2504.19148v1 Announce Type: new 
Abstract: This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework designed to address the challenges posed by high-dimensional data in neuro-fuzzy inference systems. By integrating dual weighting mechanisms-assigning adaptive importance to both attributes and rules-together with automated growth and pruning strategies, ADAR adaptively streamlines complex fuzzy models without sacrificing performance or interpretability. Experimental evaluations on four diverse datasets - Auto MPG (7 variables), Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances Energy Consumption (27 variables) show that ADAR-based models achieve consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN [16] models. Similarly, on the high-dimensional Appliances Energy dataset, ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established fuzzy logic approaches and interpretability-focused methods such as APLR. Ablation studies further reveal that combining rule-level and attribute-level weight assignment significantly reduces model overlap while preserving essential features, thereby enhancing explainability. These results highlight ADAR's effectiveness in dynamically balancing rule complexity and feature importance, paving the way for scalable, high-accuracy, and transparent neuro-fuzzy systems applicable to a range of real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption</title>
<link>https://arxiv.org/abs/2504.19179</link>
<guid>https://arxiv.org/abs/2504.19179</guid>
<content:encoded><![CDATA[
<div> AI, healthcare, Trustworthy AI, medical AI systems, cardiovascular diseases
Summary:
This article discusses the potential of Artificial Intelligence (AI) in transforming healthcare, particularly in disease diagnosis, prognosis, and patient care. It emphasizes the challenges beyond technical performance, including ethical concerns, regulatory barriers, and lack of trust that limit widespread clinical adoption of AI in healthcare. To address these issues, the article proposes a design framework to support developers in embedding Trustworthy AI (TAI) principles into medical AI systems. The framework includes requirements for each stakeholder involved in healthcare processes to adhere to TAI principles, such as human agency, algorithmic robustness, privacy, transparency, bias avoidance, and accountability. The article focuses on cardiovascular diseases as a case study to demonstrate the application of TAI principles and highlights key obstacles that persist in implementing these principles in practice. <div>
arXiv:2504.19179v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) holds great promise for transforming healthcare, particularly in disease diagnosis, prognosis, and patient care. The increasing availability of digital medical data, such as images, omics, biosignals, and electronic health records, combined with advances in computing, has enabled AI models to approach expert-level performance. However, widespread clinical adoption remains limited, primarily due to challenges beyond technical performance, including ethical concerns, regulatory barriers, and lack of trust. To address these issues, AI systems must align with the principles of Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic robustness, privacy and data governance, transparency, bias and discrimination avoidance, and accountability. Yet, the complexity of healthcare processes (e.g., screening, diagnosis, prognosis, and treatment) and the diversity of stakeholders (clinicians, patients, providers, regulators) complicate the integration of TAI principles. To bridge the gap between TAI theory and practical implementation, this paper proposes a design framework to support developers in embedding TAI principles into medical AI systems. Thus, for each stakeholder identified across various healthcare processes, we propose a disease-agnostic collection of requirements that medical AI systems should incorporate to adhere to the principles of TAI. Additionally, we examine the challenges and tradeoffs that may arise when applying these principles in practice. To ground the discussion, we focus on cardiovascular diseases, a field marked by both high prevalence and active AI innovation, and demonstrate how TAI principles have been applied and where key obstacles persist.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach</title>
<link>https://arxiv.org/abs/2504.19255</link>
<guid>https://arxiv.org/abs/2504.19255</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, ethical reasoning, moral priorities, analysis  
Summary:  
- The paper introduces the PRIME framework for analyzing moral priorities in large language models (LLMs) regarding ethical reasoning.  
- The study applied this framework to six prominent LLMs and found that they prioritize care/harm and fairness/cheating foundations but undervalue authority, loyalty, and sanctity dimensions.  
- The analysis showed that current LLMs make clear ethical judgments, exhibit alignment in moral decision-making, and generally align with human moral preferences.  
- The research provides a scalable methodology for ethical benchmarking and highlights both the capabilities and limitations of AI moral reasoning systems.  
- This insight is crucial for the responsible development of these systems as they take on more significant societal roles.  
<br /><br /> <div>
arXiv:2504.19255v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in consequential decision-making contexts, systematically assessing their ethical reasoning capabilities becomes a critical imperative. This paper introduces the Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a comprehensive methodology for analyzing moral priorities across foundational ethical dimensions including consequentialist-deontological reasoning, moral foundations theory, and Kohlberg's developmental stages. We apply this framework to six leading LLMs through a dual-protocol approach combining direct questioning and response analysis to established ethical dilemmas. Our analysis reveals striking patterns of convergence: all evaluated models demonstrate strong prioritization of care/harm and fairness/cheating foundations while consistently underweighting authority, loyalty, and sanctity dimensions. Through detailed examination of confidence metrics, response reluctance patterns, and reasoning consistency, we establish that contemporary LLMs (1) produce decisive ethical judgments, (2) demonstrate notable cross-model alignment in moral decision-making, and (3) generally correspond with empirically established human moral preferences. This research contributes a scalable, extensible methodology for ethical benchmarking while highlighting both the promising capabilities and systematic limitations in current AI moral reasoning architectures--insights critical for responsible development as these systems assume increasingly significant societal roles.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling</title>
<link>https://arxiv.org/abs/2504.19277</link>
<guid>https://arxiv.org/abs/2504.19277</guid>
<content:encoded><![CDATA[
<div> function calling, language models, edge devices, function call generation, empirical study

Summary:
In an empirical study, the efficacy of Small Language Models (SLMs) in generating function calls across various domains was evaluated using zero-shot, few-shot, and fine-tuning approaches. The study also included experiments with prompt injection and analysis of model responses. SLMs showed improvement from zero-shot to few-shot, with the best performance achieved through fine-tuning. However, the models struggled with adhering to the specified output format. Prompt injection experiments indicated the models' robustness with only a slight decline in performance. Performance evaluation on an edge device showed insights into latency and memory usage, highlighting potential practical applicability. While SLMs demonstrate promise for function call generation, further refinement is needed for real-time functioning. <br /><br />Summary: <div>
arXiv:2504.19277v1 Announce Type: new 
Abstract: Function calling is a complex task with widespread applications in domains such as information retrieval, software engineering and automation. For example, a query to book the shortest flight from New York to London on January 15 requires identifying the correct parameters to generate accurate function calls. Large Language Models (LLMs) can automate this process but are computationally expensive and impractical in resource-constrained settings. In contrast, Small Language Models (SLMs) can operate efficiently, offering faster response times, and lower computational demands, making them potential candidates for function calling on edge devices. In this exploratory empirical study, we evaluate the efficacy of SLMs in generating function calls across diverse domains using zero-shot, few-shot, and fine-tuning approaches, both with and without prompt injection, while also providing the finetuned models to facilitate future applications. Furthermore, we analyze the model responses across a range of metrics, capturing various aspects of function call generation. Additionally, we perform experiments on an edge device to evaluate their performance in terms of latency and memory usage, providing useful insights into their practical applicability. Our findings show that while SLMs improve from zero-shot to few-shot and perform best with fine-tuning, they struggle significantly with adhering to the given output format. Prompt injection experiments further indicate that the models are generally robust and exhibit only a slight decline in performance. While SLMs demonstrate potential for the function call generation task, our results also highlight areas that need further refinement for real-time functioning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics</title>
<link>https://arxiv.org/abs/2504.19320</link>
<guid>https://arxiv.org/abs/2504.19320</guid>
<content:encoded><![CDATA[
<div> categorical logic, artificial intelligent agents, Johnstone's sequent calculus, forward chaining, normal form algorithms<br />
<br />
Summary: This paper explores the use of categorical logic in designing artificial intelligent agents capable of symbolically reasoning about complexly structured objects beyond sets. By incorporating Johnstone's sequent calculus and adapting first-order unification, the authors develop forward chaining and normal form algorithms for reasoning about objects in cartesian categories using Horn logic rules. These algorithms support multi-sorted theories, contexts, and fragments of first-order logic, allowing for rich symbolic reasoning even in semantic categories that do not fully support classical logic or its connectives. This innovative approach opens up possibilities for more advanced reasoning capabilities in artificial intelligence systems, enabling them to handle more complex and diverse types of information and relationships among objects. <div>
arXiv:2504.19320v1 Announce Type: new 
Abstract: This paper seeks to apply categorical logic to the design of artificial intelligent agents that reason symbolically about objects more richly structured than sets. Using Johnstone's sequent calculus of terms- and formulae-in-context, we develop forward chaining and normal form algorithms for reasoning about objects in cartesian categories with the rules for Horn logic. We also adapt first-order unification to support multi-sorted theories, contexts, and fragments of first-order logic. The significance of these reformulations rests in the fact that they can be applied to reasoning about objects in semantic categories that do not support classical logic or even all its connectives.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neurosymbolic Association Rule Mining from Tabular Data</title>
<link>https://arxiv.org/abs/2504.19354</link>
<guid>https://arxiv.org/abs/2504.19354</guid>
<content:encoded><![CDATA[
<div> Association Rule Mining, ARM, high-dimensional datasets, Aerial+, neurosymbolic method<br />
<br />
Summary: Aerial+ is a novel neurosymbolic method for Association Rule Mining (ARM) that addresses the challenge of managing rule explosion in high-dimensional datasets. It utilizes an under-complete autoencoder to create a neural representation of data and extract rules by leveraging the model's reconstruction mechanism. Evaluation against seven baselines on five datasets showed that Aerial+ produces concise, high-quality rule sets with full data coverage, outperforming existing methods. Integration of Aerial+ into interpretable machine learning models leads to significant reductions in execution time while maintaining or improving accuracy. This demonstrates the effectiveness of Aerial+ in generating efficient and effective association rules for various applications in data mining and machine learning. <br /><br /> <div>
arXiv:2504.19354v1 Announce Type: new 
Abstract: Association Rule Mining (ARM) is the task of mining patterns among data features in the form of logical rules, with applications across a myriad of domains. However, high-dimensional datasets often result in an excessive number of rules, increasing execution time and negatively impacting downstream task performance. Managing this rule explosion remains a central challenge in ARM research. To address this, we introduce Aerial+, a novel neurosymbolic ARM method. Aerial+ leverages an under-complete autoencoder to create a neural representation of the data, capturing associations between features. It extracts rules from this neural representation by exploiting the model's reconstruction mechanism. Extensive evaluations on five datasets against seven baselines demonstrate that Aerial+ achieves state-of-the-art results by learning more concise, high-quality rule sets with full data coverage. When integrated into rule-based interpretable machine learning models, Aerial+ significantly reduces execution time while maintaining or improving accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks</title>
<link>https://arxiv.org/abs/2504.19499</link>
<guid>https://arxiv.org/abs/2504.19499</guid>
<content:encoded><![CDATA[
<div> Load Balancing, QoS, Wireless Networks, Graph Reinforcement Learning, Deep Q Network<br />
Summary:<br />
Next-generation wireless cellular networks aim to provide high QoS, necessitating effective load balancing to prevent cell congestion. A novel QoS-aware Load Balancing (LB) approach is introduced for multi-band Open Radio Access Networks. The solution utilizes Graph Reinforcement Learning (GRL) to optimize GBR and BE traffic under QoS and resource constraints. The LB is modeled as a Markov Decision Process with graph-based states, integrating QoS considerations. An off-policy dueling Deep Q Network (DQN) with GNN-based architecture trains the LB agent, ensuring robust policies invariant to node ordering and accommodating spatial dependencies. Performance comparisons with baseline methods demonstrate significant improvements, including a 53% reduction in QoS violations and fourfold increase in 5th percentile rate for BE traffic.<br />Summary: <div>
arXiv:2504.19499v1 Announce Type: new 
Abstract: Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GVPO: Group Variance Policy Optimization for Large Language Model Post-Training</title>
<link>https://arxiv.org/abs/2504.19599</link>
<guid>https://arxiv.org/abs/2504.19599</guid>
<content:encoded><![CDATA[
<div> Post-training, Large language models, Group Variance Policy Optimization, Gradient weights, KL-constrained reward maximization<br />
Summary:<br />
Group Variance Policy Optimization (GVPO) is introduced as a solution to the training instability issues faced by large language models (LLMs) during post-training. By incorporating the analytical solution to KL-constrained reward maximization into its gradient weights, GVPO ensures alignment with the optimal policy and offers a unique optimal solution. The method provides flexibility in sampling distributions, overcoming limitations of on-policy and importance sampling methods. GVPO's gradient reflects the mean squared error between central distances of implicit and actual rewards, providing both theoretical guarantees and practical adaptability for reliable LLM post-training. This approach establishes a new paradigm for refining and aligning LLMs to specific tasks and human preferences. <br /> <div>
arXiv:2504.19599v1 Announce Type: new 
Abstract: Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. To address this challenge, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evidence to Belief: A Bayesian Epistemology Approach to Language Models</title>
<link>https://arxiv.org/abs/2504.19622</link>
<guid>https://arxiv.org/abs/2504.19622</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, Bayesian epistemology, evidence, confidence, reliability

Summary: 
Language models were analyzed through the lens of Bayesian epistemology to investigate how they adjust their confidence and responses when presented with evidence of varying informativeness and reliability. A dataset with different types of evidence was created for this study, and the analysis was conducted using verbalized confidence, token probability, and sampling methods. The findings revealed that while language models are consistent with the Bayesian confirmation assumption when provided with true evidence, they deviate from other Bayesian assumptions when faced with different evidence types. Despite exhibiting high confidence levels with strong evidence, it does not always guarantee high accuracy. Language models also show a bias towards "golden" evidence and their performance varies based on the degree of irrelevance. This study sheds light on why language models may not always adhere to Bayesian assumptions in their decision-making processes. 

<br /><br />Summary: <div>
arXiv:2504.19622v1 Announce Type: new 
Abstract: This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models' responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different evidence types. Also, we demonstrated that language models can exhibit high confidence when given strong evidence, but this does not always guarantee high accuracy. Our analysis also reveals that language models are biased toward golden evidence and show varying performance depending on the degree of irrelevance, helping explain why they deviate from Bayesian assumptions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search</title>
<link>https://arxiv.org/abs/2504.19636</link>
<guid>https://arxiv.org/abs/2504.19636</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Algorithm Search, Fitness Landscape, Graph-based Approach, Multimodal 

Summary: 
- The study explores the fitness landscape of Large Language Models (LLMs) integrated into Algorithm Search (LAS) using a graph-based approach, analyzing transitions between algorithms.
- Evaluations across six tasks and six LLMs reveal highly multimodal and rugged landscapes, with variations based on tasks and LLMs.
- Combining heuristic and symbolic regression tasks, LAS landscapes show dense clusters and sparse distributions respectively of high-performing algorithms.
- Exploration-exploitation trade-offs are influenced by population size, impacting the trajectory of elite algorithms.
- Insights from the study improve understanding of LAS landscapes and offer guidance for designing more effective LAS methods.<br /><br /> <div>
arXiv:2504.19636v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2504.19678</link>
<guid>https://arxiv.org/abs/2504.19678</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, AI agents, evaluation benchmarks, autonomous decision-making, collaboration protocols

Summary:
This article provides a comprehensive review of the landscape of large language models and autonomous AI agents, focusing on evaluation benchmarks, frameworks, and collaboration protocols. It includes a comparison of benchmarks developed between 2019 and 2025 across various domains, proposing a taxonomy of approximately 60 benchmarks. The review also covers AI-agent frameworks introduced from 2023 to 2025, highlighting real-world applications in multiple fields such as materials science, healthcare, and finance. The article discusses key collaboration protocols for agent-to-agent communication and presents recommendations for future research areas, including advanced reasoning strategies, failure modes in multi-agent systems, automated scientific discovery, reinforcement learning for tool integration, integrated search capabilities, and security vulnerabilities in agent protocols.<br /><br />Summary: <div>
arXiv:2504.19678v1 Announce Type: new 
Abstract: Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Efficiency Meets Symmetry Breaking</title>
<link>https://arxiv.org/abs/2504.19738</link>
<guid>https://arxiv.org/abs/2504.19738</guid>
<content:encoded><![CDATA[
<div> Keywords: Learning-based planners, Graph Neural Networks, Symmetries, Action pruning, State pruning

Summary: 
Learning-based planners that utilize Graph Neural Networks have the capability to learn search guidance for large search spaces. This paper introduces a graph representation of planning problems that combines learning efficiency with the ability to detect symmetries. Two pruning methods, action pruning, and state pruning, are proposed to effectively manage symmetries during search. By integrating these techniques into Fast Downward, the system achieves success over LAMA on the latest IPC learning track dataset. The code for this system is released on GitHub for further exploration and implementation. The framework presented in this paper offers a novel approach to addressing symmetries in planning problems, showcasing the potential of leveraging symmetry detection in learning-based planners. <div>
arXiv:2504.19738v1 Announce Type: new 
Abstract: Learning-based planners leveraging Graph Neural Networks can learn search guidance applicable to large search spaces, yet their potential to address symmetries remains largely unexplored. In this paper, we introduce a graph representation of planning problems allying learning efficiency with the ability to detect symmetries, along with two pruning methods, action pruning and state pruning, designed to manage symmetries during search. The integration of these techniques into Fast Downward achieves a first-time success over LAMA on the latest IPC learning track dataset. Code is released at: https://github.com/bybeye/Distincter.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Agents Design and Implement Drug Discovery Pipelines?</title>
<link>https://arxiv.org/abs/2504.19912</link>
<guid>https://arxiv.org/abs/2504.19912</guid>
<content:encoded><![CDATA[
<div> benchmark, AI agents, drug discovery, decision-making, multi-objective context

Summary: 
The article introduces the DO Challenge benchmark to evaluate AI agents' decision-making abilities in drug discovery scenarios. It highlights the potential of AI-driven systems, such as the Deep Thought multi-agent system, to accelerate pharmaceutical design. The article discusses the diverse strategies explored in the DO Challenge 2025 competition and the performance of various language models in primary and auxiliary agent roles. While AI systems show promise in transforming drug discovery, their current performance falls short of expert-designed solutions and faces high instability. The study emphasizes the opportunities and limitations of AI methodologies in revolutionizing scientific research, particularly in the field of drug discovery. <br /><br /> <div>
arXiv:2504.19912v1 Announce Type: new 
Abstract: The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated decision-making for dynamic task assignment at scale</title>
<link>https://arxiv.org/abs/2504.19933</link>
<guid>https://arxiv.org/abs/2504.19933</guid>
<content:encoded><![CDATA[
<div> Dynamic Task Assignment Problem, Deep Reinforcement Learning, Decision Support System, real-world scale, process mining <br />
Summary: 
This study addresses the Dynamic Task Assignment Problem (DTAP) where tasks are stochastic sequences of activities, solving it through a Deep Reinforcement Learning (DRL) based Decision Support System (DSS). The DRL agent utilizes a graph structure for observations and actions and a reward function targeting average cycle time minimization. Tested on real-world DTAP instances derived from process mining, the DSS showcases superior or comparable performance to baseline methods across different time horizons and scenarios. The proposed system demonstrates the ability to learn effective assignment policies for real-world scale DTAPs, highlighting the potential of DRL in tackling complex operational problems efficiently. <div>
arXiv:2504.19933v1 Announce Type: new 
Abstract: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Group Lives Go Well</title>
<link>https://arxiv.org/abs/2504.19968</link>
<guid>https://arxiv.org/abs/2504.19968</guid>
<content:encoded><![CDATA[
<div> group well-being, ontology engineering, Counterfactual Account, group functional, group flourishing<br />
<br />
This paper introduces a framework for representing group well-being within an ontology engineering context. Traditional well-being theories focus on individual states, but struggle to account for cases where individual sacrifices contribute to broader social progress. The paper refines and extends the Counterfactual Account (CT) of well-being, which compares an individual's actual well-being with a hypothetical counterpart in a nearby possible world. However, this framework is not sufficient for group-level ontologies, where well-being depends on functional persistence, institutional roles, and historical impact rather than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the paper introduces a model where group flourishing is evaluated in terms of group function, likened to biological systems or designed artifacts. This approach allows for structured reasoning about group welfare, social institutions, and group flourishing over time.<br /><br />Summary: <div>
arXiv:2504.19968v1 Announce Type: new 
Abstract: This paper explores the ontological space of group well being, proposing a framework for representing collective welfare, group functions, and long term contributions within an ontology engineering context. Traditional well being theories focus on individual states, often relying on hedonistic, desire satisfaction, or objective list models. Such approaches struggle to account for cases where individual sacrifices contribute to broader social progress, a critical challenge in modeling group flourishing. To address this, the paper refines and extends the Counterfactual Account (CT) of well being, which evaluates goodness of an event by comparing an individual's actual well being with a hypothetical counterpart in a nearby possible world. While useful, this framework is insufficient for group level ontologies, where well being depends on functional persistence, institutional roles, and historical impact rather than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the paper introduces a model in which group flourishing is evaluated in terms of group functional, where members bear roles and exhibit persistence conditions akin to biological systems or designed artifacts. This approach enables semantic interoperability for modeling longitudinal social contributions, allowing for structured reasoning about group welfare, social institutions, and group flourishing over time.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[
<div> Keywords: police body-worn camera, artificial intelligence, machine learning, interaction analysis, data integration <br />
Summary: <br />
This paper introduces a new interdisciplinary framework for analyzing police body-worn camera footage from the Rochester Police Department using advanced AI and ML techniques. The goal is to detect and classify patterns of interaction between police officers and civilians, focusing on behaviors like respect, disrespect, escalation, and de-escalation. The methodology involves integrating video, audio, and NLP techniques to extract insights from the footage. The study aims to provide practical approaches for law enforcement while contributing to knowledge discovery in this area. <div>
arXiv:2504.20007v1 Announce Type: new 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Scoping of AI for Social Good Projects</title>
<link>https://arxiv.org/abs/2504.20010</link>
<guid>https://arxiv.org/abs/2504.20010</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Social Good, Problem Scoping Agent, Large Language Models, Project Proposals<br />
Summary: <br />
Artificial Intelligence for Social Good (AI4SG) aims to tackle societal challenges using AI systems. The bottleneck for many initiatives is the laborious process of problem scoping due to a lack of professionals with technical and domain expertise. To address this, the authors propose a Problem Scoping Agent (PSA) that utilizes a Large Language Model (LLM) to generate project proposals based on scientific literature and real-world knowledge. The framework is tested through blind reviews and AI evaluations, demonstrating its ability to produce comparable proposals to those written by experts. The challenges of real-world problem scoping are acknowledged, highlighting the need for further research in this area. Future work could focus on refining the PSA framework and addressing the limitations identified in the study. <br /><br />Summary: <div>
arXiv:2504.20010v1 Announce Type: new 
Abstract: Artificial Intelligence for Social Good (AI4SG) is an emerging effort that aims to address complex societal challenges with the powerful capabilities of AI systems. These challenges range from local issues with transit networks to global wildlife preservation. However, regardless of scale, a critical bottleneck for many AI4SG initiatives is the laborious process of problem scoping -- a complex and resource-intensive task -- due to a scarcity of professionals with both technical and domain expertise. Given the remarkable applications of large language models (LLM), we propose a Problem Scoping Agent (PSA) that uses an LLM to generate comprehensive project proposals grounded in scientific literature and real-world knowledge. We demonstrate that our PSA framework generates proposals comparable to those written by experts through a blind review and AI evaluations. Finally, we document the challenges of real-world problem scoping and note several areas for future work.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review</title>
<link>https://arxiv.org/abs/2504.18544</link>
<guid>https://arxiv.org/abs/2504.18544</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, tabular data, evaluation methods, health data, guidelines

Summary:
Generating synthetic tabular data is a complex task, and evaluating its quality is equally challenging. This systematic review emphasizes the importance of rigorous evaluation of synthetic health data to ensure credibility and suitability for use. The review of 101 papers highlights key challenges such as the lack of consensus on evaluation methods, incorrect use of metrics, limited input from domain experts, incomplete reporting of dataset characteristics, and low result reproducibility. In response, guidelines are proposed for generating and evaluating synthetic data, aiming to help the community fully utilize synthetic data's potential and drive innovation in the healthcare field.<br /><br />Summary: <div>
arXiv:2504.18544v1 Announce Type: cross 
Abstract: Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>