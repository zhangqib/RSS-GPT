<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>


<item>
<title>SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</title>
<link>https://arxiv.org/abs/2508.04700</link>
<guid>https://arxiv.org/abs/2508.04700</guid>
<content:encoded><![CDATA[
<div> framework, computer-use agents, experiential learning, autonomous evolution, software environments<br />
Summary:<br />
The SEAgent framework addresses the challenge of large vision-language models struggling with unfamiliar and specialized software. It enables computer-use agents to autonomously evolve through experiential learning in novel software environments. SEAgent incorporates a World State Model for trajectory assessment, a Curriculum Generator for task generation, and a policy updating mechanism through experiential learning techniques. A specialist-to-generalist training strategy facilitates the development of a stronger generalist CUA by integrating individual experiential insights. The unified agent outperforms ensembles of specialist agents on specialized software, achieving a significant 23.2% improvement in success rate over a competitive open-source CUA. The validation of SEAgent in five novel software environments within OS-World demonstrates its effectiveness in mastering unfamiliar software through autonomous evolution. <br />Summary: <div>
arXiv:2508.04700v2 Announce Type: replace 
Abstract: Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms</title>
<link>https://arxiv.org/abs/2508.05387</link>
<guid>https://arxiv.org/abs/2508.05387</guid>
<content:encoded><![CDATA[
<div> RL, large language models, Echo, heterogeneous resources, decentralised.

Summary:
The article introduces Echo, a system for large language models that decouples trajectory sampling and policy optimization in reinforcement learning. By separating these phases across "inference" and "training" swarms, Echo improves efficiency by introducing two synchronization protocols: sequential pull mode and asynchronous push-pull mode. The system successfully trains four representative RL workloads on a geographically distributed cluster while off-loading trajectory generation to commodity edge hardware. Results show that Echo matches the performance of a fully co-located baseline in convergence speed and final reward, demonstrating that decentralized, heterogeneous resources could achieve data center-grade performance for large-scale RL models. <div>
arXiv:2508.05387v3 Announce Type: replace-cross 
Abstract: Modern RL-based post-training for large language models (LLMs) co-locate trajectory sampling and policy optimisation on the same GPU cluster, forcing the system to switch between inference and training workloads. This serial context switching violates the single-program-multiple-data (SPMD) assumption underlying today's distributed training systems. We present Echo, the RL system that cleanly decouples these two phases across heterogeneous "inference" and "training" swarms while preserving statistical efficiency. Echo introduces two lightweight synchronization protocols: a sequential pull mode that refreshes policy weights according to API call for minimal bias, and an asynchronous push-pull mode that streams version-tagged rollouts through a replay buffer to maximise hardware utilisation. Training four representative RL workloads with Qwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a geographically distributed cluster, Echo matches a fully co-located Verl baseline in convergence speed and final reward while off-loading trajectory generation to commodity edge hardware. These promising results demonstrate that large-scale RL for LLMs could achieve datacentre-grade performance using decentralised, heterogeneous resources.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topos Theory for Generative AI and LLMs</title>
<link>https://arxiv.org/abs/2508.08293</link>
<guid>https://arxiv.org/abs/2508.08293</guid>
<content:encoded><![CDATA[
<div> novel categorical generative AI architectures, topos theory, universal sequence-to-sequence function approximator, large language models, category theory<br />
<br />
Summary: <br />
The article introduces the design of novel categorical generative AI architectures (GAIAs) using topos theory. It explores the use of universal constructions in category theory to create new architectures for large language models (LLMs). The research builds on the theoretical foundation that the Transformer model is a universal sequence-to-sequence function approximator and investigates novel compositional structures derived from LLM categories. It demonstrates the (co)completeness of the category of LLMs and establishes that it forms a topos, a "set-like" category with exponential objects and subobject classifiers. The study proposes potential implementation of an LLM topos architecture based on functorial characterization of backpropagation. <div>
arXiv:2508.08293v1 Announce Type: new 
Abstract: We propose the design of novel categorical generative AI architectures (GAIAs) using topos theory, a type of category that is ``set-like": a topos has all (co)limits, is Cartesian closed, and has a subobject classifier. Previous theoretical results on the Transformer model have shown that it is a universal sequence-to-sequence function approximator, and dense in the space of all continuous functions with compact support on the Euclidean space of embeddings of tokens. Building on this theoretical result, we explore novel architectures for LLMs that exploit the property that the category of LLMs, viewed as functions, forms a topos. Previous studies of large language models (LLMs) have focused on daisy-chained linear architectures or mixture-of-experts. In this paper, we use universal constructions in category theory to construct novel LLM architectures based on new types of compositional structures. In particular, these new compositional structures are derived from universal properties of LLM categories, and include pullback, pushout, (co) equalizers, exponential objects, and subobject classifiers. We theoretically validate these new compositional structures by showing that the category of LLMs is (co)complete, meaning that all diagrams have solutions in the form of (co)limits. Building on this completeness result, we then show that the category of LLMs forms a topos, a ``set-like" category, which requires showing the existence of exponential objects as well as subobject classifiers. We use a functorial characterization of backpropagation to define a potential implementation of an LLM topos architecture.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topos Causal Models</title>
<link>https://arxiv.org/abs/2508.08295</link>
<guid>https://arxiv.org/abs/2508.08295</guid>
<content:encoded><![CDATA[
<div> Keywords: topos causal models, (co)complete, subobject classifier, exponential objects, causal inference

Summary:
Topos causal models (TCMs) are a new class of causal models that leverage the properties of a topos category. TCMs are (co)complete, allowing for the existence of all limits and colimits, and admit a subobject classifier, facilitating a categorical formulation of causal intervention. The use of exponential objects enables reasoning about equivalence classes of operations on causal models. TCMs, like structural causal models (SCMs), are defined by local autonomous causal mechanisms that combine to create a unique global function. The (co)completeness of the category of TCMs ensures that every causal diagram has a "solution" in the form of a (co)limit, allowing for the approximation of arbitrary causal models. Causal interventions are modeled through subobject classifiers, and reasoning about causal equivalences and interventions is enabled by exponential objects. TCMs also have an internal logic based on a Mitchell-Benabou language with a Kripke-Joyal semantics, facilitating reasoning about causal models within the framework of TCMs.

<br /><br />Summary: <div>
arXiv:2508.08295v1 Announce Type: new 
Abstract: We propose topos causal models (TCMs), a novel class of causal models that exploit the key properties of a topos category: they are (co)complete, meaning all (co)limits exist, they admit a subobject classifier, and allow exponential objects. The main goal of this paper is to show that these properties are central to many applications in causal inference. For example, subobject classifiers allow a categorical formulation of causal intervention, which creates sub-models. Limits and colimits allow causal diagrams of arbitrary complexity to be ``solved", using a novel interpretation of causal approximation. Exponential objects enable reasoning about equivalence classes of operations on causal models, such as covered edge reversal and causal homotopy. Analogous to structural causal models (SCMs), TCMs are defined by a collection of functions, each defining a ``local autonomous" causal mechanism that assemble to induce a unique global function from exogenous to endogenous variables. Since the category of TCMs is (co)complete, which we prove in this paper, every causal diagram has a ``solution" in the form of a (co)limit: this implies that any arbitrary causal model can be ``approximated" by some global function with respect to the morphisms going into or out of the diagram. Natural transformations are crucial in measuring the quality of approximation. In addition, we show that causal interventions are modeled by subobject classifiers: any sub-model is defined by a monic arrow into its parent model. Exponential objects permit reasoning about entire classes of causal equivalences and interventions. Finally, as TCMs form a topos, they admit an internal logic defined as a Mitchell-Benabou language with an associated Kripke-Joyal semantics. We show how to reason about causal models in TCMs using this internal logic.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Application of Goal Programming to Tackle Multiobjective Problems with Recurring Fitness Landscapes</title>
<link>https://arxiv.org/abs/2508.08297</link>
<guid>https://arxiv.org/abs/2508.08297</guid>
<content:encoded><![CDATA[
<div> Keywords: decision-making, multiobjective optimization, goal programming, vehicle routing problem, fitness landscapes

Summary: 
The article discusses the challenges of solving highly constrained many-objective problems and proposes a methodology to address this issue by exploiting similarities in fitness landscapes across problem instances. The approach involves using computationally expensive multiobjective algorithms to solve one instance of a given problem scenario and then applying Goal Programming with efficient single-objective algorithms to solve other instances. Three goal-based objective functions are utilized in the methodology. Benchmark instances of the multiobjective vehicle routing problem with time windows are used to demonstrate the effectiveness of the proposed approach in producing good results within a short computation time. By combining the strengths of state-of-the-art multiobjective algorithms with the efficiency of goal programming, the methodology can find optimal compromise solutions in problem scenarios where instances share similar fitness landscapes. <div>
arXiv:2508.08297v1 Announce Type: new 
Abstract: Many real-world applications require decision-makers to assess the quality of solutions while considering multiple conflicting objectives. Obtaining good approximation sets for highly constrained many-objective problems is often a difficult task even for modern multiobjective algorithms. In some cases, multiple instances of the problem scenario present similarities in their fitness landscapes. That is, there are recurring features in the fitness landscapes when searching for solutions to different problem instances. We propose a methodology to exploit this characteristic by solving one instance of a given problem scenario using computationally expensive multiobjective algorithms to obtain a good approximation set and then using Goal Programming with efficient single-objective algorithms to solve other instances of the same problem scenario. We use three goal-based objective functions and show that on benchmark instances of the multiobjective vehicle routing problem with time windows, the methodology is able to produce good results in short computation time. The methodology allows to combine the effectiveness of state-of-the-art multiobjective algorithms with the efficiency of goal programming to find good compromise solutions in problem scenarios where instances have similar fitness landscapes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2508.08300</link>
<guid>https://arxiv.org/abs/2508.08300</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian inference, Large Language Model, automation, prior distributions, likelihoods <br />
Summary: 
This paper explores the use of Large Language Models (LLMs) to automate Bayesian inference, addressing the challenge of specifying prior distributions and likelihoods. The proposed pipeline, LLM-BI, leverages LLMs to streamline Bayesian workflows. Two experiments on Bayesian linear regression showcase the effectiveness of LLMs in eliciting prior distributions from natural language and specifying the entire model structure from a problem description. These results demonstrate the capability of LLMs to automate critical steps in Bayesian modeling, paving the way for an automated inference pipeline in probabilistic programming. By harnessing the power of LLMs, this research opens up possibilities for broader adoption of Bayesian inference by simplifying the technical requirements and specialized statistical expertise traditionally needed in this field. <br /><br />Summary: <div>
arXiv:2508.08300v1 Announce Type: new 
Abstract: A significant barrier to the widespread adoption of Bayesian inference is the specification of prior distributions and likelihoods, which often requires specialized statistical expertise. This paper investigates the feasibility of using a Large Language Model (LLM) to automate this process. We introduce LLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline for automating Bayesian workflows. As a proof-of-concept, we present two experiments focused on Bayesian linear regression. In Experiment I, we demonstrate that an LLM can successfully elicit prior distributions from natural language. In Experiment II, we show that an LLM can specify the entire model structure, including both priors and the likelihood, from a single high-level problem description. Our results validate the potential of LLMs to automate key steps in Bayesian modeling, enabling the possibility of an automated inference pipeline for probabilistic programming.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models</title>
<link>https://arxiv.org/abs/2508.08308</link>
<guid>https://arxiv.org/abs/2508.08308</guid>
<content:encoded><![CDATA[
<div> interaction paradigm, large language models, multidimensional supplementary questions, response quality, user participation
Summary: 
The article introduces a new interaction paradigm called First Ask Then Answer (FATA) for Large Language Models (LLMs) to improve response accuracy. FATA guides LLMs to generate supplementary questions for users before responding, enhancing completeness and user participation. Unlike existing approaches, FATA focuses on generating all clarifying questions at once, reducing dialogue length and improving efficiency. By integrating user-provided information with the original query, FATA significantly enhances response quality compared to baseline methods. Experimental results demonstrate FATA's superiority, outperforming baseline prompts by 40% in aggregate metrics and exhibiting greater stability. Overall, FATA leverages LLM reasoning to support user expression and enable more comprehensive and relevant queries. 
<br /><br />Summary: <div>
arXiv:2508.08308v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often struggle to deliver accurate and actionable answers when user-provided information is incomplete or ill-specified. We propose a new interaction paradigm, First Ask Then Answer (FATA), in which, through prompt words, LLMs are guided to proactively generate multidimensional supplementary questions for users prior to response generation. Subsequently, by integrating user-provided supplementary information with the original query through sophisticated prompting techniques, we achieve substantially improved response quality and relevance. In contrast to existing clarification approaches -- such as the CLAM framework oriented to ambiguity and the self-interrogation Self-Ask method -- FATA emphasizes completeness (beyond mere disambiguation) and user participation (inviting human input instead of relying solely on model-internal reasoning). It also adopts a single-turn strategy: all clarifying questions are produced at once, thereby reducing dialogue length and improving efficiency. Conceptually, FATA uses the reasoning power of LLMs to scaffold user expression, enabling non-expert users to formulate more comprehensive and contextually relevant queries. To evaluate FATA, we constructed a multi-domain benchmark and compared it with two controls: a baseline prompt (B-Prompt) and a context-enhanced expert prompt (C-Prompt). Experimental results show that FATA outperforms B-Prompt by approximately 40% in aggregate metrics and exhibits a coefficient of variation 8% lower than C-Prompt, indicating superior stability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge</title>
<link>https://arxiv.org/abs/2508.08344</link>
<guid>https://arxiv.org/abs/2508.08344</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph-based Retrieval-Augmented Generation, evaluation practices, benchmarks, reasoning ability, knowledge incompleteness <br />
Summary: <br />
Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) combines language models with knowledge graphs. Current evaluation methods are insufficient as they include questions easily answered with existing knowledge graph data, leading to ambiguity about the model's reasoning abilities. Inconsistent metrics and lenient matching criteria further complicate comparisons. This study introduces a benchmark construction method and evaluation protocol to assess KG-RAG models under incomplete knowledge. Results show limited reasoning ability in handling missing knowledge, reliance on memorization, and varying generalization capacities based on model design. <div>
arXiv:2508.08344v1 Announce Type: new 
Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an increasingly explored approach for combining the reasoning capabilities of large language models with the structured evidence of knowledge graphs. However, current evaluation practices fall short: existing benchmarks often include questions that can be directly answered using existing triples in KG, making it unclear whether models perform reasoning or simply retrieve answers directly. Moreover, inconsistent evaluation metrics and lenient answer matching criteria further obscure meaningful comparisons. In this work, we introduce a general method for constructing benchmarks, together with an evaluation protocol, to systematically assess KG-RAG methods under knowledge incompleteness. Our empirical results show that current KG-RAG methods have limited reasoning ability under missing knowledge, often rely on internal memorization, and exhibit varying degrees of generalization depending on their design.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games</title>
<link>https://arxiv.org/abs/2508.08382</link>
<guid>https://arxiv.org/abs/2508.08382</guid>
<content:encoded><![CDATA[
<div> drafting decisions, Magic: The Gathering, domain-adapted, large language model, AI, 

Summary:
Collectible card games pose challenges to AI due to their complexity. Current AI models struggle in tasks like deckbuilding and gameplay compared to humans. This study introduces UrzaGPT, a domain-adapted large language model specifically designed to recommend drafting decisions in real-time for Magic: The Gathering. By fine-tuning a large language model on annotated draft logs using Low-Rank Adaptation, UrzaGPT shows promising performance in comparison to zero-shot models and existing domain-specific models. While smaller LLMs like Llama-3-8B struggle, GPT-4o achieves 43% accuracy in zero-shot drafting. Through fine-tuning smaller models with UrzaGPT, accuracy improves to 66.2% with just 10,000 steps. Although not yet reaching the level of domain-specific models, the study demonstrates the potential for using LLMs to create effective, versatile, and easily updateable drafting AIs in the future. 

<br /><br />Summary: <div>
arXiv:2508.08382v1 Announce Type: new 
Abstract: Collectible card games (CCGs) are a difficult genre for AI due to their partial observability, long-term decision-making, and evolving card sets. Due to this, current AI models perform vastly worse than human players at CCG tasks such as deckbuilding and gameplay. In this work, we introduce $\textit{UrzaGPT}$, a domain-adapted large language model that recommends real-time drafting decisions in $\textit{Magic: The Gathering}$. Starting from an open-weight LLM, we use Low-Rank Adaptation fine-tuning on a dataset of annotated draft logs. With this, we leverage the language modeling capabilities of LLM, and can quickly adapt to different expansions of the game. We benchmark $\textit{UrzaGPT}$ in comparison to zero-shot LLMs and the state-of-the-art domain-specific model. Untuned, small LLMs like Llama-3-8B are completely unable to draft, but the larger GPT-4o achieves a zero-shot performance of $43\%$. Using UrzaGPT to fine-tune smaller models, we achieve an accuracy of $66.2\%$ using only 10,000 steps. Despite this not reaching the capability of domain-specific models, we show that solely using LLMs to draft is possible and conclude that using LLMs can enable performant, general, and update-friendly drafting AIs in the future.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</title>
<link>https://arxiv.org/abs/2508.08385</link>
<guid>https://arxiv.org/abs/2508.08385</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Armed Bandit, Monte-Carlo Tree Search, classical planning, node selection, Tree Collapsing

Summary: 
In the study, an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning is presented. The focus is on addressing the bottleneck in node selection, which becomes significant in classical planning due to the potentially large search depth. To improve efficiency, a bilevel modification to MCTS is proposed, involving running a best-first search from selected leaf nodes with an expansion budget proportional to the search depth. This modification results in an amortized runtime complexity of O(1) for node selection. Additionally, a technique called Tree Collapsing is introduced, which reduces action selection steps and further enhances performance. These enhancements aim to make MCTS more efficient in classical planning scenarios where the search depth can be arbitrarily large. <div>
arXiv:2508.08385v1 Announce Type: new 
Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solver-Aided Expansion of Loops to Avoid Generate-and-Test</title>
<link>https://arxiv.org/abs/2508.08442</link>
<guid>https://arxiv.org/abs/2508.08442</guid>
<content:encoded><![CDATA[
<div> optimization, constraint modelling, MiniZinc, induction variables, solver

Summary: 
The article introduces a new approach to constraint modelling languages like MiniZinc and Essence that improves the efficiency of loop unrolling during compilation. The standard approach involves generating all combinations of induction variables and using partial evaluation to discard irrelevant ones. However, this method can be inefficient for problems where most combinations are ultimately irrelevant. The proposed method avoids full enumeration by using a solver to compute only the combinations necessary to generate the final set of constraints. This results in a faster compilation process while producing the same model as conventional flattening. The approach is particularly beneficial for problems with induction variables ranging over large domains with selective preconditions. This advancement enhances the translation of high-level user models into solver-ready form, improving the overall efficiency of constraint modelling languages. 

<br /><br />Summary: <div>
arXiv:2508.08442v1 Announce Type: new 
Abstract: Constraint modelling languages like MiniZinc and Essence rely on unrolling loops (in the form of quantified expressions and comprehensions) during compilation. Standard approaches generate all combinations of induction variables and use partial evaluation to discard those that simplify to identity elements of associative-commutative operators (e.g. true for conjunction, 0 for summation). This can be inefficient for problems where most combinations are ultimately irrelevant. We present a method that avoids full enumeration by using a solver to compute only the combinations required to generate the final set of constraints. The resulting model is identical to that produced by conventional flattening, but compilation can be significantly faster. This improves the efficiency of translating high-level user models into solver-ready form, particularly when induction variables range over large domains with selective preconditions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OverFill: Two-Stage Models for Efficient Language Model Decoding</title>
<link>https://arxiv.org/abs/2508.08446</link>
<guid>https://arxiv.org/abs/2508.08446</guid>
<content:encoded><![CDATA[
<div> Inference, Language Models, OverFill, Efficiency, Latency  
Summary:  
- Large language models (LLMs) face challenges in deployment due to high inference costs, with the decode stage dominating latency for long sequences.  
- The proposed OverFill model decouples prefill and decode stages to optimize accuracy-efficiency tradeoffs.  
- OverFill uses a full model for prefill and switches to a dense pruned model for generating tokens sequentially, improving generation quality with minimal latency overhead.  
- The 3B-to-1B and 8B-to-3B configurations of OverFill outperform pruned models by 83.2% and 79.2%, respectively, across standard benchmarks.  
- OverFill matches same-sized models trained from scratch in performance while using significantly less training data.  

<br /><br />Summary: <div>
arXiv:2508.08446v1 Announce Type: new 
Abstract: Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search</title>
<link>https://arxiv.org/abs/2508.08477</link>
<guid>https://arxiv.org/abs/2508.08477</guid>
<content:encoded><![CDATA[
<div> Dynamic arc costs, Trigger Arc Traveling Salesman Problem, GRASP metaheuristic, mixed-integer programming, real-time routing<br />
<br />
Summary:<br />
The Trigger Arc Traveling Salesman Problem (TA-TSP) is addressed in this paper through a GRASP-based metaheuristic that combines construction heuristics with a multi-neighborhood local search. The method leverages mixed-integer programming techniques in the construction phase to transform the TA-TSP into tailored TSP instances, followed by the application of 2-Opt, Swap, and Relocate operators in the improvement phase. Computational experiments on MESS 2024 competition instances showed impressively low optimality gaps relative to the best-known solutions, with solutions outperforming the Gurobi solver by 11.3% on synthetic datasets. The algorithm also performed well in real-time routing applications with state-dependent travel costs, achieving a top-three finish at MESS 2024. The results demonstrate the effectiveness of the proposed approach in solving the TA-TSP efficiently and producing high-quality solutions within time constraints. <br /><br /> <div>
arXiv:2508.08477v1 Announce Type: new 
Abstract: The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific \textit{trigger} arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77\% and 0.40\% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3\% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback</title>
<link>https://arxiv.org/abs/2508.08486</link>
<guid>https://arxiv.org/abs/2508.08486</guid>
<content:encoded><![CDATA[
<div> impossibility result, ordinal data, cardinal feedback, willingness-to-pay elicitations, dataset

Summary: This article discusses the limitations of alignment techniques for Language Model Fine-Tuning (LLMs) that rely on ordinal comparisons of preferences. The study proves that no algorithm based on ordinal comparisons alone can consistently recover the most preferred model. This is because ordinal data lacks the necessary information to resolve trade-offs in model evaluation, such as balancing factual accuracy with stylistic improvements. The research highlights the importance of incorporating cardinal feedback, such as willingness-to-pay elicitations, to capture preferences over models rather than just responses. The authors conducted experiments with 25,000 cardinal judgments and found that integrating cardinal feedback into model fine-tuning allows for prioritizing high-impact improvements, leading to superior performance on benchmarks like Arena-Hard. This indicates the significance of using cardinal feedback in preference elicitation for enhancing model performance in LLMs.

<br /><br />Summary: <div>
arXiv:2508.08486v1 Announce Type: new 
Abstract: Alignment techniques for LLMs rely on optimizing preference-based objectives -- where these preferences are typically elicited as ordinal, binary choices between responses. Recent work has focused on improving label quality or mitigating particular biases, but we identify a more fundamental limitation: these methods collect the wrong kind of data. We prove an impossibility result: no algorithm relying solely on ordinal comparisons can systematically recover the most preferred model. Intuitively, ordinal data lacks the information needed to resolve tradeoffs -- e.g., fixing a factual error on one prompt versus improving style on another. We show that selecting the optimal model requires recovering preferences over \emph{models} (rather than just responses), which can only be identified given cardinal feedback about response quality. To address this, we collect and publicly release a dataset of 25,000 cardinal judgments using willingness-to-pay elicitations, a well-established tool from experimental economics. Empirically, we find that incorporating cardinal feedback into preference fine-tuning allows models to prioritize high-impact improvements and outperform ordinal-only methods on downstream benchmarks, such as Arena-Hard.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2508.08493</link>
<guid>https://arxiv.org/abs/2508.08493</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, POMO, combinatorial problems, Vehicle Routing Problem, improved algorithm

Summary: 
Reinforcement learning methods have shown promise in solving combinatorial problems, with POMO being a strong performer on tasks like the Vehicle Routing Problem. However, there is room for enhancement in these tasks. In this study, a new method called POMO+ was developed to improve upon the existing POMO model. By utilizing initial nodes more effectively, POMO+ demonstrated faster convergence and better results in experiments. The models were validated using the CVRPLIB dataset, showing improvements in instances with up to 100 customers. The research has potential to drive further advancements in the field of combinatorial problem-solving using reinforcement learning.

<br /><br />Summary: <div>
arXiv:2508.08493v1 Announce Type: new 
Abstract: In recent years, reinforcement learning (RL) methods have emerged as a promising approach for solving combinatorial problems. Among RL-based models, POMO has demonstrated strong performance on a variety of tasks, including variants of the Vehicle Routing Problem (VRP). However, there is room for improvement for these tasks. In this work, we improved POMO, creating a method (\textbf{POMO+}) that leverages the initial nodes to find a solution in a more informed way. We ran experiments on our new model and observed that our solution converges faster and achieves better results. We validated our models on the CVRPLIB dataset and noticed improvements in problem instances with up to 100 customers. We hope that our research in this project can lead to further advancements in the field.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Oracles for Ontology Alignment</title>
<link>https://arxiv.org/abs/2508.08500</link>
<guid>https://arxiv.org/abs/2508.08500</guid>
<content:encoded><![CDATA[
<div> Ontology alignment, Large Language Models, Human-in-the-loop, Ontology Alignment Evaluation Initiative, state-of-the-art<br />
Summary:<br />
Ontology alignment is crucial for integrating data sources across domains. While many systems exist, accuracy remains a challenge, especially for large ontologies. This study explores using Large Language Models (LLM) to validate uncertain correspondences. Evaluating LLM performance on OAEI tasks with various prompts, results are compared to simulated Oracles. LLMs show promise as alternatives to human experts, especially for validating uncertain mappings. <div>
arXiv:2508.08500v1 Announce Type: new 
Abstract: Ontology alignment plays a crucial role in integrating diverse data sources across domains. There is a large plethora of systems that tackle the ontology alignment problem, yet challenges persist in producing highly quality correspondences among a set of input ontologies. Human-in-the-loop during the alignment process is essential in applications requiring very accurate mappings. User involvement is, however, expensive when dealing with large ontologies. In this paper, we explore the feasibility of using Large Language Models (LLM) as an alternative to the domain expert. The use of the LLM focuses only on the validation of the subset of correspondences where an ontology alignment system is very uncertain. We have conducted an extensive evaluation over several matching tasks of the Ontology Alignment Evaluation Initiative (OAEI), analysing the performance of several state-of-the-art LLMs using different ontology-driven prompt templates. The LLM results are also compared against simulated Oracles with variable error rates.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games</title>
<link>https://arxiv.org/abs/2508.08501</link>
<guid>https://arxiv.org/abs/2508.08501</guid>
<content:encoded><![CDATA[
<div> benchmark, reasoning, problem-solving, language models, spatial reasoning

Summary:
The article introduces GVGAI-LLM, a video game benchmark designed to evaluate the reasoning and problem-solving abilities of large language models (LLMs). It is based on the General Video Game AI framework and includes arcade-style games to test models on tasks different from existing benchmarks. The benchmark uses a game description language for rapid game creation to prevent overfitting. Each game scene is represented in ASCII characters for efficient processing by LLMs. Various metrics such as step ratio, step efficiency, and overall score are defined to evaluate model behavior. Zero-shot evaluations on diverse games reveal persistent limitations in spatial reasoning and basic planning in LLMs. The study suggests structured prompting and spatial grounding techniques to improve model performance but notes the benchmark remains challenging. GVGAI-LLM serves as a reproducible testbed for advancing research on LLM capabilities, emphasizing agentic behavior and contextual reasoning. 

<br /><br />Summary: <div>
arXiv:2508.08501v1 Announce Type: new 
Abstract: We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). Built on the General Video Game AI framework, it features a diverse collection of arcade-style games designed to test a model's ability to handle tasks that differ from most existing LLM benchmarks. The benchmark leverages a game description language that enables rapid creation of new games and levels, helping to prevent overfitting over time. Each game scene is represented by a compact set of ASCII characters, allowing for efficient processing by language models. GVGAI-LLM defines interpretable metrics, including the meaningful step ratio, step efficiency, and overall score, to assess model behavior. Through zero-shot evaluations across a broad set of games and levels with diverse challenges and skill depth, we reveal persistent limitations of LLMs in spatial reasoning and basic planning. Current models consistently exhibit spatial and logical errors, motivating structured prompting and spatial grounding techniques. While these interventions lead to partial improvements, the benchmark remains very far from solved. GVGAI-LLM provides a reproducible testbed for advancing research on language model capabilities, with a particular emphasis on agentic behavior and contextual reasoning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering</title>
<link>https://arxiv.org/abs/2508.08529</link>
<guid>https://arxiv.org/abs/2508.08529</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, medical tabular data, Large Language Models, privacy preservation, healthcare research

Summary: 
The paper introduces SynLLM, a framework for generating synthetic medical tabular data using state-of-the-art Large Language Models (LLMs). Four types of prompts are proposed to encode schema, metadata, and domain knowledge, controlling data generation without fine-tuning the models. The framework includes a rigorous evaluation pipeline assessing data quality across statistical fidelity, clinical consistency, and privacy preservation. Evaluation on three public medical datasets shows that prompt engineering significantly impacts data quality and privacy risk, with rule-based prompts achieving the best balance. SynLLM demonstrates that with well-designed prompts and robust evaluation criteria, LLMs can generate clinically plausible and privacy-aware synthetic medical data, promoting safer and more effective data sharing in healthcare research.

<br /><br />Summary: <div>
arXiv:2508.08529v1 Announce Type: new 
Abstract: Access to real-world medical data is often restricted due to privacy regulations, posing a significant barrier to the advancement of healthcare research. Synthetic data offers a promising alternative; however, generating realistic, clinically valid, and privacy-conscious records remains a major challenge. Recent advancements in Large Language Models (LLMs) offer new opportunities for structured data generation; however, existing approaches frequently lack systematic prompting strategies and comprehensive, multi-dimensional evaluation frameworks.
  In this paper, we present SynLLM, a modular framework for generating high-quality synthetic medical tabular data using 20 state-of-the-art open-source LLMs, including LLaMA, Mistral, and GPT variants, guided by structured prompts. We propose four distinct prompt types, ranging from example-driven to rule-based constraints, that encode schema, metadata, and domain knowledge to control generation without model fine-tuning. Our framework features a comprehensive evaluation pipeline that rigorously assesses generated data across statistical fidelity, clinical consistency, and privacy preservation.
  We evaluate SynLLM across three public medical datasets, including Diabetes, Cirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt engineering significantly impacts data quality and privacy risk, with rule-based prompts achieving the best privacy-quality balance. SynLLM establishes that, when guided by well-designed prompts and evaluated with robust, multi-metric criteria, LLMs can generate synthetic medical data that is both clinically plausible and privacy-aware, paving the way for safer and more effective data sharing in healthcare research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss</title>
<link>https://arxiv.org/abs/2508.08615</link>
<guid>https://arxiv.org/abs/2508.08615</guid>
<content:encoded><![CDATA[
<div> Mesh movement network, partial differential equations, unsupervised learning, generalization, physics-constrained loss function

Summary: 
The paper introduces the Unsupervised and Generalizable Mesh Movement Network (UGM2N) for solving partial differential equations (PDEs) in a more efficient and accurate manner. The network utilizes unsupervised mesh adaptation and localized geometric feature learning to eliminate the need for pre-adapted meshes. A physics-constrained loss function, known as the M-Uniform loss, is developed to ensure mesh equidistribution at the nodal level. Experimental results show that the UGM2N outperforms existing methods in terms of generalization across diverse PDEs and mesh topologies, scalability to multi-scale resolutions, and guaranteed error reduction without mesh tangling. The network demonstrates superior performance, robustness, and geometric independence in efficiently adapting meshes for simulations of physical systems in science and engineering. <div>
arXiv:2508.08615v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) form the mathematical foundation for modeling physical systems in science and engineering, where numerical solutions demand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address this challenge by dynamically relocating mesh nodes to rapidly-varying regions, enhancing both simulation accuracy and computational efficiency. However, traditional approaches suffer from high computational complexity and geometric inflexibility, limiting their applicability, and existing supervised learning-based approaches face challenges in zero-shot generalization across diverse PDEs and mesh topologies.In this paper, we present an Unsupervised and Generalizable Mesh Movement Network (UGM2N). We first introduce unsupervised mesh adaptation through localized geometric feature learning, eliminating the dependency on pre-adapted meshes. We then develop a physics-constrained loss function, M-Uniform loss, that enforces mesh equidistribution at the nodal level.Experimental results demonstrate that the proposed network exhibits equation-agnostic generalization and geometric independence in efficient mesh adaptation. It demonstrates consistent superiority over existing methods, including robust performance across diverse PDEs and mesh geometries, scalability to multi-scale resolutions and guaranteed error reduction without mesh tangling.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriGPT: a Large Language Model Ecosystem for Agriculture</title>
<link>https://arxiv.org/abs/2508.08632</link>
<guid>https://arxiv.org/abs/2508.08632</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Agriculture, AgriGPT, Retrieval-Augmented Generation, AgriBench-13K

Summary:
AgriGPT is a specialized Large Language Model ecosystem designed for agricultural applications. It includes a high-quality standardized question-answer dataset, Agri-342K, compiled from credible data sources. The model utilizes Tri-RAG, a Retrieval-Augmented Generation framework, for improved reasoning reliability. An evaluation suite, AgriBench-13K, consisting of 13 tasks, is introduced to assess the model's performance. AgriGPT outperforms general-purpose LLMs in domain adaptation and reasoning tasks. The ecosystem is designed to be modular and extensible, focusing on structured data construction, retrieval-enhanced generation, and domain-specific evaluation. The work aims to empower agricultural communities, particularly in underserved regions, and promote open research in the field. All models, datasets, and code will be made publicly available. 

<br /><br />Summary: 
AgriGPT is a specialized LLM ecosystem for agriculture, featuring a curated QA dataset, advanced reasoning techniques, and a comprehensive evaluation suite. The model surpasses general-purpose LLMs in domain-specific tasks, showcasing its potential for agricultural applications. Its modular design allows for flexibility and extensibility, catering to diverse agricultural stakeholders. The release of all resources aims to facilitate impactful research in agriculture and support underserved communities. <div>
arXiv:2508.08632v1 Announce Type: new 
Abstract: Despite the rapid progress of Large Language Models (LLMs), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized LLM ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning, thereby improving the LLM's reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose LLMs on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible LLM ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized LLMs. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diminution: On Reducing the Size of Grounding ASP Programs</title>
<link>https://arxiv.org/abs/2508.08633</link>
<guid>https://arxiv.org/abs/2508.08633</guid>
<content:encoded><![CDATA[
<div> diminution, answer set programming, grounding bottleneck, Herbrand universe, performance improvement 
<br />
Answer Set Programming (ASP) often faces challenges due to the grounding bottleneck caused by large Herbrand universes generating massive ground programs. To address this issue, a formal and generalizable strategy called diminution is introduced. Diminution involves selecting a subset of the Herbrand universe to generate a reduced ground program before solving. The concept is formally defined, key properties are analyzed, and the complexity of identifying diminution is studied. An encoding method is used to enable off-the-shelf ASP solvers to evaluate candidate subsets. By applying diminutions selected through their strategy, significant performance improvements are observed in experiments on various benchmarks. Grounding time is reduced by up to 70% on average, and grounding file sizes decrease by up to 85%. These findings highlight the effectiveness of leveraging diminutions to address the grounding bottleneck in ASP. 
<br /><br />Summary: <div>
arXiv:2508.08633v1 Announce Type: new 
Abstract: Answer Set Programming (ASP) is often hindered by the grounding bottleneck: large Herbrand universes generate ground programs so large that solving becomes difficult. Many methods employ ad-hoc heuristics to improve grounding performance, motivating the need for a more formal and generalizable strategy. We introduce the notion of diminution, defined as a selected subset of the Herbrand universe used to generate a reduced ground program before solving. We give a formal definition of diminution, analyze its key properties, and study the complexity of identifying it. We use a specific encoding that enables off-the-shelf ASP solver to evaluate candidate subsets. Our approach integrates seamlessly with existing grounders via domain predicates. In extensive experiments on five benchmarks, applying diminutions selected by our strategy yields significant performance improvements, reducing grounding time by up to 70% on average and decreasing the size of grounding files by up to 85%. These results demonstrate that leveraging diminutions constitutes a robust and general-purpose approach for alleviating the grounding bottleneck in ASP.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records</title>
<link>https://arxiv.org/abs/2508.08646</link>
<guid>https://arxiv.org/abs/2508.08646</guid>
<content:encoded><![CDATA[
<div> Feature selection, EHR, personalized, online, cost-aware<br />
<br />
Summary:
The article introduces a novel personalized, online, and cost-aware feature selection framework designed for Electronic Health Records (EHR) datasets. Traditional feature selection methods struggle with sparse and heterogeneous EHR data, making it challenging to extract valuable insights for clinical applications. The proposed framework addresses these challenges by acquiring features online for individual patients, considering budget constraints and feature variability costs. This approach aims to support physicians in decision-making during patient screenings by guiding them towards acquiring the most informative features within budget limitations. By optimizing resource utilization and increasing diagnostic confidence, the framework enhances clinical workflows and patient care outcomes. <div>
arXiv:2508.08646v1 Announce Type: new 
Abstract: Electronic Health Records (EHR) have revolutionized healthcare by digitizing patient data, improving accessibility, and streamlining clinical workflows. However, extracting meaningful insights from these complex and multimodal datasets remains a significant challenge for researchers. Traditional feature selection methods often struggle with the inherent sparsity and heterogeneity of EHR data, especially when accounting for patient-specific variations and feature costs in clinical applications. To address these challenges, we propose a novel personalized, online and cost-aware feature selection framework tailored specifically for EHR datasets. The features are aquired in an online fashion for individual patients, incorporating budgetary constraints and feature variability costs. The framework is designed to effectively manage sparse and multimodal data, ensuring robust and scalable performance in diverse healthcare contexts. A primary application of our proposed method is to support physicians' decision making in patient screening scenarios. By guiding physicians toward incremental acquisition of the most informative features within budget constraints, our approach aims to increase diagnostic confidence while optimizing resource utilization.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</title>
<link>https://arxiv.org/abs/2508.08652</link>
<guid>https://arxiv.org/abs/2508.08652</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural communication compliance, simulation-based training, large language models, prompt-based inference, maritime domain <br />
Summary: 
This paper introduces a new method, Prompt-and-Check, for evaluating procedural communication compliance in simulation-based training using large language models (LLMs). The method utilizes context-rich prompts to assess checklist items based on verbal exchanges in a simulation task in the maritime domain. The study utilized models such as LLama 2 7B, LLaMA 3 8B, and Mistral 7B running on an RTX 4070 GPU. Results show that prompt-based inference allows for effective context-aware reasoning without task-specific training. This approach has practical applications in enhancing debriefing, providing performance feedback, and automating assessments in training environments. Overall, the study demonstrates the usefulness of LLMs in improving evaluation processes in safety-critical domains. <br /><br /> Summary: <div>
arXiv:2508.08652v1 Announce Type: new 
Abstract: Accurate evaluation of procedural communication compliance is essential in simulation-based training, particularly in safety-critical domains where adherence to compliance checklists reflects operational competence. This paper explores a lightweight, deployable approach using prompt-based inference with open-source large language models (LLMs) that can run efficiently on consumer-grade GPUs. We present Prompt-and-Check, a method that uses context-rich prompts to evaluate whether each checklist item in a protocol has been fulfilled, solely based on transcribed verbal exchanges. We perform a case study in the maritime domain with participants performing an identical simulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and Mistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a prompt incorporating relevant transcript excerpts is fed into the model, which outputs a compliance judgment. We assess model outputs against expert-annotated ground truth using classification accuracy and agreement scores. Our findings demonstrate that prompting enables effective context-aware reasoning without task-specific training. This study highlights the practical utility of LLMs in augmenting debriefing, performance feedback, and automated assessment in training environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Node-Destroyer Model with Large Neighborhood Search for Solving the Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2508.08659</link>
<guid>https://arxiv.org/abs/2508.08659</guid>
<content:encoded><![CDATA[
<div> Keywords: iterative learning, hybrid optimization, Capacitated Vehicle Routing Problem, Node-Destroyer Model, Graph Neural Networks

Summary:
This research introduces an iterative learning hybrid optimization solver designed to enhance the performance of metaheuristic algorithms in solving the Capacitated Vehicle Routing Problem (CVRP). The proposed Node-Destroyer Model, incorporating Graph Neural Networks (GNNs), aids in identifying and selecting customer nodes to guide the Large Neighborhood Search (LNS) operator within metaheuristic optimization frameworks. By leveraging the structural properties of the problem and solution as a graph, the model strategically selects nodes for removal, reducing operational complexity and scaling down the search space. This approach, specifically tailored for the CVRP, does not require retraining across different problem sizes and demonstrates improved solution quality for standard CVRP benchmarks. Moreover, the hybrid mechanism exhibits scalability on large-scale instances with up to 30,000 customer nodes, outperforming baseline algorithms and achieving higher solution quality under similar conditions. 

<br /><br />Summary: <div>
arXiv:2508.08659v1 Announce Type: new 
Abstract: In this research, we propose an iterative learning hybrid optimization solver developed to strengthen the performance of metaheuristic algorithms in solving the Capacitated Vehicle Routing Problem (CVRP). The iterative hybrid mechanism integrates the proposed Node-Destroyer Model, a machine learning hybrid model that utilized Graph Neural Networks (GNNs) such identifies and selects customer nodes to guide the Large Neighborhood Search (LNS) operator within the metaheuristic optimization frameworks. This model leverages the structural properties of the problem and solution that can be represented as a graph, to guide strategic selections concerning node removal. The proposed approach reduces operational complexity and scales down the search space involved in the optimization process. The hybrid approach is applied specifically to the CVRP and does not require retraining across problem instances of different sizes. The proposed hybrid mechanism is able to improve the performance of baseline metaheuristic algorithms. Our approach not only enhances the solution quality for standard CVRP benchmarks but also proves scalability on very large-scale instances with up to 30,000 customer nodes. Experimental evaluations on benchmark datasets show that the proposed hybrid mechanism is capable of improving different baseline algorithms, achieving better quality of solutions under similar settings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aryabhata: An exam-focused language model for JEE Math</title>
<link>https://arxiv.org/abs/2508.08665</link>
<guid>https://arxiv.org/abs/2508.08665</guid>
<content:encoded><![CDATA[
<div> Keywords: Aryabhata 1.0, math reasoning model, Indian academic exam, Joint Entrance Examination, supervised fine-tuning, reinforcement learning

Summary: 
Aryabhata 1.0 is a compact math reasoning model designed specifically for the Indian academic exam, the Joint Entrance Examination (JEE). It merges strong open-weight reasoning models and undergoes supervised fine-tuning with curriculum learning on verified chain-of-thought traces. Additionally, reinforcement learning with verifiable rewards is applied using novel exploration strategies to enhance performance. The model outperforms existing ones in accuracy and efficiency on in-distribution and out-of-distribution benchmarks. Aryabhata offers pedagogically useful step-by-step reasoning and has been released as an open-source foundation model for the community to provide feedback. PhysicsWallah AI is actively working on training future models to further improve learning outcomes for students. 

<br /><br />Summary: <div>
arXiv:2508.08665v1 Announce Type: new 
Abstract: We present $\textbf{Aryabhata 1.0}$, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-$n$ rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as $\textit{Adaptive Group Resizing}$ and $\textit{Temperature Scaling}$. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback ($\href{https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0}{Aryabhata\ 1.0\ on\ Hugging\ Face}$); PW is actively training future models to further improve learning outcomes for students.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</title>
<link>https://arxiv.org/abs/2508.08688</link>
<guid>https://arxiv.org/abs/2508.08688</guid>
<content:encoded><![CDATA[
<div> Vision-language models, Topology-aware reasoning, Synthetic data pipeline, Frugal Learning, Generalization<br />
Summary:
STELAR-Vision introduces a training framework for vision-language models focusing on topology-aware reasoning. The core component is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Through supervised fine-tuning and reinforcement learning, Qwen2VL models are post-trained with a focus on accuracy and efficiency. Frugal Learning is proposed to reduce output length with minimal accuracy loss. Results show that STELAR-Vision improves accuracy by 9.7% over the base model and outperforms larger models by up to 7.3%. The approach demonstrates strong generalization on out-of-distribution benchmarks, surpassing existing models by significant margins. Compared to Chain-Only training, STELAR-Vision achieves higher overall accuracy on in-distribution datasets and consistently outperforms across all out-of-distribution benchmarks. Datasets have been released, and code will be made available to the public. <br /><br />Summary: <div>
arXiv:2508.08688v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks. We have released datasets, and code will be available.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Generative Social Agents via Theory-Informed Workflow Design</title>
<link>https://arxiv.org/abs/2508.08726</link>
<guid>https://arxiv.org/abs/2508.08726</guid>
<content:encoded><![CDATA[
<div> framework, large language models, social simulations, agents, theory-driven<br />
<br />
Summary: This article introduces a new framework for designing social agents based on large language models. The framework is grounded in Social Cognition Theory and consists of three key modules: motivation, action planning, and learning. By incorporating these modules, the agents can better reason about their goals, plan coherent actions, and adapt their behavior over time, resulting in more flexible and contextually appropriate responses. Comprehensive experiments show that the theory-driven agents exhibit realistic human behavior patterns under complex conditions, with up to 75% lower deviation from real-world data compared to traditional generative baselines. Ablation studies confirm the importance of the motivation, planning, and learning modules, as removing any of them leads to increased errors. Ultimately, this framework enhances the generalizability and coherence of social agents in agent-based simulations. <br /><br /> <div>
arXiv:2508.08726v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated strong reasoning and role-playing capabilities, opening new opportunities for agent-based social simulations. However, most existing agents' implementations are scenario-tailored, without a unified framework to guide the design. This lack of a general social agent limits their ability to generalize across different social contexts and to produce consistent, realistic behaviors. To address this challenge, we propose a theory-informed framework that provides a systematic design process for LLM-based social agents. Our framework is grounded in principles from Social Cognition Theory and introduces three key modules: motivation, action planning, and learning. These modules jointly enable agents to reason about their goals, plan coherent actions, and adapt their behavior over time, leading to more flexible and contextually appropriate responses. Comprehensive experiments demonstrate that our theory-driven agents reproduce realistic human behavior patterns under complex conditions, achieving up to 75% lower deviation from real-world behavioral data across multiple fidelity metrics compared to classical generative baselines. Ablation studies further show that removing motivation, planning, or learning modules increases errors by 1.5 to 3.2 times, confirming their distinct and essential contributions to generating realistic and coherent social behaviors.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</title>
<link>https://arxiv.org/abs/2508.08774</link>
<guid>https://arxiv.org/abs/2508.08774</guid>
<content:encoded><![CDATA[
<div> Perception Module, Memory Module, Spatiotemporal Reasoning Module, Actuator Module, Augmented Reality<br />
Summary:<br />
This article introduces a framework for memory-augmented AR agents to improve user experiences by integrating MLLMs. Current AR agents struggle with complex tasks that require understanding long-term user experiences. The proposed framework includes Perception, Memory, Spatiotemporal Reasoning, and Actuator modules. These modules aim to capture, retain, and reason over historical user interactions for personalized task assistance. An implementation roadmap, evaluation strategy, target applications, and use cases are provided to demonstrate the framework's practicality across diverse domains. This work aims to inspire further research in creating more intelligent AR systems that can effectively utilize user interaction history for adaptive, context-aware task support.<br /><br />Summary: <div>
arXiv:2508.08774v1 Announce Type: new 
Abstract: Augmented Reality (AR) systems are increasingly integrating foundation models, such as Multimodal Large Language Models (MLLMs), to provide more context-aware and adaptive user experiences. This integration has led to the development of AR agents to support intelligent, goal-directed interactions in real-world environments. While current AR agents effectively support immediate tasks, they struggle with complex multi-step scenarios that require understanding and leveraging user's long-term experiences and preferences. This limitation stems from their inability to capture, retain, and reason over historical user interactions in spatiotemporal contexts. To address these challenges, we propose a conceptual framework for memory-augmented AR agents that can provide personalized task assistance by learning from and adapting to user-specific experiences over time. Our framework consists of four interconnected modules: (1) Perception Module for multimodal sensor processing, (2) Memory Module for persistent spatiotemporal experience storage, (3) Spatiotemporal Reasoning Module for synthesizing past and present contexts, and (4) Actuator Module for effective AR communication. We further present an implementation roadmap, a future evaluation strategy, a potential target application and use cases to demonstrate the practical applicability of our framework across diverse domains. We aim for this work to motivate future research toward developing more intelligent AR systems that can effectively bridge user's interaction history with adaptive, context-aware task assistance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions</title>
<link>https://arxiv.org/abs/2508.08795</link>
<guid>https://arxiv.org/abs/2508.08795</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, knowledge editing, function-based taxonomy, knowledge types, evaluation tasks

Summary: 
This article discusses the importance of knowledge editing in large language models (LLMs) to update outdated or inaccurate information without the need for full retraining. A novel function-based taxonomy is introduced to classify different types of knowledge being edited, such as factual, temporal, conceptual, commonsense, and social knowledge. The effectiveness of editing methods relies on the nature of the target knowledge. The article reviews existing methods, highlights their strengths and limitations, formalizes the problem of knowledge editing, surveys evaluation tasks and datasets, and outlines open challenges and future directions. This comprehensive overview sheds light on the current landscape of knowledge editing in LLMs and provides insights for further research in this area. 

<br /><br />Summary: <div>
arXiv:2508.08795v1 Announce Type: new 
Abstract: Large language models (LLMs) acquire vast knowledge from large text corpora, but this information can become outdated or inaccurate. Since retraining is computationally expensive, knowledge editing offers an efficient alternative -- modifying internal knowledge without full retraining. These methods aim to update facts precisely while preserving the model's overall capabilities. While existing surveys focus on the mechanism of editing (e.g., parameter changes vs. external memory), they often overlook the function of the knowledge being edited. This survey introduces a novel, complementary function-based taxonomy to provide a more holistic view. We examine how different mechanisms apply to various knowledge types -- factual, temporal, conceptual, commonsense, and social -- highlighting how editing effectiveness depends on the nature of the target knowledge. By organizing our review along these two axes, we map the current landscape, outline the strengths and limitations of existing methods, define the problem formally, survey evaluation tasks and datasets, and conclude with open challenges and future directions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRainsaCK: a Comprehensive Software Library for Benchmarking Explanations of Link Prediction Tasks on Knowledge Graphs</title>
<link>https://arxiv.org/abs/2508.08815</link>
<guid>https://arxiv.org/abs/2508.08815</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graphs, Link Prediction, Explanation Methods, Evaluation Protocol, Benchmarking

Summary:
Knowledge Graphs often have missing information, and link prediction methods are used to predict these missing facts. While embedding based solutions are scalable, they lack comprehensibility, which is important in various domains. Explanation methods aim to address this issue by identifying supporting knowledge for predicted facts. However, quantitatively evaluating and comparing these explanations is challenging due to the lack of a standard evaluation protocol and benchmarking resource. To fill this gap, the authors propose GRainsaCK, a software resource that simplifies the benchmarking of explanations by streamlining tasks from model training to evaluation. GRainsaCK is designed for modularity and extensibility, allowing easy replacement of components. Extensive documentation, including a tutorial, is provided to encourage reuse of the resource. 

<br /><br />Summary: 
1. Knowledge Graphs often lack complete information, leading to the use of link prediction methods.
2. Embedding based solutions for link prediction may lack comprehensibility in various domains.
3. Explanation methods aim to identify supporting knowledge for predicted facts to enhance understanding.
4. Evaluating and comparing explanations quantitatively is challenging due to the lack of a standard evaluation protocol and benchmarking resource.
5. GRainsaCK is proposed as a software resource to streamline the benchmarking of explanations, promoting modularity and extensibility, with extensive documentation provided to facilitate reuse. <div>
arXiv:2508.08815v1 Announce Type: new 
Abstract: Since Knowledge Graphs are often incomplete, link prediction methods are adopted for predicting missing facts. Scalable embedding based solutions are mostly adopted for this purpose, however, they lack comprehensibility, which may be crucial in several domains. Explanation methods tackle this issue by identifying supporting knowledge explaining the predicted facts. Regretfully, evaluating/comparing quantitatively the resulting explanations is challenging as there is no standard evaluation protocol and overall benchmarking resource. We fill this important gap by proposing GRainsaCK, a reusable software resource that fully streamlines all the tasks involved in benchmarking explanations, i.e., from model training to evaluation of explanations along the same evaluation protocol. Moreover, GRainsaCK furthers modularity/extensibility by implementing the main components as functions that can be easily replaced. Finally, fostering its reuse, we provide extensive documentation including a tutorial.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2508.08816</link>
<guid>https://arxiv.org/abs/2508.08816</guid>
<content:encoded><![CDATA[
<div> planning, multimodal retrieval, dynamic orchestration, task execution, benchmarking

Summary:
E-Agent is a new agent framework designed to improve multimodal retrieval-augmented generation (mRAG) systems by dynamically orchestrating multimodal tools and optimizing workflows. It features a mRAG planner that uses contextual reasoning to efficiently retrieve information and a task executor that sequences tool execution for optimal performance. The framework follows a one-time planning strategy to minimize redundant tool invocations. The researchers also introduce the Real-World mRAG Planning (RemPlan) benchmark, which assesses mRAG planning capabilities through systematically annotated question types. E-Agent outperforms existing methods in experiments across RemPlan and other benchmarks, achieving a 13% accuracy gain while reducing redundant searches by 37%. This innovation addresses limitations in current mRAG systems, offering a more efficient and effective approach to multimodal information retrieval. 

<br /><br />Summary: <div>
arXiv:2508.08816v1 Announce Type: new 
Abstract: Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising solution to address the temporal limitations of Multimodal Large Language Models (MLLMs) in real-world scenarios like news analysis and trending topics. However, existing approaches often suffer from rigid retrieval strategies and under-utilization of visual information. To bridge this gap, we propose E-Agent, an agent framework featuring two key innovations: a mRAG planner trained to dynamically orchestrate multimodal tools based on contextual reasoning, and a task executor employing tool-aware execution sequencing to implement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning strategy that enables efficient information retrieval while minimizing redundant tool invocations. To rigorously assess the planning capabilities of mRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark. This novel benchmark contains both retrieval-dependent and retrieval-independent question types, systematically annotated with essential retrieval tools required for each instance. The benchmark's explicit mRAG planning annotations and diverse question design enhance its practical relevance by simulating real-world scenarios requiring dynamic mRAG decisions. Experiments across RemPlan and three established benchmarks demonstrate E-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods while reducing redundant searches by 37%.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition</title>
<link>https://arxiv.org/abs/2508.08830</link>
<guid>https://arxiv.org/abs/2508.08830</guid>
<content:encoded><![CDATA[
<div> emotion recognition, artificial intelligence, Reading the Mind in the Eyes Test, human-AI collaboration, collective intelligence

Summary:
Multimodal large language models (MLLMs) were evaluated for their emotion recognition abilities using the Reading the Mind in the Eyes Test (RMET) and its multiracial counterpart (MRMET), surpassing human performance. However, human groups collectively outperformed MLLMs, demonstrating the wisdom of the crowd. A collaborative approach combining human and MLLM predictions showed the highest accuracy, highlighting the potential of human-AI collaboration in emotional intelligence. This suggests that while MLLMs excel at individual emotion recognition, the collective intelligence of humans and synergistic potential of human-AI collaboration offer the most effective path for emotionally intelligent AI systems. Future research should focus on developing emotionally intelligent AI systems that leverage both human expertise and AI capabilities.<br /><br />Summary: <div>
arXiv:2508.08830v1 Announce Type: new 
Abstract: The ability to discern subtle emotional cues is fundamental to human social intelligence. As artificial intelligence (AI) becomes increasingly common, AI's ability to recognize and respond to human emotions is crucial for effective human-AI interactions. In particular, whether such systems can match or surpass human experts remains to be seen. However, the emotional intelligence of AI, particularly multimodal large language models (MLLMs), remains largely unexplored. This study evaluates the emotion recognition abilities of MLLMs using the Reading the Mind in the Eyes Test (RMET) and its multiracial counterpart (MRMET), and compares their performance against human participants. Results show that, on average, MLLMs outperform humans in accurately identifying emotions across both tests. This trend persists even when comparing performance across low, medium, and expert-level performing groups. Yet when we aggregate independent human decisions to simulate collective intelligence, human groups significantly surpass the performance of aggregated MLLM predictions, highlighting the wisdom of the crowd. Moreover, a collaborative approach (augmented intelligence) that combines human and MLLM predictions achieves greater accuracy than either humans or MLLMs alone. These results suggest that while MLLMs exhibit strong emotion recognition at the individual level, the collective intelligence of humans and the synergistic potential of human-AI collaboration offer the most promising path toward effective emotional AI. We discuss the implications of these findings for the development of emotionally intelligent AI systems and future research directions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation</title>
<link>https://arxiv.org/abs/2508.08882</link>
<guid>https://arxiv.org/abs/2508.08882</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematical reasoning, cognitive load interference, dual-agent hybrid framework, imitation learning, reinforcement learning

Summary:
This article discusses the limitations of current tool-integrated mathematical reasoning systems that adopt a single-agent paradigm for problem reasoning, code generation, and execution. The study compares a reasoning-only agent with a reasoning-plus-code agent and finds that the latter struggles to produce correct reasoning paths. To address this issue, the authors propose a dual-agent hybrid framework with a Reasoning Agent and a Code Agent. The training approach combines imitation learning and reinforcement learning, with the Code Agent receiving rewards for matching intermediate ground-truth programs and valid execution, while the Reasoning Agent focuses on final-answer accuracy. This decoupled role design aims to reduce cognitive interference and improve reasoning-coding coordination. The proposed framework seeks to provide a more efficient and effective approach to mathematical problem-solving. 

<br /><br />Summary: <div>
arXiv:2508.08882v1 Announce Type: new 
Abstract: Current tool-integrated mathematical reasoning systems often adopt a single-agent paradigm, where one large language model handles problem reasoning, code generation, and code execution in an integrated workflow. While this design eases coordination, we hypothesize that it imposes cognitive load interference, as the agent must interleave long-horizon reasoning with precise program synthesis. We validate this hypothesis through a controlled comparison between a reasoning-only agent and a reasoning-plus-code agent, finding that the latter produces significantly fewer correct reasoning paths despite having tool-calling capabilities. To address this, we propose a dual-agent hybrid framework: a Reasoning Agent performs stepwise problem decomposition, and a Code Agent handles code generation and execution. Training combines imitation learning and reinforcement learning: the Code Agent receives strong rewards for matching intermediate ground-truth programs and weaker rewards for valid execution, while the Reasoning Agent is optimized chiefly via final-answer accuracy using advantage estimation to credit intermediate steps. This decoupled role design reduces cognitive interference and promotes stable reasoning-coding coordination.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compass-Thinker-7B Technical Report</title>
<link>https://arxiv.org/abs/2508.08909</link>
<guid>https://arxiv.org/abs/2508.08909</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Compass-Thinker-7B, Mathematics Problems, AIME2024 Evaluation 

Summary: 
The research introduces the Compass-Thinker-7B model, designed to explore Reinforcement Learning capabilities with reduced computational resources. By training the model using a specially designed Reinforcement Learning Pipeline on a dataset of 30k mathematics problems, the model's potential is gradually unlocked through different difficulty distributions. Compass-Thinker-7B demonstrates exceptional reasoning abilities and outperforms other models in mathematics tasks, achieving 40% accuracy in the challenging AIME2024 evaluation. This approach highlights the effectiveness of leveraging Reinforcement Learning to enhance reasoning capabilities in large language models while mitigating computational costs and resource demands. <div>
arXiv:2508.08909v1 Announce Type: new 
Abstract: Recent R1-Zero-like research further demonstrates that reasoning extension has given large language models (LLMs) unprecedented reasoning capabilities, and Reinforcement Learning is the core tech- nology to elicit its complex reasoning. However, conducting RL experiments directly on hyperscale models involves high computational costs and resource demands, posing significant risks. We pro- pose the Compass-Thinker-7B model, which aims to explore the potential of Reinforcement Learn- ing with less computational resources and costs, and provides insights for further research into RL recipes for larger models. Compass-Thinker-7B is trained from an open source model through a spe- cially designed Reinforcement Learning Pipeline. we curate a dataset of 30k verifiable mathematics problems for the Reinforcement Learning Pipeline. By configuring data and training settings with dif- ferent difficulty distributions for different stages, the potential of the model is gradually released and the training efficiency is improved. Extensive evaluations show that Compass-Thinker-7B possesses exceptional reasoning potential, and achieves superior performance on mathematics compared to the same-sized RL model.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B achieves 40% accuracy.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.08926</link>
<guid>https://arxiv.org/abs/2508.08926</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Implicit Reasoning Safety, Multimodal inputs, Safe Semantics, Unsafe Interpretations


Summary:
Large Vision-Language Models (LVLMs) are facing safety challenges due to vulnerabilities in their implicit reasoning. This paper introduces the concept of Implicit Reasoning Safety, where benign combined inputs can trigger unsafe LVLM outputs. The authors developed the Safe Semantics, Unsafe Interpretations dataset to demonstrate this issue. Their findings show that even simple In-Context Learning with this dataset can significantly mitigate implicit multimodal threats. This highlights the critical need to improve cross-modal implicit reasoning in LVLMs to ensure their safety and reliability. <br /><br />Summary: <div>
arXiv:2508.08926v1 Announce Type: new 
Abstract: Large Vision-Language Models face growing safety challenges with multimodal inputs. This paper introduces the concept of Implicit Reasoning Safety, a vulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due to flawed or hidden reasoning. To showcase this, we developed Safe Semantics, Unsafe Interpretations, the first dataset for this critical issue. Our demonstrations show that even simple In-Context Learning with SSUI significantly mitigates these implicit multimodal threats, underscoring the urgent need to improve cross-modal implicit reasoning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2508.08992</link>
<guid>https://arxiv.org/abs/2508.08992</guid>
<content:encoded><![CDATA[
<div> Keywords: Prospect Theory, Large Language Models, epistemic markers, decision-making behavior, uncertainty

Summary:
Prospect Theory (PT) is used to model human decision-making under uncertainty, while epistemic markers express uncertainty in language. This study investigates whether PT applies to contemporary Large Language Models (LLMs) and how epistemic markers affect their decision-making behavior. An experiment based on economic questionnaires was designed to evaluate LLMs' decision-making behavior under PT. The study introduces uncertainty through empirical probability values associated with epistemic markers and examines their influence on LLM decision-making behavior. The findings suggest that modeling LLMs' decision-making with PT is not consistently reliable, especially when uncertainty is expressed in diverse linguistic forms. The research provides a more general and precise evaluation framework for studying LLM behavior under uncertainty. The code for the study is available on GitHub at https://github.com/HKUST-KnowComp/MarPT. 

<br /><br />Summary: Prospect Theory is used to model human decision-making under uncertainty, with this study exploring its applicability to Large Language Models (LLMs) and the impact of epistemic markers on their decision-making behavior. An experiment based on economic questionnaires evaluates LLMs' decision-making under PT, incorporating uncertainty through empirical probability values of epistemic markers. The findings reveal inconsistencies in modeling LLM behavior with PT, particularly when uncertainty is expressed in different linguistic forms. The study offers a precise evaluation framework for studying LLM decision-making under uncertainty, contributing to our understanding of how epistemic markers influence LLM behavior. <div>
arXiv:2508.08992v1 Announce Type: new 
Abstract: Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory</title>
<link>https://arxiv.org/abs/2508.08997</link>
<guid>https://arxiv.org/abs/2508.08997</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent systems, Large Language Models, Intrinsic Memory Agents, PDDL dataset, Structured planning tasks

Summary: 
In this paper, a novel framework called Intrinsic Memory Agents is introduced to address limitations faced by multi-agent systems built on Large Language Models (LLMs). These limitations include memory consistency, role adherence, and procedural integrity due to context window restrictions. The proposed approach maintains agent-specific memories that evolve with agent outputs, preserving specialized perspectives and focusing on task-relevant information. By benchmarking on the PDDL dataset, the study shows a significant improvement of 38.6% in performance compared to existing approaches, with high token efficiency. The framework is further evaluated on a complex data pipeline design task, demonstrating higher quality designs in terms of scalability, reliability, usability, cost-effectiveness, and documentation. The results indicate that structured, intrinsic memory approaches can enhance the capabilities of multi-agent LLM systems in structured planning tasks.

Summary: <div>
arXiv:2508.08997v1 Announce Type: new 
Abstract: Multi-agent systems built on Large Language Models (LLMs) show exceptional promise for complex collaborative problem-solving, yet they face fundamental challenges stemming from context window limitations that impair memory consistency, role adherence, and procedural integrity. This paper introduces Intrinsic Memory Agents, a novel framework that addresses these limitations through structured agent-specific memories that evolve intrinsically with agent outputs. Specifically, our method maintains role-aligned memory templates that preserve specialized perspectives while focusing on task-relevant information. We benchmark our approach on the PDDL dataset, comparing its performance to existing state-of-the-art multi-agentic memory approaches and showing an improvement of 38.6\% with the highest token efficiency. An additional evaluation is performed on a complex data pipeline design task, we demonstrate that our approach produces higher quality designs when comparing 5 metrics: scalability, reliability, usability, cost-effectiveness and documentation with additional qualitative evidence of the improvements. Our findings suggest that addressing memory limitations through structured, intrinsic approaches can improve the capabilities of multi-agent LLM systems on structured planning tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs</title>
<link>https://arxiv.org/abs/2508.09019</link>
<guid>https://arxiv.org/abs/2508.09019</guid>
<content:encoded><![CDATA[
<div> probes, bias detection, mechanistic interpretability, steering vectors, activation steering<br />
Summary:<br />
This study introduces an end-to-end system to address biases in large language models (LLMs). By using mechanistic interpretability techniques, the system can identify and mitigate biases within the model's internal workings. The first stage involves training linear probes on the model's internal activations to detect biases like gender, race, and age. These probes accurately detect biased content, particularly in the model's later layers. In the second stage, steering vectors are computed based on contrasting biased and neutral statements to steer the model away from producing biased content during inference. The activation steering technique successfully guides the model to generate more neutral outputs. Overall, this system provides a transparent and interpretable approach to enhancing the safety and accountability of LLMs.<br /> <div>
arXiv:2508.09019v1 Announce Type: new 
Abstract: As large language models (LLMs) become more integrated into societal systems, the risk of them perpetuating and amplifying harmful biases becomes a critical safety concern. Traditional methods for mitigating bias often rely on data filtering or post-hoc output moderation, which treat the model as an opaque black box. In this work, we introduce a complete, end-to-end system that uses techniques from mechanistic interpretability to both identify and actively mitigate bias directly within a model's internal workings. Our method involves two primary stages. First, we train linear "probes" on the internal activations of a model to detect the latent representations of various biases (e.g., gender, race, age). Our experiments on \texttt{gpt2-large} demonstrate that these probes can identify biased content with near-perfect accuracy, revealing that bias representations become most salient in the model's later layers. Second, we leverage these findings to compute "steering vectors" by contrasting the model's activation patterns for biased and neutral statements. By adding these vectors during inference, we can actively steer the model's generative process away from producing harmful, stereotypical, or biased content in real-time. We demonstrate the efficacy of this activation steering technique, showing that it successfully alters biased completions toward more neutral alternatives. We present our work as a robust and reproducible system that offers a more direct and interpretable approach to building safer and more accountable LLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A First Look at Predictability and Explainability of Pre-request Passenger Waiting Time in Ridesharing Systems</title>
<link>https://arxiv.org/abs/2508.09027</link>
<guid>https://arxiv.org/abs/2508.09027</guid>
<content:encoded><![CDATA[
<div> passenger waiting time prediction, ridesharing, pre-request prediction, explainability, FiXGBoost <br />
Summary: <br />
This study focuses on pre-request passenger waiting time prediction in ridesharing systems, aiming to enhance user experience and platform efficiency. The research explores the impact of demand & supply dynamics on waiting time and proposes FiXGBoost, a feature interaction-based XGBoost model for prediction without matched driver information. The model is designed to provide accurate predictions with high explainability. Experiments conducted on a large-scale real-world ridesharing dataset demonstrate the effectiveness of FiXGBoost in predicting waiting time, highlighting its potential to improve trip planning for passengers and enhance the overall experience for both passengers and drivers. <div>
arXiv:2508.09027v1 Announce Type: new 
Abstract: Passenger waiting time prediction plays a critical role in enhancing both ridesharing user experience and platform efficiency. While most existing research focuses on post-request waiting time prediction with knowing the matched driver information, pre-request waiting time prediction (i.e., before submitting a ride request and without matching a driver) is also important, as it enables passengers to plan their trips more effectively and enhance the experience of both passengers and drivers. However, it has not been fully studied by existing works. In this paper, we take the first step toward understanding the predictability and explainability of pre-request passenger waiting time in ridesharing systems. Particularly, we conduct an in-depth data-driven study to investigate the impact of demand&amp;supply dynamics on passenger waiting time. Based on this analysis and feature engineering, we propose FiXGBoost, a novel feature interaction-based XGBoost model designed to predict waiting time without knowing the assigned driver information. We further perform an importance analysis to quantify the contribution of each factor. Experiments on a large-scale real-world ridesharing dataset including over 30 million trip records show that our FiXGBoost can achieve a good performance for pre-request passenger waiting time prediction with high explainability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.09054</link>
<guid>https://arxiv.org/abs/2508.09054</guid>
<content:encoded><![CDATA[
<div> predictive maintenance, deep neural networks, failure detection, track circuits, railway systems
<br />
Predictive maintenance using deep neural networks is proposed to classify anomalies in track circuits, specifically Continuous Variable Current Modulation (CVCM) technology, before they escalate into failures. This approach aims to improve maintenance planning and minimize downtime and revenue loss in railway operations. Through the use of deep neural networks, the system achieves 99.31% overall accuracy in detecting anomalies within 1% of their onset, outperforming conventional techniques. The method is ISO-17359 compliant and provides uncertainty estimates through conformal prediction, reaching 99% confidence with consistent coverage across different classes of anomalies. With the global deployment of CVCMs, this approach is scalable and adaptable to enhance operational reliability in various track circuits and railway systems. 
<br /><br />Summary: <div>
arXiv:2508.09054v1 Announce Type: new 
Abstract: Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling</title>
<link>https://arxiv.org/abs/2508.09105</link>
<guid>https://arxiv.org/abs/2508.09105</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Multimodal Retrieval-Augmented Generation, Large Language Models, privacy leakage accountability, Source-aware Membership Audit<br />
Summary:<br /> 
The article introduces Source-aware Membership Audit (SMA) to attribute generated content in Large Language Models to pre-training, external retrieval, or user input. It addresses challenges in content provenance attribution by designing an attribution estimation mechanism based on zero-order optimization. SMA also includes a cross-modal attribution technique for token-level attribution in MRAG systems. This shift in focus from data memorization to content sourcing offers a novel approach to auditing data provenance in generative systems. <div>
arXiv:2508.09105v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented Generation (MRAG) significantly improve the knowledge coverage and contextual understanding of Large Language Models (LLMs) by introducing external knowledge sources. However, retrieval and multimodal fusion obscure content provenance, rendering existing membership inference methods unable to reliably attribute generated outputs to pre-training, external retrieval, or user input, thus undermining privacy leakage accountability
  To address these challenges, we propose the first Source-aware Membership Audit (SMA) that enables fine-grained source attribution of generated content in a semi-black-box setting with retrieval control capabilities.To address the environmental constraints of semi-black-box auditing, we further design an attribution estimation mechanism based on zero-order optimization, which robustly approximates the true influence of input tokens on the output through large-scale perturbation sampling and ridge regression modeling. In addition, SMA introduces a cross-modal attribution technique that projects image inputs into textual descriptions via MLLMs, enabling token-level attribution in the text modality, which for the first time facilitates membership inference on image retrieval traces in MRAG systems. This work shifts the focus of membership inference from 'whether the data has been memorized' to 'where the content is sourced from', offering a novel perspective for auditing data provenance in complex generative systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenCUA: Open Foundations for Computer-Use Agents</title>
<link>https://arxiv.org/abs/2508.09123</link>
<guid>https://arxiv.org/abs/2508.09123</guid>
<content:encoded><![CDATA[
<div> Vision-language models, computer-use agents, open-source framework, AgentNet dataset, Chain-of-Thought reasoning
Summary:
OpenCUA is an open-source framework designed to provide access to critical details of computer-use agents (CUAs) for study and research. It includes an annotation infrastructure, AgentNet dataset spanning 3 operating systems and 200+ applications, and a scalable pipeline for transforming demonstrations into state-action pairs. The end-to-end agent models in OpenCUA demonstrate strong performance and outperform existing open-source models like OpenAI CUA (GPT-4o). The OpenCUA-32B model achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art. The approach generalizes well across domains and benefits from increased test-time computation. The release of the annotation tool, datasets, code, and models aims to open foundations for further research in the field of computer-use agents. <br /><br />Summary: <div>
arXiv:2508.09123v1 Announce Type: new 
Abstract: Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair</title>
<link>https://arxiv.org/abs/2508.09129</link>
<guid>https://arxiv.org/abs/2508.09129</guid>
<content:encoded><![CDATA[
<div> Keywords: information seeking, large language model, strategic reasoning, search strategies, multi-step reasoning <br />
Summary: BrowseMaster is a framework designed to enhance information seeking in the digital landscape by combining expansive search with strategic reasoning. It consists of a planner and executor agent pair that work together to formulate search strategies based on task constraints and efficiently retrieve relevant evidence. This division of labor allows for coherent long-horizon reasoning while maintaining broad exploration. Through extensive experiments on challenging English and Chinese benchmarks, BrowseMaster outperformed existing baselines in complex information-seeking tasks, achieving high scores on BrowseComp-en and BrowseComp-zh. The framework's ability to balance breadth of search with depth of reasoning makes it a promising solution for navigating the vast and ever-growing digital landscape. <br /><br />Summary: <div>
arXiv:2508.09129v1 Announce Type: new 
Abstract: Effective information seeking in the vast and ever-growing digital landscape requires balancing expansive search with strategic reasoning. Current large language model (LLM)-based agents struggle to achieve this balance due to limitations in search breadth and reasoning depth, where slow, serial querying restricts coverage of relevant sources and noisy raw inputs disrupt the continuity of multi-step reasoning. To address these challenges, we propose BrowseMaster, a scalable framework built around a programmatically augmented planner-executor agent pair. The planner formulates and adapts search strategies based on task constraints, while the executor conducts efficient, targeted retrieval to supply the planner with concise, relevant evidence. This division of labor preserves coherent, long-horizon reasoning while sustaining broad and systematic exploration, overcoming the trade-off that limits existing agents. Extensive experiments on challenging English and Chinese benchmarks show that BrowseMaster consistently outperforms open-source and proprietary baselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh, which demonstrates its strong capability in complex, reasoning-heavy information-seeking tasks at scale.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Parallel Cooperative Landscape Smoothing Algorithm and Its Applications on TSP and UBQP</title>
<link>https://arxiv.org/abs/2401.03237</link>
<guid>https://arxiv.org/abs/2401.03237</guid>
<content:encoded><![CDATA[
<div> Keywords: combinatorial optimization problem, homotopic convex transformation, unconstrained binary quadratic programming, iterated local search, parallel cooperative LSILS

Summary:
This paper introduces a method to smooth the solution space of unconstrained binary quadratic programming problems using the homotopic convex (HC) transformation approach. The authors construct a unimodal toy UBQP and theoretically prove its unimodality. They demonstrate the effectiveness of this approach through empirical verification, introducing a landscape smoothing iterated local search (LSILS) algorithmic framework. Experimental analyses on various UBQP instances show the success of LSILS in enhancing solution quality. Furthermore, the paper proposes a parallel cooperative variant of LSILS (PC-LSILS) and applies it to both UBQP and the traveling salesman problem (TSP). Results indicate that PC-LSILS improves the performance of the HC transformation, leading to overall enhanced algorithm performance.<br /><br />Summary: <div>
arXiv:2401.03237v2 Announce Type: cross 
Abstract: Combinatorial optimization problem (COP) is difficult to solve because of the massive number of local optimal solutions in his solution space. Various methods have been put forward to smooth the solution space of COPs, including homotopic convex (HC) transformation for the traveling salesman problem (TSP). This paper extends the HC transformation approach to unconstrained binary quadratic programming (UBQP) by proposing a method to construct a unimodal toy UBQP of any size. We theoretically prove the unimodality of the constructed toy UBQP. After that, we apply this unimodal toy UBQP to smooth the original UBQP by using the HC transformation framework and empirically verify the smoothing effects. Subsequently, we introduce an iterative algorithmic framework incorporating HC transformation, referred as landscape smoothing iterated local search (LSILS). Our experimental analyses, conducted on various UBQP instances show the effectiveness of LSILS. Furthermore, this paper proposes a parallel cooperative variant of LSILS, denoted as PC-LSILS and apply it to both the UBQP and the TSP. Our experimental findings highlight that PC-LSILS improves the smoothing performance of the HC transformation, and further improves the overall performance of the algorithm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effects of Smoothing Rugged Landscape by Different Toy Problems: A Case Study on UBQP</title>
<link>https://arxiv.org/abs/2407.19676</link>
<guid>https://arxiv.org/abs/2407.19676</guid>
<content:encoded><![CDATA[
<div> Keywords: Unconstrained Binary Quadratic Program, Landscape Smoothing Iterated Local Search, rugged landscape, toy UBQP, flatness

Summary:
The Unconstrained Binary Quadratic Program (UBQP) problem is challenging due to its rugged landscape. The Landscape Smoothing Iterated Local Search (LSILS) algorithm aims to smooth this rugged landscape by combining the original UBQP with a toy UBQP. This study explores the impact of using different toy UBQPs to smooth rugged landscapes. Three toy UBQPs were examined, with results showing that the toy UBQP with matrix ^Q2 (constructed by "+/-i") provided the most effective smoothing, while the randomly constructed ^Q3 had the least favorable outcome. The landscape flatness varied among the toy UBQPs, with the ^Q1 (constructed by "+/-1") being the flattest. These findings enhance understanding of landscape smoothing techniques for optimizing UBQP.<br /><br />Summary: <div>
arXiv:2407.19676v1 Announce Type: cross 
Abstract: The hardness of the Unconstrained Binary Quadratic Program (UBQP) problem is due its rugged landscape. Various algorithms have been proposed for UBQP, including the Landscape Smoothing Iterated Local Search (LSILS). Different from other UBQP algorithms, LSILS tries to smooth the rugged landscape by building a convex combination of the original UBQP and a toy UBQP. In this paper, our study further investigates the impact of smoothing rugged landscapes using different toy UBQP problems, including a toy UBQP with matrix ^Q1 (construct by "+/-1"), a toy UBQP with matrix ^Q2 (construct by "+/-i") and a toy UBQP with matrix ^Q3 (construct randomly). We first assess the landscape flatness of the three toy UBQPs. Subsequently, we test the efficiency of LSILS with different toy UBQPs. Results reveal that the toy UBQP with ^Q1 (construct by "+/-1") exhibits the flattest landscape among the three, while the toy UBQP with ^Q3 (construct randomly) presents the most non-flat landscape. Notably, LSILS using the toy UBQP with ^Q2 (construct by "+/-i") emerges as the most effective, while ^Q3 (construct randomly) has the poorest result. These findings contribute to a detailed understanding of landscape smoothing techniques in optimizing UBQP.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection</title>
<link>https://arxiv.org/abs/2508.08265</link>
<guid>https://arxiv.org/abs/2508.08265</guid>
<content:encoded><![CDATA[
<div> detecting scientific claims, mentioning scientific studies, council debate, large language models, CheckThat! 2025

Summary:
The paper presents a novel council debate method for identifying scientific claims, references to scientific studies, and mentions of scientific entities in tweets. Three debating methods were explored: single debate, team debate, and council debate, with the council debate model outperforming the others. While the proposed method did not rank highly in identifying scientific claims or mentions of scientific entities, it excelled in detecting references to scientific studies. The council debate involved multiple expert models deliberating together to reach a consensus, moderated by a chairperson model. This approach was developed for the scientific web discourse detection task of CheckThat! 2025, demonstrating the effectiveness of structured academic discussions among large language models in identifying scientific content in online discourse. <div>
arXiv:2508.08265v1 Announce Type: cross 
Abstract: In this paper, we present our work developed for the scientific web discourse detection task (Task 4a) of CheckThat! 2025. We propose a novel council debate method that simulates structured academic discussions among multiple large language models (LLMs) to identify whether a given tweet contains (i) a scientific claim, (ii) a reference to a scientific study, or (iii) mentions of scientific entities. We explore three debating methods: i) single debate, where two LLMs argue for opposing positions while a third acts as a judge; ii) team debate, in which multiple models collaborate within each side of the debate; and iii) council debate, where multiple expert models deliberate together to reach a consensus, moderated by a chairperson model. We choose council debate as our primary model as it outperforms others in the development test set. Although our proposed method did not rank highly for identifying scientific claims (8th out of 10) or mentions of scientific entities (9th out of 10), it ranked first in detecting references to scientific studies.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants</title>
<link>https://arxiv.org/abs/2508.08266</link>
<guid>https://arxiv.org/abs/2508.08266</guid>
<content:encoded><![CDATA[
<div> Virginia, land patents, large language models, georeferencing, historical<br />
Summary:<br />
- Evaluation of large language models (LLMs) in converting narrative land patent descriptions to geographically accurate coordinates in Virginia<br />
- Testing six OpenAI models on a corpus of 5,471 patent abstracts from 1695-1732<br />
- Best-performing LLM achieving a mean error of 23 km, outperforming external baselines and improving with an ensemble approach<br />
- Ablation study showing reliance on textual descriptions for georeferencing<br />
- Cost-efficient model maintaining a 28 km mean error at a low cost per grant<br />
- Demonstrating the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing<br /> 
Summary: <div>
arXiv:2508.08266v1 Announce Type: cross 
Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily as narrative metes-and-bounds descriptions, limiting spatial analysis. This study systematically evaluates current-generation large language models (LLMs) in converting these prose abstracts into geographically accurate latitude/longitude coordinates within a focused evaluation context. A digitized corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43 rigorously verified test cases serving as an initial, geographically focused benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class, and GPT-3.5) were tested under two paradigms: direct-to-coordinate and tool-augmented chain-of-thought invoking external geocoding APIs. Results were compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3, and a county-centroid heuristic.
  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km (median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70% (Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12 km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the median LLM by 48.6%. A patentee-name-redaction ablation increased error by about 9%, indicating reliance on textual landmark and adjacency descriptions rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong cost-accuracy benchmark; external geocoding tools offered no measurable benefit in this evaluation.
  These findings demonstrate the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands</title>
<link>https://arxiv.org/abs/2508.08269</link>
<guid>https://arxiv.org/abs/2508.08269</guid>
<content:encoded><![CDATA[
<div> Keywords: tendon-driven robotic hands, surface electromyography (sEMG) sensors, EMG-to-Tendon Control dataset, regression models, dexterous robotic manipulation

Summary: 
Tendon-driven robotic hands offer advanced dexterity for manipulation tasks, but learning control policies can be challenging due to the lack of direct mapping between motion capture data and tendon controls. Visual tracking methods for tendon-driven systems are often unreliable, leading to the exploration of surface electromyography (sEMG) sensors as an alternative for capturing hand motion. This study introduces a large-scale EMG-to-Tendon Control dataset, extending existing datasets and addressing limitations in predicting tendon control from sEMG recordings. Three baseline regression models showcase the utility of the dataset, and a novel diffusion-based regression model is proposed for accurate prediction of tendon controls from sEMG signals. This dataset and modeling framework are crucial advancements in achieving scalable and precise tendon control in robotic hands. <br /><br />Summary: <div>
arXiv:2508.08269v1 Announce Type: cross 
Abstract: Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose data sets and regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic hands, extending the emg2pose dataset, which includes recordings from 193 subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset incorporates tendon control signals derived using the MyoSuite MyoHand model, addressing limitations such as invalid poses in prior methods. We provide three baseline regression models to demonstrate emg2tendon utility and propose a novel diffusion-based regression model for predicting tendon control from sEMG recordings. This dataset and modeling framework marks a significant step forward for tendon-driven dexterous robotic manipulation, laying the groundwork for scalable and accurate tendon control in robotic hands. https://emg2tendon.github.io/
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI</title>
<link>https://arxiv.org/abs/2508.08270</link>
<guid>https://arxiv.org/abs/2508.08270</guid>
<content:encoded><![CDATA[
<div> medical multimodal models, pathology analysis, radiology report generation, biomedical assistance, text-image relationship

Summary:
Doctor Sun is a new large multimodal generative model specifically designed for the medical field. It aims to better understand complex medical concepts by integrating text and image data modalities. The model is trained on various medical datasets through a two-stage process focusing on feature alignment and instruction tuning. Doctor Sun combines a pre-trained vision encoder with a medical language model to effectively capture the relationship between text and images in medical data. The researchers also provide a bilingual medical multimodal dataset, SunMed-VL, along with all the models, code, and resources to foster further research in biomedical multimodal AI. <div>
arXiv:2508.08270v1 Announce Type: cross 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition</title>
<link>https://arxiv.org/abs/2508.08274</link>
<guid>https://arxiv.org/abs/2508.08274</guid>
<content:encoded><![CDATA[
<div> adjectives, hate speech, counter speech, SCBM, language model <br />
<br />
Summary: 
The article presents a novel transparent method called Speech Concept Bottleneck Model (SCBM) for automated hate and counter speech recognition. SCBM utilizes adjectives as bottleneck concepts to create an interpretable representation of input texts using large language models (LLMs). This representation is then fed into a lightweight classifier for identifying hate and counter speech. The SCBM approach achieves an average macro-F1 score of 0.69 across multiple datasets, outperforming previous results on four out of five datasets. The method offers both local and global interpretability and can be extended to other NLP tasks. By combining adjective-based concept representations with transformer embeddings, a performance improvement of 1.8% on average is observed. The results demonstrate the effectiveness of using adjective-based concept representations as compact and interpretable encodings for hate and counter speech recognition. <div>
arXiv:2508.08274v1 Announce Type: cross 
Abstract: The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., "Speech Concept Bottleneck Model" (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis</title>
<link>https://arxiv.org/abs/2508.08275</link>
<guid>https://arxiv.org/abs/2508.08275</guid>
<content:encoded><![CDATA[
<div> Evaluation Benchmark, Multimodal Large Language Models, Continual Learning Algorithms, Reasoning Quality Assessment, Task Order
<br />
Summary:
Models like Multimodal Large Language Models (MLLMs) need continual tuning for real-world applications, but a lack of benchmarks hinders progress. MLLM-CTBench addresses this by offering a multidimensional evaluation that combines final answer accuracy with CoT reasoning quality assessment. It also benchmarks eight continual learning algorithms and compares reinforcement learning with supervised fine-tuning. Tasks are carefully curated from 16 datasets across six domains. Key findings include models with greater capabilities being more robust to forgetting and reasoning chains degrading slower than final answers. The effectiveness of continual learning algorithms depends on model capability and task order, with KL-divergence constraints helping in reinforcement learning settings. MLLM-CTBench sets a standard for MLLM tuning and offers insight into algorithm design and evaluation. 
<br /><br />Summary: <div>
arXiv:2508.08275v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) rely on continual instruction tuning to adapt to the evolving demands of real-world applications. However, progress in this area is hindered by the lack of rigorous and systematic benchmarks. To address this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark with three key contributions: (1) Multidimensional Evaluation: We combine final answer accuracy with fine-grained CoT reasoning quality assessment, enabled by a specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms and Training Paradigms: We benchmark eight continual learning algorithms across four major categories and systematically compare reinforcement learning with supervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and organize 16 datasets from existing work, covering six challenging domains. Our key findings include: (i) Models with stronger general capabilities exhibit greater robustness to forgetting during continual learning; (ii) Reasoning chains degrade more slowly than final answers, supporting the hierarchical forgetting hypothesis; (iii) The effectiveness of continual learning algorithms is highly dependent on both model capability and task order; (iv) In reinforcement learning settings, incorporating KL-divergence constraints helps maintain policy stability and plays a crucial role in mitigating forgetting. MLLM-CTBench establishes a rigorous standard for continual instruction tuning of MLLMs and offers practical guidance for algorithm design and evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Contrast Localizer for Identifying Causal Unitsin Social &amp; Mathematical Tasks in Language Models</title>
<link>https://arxiv.org/abs/2508.08276</link>
<guid>https://arxiv.org/abs/2508.08276</guid>
<content:encoded><![CDATA[
<div> localizer, Theory of Mind, mathematical reasoning, large language models, vision-language models
Summary:
Contrast localizer techniques were adapted in this study to identify causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs. Top-activated units were localized across various models and their causal role was evaluated through targeted ablations. Surprisingly, low-activation units sometimes had a greater impact on performance compared to highly activated ones. Furthermore, units identified through mathematical localizers sometimes impaired ToM performance more than those from ToM localizers. The study suggests the need for broader stimulus sets to more accurately capture task-specific units and questions the causal relevance of contrast-based localizers in these models. <div>
arXiv:2508.08276v1 Announce Type: cross 
Abstract: This work adapts a neuroscientific contrast localizer to pinpoint causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated units using contrastive stimulus sets and assess their causal role via targeted ablations. We compare the effect of lesioning functionally selected units against low-activation and randomly selected units on downstream accuracy across established ToM and mathematical benchmarks. Contrary to expectations, low-activation units sometimes produced larger performance drops than the highly activated ones, and units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer. These findings call into question the causal relevance of contrast-based localizers and highlight the need for broader stimulus sets and more accurately capture task-specific units.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Heterogeneity-Aware and Energy-Efficient Topology Optimization for Decentralized Federated Learning in Edge Environment</title>
<link>https://arxiv.org/abs/2508.08278</link>
<guid>https://arxiv.org/abs/2508.08278</guid>
<content:encoded><![CDATA[
arXiv:2508.08278v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a promising paradigm within edge computing (EC) systems, enabling numerous edge devices to collaboratively train artificial intelligence (AI) models while maintaining data privacy. To overcome the communication bottlenecks associated with centralized parameter servers, decentralized federated learning (DFL), which leverages peer-to-peer (P2P) communication, has been extensively explored in the research community. Although researchers design a variety of DFL approach to ensure model convergence, its iterative learning process inevitably incurs considerable cost along with the growth of model complexity and the number of participants. These costs are largely influenced by the dynamic changes of topology in each training round, particularly its sparsity and connectivity conditions. Furthermore, the inherent resources heterogeneity in the edge environments affects energy efficiency of learning process, while data heterogeneity degrades model performance. These factors pose significant challenges to the design of an effective DFL framework for EC systems. To this end, we propose Hat-DFed, a heterogeneity-aware and coset-effective decentralized federated learning (DFL) framework. In Hat-DFed, the topology construction is formulated as a dual optimization problem, which is then proven to be NP-hard, with the goal of maximizing model performance while minimizing cumulative energy consumption in complex edge environments. To solve this problem, we design a two-phase algorithm that dynamically constructs optimal communication topologies while unbiasedly estimating their impact on both model performance and energy cost. Additionally, the algorithm incorporates an importance-aware model aggregation mechanism to mitigate performance degradation caused by data heterogeneity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XFMNet: Decoding Cross-Site and Nonstationary Water Patterns via Stepwise Multimodal Fusion for Long-Term Water Quality Forecasting</title>
<link>https://arxiv.org/abs/2508.08279</link>
<guid>https://arxiv.org/abs/2508.08279</guid>
<content:encoded><![CDATA[
arXiv:2508.08279v1 Announce Type: cross 
Abstract: Long-term time-series forecasting is critical for environmental monitoring, yet water quality prediction remains challenging due to complex periodicity, nonstationarity, and abrupt fluctuations induced by ecological factors. These challenges are further amplified in multi-site scenarios that require simultaneous modeling of temporal and spatial dynamics. To tackle this, we introduce XFMNet, a stepwise multimodal fusion network that integrates remote sensing precipitation imagery to provide spatial and environmental context in river networks. XFMNet first aligns temporal resolutions between water quality series and remote sensing inputs via adaptive downsampling, followed by locally adaptive decomposition to disentangle trend and cycle components. A cross-attention gated fusion module dynamically integrates temporal patterns with spatial and ecological cues, enhancing robustness to nonstationarity and site-specific anomalies. Through progressive and recursive fusion, XFMNet captures both long-term trends and short-term fluctuations. Extensive experiments on real-world datasets demonstrate substantial improvements over state-of-the-art baselines, highlighting the effectiveness of XFMNet for spatially distributed time series prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder</title>
<link>https://arxiv.org/abs/2508.08280</link>
<guid>https://arxiv.org/abs/2508.08280</guid>
<content:encoded><![CDATA[
arXiv:2508.08280v1 Announce Type: cross 
Abstract: Deep learning has emerged as the most promising approach in various fields; however, when the distributions of training and test data are different (domain shift), the performance of deep learning models can degrade. Semi-supervised domain adaptation (SSDA) is a major approach for addressing this issue, assuming that a fully labeled training set (source domain) is available, but the test set (target domain) provides labels only for a small subset. In this study, we propose a novel two-step momentum encoder-utilized SSDA framework, MoSSDA, for multivariate time-series classification. Time series data are highly sensitive to noise, and sequential dependencies cause domain shifts resulting in critical performance degradation. To obtain a robust, domain-invariant and class-discriminative representation, MoSSDA employs a domain-invariant encoder to learn features from both source and target domains. Subsequently, the learned features are fed to a mixup-enhanced positive contrastive module consisting of an online momentum encoder. The final classifier is trained with learned features that exhibit consistency and discriminability with limited labeled target domain data, without data augmentation. We applied a two-stage process by separating the gradient flow between the encoders and the classifier to obtain rich and complex representations. Through extensive experiments on six diverse datasets, MoSSDA achieved state-of-the-art performance for three different backbones and various unlabeled ratios in the target domain data. The Ablation study confirms that each module, including two-stage learning, is effective in improving the performance. Our code is available at https://github.com/seonyoungKimm/MoSSDA
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction</title>
<link>https://arxiv.org/abs/2508.08281</link>
<guid>https://arxiv.org/abs/2508.08281</guid>
<content:encoded><![CDATA[
arXiv:2508.08281v1 Announce Type: cross 
Abstract: Knowledge discovered from telecom data can facilitate proactive understanding of network dynamics and user behaviors, which in turn empowers service providers to optimize cellular traffic scheduling and resource allocation. Nevertheless, the telecom industry still heavily relies on manual expert intervention. Existing studies have been focused on exhaustively explore the spatial-temporal correlations. However, they often overlook the underlying characteristics of cellular traffic, which are shaped by the sporadic and bursty nature of telecom services. Additionally, concept drift creates substantial obstacles to maintaining satisfactory accuracy in continuous cellular forecasting tasks. To resolve these problems, we put forward an online cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal feature Complementarity (MGSTC). The proposed method is devised to achieve high-precision predictions in practical continuous forecasting scenarios. Concretely, MGSTC segments historical data into chunks and employs the coarse-grained temporal attention to offer a trend reference for the prediction horizon. Subsequently, fine-grained spatial attention is utilized to capture detailed correlations among network elements, which enables localized refinement of the established trend. The complementarity of these multi-grained spatial-temporal features facilitates the efficient transmission of valuable information. To accommodate continuous forecasting needs, we implement an online learning strategy that can detect concept drift in real-time and promptly switch to the appropriate parameter update stage. Experiments carried out on four real-world datasets demonstrate that MGSTC outperforms eleven state-of-the-art baselines consistently.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language</title>
<link>https://arxiv.org/abs/2508.08283</link>
<guid>https://arxiv.org/abs/2508.08283</guid>
<content:encoded><![CDATA[
arXiv:2508.08283v1 Announce Type: cross 
Abstract: This paper presents MinionsLLM, a novel framework that integrates Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable natural language control of multi-agent systems within arbitrary, user-defined environments. MinionsLLM provides standardized interfaces for defining environments, agents, and behavioral primitives, and introduces two synthetic dataset generation methods (Method A and Method B) to fine-tune LLMs for improved syntactic validity and semantic task relevance. We validate our approach using Google's Gemma 3 model family at three parameter scales (1B, 4B, and 12B) and demonstrate substantial gains: Method B increases syntactic validity to 92.6% and achieves a mean task performance improvement of 33% over baseline. Notably, our experiments show that smaller models benefit most from fine-tuning, suggesting promising directions for deploying compact, locally hosted LLMs in resource-constrained multi-agent control scenarios. The framework and all resources are released open-source to support reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2508.08285</link>
<guid>https://arxiv.org/abs/2508.08285</guid>
<content:encoded><![CDATA[
arXiv:2508.08285v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their tendency to hallucinate poses serious challenges for reliable deployment. Despite numerous hallucination detection methods, their evaluations often rely on ROUGE, a metric based on lexical overlap that misaligns with human judgments. Through comprehensive human studies, we demonstrate that while ROUGE exhibits high recall, its extremely low precision leads to misleading performance estimates. In fact, several established detection methods show performance drops of up to 45.9\% when assessed using human-aligned metrics like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based on response length can rival complex detection techniques, exposing a fundamental flaw in current evaluation practices. We argue that adopting semantically aware and robust evaluation frameworks is essential to accurately gauge the true performance of hallucination detection methods, ultimately ensuring the trustworthiness of LLM outputs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions</title>
<link>https://arxiv.org/abs/2508.08287</link>
<guid>https://arxiv.org/abs/2508.08287</guid>
<content:encoded><![CDATA[
arXiv:2508.08287v1 Announce Type: cross 
Abstract: Despite the increasing usage of Large Language Models (LLMs) in answering questions in a variety of domains, their reliability and accuracy remain unexamined for a plethora of domains including the religious domains. In this paper, we introduce a novel benchmark FiqhQA focused on the LLM generated Islamic rulings explicitly categorized by the four major Sunni schools of thought, in both Arabic and English. Unlike prior work, which either overlooks the distinctions between religious school of thought or fails to evaluate abstention behavior, we assess LLMs not only on their accuracy but also on their ability to recognize when not to answer. Our zero-shot and abstention experiments reveal significant variation across LLMs, languages, and legal schools of thought. While GPT-4o outperforms all other models in accuracy, Gemini and Fanar demonstrate superior abstention behavior critical for minimizing confident incorrect answers. Notably, all models exhibit a performance drop in Arabic, highlighting the limitations in religious reasoning for languages other than English. To the best of our knowledge, this is the first study to benchmark the efficacy of LLMs for fine-grained Islamic school of thought specific ruling generation and to evaluate abstention for Islamic jurisprudence queries. Our findings underscore the need for task-specific evaluation and cautious deployment of LLMs in religious applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Transformers through the Lens of Pavlovian Conditioning</title>
<link>https://arxiv.org/abs/2508.08289</link>
<guid>https://arxiv.org/abs/2508.08289</guid>
<content:encoded><![CDATA[
arXiv:2508.08289v1 Announce Type: cross 
Abstract: Transformer architectures have revolutionized artificial intelligence (AI) through their attention mechanisms, yet the computational principles underlying their success remain opaque. We present a novel theoretical framework that reinterprets the core computation of attention as Pavlovian conditioning. Our model finds a direct mathematical analogue in linear attention, which simplifies the analysis of the underlying associative process. We demonstrate that attention's queries, keys, and values can be mapped to the three elements of classical conditioning: test stimuli that probe associations, conditional stimuli (CS) that serve as retrieval cues, and unconditional stimuli (US) that contain response information. Through this lens, we suggest that each attention operation constructs a transient associative memory via a Hebbian rule, where CS-US pairs form dynamic associations that test stimuli can later retrieve. Our framework yields several theoretical insights grounded in this linearized model: (1) a capacity theorem showing that attention heads can store O($\sqrt{d_k}$) associations before interference degrades retrieval; (2) an error propagation analysis revealing fundamental architectural trade-offs of balancing model depth, width, and head redundancy to maintain reliability; and (3) an understanding of how biologically plausible learning rules could enhance transformer architectures. By establishing this deep connection, we suggest that the success of modern AI may stem not from architectural novelty alone, but from implementing computational principles that biology optimized over millions of years of evolution.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Putnam-AXIOM: A Functional and Static Benchmark</title>
<link>https://arxiv.org/abs/2508.08292</link>
<guid>https://arxiv.org/abs/2508.08292</guid>
<content:encoded><![CDATA[
arXiv:2508.08292v1 Announce Type: cross 
Abstract: Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel-Wise MLPs Improve the Generalization of Recurrent Convolutional Networks</title>
<link>https://arxiv.org/abs/2508.08298</link>
<guid>https://arxiv.org/abs/2508.08298</guid>
<content:encoded><![CDATA[
arXiv:2508.08298v1 Announce Type: cross 
Abstract: We investigate the impact of channel-wise mixing via multi-layer perceptrons (MLPs) on the generalization capabilities of recurrent convolutional networks. Specifically, we compare two architectures: DARC (Depth Aware Recurrent Convolution), which employs a simple recurrent convolutional structure, and DAMP (Depth Aware Multi-layer Perceptron), which extends DARC with a gated MLP for channel mixing. Using the Re-ARC benchmark, we find that DAMP significantly outperforms DARC in both in-distribution and out-of-distribution generalization under exact-match grading criteria. These results suggest that explicit channel mixing through MLPs enables recurrent convolutional networks to learn more robust and generalizable computational patterns. Our findings have implications for neural program synthesis and highlight the potential of DAMP as a target architecture for hypernetwork approaches.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained PSLQ Search for Machin-like Identities Achieving Record-Low Lehmer Measures</title>
<link>https://arxiv.org/abs/2508.08307</link>
<guid>https://arxiv.org/abs/2508.08307</guid>
<content:encoded><![CDATA[
arXiv:2508.08307v1 Announce Type: cross 
Abstract: Machin-like arctangent relations are classical tools for computing $\pi$, with efficiency quantified by the Lehmer measure ($\lambda$). We present a framework for discovering low-measure relations by coupling the PSLQ integer-relation algorithm with number-theoretic filters derived from the algebraic structure of Gaussian integers, making large scale search tractable. Our search yields new 5 and 6 term relations with record-low Lehmer measures ($\lambda=1.4572, \lambda=1.3291$). We also demonstrate how discovered relations can serve as a basis for generating new, longer formulae through algorithmic extensions. This combined approach of a constrained PSLQ search and algorithmic extension provides a robust method for future explorations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Quality of AI-Generated Exams: A Large-Scale Field Study</title>
<link>https://arxiv.org/abs/2508.08314</link>
<guid>https://arxiv.org/abs/2508.08314</guid>
<content:encoded><![CDATA[
arXiv:2508.08314v1 Announce Type: cross 
Abstract: While large language models (LLMs) challenge conventional methods of teaching and learning, they present an exciting opportunity to improve efficiency and scale high-quality instruction. One promising application is the generation of customized exams, tailored to specific course content. There has been significant recent excitement on automatically generating questions using artificial intelligence, but also comparatively little work evaluating the psychometric quality of these items in real-world educational settings. Filling this gap is an important step toward understanding generative AI's role in effective test design. In this study, we introduce and evaluate an iterative refinement strategy for question generation, repeatedly producing, assessing, and improving questions through cycles of LLM-generated critique and revision. We evaluate the quality of these AI-generated questions in a large-scale field study involving 91 classes -- covering computer science, mathematics, chemistry, and more -- in dozens of colleges across the United States, comprising nearly 1700 students. Our analysis, based on item response theory (IRT), suggests that for students in our sample the AI-generated questions performed comparably to expert-created questions designed for standardized exams. Our results illustrate the power of AI to make high-quality assessments more readily available, benefiting both teachers and students.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EU Digital Regulation and Guatemala: AI, 5G, and Cybersecurity</title>
<link>https://arxiv.org/abs/2508.08315</link>
<guid>https://arxiv.org/abs/2508.08315</guid>
<content:encoded><![CDATA[
arXiv:2508.08315v1 Announce Type: cross 
Abstract: The paper examines how EU rules in AI, 5G, and cybersecurity operate as transnational governance and shape policy in Guatemala. It outlines the AI Act's risk approach, the 5G Action Plan and Security Toolbox, and the cybersecurity regime built on ENISA, NIS2, the Cybersecurity Act, and the Cyber Resilience Act. It traces extraterritorial channels such as the Brussels effect, private standards, supply chain clauses, and data transfer controls. Guatemala specific impacts include SME compliance costs, procurement limits, environmental trade-offs in rollout, rights risks, and capacity gaps. The paper maps current national measures and proposes five guardrails: digital constitutionalism, green IT duties, third country impact assessment, standards co-design, and recognition of regulatory diversity.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of State-of-the-Art Deep Learning Techniques for Plant Disease and Pest Detection</title>
<link>https://arxiv.org/abs/2508.08317</link>
<guid>https://arxiv.org/abs/2508.08317</guid>
<content:encoded><![CDATA[
arXiv:2508.08317v1 Announce Type: cross 
Abstract: Addressing plant diseases and pests is critical for enhancing crop production and preventing economic losses. Recent advances in artificial intelligence (AI), machine learning (ML), and deep learning (DL) have significantly improved the precision and efficiency of detection methods, surpassing the limitations of manual identification. This study reviews modern computer-based techniques for detecting plant diseases and pests from images, including recent AI developments. The methodologies are organized into five categories: hyperspectral imaging, non-visualization techniques, visualization approaches, modified deep learning architectures, and transformer models. This structured taxonomy provides researchers with detailed, actionable insights for selecting advanced state-of-the-art detection methods. A comprehensive survey of recent work and comparative studies demonstrates the consistent superiority of modern AI-based approaches, which often outperform older image analysis methods in speed and accuracy. In particular, vision transformers such as the Hierarchical Vision Transformer (HvT) have shown accuracy exceeding 99.3% in plant disease detection, outperforming architectures like MobileNetV3. The study concludes by discussing system design challenges, proposing solutions, and outlining promising directions for future research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Fear and Desire, the Monster Artificial Intelligence (AI): Analysis through the Lenses of Monster Theory</title>
<link>https://arxiv.org/abs/2508.08318</link>
<guid>https://arxiv.org/abs/2508.08318</guid>
<content:encoded><![CDATA[
arXiv:2508.08318v1 Announce Type: cross 
Abstract: With the increasing adoption of Artificial Intelligence (AI) in all fields and daily activities, a heated debate is found about the advantages and challenges of AI and the need for navigating the concerns associated with AI to make the best of it. To contribute to this literature and the ongoing debate related to it, this study draws on the Monster theory to explain the conflicting representation of AI. It suggests that studying monsters in popular culture can provide an in-depth understanding of AI and its monstrous effects. Specifically, this study aims to discuss AI perception and development through the seven theses of Monster theory. The obtained results revealed that, just like monsters, AI is complex in nature, and it should not be studied as a separate entity but rather within a given society or culture. Similarly, readers may perceive and interpret AI differently, just as readers may interpret monsters differently. The relationship between AI and monsters, as depicted in this study, does not seem to be as odd as it might be at first.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code</title>
<link>https://arxiv.org/abs/2508.08322</link>
<guid>https://arxiv.org/abs/2508.08322</guid>
<content:encoded><![CDATA[
arXiv:2508.08322v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in automating code generation and software engineering tasks, yet they often struggle with complex, multi-file projects due to context limitations and knowledge gaps. We propose a novel context engineering workflow that combines multiple AI components: an Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered semantic literature retrieval for injecting domain knowledge, NotebookLM-based document synthesis for contextual understanding, and a Claude Code multi-agent system for code generation and validation. Our integrated approach leverages intent clarification, retrieval-augmented generation, and specialized sub-agents orchestrated via Claude's agent framework. We demonstrate that this method significantly improves the accuracy and reliability of code assistants in real-world repositories, yielding higher single-shot success rates and better adherence to project context than baseline single-agent approaches. Qualitative results on a large Next.js codebase show the multi-agent system effectively plans, edits, and tests complex features with minimal human intervention. We compare our system with recent frameworks like CodePlan, MASAI, and HyperAgent, highlighting how targeted context injection and agent role decomposition lead to state-of-the-art performance. Finally, we discuss the implications for deploying LLM-based coding assistants in production, along with lessons learned on context management and future research directions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
arXiv:2508.08325v1 Announce Type: cross 
Abstract: Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming</title>
<link>https://arxiv.org/abs/2508.08332</link>
<guid>https://arxiv.org/abs/2508.08332</guid>
<content:encoded><![CDATA[
arXiv:2508.08332v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are widely used for code generation. However, commercial models like ChatGPT require significant computing power, which leads to high energy use and carbon emissions. This has raised concerns about their environmental impact. In this study, we evaluate open-source Small Language Models (SLMs) trained explicitly for code generation and compare their performance and energy efficiency against large LLMs and efficient human-written Python code. The goal is to investigate whether SLMs can match the performance of LLMs on certain types of programming problems while producing more energy-efficient code. We evaluate 150 coding problems from LeetCode, evenly distributed across three difficulty levels: easy, medium, and hard. Our comparison includes three small open-source models, StableCode-3B, StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using four key metrics: run-time, memory usage, energy consumption, and correctness. We use human-written solutions as a baseline to assess the quality and efficiency of the model-generated code. Results indicate that LLMs achieve the highest correctness across all difficulty levels, but SLMs are often more energy-efficient when their outputs are correct. In over 52% of the evaluated problems, SLMs consumed the same or less energy than LLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normative Moral Pluralism for AI: A Framework for Deliberation in Complex Moral Contexts</title>
<link>https://arxiv.org/abs/2508.08333</link>
<guid>https://arxiv.org/abs/2508.08333</guid>
<content:encoded><![CDATA[
arXiv:2508.08333v1 Announce Type: cross 
Abstract: The conceptual framework proposed in this paper centers on the development of a deliberative moral reasoning system - one designed to process complex moral situations by generating, filtering, and weighing normative arguments drawn from diverse ethical perspectives. While the framework is rooted in Machine Ethics, it also makes a substantive contribution to Value Alignment by outlining a system architecture that links structured moral reasoning to action under time constraints. Grounded in normative moral pluralism, this system is not constructed to imitate behavior but is built on reason-sensitive deliberation over structured moral content in a transparent and principled manner. Beyond its role as a deliberative system, it also serves as the conceptual foundation for a novel two-level architecture: functioning as a moral reasoning teacher envisioned to train faster models that support real-time responsiveness without reproducing the full structure of deliberative reasoning. Together, the deliberative and intuitive components are designed to enable both deep reflection and responsive action. A key design feature is the dual-hybrid structure: a universal layer that defines a moral threshold through top-down and bottom-up learning, and a local layer that learns to weigh competing considerations in context while integrating culturally specific normative content, so long as it remains within the universal threshold. By extending the notion of moral complexity to include not only conflicting beliefs but also multifactorial dilemmas, multiple stakeholders, and the integration of non-moral considerations, the framework aims to support morally grounded decision-making in realistic, high-stakes contexts.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling</title>
<link>https://arxiv.org/abs/2508.08334</link>
<guid>https://arxiv.org/abs/2508.08334</guid>
<content:encoded><![CDATA[
arXiv:2508.08334v1 Announce Type: cross 
Abstract: Molecular representation learning, a cornerstone for downstream tasks like molecular captioning and molecular property prediction, heavily relies on Graph Neural Networks (GNN). However, GNN suffers from the over-smoothing problem, where node-level features collapse in deep GNN layers. While existing feature projection methods with cross-attention have been introduced to mitigate this issue, they still perform poorly in deep features. This motivated our exploration of using Mamba as an alternative projector for its ability to handle complex sequences. However, we observe that while Mamba excels at preserving global topological information from deep layers, it neglects fine-grained details in shallow layers. The capabilities of Mamba and cross-attention exhibit a global-local trade-off. To resolve this critical global-local trade-off, we propose Hierarchical and Structure-Aware Network (HSA-Net), a novel framework with two modules that enables a hierarchical feature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP) module is introduced to process features from different graph layers. It learns to dynamically switch between a cross-attention projector for shallow layers and a structure-aware Graph-Mamba projector for deep layers, producing high-quality, multi-level features. Secondly, to adaptively merge these multi-level features, we design a Source-Aware Fusion (SAF) module, which flexibly selects fusion experts based on the characteristics of the aggregation features, ensuring a precise and effective final representation fusion. Extensive experiments demonstrate that our HSA-Net framework quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Fairness amid Social Determinants: Reflection, Characterization, and Approach</title>
<link>https://arxiv.org/abs/2508.08337</link>
<guid>https://arxiv.org/abs/2508.08337</guid>
<content:encoded><![CDATA[
arXiv:2508.08337v1 Announce Type: cross 
Abstract: Social determinants are variables that, while not directly pertaining to any specific individual, capture key aspects of contexts and environments that have direct causal influences on certain attributes of an individual. Previous algorithmic fairness literature has primarily focused on sensitive attributes, often overlooking the role of social determinants. Our paper addresses this gap by introducing formal and quantitative rigor into a space that has been shaped largely by qualitative proposals regarding the use of social determinants. To demonstrate theoretical perspectives and practical applicability, we examine a concrete setting of college admissions, using region as a proxy for social determinants. Our approach leverages a region-based analysis with Gamma distribution parameterization to model how social determinants impact individual outcomes. Despite its simplicity, our method quantitatively recovers findings that resonate with nuanced insights in previous qualitative debates, that are often missed by existing algorithmic fairness approaches. Our findings suggest that mitigation strategies centering solely around sensitive attributes may introduce new structural injustice when addressing existing discrimination. Considering both sensitive attributes and social determinants facilitates a more comprehensive explication of benefits and burdens experienced by individuals from diverse demographic backgrounds as well as contextual environments, which is essential for understanding and achieving fairness effectively and transparently.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction</title>
<link>https://arxiv.org/abs/2508.08338</link>
<guid>https://arxiv.org/abs/2508.08338</guid>
<content:encoded><![CDATA[
arXiv:2508.08338v1 Announce Type: cross 
Abstract: To mitigate the potential adverse health effects of simultaneous multi-drug use, including unexpected side effects and interactions, accurately identifying and predicting drug-drug interactions (DDIs) is considered a crucial task in the field of deep learning. Although existing methods have demonstrated promising performance, they suffer from the bottleneck of limited functional motif-based representation learning, as DDIs are fundamentally caused by motif interactions rather than the overall drug structures. In this paper, we propose an Image-enhanced molecular motif sequence representation framework for \textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs from both global and local structures. Specifically, ImageDDI tokenizes molecules into functional motifs. To effectively represent a drug pair, their motifs are combined into a single sequence and embedded using a transformer-based encoder, starting from the local structure representation. By leveraging the associations between drug pairs, ImageDDI further enhances the spatial representation of molecules using global molecular image information (e.g. texture, shadow, color, and planar spatial relationships). To integrate molecular visual information into functional motif sequence, ImageDDI employs Adaptive Feature Fusion, enhancing the generalization of ImageDDI by dynamically adapting the fusion process of feature representations. Experimental results on widely used datasets demonstrate that ImageDDI outperforms state-of-the-art methods. Moreover, extensive experiments show that ImageDDI achieved competitive performance in both 2D and 3D image-enhanced scenarios compared to other models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving</title>
<link>https://arxiv.org/abs/2508.08343</link>
<guid>https://arxiv.org/abs/2508.08343</guid>
<content:encoded><![CDATA[
arXiv:2508.08343v1 Announce Type: cross 
Abstract: Serving LLM adapters has gained significant attention as an effective approach to adapt general-purpose language models to diverse, task-specific use cases. However, serving a wide range of adapters introduces several and substantial overheads, leading to performance degradation and challenges in optimal placement. To address these challenges, we present an analytical, AI-driven pipeline that accurately determines the optimal allocation of adapters in single-node setups. This allocation maximizes performance, effectively using GPU resources, while preventing request starvation. Crucially, the proposed allocation is given based on current workload patterns. These insights in single-node setups can be leveraged in multi-replica deployments for overall placement, load balancing and server configuration, ultimately enhancing overall performance and improving resource efficiency. Our approach builds on an in-depth analysis of LLM adapter serving, accounting for overheads and performance variability, and includes the development of the first Digital Twin capable of replicating online LLM-adapter serving systems with matching key performance metrics. The experimental results demonstrate that the Digital Twin achieves a SMAPE difference of no more than 5.5% in throughput compared to real results, and the proposed pipeline accurately predicts the optimal placement with minimal latency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do AI Companies Make Good on Voluntary Commitments to the White House?</title>
<link>https://arxiv.org/abs/2508.08345</link>
<guid>https://arxiv.org/abs/2508.08345</guid>
<content:encoded><![CDATA[
arXiv:2508.08345v1 Announce Type: cross 
Abstract: Voluntary commitments are central to international AI governance, as demonstrated by recent voluntary guidelines from the White House to the G7, from Bletchley Park to Seoul. How do major AI companies make good on their commitments? We score companies based on their publicly disclosed behavior by developing a detailed rubric based on their eight voluntary commitments to the White House in 2023. We find significant heterogeneity: while the highest-scoring company (OpenAI) scores a 83% overall on our rubric, the average score across all companies is just 52%. The companies demonstrate systemically poor performance for their commitment to model weight security with an average score of 17%: 11 of the 16 companies receive 0% for this commitment. Our analysis highlights a clear structural shortcoming that future AI governance initiatives should correct: when companies make public commitments, they should proactively disclose how they meet their commitments to provide accountability, and these disclosures should be verifiable. To advance policymaking on corporate AI governance, we provide three directed recommendations that address underspecified commitments, the role of complex AI supply chains, and public transparency that could be applied towards AI governance initiatives worldwide.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy-Pattern Tsetlin Machine</title>
<link>https://arxiv.org/abs/2508.08350</link>
<guid>https://arxiv.org/abs/2508.08350</guid>
<content:encoded><![CDATA[
arXiv:2508.08350v1 Announce Type: cross 
Abstract: The "all-or-nothing" clause evaluation strategy is a core mechanism in the Tsetlin Machine (TM) family of algorithms. In this approach, each clause - a logical pattern composed of binary literals mapped to input data - is disqualified from voting if even a single literal fails. Due to this strict requirement, standard TMs must employ thousands of clauses to achieve competitive accuracy. This paper introduces the Fuzzy-Pattern Tsetlin Machine (FPTM), a novel variant where clause evaluation is fuzzy rather than strict. If some literals in a clause fail, the remaining ones can still contribute to the overall vote with a proportionally reduced score. As a result, each clause effectively consists of sub-patterns that adapt individually to the input, enabling more flexible, efficient, and robust pattern matching. The proposed fuzzy mechanism significantly reduces the required number of clauses, memory footprint, and training time, while simultaneously improving accuracy. On the IMDb dataset, FPTM achieves 90.15% accuracy with only one clause per class, a 50x reduction in clauses and memory over the Coalesced Tsetlin Machine. FPTM trains up to 316x faster (45 seconds vs. 4 hours) and fits within 50 KB, enabling online learning on microcontrollers. Inference throughput reaches 34.5 million predictions/second (51.4 GB/s). On Fashion-MNIST, accuracy reaches 92.18% (2 clauses), 93.19% (20 clauses) and 94.68% (8000 clauses), a ~400x clause reduction compared to the Composite TM's 93.00% (8000 clauses). On the Amazon Sales dataset with 20% noise, FPTM achieves 85.22% accuracy, significantly outperforming the Graph Tsetlin Machine (78.17%) and a Graph Convolutional Neural Network (66.23%).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Processing of synthetic data in AI development for healthcare and the definition of personal data in EU law</title>
<link>https://arxiv.org/abs/2508.08353</link>
<guid>https://arxiv.org/abs/2508.08353</guid>
<content:encoded><![CDATA[
arXiv:2508.08353v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has the potential to transform healthcare, but it requires access to health data. Synthetic data that is generated through machine learning models trained on real data, offers a way to share data while preserving privacy. However, uncertainties in the practical application of the General Data Protection Regulation (GDPR) create an administrative burden, limiting the benefits of synthetic data. Through a systematic analysis of relevant legal sources and an empirical study, this article explores whether synthetic data should be classified as personal data under the GDPR. The study investigates the residual identification risk through generating synthetic data and simulating inference attacks, challenging common perceptions of technical identification risk. The findings suggest synthetic data is likely anonymous, depending on certain factors, but highlights uncertainties about what constitutes reasonably likely risk. To promote innovation, the study calls for clearer regulations to balance privacy protection with the advancement of AI in healthcare.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The DNA of nuclear models: How AI predicts nuclear masses</title>
<link>https://arxiv.org/abs/2508.08370</link>
<guid>https://arxiv.org/abs/2508.08370</guid>
<content:encoded><![CDATA[
arXiv:2508.08370v1 Announce Type: cross 
Abstract: Obtaining high-precision predictions of nuclear masses, or equivalently nuclear binding energies, $E_b$, remains an important goal in nuclear-physics research. Recently, many AI-based tools have shown promising results on this task, some achieving precision that surpasses the best physics models. However, the utility of these AI models remains in question given that predictions are only useful where measurements do not exist, which inherently requires extrapolation away from the training (and testing) samples. Since AI models are largely black boxes, the reliability of such an extrapolation is difficult to assess. We present an AI model that not only achieves cutting-edge precision for $E_b$, but does so in an interpretable manner. For example, we find (and explain why) that the most important dimensions of its internal representation form a double helix, where the analog of the hydrogen bonds in DNA here link the number of protons and neutrons found in the most stable nucleus of each isotopic chain. Furthermore, we show that the AI prediction of $E_b$ can be factorized and ordered hierarchically, with the most important terms corresponding to well-known symbolic models (such as the famous liquid drop). Remarkably, the improvement of the AI model over symbolic ones can almost entirely be attributed to an observation made by Jaffe in 1969. The end result is a fully interpretable data-driven model of nuclear masses.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors</title>
<link>https://arxiv.org/abs/2508.08384</link>
<guid>https://arxiv.org/abs/2508.08384</guid>
<content:encoded><![CDATA[
arXiv:2508.08384v1 Announce Type: cross 
Abstract: Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Query-Relevant Document Summaries via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.08404</link>
<guid>https://arxiv.org/abs/2508.08404</guid>
<content:encoded><![CDATA[
arXiv:2508.08404v1 Announce Type: cross 
Abstract: E-commerce search engines often rely solely on product titles as input for ranking models with latency constraints. However, this approach can result in suboptimal relevance predictions, as product titles often lack sufficient detail to capture query intent. While product descriptions provide richer information, their verbosity and length make them unsuitable for real-time ranking, particularly for computationally expensive architectures like cross-encoder ranking models. To address this challenge, we propose ReLSum, a novel reinforcement learning framework designed to generate concise, query-relevant summaries of product descriptions optimized for search relevance. ReLSum leverages relevance scores as rewards to align the objectives of summarization and ranking, effectively overcoming limitations of prior methods, such as misaligned learning targets. The framework employs a trainable large language model (LLM) to produce summaries, which are then used as input for a cross-encoder ranking model. Experimental results demonstrate significant improvements in offline metrics, including recall and NDCG, as well as online user engagement metrics. ReLSum provides a scalable and efficient solution for enhancing search relevance in large-scale e-commerce systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Tangent Knowledge Distillation for Optical Convolutional Networks</title>
<link>https://arxiv.org/abs/2508.08421</link>
<guid>https://arxiv.org/abs/2508.08421</guid>
<content:encoded><![CDATA[
arXiv:2508.08421v1 Announce Type: cross 
Abstract: Hybrid Optical Neural Networks (ONNs, typically consisting of an optical frontend and a digital backend) offer an energy-efficient alternative to fully digital deep networks for real-time, power-constrained systems. However, their adoption is limited by two main challenges: the accuracy gap compared to large-scale networks during training, and discrepancies between simulated and fabricated systems that further degrade accuracy. While previous work has proposed end-to-end optimizations for specific datasets (e.g., MNIST) and optical systems, these approaches typically lack generalization across tasks and hardware designs. To address these limitations, we propose a task-agnostic and hardware-agnostic pipeline that supports image classification and segmentation across diverse optical systems. To assist optical system design before training, we estimate achievable model accuracy based on user-specified constraints such as physical size and the dataset. For training, we introduce Neural Tangent Knowledge Distillation (NTKD), which aligns optical models with electronic teacher networks, thereby narrowing the accuracy gap. After fabrication, NTKD also guides fine-tuning of the digital backend to compensate for implementation errors. Experiments on multiple datasets (e.g., MNIST, CIFAR, Carvana Masking) and hardware configurations show that our pipeline consistently improves ONN performance and enables practical deployment in both pre-fabrication simulations and physical implementations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment</title>
<link>https://arxiv.org/abs/2508.08424</link>
<guid>https://arxiv.org/abs/2508.08424</guid>
<content:encoded><![CDATA[
arXiv:2508.08424v1 Announce Type: cross 
Abstract: Prior work on language modeling showed conflicting findings about whether morphologically aligned approaches to tokenization improve performance, particularly for languages with complex morphology. To investigate this, we select a typologically diverse set of languages: Telugu (agglutinative), Hindi (primarily fusional with some agglutination), and English (fusional). We conduct a comprehensive evaluation of language models -- starting from tokenizer training and extending through the finetuning and downstream task evaluation. To account for the consistent performance differences observed across tokenizer variants, we focus on two key factors: morphological alignment and tokenization quality. To assess morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.
  Our experiments reveal that better morphological alignment correlates positively -- though moderately -- with performance in syntax-based tasks such as Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing. However, we also find that the tokenizer algorithm (Byte-pair Encoding vs. Unigram) plays a more significant role in influencing downstream performance than morphological alignment alone. Naive Unigram tokenizers outperform others across most settings, though hybrid tokenizers that incorporate morphological segmentation significantly improve performance within the BPE framework. In contrast, intrinsic metrics like Corpus Token Count (CTC) and R\'enyi entropy showed no correlation with downstream performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
<link>https://arxiv.org/abs/2508.08435</link>
<guid>https://arxiv.org/abs/2508.08435</guid>
<content:encoded><![CDATA[
arXiv:2508.08435v1 Announce Type: cross 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal User Profiling with LLMs: Balancing Short-Term and Long-Term Preferences for Recommendations</title>
<link>https://arxiv.org/abs/2508.08454</link>
<guid>https://arxiv.org/abs/2508.08454</guid>
<content:encoded><![CDATA[
arXiv:2508.08454v1 Announce Type: cross 
Abstract: Accurately modeling user preferences is crucial for improving the performance of content-based recommender systems. Existing approaches often rely on simplistic user profiling methods, such as averaging or concatenating item embeddings, which fail to capture the nuanced nature of user preference dynamics, particularly the interactions between long-term and short-term preferences. In this work, we propose LLM-driven Temporal User Profiling (LLM-TUP), a novel method for user profiling that explicitly models short-term and long-term preferences by leveraging interaction timestamps and generating natural language representations of user histories using a large language model (LLM). These representations are encoded into high-dimensional embeddings using a pre-trained BERT model, and an attention mechanism is applied to dynamically fuse the short-term and long-term embeddings into a comprehensive user profile. Experimental results on real-world datasets demonstrate that LLM-TUP achieves substantial improvements over several baselines, underscoring the effectiveness of our temporally aware user-profiling approach and the use of semantically rich user profiles, generated by LLMs, for personalized content-based recommendation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Children to Create AI-Enabled Augmented Reality Experiences</title>
<link>https://arxiv.org/abs/2508.08467</link>
<guid>https://arxiv.org/abs/2508.08467</guid>
<content:encoded><![CDATA[
arXiv:2508.08467v1 Announce Type: cross 
Abstract: Despite their potential to enhance children's learning experiences, AI-enabled AR technologies are predominantly used in ways that position children as consumers rather than creators. We introduce Capybara, an AR-based and AI-powered visual programming environment that empowers children to create, customize, and program 3D characters overlaid onto the physical world. Capybara enables children to create virtual characters and accessories using text-to-3D generative AI models, and to animate these characters through auto-rigging and body tracking. In addition, our system employs vision-based AI models to recognize physical objects, allowing children to program interactive behaviors between virtual characters and their physical surroundings. We demonstrate the expressiveness of Capybara through a set of novel AR experiences. We conducted user studies with 20 children in the United States and Argentina. Our findings suggest that Capybara can empower children to harness AI in authoring personalized and engaging AR experiences that seamlessly bridge the virtual and physical worlds.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling</title>
<link>https://arxiv.org/abs/2508.08487</link>
<guid>https://arxiv.org/abs/2508.08487</guid>
<content:encoded><![CDATA[
arXiv:2508.08487v1 Announce Type: cross 
Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum Point-Perplexity Mechanics in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08492</link>
<guid>https://arxiv.org/abs/2508.08492</guid>
<content:encoded><![CDATA[
arXiv:2508.08492v1 Announce Type: cross 
Abstract: We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this "energy" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this "log-Lagrangian" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise: Real-World Lessons from LLM Co-Design in a Safety-Net Hospital</title>
<link>https://arxiv.org/abs/2508.08504</link>
<guid>https://arxiv.org/abs/2508.08504</guid>
<content:encoded><![CDATA[
arXiv:2508.08504v1 Announce Type: cross 
Abstract: Large language models (LLMs) have the potential to address social and behavioral determinants of health by transforming labor intensive workflows in resource-constrained settings. Creating LLM-based applications that serve the needs of underserved communities requires a deep understanding of their local context, but it is often the case that neither LLMs nor their developers possess this local expertise, and the experts in these communities often face severe time/resource constraints. This creates a disconnect: how can one engage in meaningful co-design of an LLM-based application for an under-resourced community when the communication channel between the LLM developer and domain expert is constrained? We explored this question through a real-world case study, in which our data science team sought to partner with social workers at a safety net hospital to build an LLM application that summarizes patients' social needs. Whereas prior works focus on the challenge of prompt tuning, we found that the most critical challenge in this setting is the careful and precise specification of \what information to surface to providers so that the LLM application is accurate, comprehensive, and verifiable. Here we present a novel co-design framework for settings with limited access to domain experts, in which the summary generation task is first decomposed into individually-optimizable attributes and then each attribute is efficiently refined and validated through a multi-tier cascading approach.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression</title>
<link>https://arxiv.org/abs/2508.08509</link>
<guid>https://arxiv.org/abs/2508.08509</guid>
<content:encoded><![CDATA[
arXiv:2508.08509v1 Announce Type: cross 
Abstract: Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs to Capture Users' Temporal Context for Recommendation</title>
<link>https://arxiv.org/abs/2508.08512</link>
<guid>https://arxiv.org/abs/2508.08512</guid>
<content:encoded><![CDATA[
arXiv:2508.08512v1 Announce Type: cross 
Abstract: Effective recommender systems demand dynamic user understanding, especially in complex, evolving environments. Traditional user profiling often fails to capture the nuanced, temporal contextual factors of user preferences, such as transient short-term interests and enduring long-term tastes. This paper presents an assessment of Large Language Models (LLMs) for generating semantically rich, time-aware user profiles. We do not propose a novel end-to-end recommendation architecture; instead, the core contribution is a systematic investigation into the degree of LLM effectiveness in capturing the dynamics of user context by disentangling short-term and long-term preferences. This approach, framing temporal preferences as dynamic user contexts for recommendations, adaptively fuses these distinct contextual components into comprehensive user embeddings. The evaluation across Movies&amp;TV and Video Games domains suggests that while LLM-generated profiles offer semantic depth and temporal structure, their effectiveness for context-aware recommendations is notably contingent on the richness of user interaction histories. Significant gains are observed in dense domains (e.g., Movies&amp;TV), whereas improvements are less pronounced in sparse environments (e.g., Video Games). This work highlights LLMs' nuanced potential in enhancing user profiling for adaptive, context-aware recommendations, emphasizing the critical role of dataset characteristics for practical applicability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.08521</link>
<guid>https://arxiv.org/abs/2508.08521</guid>
<content:encoded><![CDATA[
arXiv:2508.08521v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) are increasingly being used in a broad range of applications, bringing their security and behavioral control to the forefront. While existing approaches for behavioral control or output redirection, like system prompting in VLMs, are easily detectable and often ineffective, activation-based steering vectors require invasive runtime access to model internals--incompatible with API-based services and closed-source deployments. We introduce VISOR (Visual Input-based Steering for Output Redirection), a novel method that achieves sophisticated behavioral control through optimized visual inputs alone. By crafting universal steering images that induce target activation patterns, VISOR enables practical deployment across all VLM serving modalities while remaining imperceptible compared to explicit textual instructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment tasks: refusal, sycophancy and survival instinct. A single 150KB steering image matches steering vector performance within 1-2% for positive behavioral shifts while dramatically exceeding it for negative steering--achieving up to 25% shifts from baseline compared to steering vectors' modest changes. Unlike system prompting (3-4% shifts), VISOR provides robust bidirectional control while maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond eliminating runtime overhead and model access requirements, VISOR exposes a critical security vulnerability: adversaries can achieve sophisticated behavioral manipulation through visual channels alone, bypassing text-based defenses. Our work fundamentally re-imagines multimodal model control and highlights the urgent need for defenses against visual steering attacks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI</title>
<link>https://arxiv.org/abs/2508.08524</link>
<guid>https://arxiv.org/abs/2508.08524</guid>
<content:encoded><![CDATA[
arXiv:2508.08524v1 Announce Type: cross 
Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing Atari Space Invaders with Sparse Cosine Optimized Policy Evolution</title>
<link>https://arxiv.org/abs/2508.08526</link>
<guid>https://arxiv.org/abs/2508.08526</guid>
<content:encoded><![CDATA[
arXiv:2508.08526v1 Announce Type: cross 
Abstract: Evolutionary approaches have previously been shown to be effective learning methods for a diverse set of domains. However, the domain of game-playing poses a particular challenge for evolutionary methods due to the inherently large state space of video games. As the size of the input state expands, the size of the policy must also increase in order to effectively learn the temporal patterns in the game space. Consequently, a larger policy must contain more trainable parameters, exponentially increasing the size of the search space. Any increase in search space is highly problematic for evolutionary methods, as increasing the number of trainable parameters is inversely correlated with convergence speed. To reduce the size of the input space while maintaining a meaningful representation of the original space, we introduce Sparse Cosine Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine Transform (DCT) as a pseudo attention mechanism, transforming an input state into a coefficient matrix. By truncating and applying sparsification to this matrix, we reduce the dimensionality of the input space while retaining the highest energy features of the original input. We demonstrate the effectiveness of SCOPE as the policy for the Atari game Space Invaders. In this task, SCOPE with CMA-ES outperforms evolutionary methods that consider an unmodified input state, such as OpenAI-ES and HyperNEAT. SCOPE also outperforms simple reinforcement learning methods, such as DQN and A3C. SCOPE achieves this result through reducing the input size by 53% from 33,600 to 15,625 then using a bilinear affine mapping of sparse DCT coefficients to policy actions learned by the CMA-ES algorithm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework</title>
<link>https://arxiv.org/abs/2508.08535</link>
<guid>https://arxiv.org/abs/2508.08535</guid>
<content:encoded><![CDATA[
arXiv:2508.08535v1 Announce Type: cross 
Abstract: Wireless Body Area Networks (WBANs) enable continuous monitoring of physiological signals for applications ranging from chronic disease management to emergency response. Recent advances in 6G communications, post-quantum cryptography, and energy harvesting have the potential to enhance WBAN performance. However, integrating these technologies into a unified, adaptive system remains a challenge. This paper surveys some of the most well-known Wireless Body Area Network (WBAN) architectures, routing strategies, and security mechanisms, identifying key gaps in adaptability, energy efficiency, and quantum-resistant security. We propose a novel Large Language Model-driven adaptive WBAN framework in which a Large Language Model acts as a cognitive control plane, coordinating routing, physical layer selection, micro-energy harvesting, and post-quantum security in real time. Our review highlights the limitations of current heuristic-based designs and outlines a research agenda for resource-constrained, 6G-ready medical systems. This approach aims to enable ultra-reliable, secure, and self-optimizing WBANs for next-generation mobile health applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
<link>https://arxiv.org/abs/2508.08543</link>
<guid>https://arxiv.org/abs/2508.08543</guid>
<content:encoded><![CDATA[
arXiv:2508.08543v1 Announce Type: cross 
Abstract: Achieving accurate traffic prediction is a fundamental but crucial task in the development of current intelligent transportation systems.Most of the mainstream methods that have made breakthroughs in traffic prediction rely on spatio-temporal graph neural networks, spatio-temporal attention mechanisms, etc. The main challenges of the existing deep learning approaches are that they either depend on a complete traffic network structure or require intricate model designs to capture complex spatio-temporal dependencies. These limitations pose significant challenges for the efficient deployment and operation of deep learning models on large-scale datasets. To address these challenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP) based model M3-Net for traffic prediction. Our proposed model not only employs time series and spatio-temporal embeddings for efficient feature processing but also first introduces a novel MLP-Mixer architecture with a mixture of experts (MoE) mechanism. Extensive experiments conducted on multiple real datasets demonstrate the superiority of the proposed model in terms of prediction performance and lightweight deployment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents and the Law</title>
<link>https://arxiv.org/abs/2508.08544</link>
<guid>https://arxiv.org/abs/2508.08544</guid>
<content:encoded><![CDATA[
arXiv:2508.08544v1 Announce Type: cross 
Abstract: As AI becomes more "agentic," it faces technical and socio-legal issues it must address if it is to fulfill its promise of increased economic productivity and efficiency. This paper uses technical and legal perspectives to explain how things change when AI systems start being able to directly execute tasks on behalf of a user. We show how technical conceptions of agents track some, but not all, socio-legal conceptions of agency. That is, both computer science and the law recognize the problems of under-specification for an agent, and both disciplines have robust conceptions of how to address ensuring an agent does what the programmer, or in the law, the principal desires and no more. However, to date, computer science has under-theorized issues related to questions of loyalty and to third parties that interact with an agent, both of which are central parts of the law of agency. First, we examine the correlations between implied authority in agency law and the principle of value-alignment in AI, wherein AI systems must operate under imperfect objective specification. Second, we reveal gaps in the current computer science view of agents pertaining to the legal concepts of disclosure and loyalty, and how failure to account for them can result in unintended effects in AI ecommerce agents. In surfacing these gaps, we show a path forward for responsible AI agent development and deployment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval</title>
<link>https://arxiv.org/abs/2508.08545</link>
<guid>https://arxiv.org/abs/2508.08545</guid>
<content:encoded><![CDATA[
arXiv:2508.08545v1 Announce Type: cross 
Abstract: Developers insert logging statements in source code to capture relevant runtime information essential for maintenance and debugging activities. Log level choice is an integral, yet tricky part of the logging activity as it controls log verbosity and therefore influences systems' observability and performance. Recent advances in ML-based log level prediction have leveraged large language models (LLMs) to propose log level predictors (LLPs) that demonstrated promising performance improvements (AUC between 0.64 and 0.8). Nevertheless, current LLM-based LLPs rely on randomly selected in-context examples, overlooking the structure and the diverse logging practices within modern software projects. In this paper, we propose OmniLLP, a novel LLP enhancement framework that clusters source files based on (1) semantic similarity reflecting the code's functional purpose, and (2) developer ownership cohesion. By retrieving in-context learning examples exclusively from these semantic and ownership aware clusters, we aim to provide more coherent prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy. Our results show that both semantic and ownership-aware clusterings statistically significantly improve the accuracy (by up to 8\% AUC) of the evaluated LLM-based LLPs compared to random predictors (i.e., leveraging randomly selected in-context examples from the whole project). Additionally, our approach that combines the semantic and ownership signal for in-context prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated projects. Our findings highlight the value of integrating software engineering-specific context, such as code semantic and developer ownership signals into LLM-LLPs, offering developers a more accurate, contextually-aware approach to logging and therefore, enhancing system maintainability and observability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2508.08551</link>
<guid>https://arxiv.org/abs/2508.08551</guid>
<content:encoded><![CDATA[
arXiv:2508.08551v1 Announce Type: cross 
Abstract: Spatiotemporal prediction plays a critical role in numerous real-world applications such as urban planning, transportation optimization, disaster response, and pandemic control. In recent years, researchers have made significant progress by developing advanced deep learning models for spatiotemporal prediction. However, most existing models are deterministic, i.e., predicting only the expected mean values without quantifying uncertainty, leading to potentially unreliable and inaccurate outcomes. While recent studies have introduced probabilistic models to quantify uncertainty, they typically focus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes), thereby neglecting the inherent correlations among heterogeneous urban phenomena. To address the research gap, we propose a novel Graph Neural Network with Uncertainty Quantification, termed UQGNN for multivariate spatiotemporal prediction. UQGNN introduces two key innovations: (i) an Interaction-aware Spatiotemporal Embedding Module that integrates a multivariate diffusion graph convolutional network and an interaction-aware temporal convolutional network to effectively capture complex spatial and temporal interaction patterns, and (ii) a multivariate probabilistic prediction module designed to estimate both expected mean values and associated uncertainties. Extensive experiments on four real-world multivariate spatiotemporal datasets from Shenzhen, New York City, and Chicago demonstrate that UQGNN consistently outperforms state-of-the-art baselines in both prediction accuracy and uncertainty quantification. For example, on the Shenzhen dataset, UQGNN achieves a 5% improvement in both prediction accuracy and uncertainty quantification.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation</title>
<link>https://arxiv.org/abs/2508.08570</link>
<guid>https://arxiv.org/abs/2508.08570</guid>
<content:encoded><![CDATA[
arXiv:2508.08570v1 Announce Type: cross 
Abstract: To enhance group robustness to spurious correlations, prior work often relies on auxiliary annotations for groups or spurious features and assumes identical sets of groups across source and target domains. These two requirements are both unnatural and impractical in real-world settings. To overcome these limitations, we propose a method that leverages the semantic structure inherent in class labels--specifically, superclass information--to naturally reduce reliance on spurious features. Our model employs gradient-based attention guided by a pre-trained vision-language model to disentangle superclass-relevant and irrelevant features. Then, by promoting the use of all superclass-relevant features for prediction, our approach achieves robustness to more complex spurious correlations without the need to annotate any source samples. Experiments across diverse datasets demonstrate that our method significantly outperforms baselines in domain generalization tasks, with clear improvements in both quantitative metrics and qualitative visualizations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who pays the RENT? Implications of Spatial Inequality for Prediction-Based Allocation Policies</title>
<link>https://arxiv.org/abs/2508.08573</link>
<guid>https://arxiv.org/abs/2508.08573</guid>
<content:encoded><![CDATA[
arXiv:2508.08573v1 Announce Type: cross 
Abstract: AI-powered scarce resource allocation policies rely on predictions to target either specific individuals (e.g., high-risk) or settings (e.g., neighborhoods). Recent research on individual-level targeting demonstrates conflicting results; some models show that targeting is not useful when inequality is high, while other work demonstrates potential benefits. To study and reconcile this apparent discrepancy, we develop a stylized framework based on the Mallows model to understand how the spatial distribution of inequality affects the effectiveness of door-to-door outreach policies. We introduce the RENT (Relative Efficiency of Non-Targeting) metric, which we use to assess the effectiveness of targeting approaches compared with neighborhood-based approaches in preventing tenant eviction when high-risk households are more versus less spatially concentrated. We then calibrate the model parameters to eviction court records collected in a medium-sized city in the USA. Results demonstrate considerable gains in the number of high-risk households canvassed through individually targeted policies, even in a highly segregated metro area with concentrated risks of eviction. We conclude that apparent discrepancies in the prior literature can be reconciled by considering 1) the source of deployment costs and 2) the observed versus modeled concentrations of risk. Our results inform the deployment of AI-based solutions in social service provision that account for particular applications and geographies.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders</title>
<link>https://arxiv.org/abs/2508.08583</link>
<guid>https://arxiv.org/abs/2508.08583</guid>
<content:encoded><![CDATA[
arXiv:2508.08583v1 Announce Type: cross 
Abstract: As the social implementation of AI has been steadily progressing, research and development related to AI security has also been increasing. However, existing studies have been limited to organizing related techniques, attacks, defenses, and risks in terms of specific domains or AI elements. Thus, it extremely difficult to understand the relationships among them and how negative impacts on stakeholders are brought about. In this paper, we argue that the knowledge, technologies, and social impacts related to AI security should be holistically organized to help understand relationships among them. To this end, we first develop an AI security map that holistically organizes interrelationships among elements related to AI security as well as negative impacts on information systems and stakeholders. This map consists of the two aspects, namely the information system aspect (ISA) and the external influence aspect (EIA). The elements that AI should fulfill within information systems are classified under the ISA. The EIA includes elements that affect stakeholders as a result of AI being attacked or misused. For each element, corresponding negative impacts are identified. By referring to the AI security map, one can understand the potential negative impacts, along with their causes and countermeasures. Additionally, our map helps clarify how the negative impacts on AI-based systems relate to those on stakeholders. We show some findings newly obtained by referring to our map. We also provide several recommendations and open problems to guide future AI security communities.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives</title>
<link>https://arxiv.org/abs/2508.08591</link>
<guid>https://arxiv.org/abs/2508.08591</guid>
<content:encoded><![CDATA[
arXiv:2508.08591v1 Announce Type: cross 
Abstract: Advances in large language models (LLMs) have enabled a wide range of applications. However, depression prediction is hindered by the lack of large-scale, high-quality, and rigorously annotated datasets. This study introduces DepressLLM, trained and evaluated on a novel corpus of 3,699 autobiographical narratives reflecting both happiness and distress. DepressLLM provides interpretable depression predictions and, via its Score-guided Token Probability Summation (SToPS) module, delivers both improved classification performance and reliable confidence estimates, achieving an AUC of 0.789, which rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets, including an Ecological Momentary Assessment (EMA) corpus of daily stress and mood recordings, and on public clinical interview data. Finally, a psychiatric review of high-confidence misclassifications highlighted key model and data limitations that suggest directions for future refinements. These findings demonstrate that interpretable AI can enable earlier diagnosis of depression and underscore the promise of medical AI in psychiatry.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Critical Infrastructure in Smart Grids: A Unified Framework for Synthetic Data Generation and Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.08593</link>
<guid>https://arxiv.org/abs/2508.08593</guid>
<content:encoded><![CDATA[
arXiv:2508.08593v1 Announce Type: cross 
Abstract: In digital substations, security events pose significant challenges to the sustained operation of power systems. To mitigate these challenges, the implementation of robust defense strategies is critically important. A thorough process of anomaly identification and detection in information and communication technology (ICT) frameworks is crucial to ensure secure and reliable communication and coordination between interconnected devices within digital substations. Hence, this paper addresses the critical cybersecurity challenges confronting IEC61850-based digital substations within modern smart grids, where the integration of advanced communication protocols, e.g., generic object-oriented substation event (GOOSE), has enhanced energy management and introduced significant vulnerabilities to cyberattacks. Focusing on the limitations of traditional anomaly detection systems (ADSs) in detecting threats, this research proposes a transformative approach by leveraging generative AI (GenAI) to develop robust ADSs. The primary contributions include the suggested advanced adversarial traffic mutation (AATM) technique to generate synthesized and balanced datasets for GOOSE messages, ensuring protocol compliance and enabling realistic zero-day attack pattern creation to address data scarcity. Then, the implementation of GenAI-based ADSs incorporating the task-oriented dialogue (ToD) processes has been explored for improved detection of attack patterns. Finally, a comparison of the GenAI-based ADS with machine learning (ML)-based ADSs has been implemented to showcase the outperformance of the GenAI-based frameworks considering the AATM-generated GOOSE datasets and standard/advanced performance evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yan: Foundational Interactive Video Generation</title>
<link>https://arxiv.org/abs/2508.08601</link>
<guid>https://arxiv.org/abs/2508.08601</guid>
<content:encoded><![CDATA[
arXiv:2508.08601v1 Announce Type: cross 
Abstract: We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: https://greatx3.github.io/Yan/.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization</title>
<link>https://arxiv.org/abs/2508.08604</link>
<guid>https://arxiv.org/abs/2508.08604</guid>
<content:encoded><![CDATA[
arXiv:2508.08604v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have been widely used in various visual recognition tasks due to their remarkable generalization capabilities. As these models grow in size and complexity, fine-tuning becomes costly, emphasizing the need to reuse adaptation knowledge from 'weaker' models to efficiently enhance 'stronger' ones. However, existing adaptation transfer methods exhibit limited transferability across models due to their model-specific design and high computational demands. To tackle this, we propose Transferable Model-agnostic adapter (TransMiter), a light-weight adapter that improves vision-language models 'without backpropagation'. TransMiter captures the knowledge gap between pre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained, this knowledge can be seamlessly transferred across different models without the need for backpropagation. Moreover, TransMiter consists of only a few layers, inducing a negligible additional inference cost. Notably, supplementing the process with a few labeled data further yields additional performance gain, often surpassing a fine-tuned stronger model, with a marginal training cost. Experimental results and analyses demonstrate that TransMiter effectively and efficiently transfers adaptation knowledge while preserving generalization abilities across VLMs of different sizes and architectures in visual recognition tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach</title>
<link>https://arxiv.org/abs/2508.08627</link>
<guid>https://arxiv.org/abs/2508.08627</guid>
<content:encoded><![CDATA[
arXiv:2508.08627v1 Announce Type: cross 
Abstract: Mobile augmented reality (MAR) is envisioned as a key immersive application in 6G, enabling virtual content rendering aligned with the physical environment through device pose estimation. In this paper, we propose a novel agent-driven communication service provisioning approach for edge-assisted MAR, aiming to reduce communication overhead between MAR devices and the edge server while ensuring the quality of experience (QoE). First, to address the inaccessibility of MAR application-specific information to the network controller, we establish a digital agent powered by large language models (LLMs) on behalf of the MAR service provider, bridging the data and function gap between the MAR service and network domains. Second, to cope with the user-dependent and dynamic nature of data traffic patterns for individual devices, we develop a user-level QoE modeling method that captures the relationship between communication resource demands and perceived user QoE, enabling personalized, agent-driven communication resource management. Trace-driven simulation results demonstrate that the proposed approach outperforms conventional LLM-based QoE-aware service provisioning methods in both user-level QoE modeling accuracy and communication resource efficiency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment</title>
<link>https://arxiv.org/abs/2508.08629</link>
<guid>https://arxiv.org/abs/2508.08629</guid>
<content:encoded><![CDATA[
arXiv:2508.08629v1 Announce Type: cross 
Abstract: Due to perceptions of efficiency and significant productivity gains, various organisations, including in education, are adopting Large Language Models (LLMs) into their workflows. Educator-facing, learner-facing, and institution-facing LLMs, collectively, Educational Large Language Models (eLLMs), complement and enhance the effectiveness of teaching, learning, and academic operations. However, their integration into an educational setting raises significant cybersecurity concerns. A comprehensive landscape of contemporary attacks on LLMs and their impact on the educational environment is missing. This study presents a generalised taxonomy of fifty attacks on LLMs, which are categorized as attacks targeting either models or their infrastructure. The severity of these attacks is evaluated in the educational sector using the DREAD risk assessment framework. Our risk assessment indicates that token smuggling, adversarial prompts, direct injection, and multi-step jailbreak are critical attacks on eLLMs. The proposed taxonomy, its application in the educational environment, and our risk assessment will help academic and industrial practitioners to build resilient solutions that protect learners and institutions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time</title>
<link>https://arxiv.org/abs/2508.08641</link>
<guid>https://arxiv.org/abs/2508.08641</guid>
<content:encoded><![CDATA[
arXiv:2508.08641v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being applied to black-box optimization tasks, from program synthesis to molecule design. Prior work typically leverages in-context learning to iteratively guide the model towards better solutions. Such methods, however, often struggle to balance exploration of new solution spaces with exploitation of high-reward ones. Recently, test-time training (TTT) with synthetic data has shown promise in improving solution quality. However, the need for hand-crafted training data tailored to each task limits feasibility and scalability across domains. To address this problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a search algorithm to adapt LLMs at inference without requiring external training data. MiGrATe operates via a mixed-policy group construction procedure that combines on-policy sampling with two off-policy data selection techniques: greedy sampling, which selects top-performing past completions, and neighborhood sampling (NS), which generates completions structurally similar to high-reward ones. Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling. We evaluate MiGrATe on three challenging domains-word search, molecule optimization, and hypothesis+program induction on the Abstraction and Reasoning Corpus (ARC)-and find that it consistently outperforms both inference-only and TTT baselines, demonstrating the potential of online TTT as a solution for complex search tasks without external supervision.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement</title>
<link>https://arxiv.org/abs/2508.08653</link>
<guid>https://arxiv.org/abs/2508.08653</guid>
<content:encoded><![CDATA[
arXiv:2508.08653v1 Announce Type: cross 
Abstract: Transforming unstructured text into structured data is a complex task, requiring semantic understanding, reasoning, and structural comprehension. While Large Language Models (LLMs) offer potential, they often struggle with handling ambiguous or domain-specific data, maintaining table structure, managing long inputs, and addressing numerical reasoning. This paper proposes an efficient system for LLM-driven text-to-table generation that leverages novel prompting techniques. Specifically, the system incorporates two key strategies: breaking down the text-to-table task into manageable, guided sub-tasks and refining the generated tables through iterative self-feedback. We show that this custom task decomposition allows the model to address the problem in a stepwise manner and improves the quality of the generated table. Furthermore, we discuss the benefits and potential risks associated with iterative self-feedback on the generated tables while highlighting the trade-offs between enhanced performance and computational cost. Our methods achieve strong results compared to baselines on two complex text-to-table generation datasets available in the public domain.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2508.08657</link>
<guid>https://arxiv.org/abs/2508.08657</guid>
<content:encoded><![CDATA[
arXiv:2508.08657v1 Announce Type: cross 
Abstract: Accurate molecular property prediction is a critical challenge with wide-ranging applications in chemistry, materials science, and drug discovery. Molecular representation methods, including fingerprints and graph neural networks (GNNs), achieve state-of-the-art results by effectively deriving features from molecular structures. However, these methods often overlook decades of accumulated semantic and contextual knowledge. Recent advancements in large language models (LLMs) demonstrate remarkable reasoning abilities and prior knowledge across scientific domains, leading us to hypothesize that LLMs can generate rich molecular representations when guided to reason in multiple perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view framework that integrates three perspectives: the molecular structure view, the molecular task view, and the molecular rules view. These views are fused dynamically to adapt to task requirements, and experiments demonstrate that $\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks across classification and regression tasks. Moreover, we demonstrate that representation derived from LLM achieves exceptional performance by leveraging two core functionalities: the generation of molecular embeddings through their encoding capabilities and the curation of molecular features through advanced reasoning processes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics</title>
<link>https://arxiv.org/abs/2508.08661</link>
<guid>https://arxiv.org/abs/2508.08661</guid>
<content:encoded><![CDATA[
arXiv:2508.08661v1 Announce Type: cross 
Abstract: Language models have shown strong capabilities across a wide range of tasks in software engineering, such as code generation, yet they suffer from hallucinations. While hallucinations have been studied independently in natural language and code generation, their occurrence in tasks involving code changes which have a structurally complex and context-dependent format of code remains largely unexplored. This paper presents the first comprehensive analysis of hallucinations in two critical tasks involving code change to natural language generation: commit message generation and code review comment generation. We quantify the prevalence of hallucinations in recent language models and explore a range of metric-based approaches to automatically detect them. Our findings reveal that approximately 50\% of generated code reviews and 20\% of generated commit messages contain hallucinations. Whilst commonly used metrics are weak detectors on their own, combining multiple metrics substantially improves performance. Notably, model confidence and feature attribution metrics effectively contribute to hallucination detection, showing promise for inference-time detection.\footnote{All code and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imposing AI: Deceptive design patterns against sustainability</title>
<link>https://arxiv.org/abs/2508.08672</link>
<guid>https://arxiv.org/abs/2508.08672</guid>
<content:encoded><![CDATA[
arXiv:2508.08672v1 Announce Type: cross 
Abstract: Generative AI is being massively deployed in digital services, at a scale that will result in significant environmental harm. We document how tech companies are transforming established user interfaces to impose AI use and show how and to what extent these strategies fit within established deceptive pattern categories. We identify two main design strategies that are implemented to impose AI use in both personal and professional contexts: imposing AI features in interfaces at the expense of existing non-AI features and promoting narratives about AI that make it harder to resist using it. We discuss opportunities for regulating the imposed adoption of AI features, which would inevitably lead to negative environmental effects.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</title>
<link>https://arxiv.org/abs/2508.08679</link>
<guid>https://arxiv.org/abs/2508.08679</guid>
<content:encoded><![CDATA[
arXiv:2508.08679v1 Announce Type: cross 
Abstract: Multimodal medical image fusion (MMIF) aims to integrate images from different modalities to produce a comprehensive image that enhances medical diagnosis by accurately depicting organ structures, tissue textures, and metabolic information. Capturing both the unique and complementary information across multiple modalities simultaneously is a key research challenge in MMIF. To address this challenge, this paper proposes a novel image fusion method, MMIF-AMIN, which features a new architecture that can effectively extract these unique and complementary features. Specifically, an Invertible Dense Network (IDN) is employed for lossless feature extraction from individual modalities. To extract complementary information between modalities, a Multi-scale Complementary Feature Extraction Module (MCFEM) is designed, which incorporates a hybrid attention mechanism, convolutional layers of varying sizes, and Transformers. An adaptive loss function is introduced to guide model learning, addressing the limitations of traditional manually-designed loss functions and enhancing the depth of data mining. Extensive experiments demonstrate that MMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior results in both quantitative and qualitative analyses. Ablation experiments confirm the effectiveness of each component of the proposed method. Additionally, extending MMIF-AMIN to other image fusion tasks also achieves promising performance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeFix: Targeted Model Repair via Controlled Image Generation</title>
<link>https://arxiv.org/abs/2508.08701</link>
<guid>https://arxiv.org/abs/2508.08701</guid>
<content:encoded><![CDATA[
arXiv:2508.08701v1 Announce Type: cross 
Abstract: Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.08712</link>
<guid>https://arxiv.org/abs/2508.08712</guid>
<content:encoded><![CDATA[
arXiv:2508.08712v1 Announce Type: cross 
Abstract: As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs</title>
<link>https://arxiv.org/abs/2508.08715</link>
<guid>https://arxiv.org/abs/2508.08715</guid>
<content:encoded><![CDATA[
arXiv:2508.08715v1 Announce Type: cross 
Abstract: Generative speech models have demonstrated significant potential in personalizing teacher-student interactions, offering valuable real-world applications for language learning in children's education. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiAiTutor, an educational multilingual generative AI tutor with child-friendly designs, leveraging LLM architecture for speech generation tailored for educational purposes. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, facilitating young children's language learning through culturally relevant image-description tasks in three low-resource languages: Singaporean-accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiAiTutor compared to baseline methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2508.08718</link>
<guid>https://arxiv.org/abs/2508.08718</guid>
<content:encoded><![CDATA[
arXiv:2508.08718v1 Announce Type: cross 
Abstract: The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial optimization task with numerous practical applications. Classic heuristic solvers can attain near-optimal performance for small problem instances, but become computationally intractable for larger problems. Real-world logistics problems such as dynamically re-routing last-mile deliveries demand a solver with fast inference time, which has led researchers to investigate specialized neural network solvers. However, neural networks struggle to generalize beyond the synthetic data they were trained on. In particular, we show that there exist TSP distributions that are realistic in practice, which also consistently lead to poor worst-case performance for existing neural approaches. To address this issue of distribution robustness, we present Combinatorial Optimization with Generative Sampling (COGS), where training data is sampled from a generative TSP model. We show that COGS provides better data coverage and interpolation in the space of TSP training distributions. We also present TSPLib50, a dataset of realistically distributed TSP samples, which tests real-world generalization ability without conflating this issue with instance size. We evaluate our method on various synthetic datasets as well as TSPLib50, and compare to state-of-the-art neural baselines. We demonstrate that COGS improves distribution robustness, with most performance gains coming from worst-case scenarios.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</title>
<link>https://arxiv.org/abs/2508.08719</link>
<guid>https://arxiv.org/abs/2508.08719</guid>
<content:encoded><![CDATA[
arXiv:2508.08719v1 Announce Type: cross 
Abstract: Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs' trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs' behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs' stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs</title>
<link>https://arxiv.org/abs/2508.08742</link>
<guid>https://arxiv.org/abs/2508.08742</guid>
<content:encoded><![CDATA[
arXiv:2508.08742v1 Announce Type: cross 
Abstract: Scientific literature question answering is a pivotal step towards new scientific discoveries. Recently, \textit{two-stage} retrieval-augmented generated large language models (RAG-LLMs) have shown impressive advancements in this domain. Such a two-stage framework, especially the second stage (reranker), is particularly essential in the scientific domain, where subtle differences in terminology may have a greatly negative impact on the final factual-oriented or knowledge-intensive answers. Despite this significant progress, the potential and limitations of these works remain unexplored. In this work, we present a Scientific Rerank-oriented RAG Benchmark (SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning five scientific subjects. To rigorously assess the reranker performance in terms of noise resilience, relevance disambiguation, and factual consistency, we develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy Contexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI), and Counterfactual Contexts (CC). Through systematic evaluation of 13 widely used rerankers on five families of LLMs, we provide detailed insights into their relative strengths and limitations. To the best of our knowledge, SciRerankBench is the first benchmark specifically developed to evaluate rerankers within RAG-LLMs, which provides valuable observations and guidance for their future development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</title>
<link>https://arxiv.org/abs/2508.08748</link>
<guid>https://arxiv.org/abs/2508.08748</guid>
<content:encoded><![CDATA[
arXiv:2508.08748v1 Announce Type: cross 
Abstract: Robotic pick-and-place tasks in convenience stores pose challenges due to dense object arrangements, occlusions, and variations in object properties such as color, shape, size, and texture. These factors complicate trajectory planning and grasping. This paper introduces a perception-action pipeline leveraging annotation-guided visual prompting, where bounding box annotations identify both pickable objects and placement locations, providing structured spatial guidance. Instead of traditional step-by-step planning, we employ Action Chunking with Transformers (ACT) as an imitation learning algorithm, enabling the robotic arm to predict chunked action sequences from human demonstrations. This facilitates smooth, adaptive, and data-driven pick-and-place operations. We evaluate our system based on success rate and visual analysis of grasping behavior, demonstrating improved grasp accuracy and adaptability in retail environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation</title>
<link>https://arxiv.org/abs/2508.08761</link>
<guid>https://arxiv.org/abs/2508.08761</guid>
<content:encoded><![CDATA[
arXiv:2508.08761v1 Announce Type: cross 
Abstract: The manual translation of unstructured team dialogue into the structured artifacts required for Information Technology (IT) project governance is a critical bottleneck in modern information systems management. We introduce DevNous, a Large Language Model-based (LLM) multi-agent expert system, to automate this unstructured-to-structured translation process. DevNous integrates directly into team chat environments, identifying actionable intents from informal dialogue and managing stateful, multi-turn workflows for core administrative tasks like automated task formalization and progress summary synthesis. To quantitatively evaluate the system, we introduce a new benchmark of 160 realistic, interactive conversational turns. The dataset was manually annotated with a multi-label ground truth and is publicly available. On this benchmark, DevNous achieves an exact match turn accuracy of 81.3\% and a multiset F1-Score of 0.845, providing strong evidence for its viability. The primary contributions of this work are twofold: (1) a validated architectural pattern for developing ambient administrative agents, and (2) the introduction of the first robust empirical baseline and public benchmark dataset for this challenging problem domain.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation</title>
<link>https://arxiv.org/abs/2508.08765</link>
<guid>https://arxiv.org/abs/2508.08765</guid>
<content:encoded><![CDATA[
arXiv:2508.08765v1 Announce Type: cross 
Abstract: The growing presence of AI-generated videos on social networks poses new challenges for deepfake detection, as detectors trained under controlled conditions often fail to generalize to real-world scenarios. A key factor behind this gap is the aggressive, proprietary compression applied by platforms like YouTube and Facebook, which launder low-level forensic cues. However, replicating these transformations at scale is difficult due to API limitations and data-sharing constraints. For these reasons, we propose a first framework that emulates the video sharing pipelines of social networks by estimating compression and resizing parameters from a small set of uploaded videos. These parameters enable a local emulator capable of reproducing platform-specific artifacts on large datasets without direct API access. Experiments on FaceForensics++ videos shared via social networks demonstrate that our emulated data closely matches the degradation patterns of real uploads. Furthermore, detectors fine-tuned on emulated videos achieve comparable performance to those trained on actual shared media. Our approach offers a scalable and practical solution for bridging the gap between lab-based training and real-world deployment of deepfake detectors, particularly in the underexplored domain of compressed video content.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2508.08777</link>
<guid>https://arxiv.org/abs/2508.08777</guid>
<content:encoded><![CDATA[
arXiv:2508.08777v1 Announce Type: cross 
Abstract: Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReQuestNet: A Foundational Learning model for Channel Estimation</title>
<link>https://arxiv.org/abs/2508.08790</link>
<guid>https://arxiv.org/abs/2508.08790</guid>
<content:encoded><![CDATA[
arXiv:2508.08790v1 Announce Type: cross 
Abstract: In this paper, we present a novel neural architecture for channel estimation (CE) in 5G and beyond, the Recurrent Equivariant UERS Estimation Network (ReQuestNet). It incorporates several practical considerations in wireless communication systems, such as ability to handle variable number of resource block (RB), dynamic number of transmit layers, physical resource block groups (PRGs) bundling size (BS), demodulation reference signal (DMRS) patterns with a single unified model, thereby, drastically simplifying the CE pipeline. Besides it addresses several limitations of the legacy linear MMSE solutions, for example, by being independent of other reference signals and particularly by jointly processing MIMO layers and differently precoded channels with unknown precoding at the receiver. ReQuestNet comprises of two sub-units, CoarseNet followed by RefinementNet. CoarseNet performs per PRG, per transmit-receive (Tx-Rx) stream channel estimation, while RefinementNet refines the CoarseNet channel estimate by incorporating correlations across differently precoded PRGs, and correlation across multiple input multiple output (MIMO) channel spatial dimensions (cross-MIMO). Simulation results demonstrate that ReQuestNet significantly outperforms genie minimum mean squared error (MMSE) CE across a wide range of channel conditions, delay-Doppler profiles, achieving up to 10dB gain at high SNRs. Notably, ReQuestNet generalizes effectively to unseen channel profiles, efficiently exploiting inter-PRG and cross-MIMO correlations under dynamic PRG BS and varying transmit layer allocations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments</title>
<link>https://arxiv.org/abs/2508.08791</link>
<guid>https://arxiv.org/abs/2508.08791</guid>
<content:encoded><![CDATA[
arXiv:2508.08791v1 Announce Type: cross 
Abstract: Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TechOps: Technical Documentation Templates for the AI Act</title>
<link>https://arxiv.org/abs/2508.08804</link>
<guid>https://arxiv.org/abs/2508.08804</guid>
<content:encoded><![CDATA[
arXiv:2508.08804v1 Announce Type: cross 
Abstract: Operationalizing the EU AI Act requires clear technical documentation to ensure AI systems are transparent, traceable, and accountable. Existing documentation templates for AI systems do not fully cover the entire AI lifecycle while meeting the technical documentation requirements of the AI Act.
  This paper addresses those shortcomings by introducing open-source templates and examples for documenting data, models, and applications to provide sufficient documentation for certifying compliance with the AI Act. These templates track the system status over the entire AI lifecycle, ensuring traceability, reproducibility, and compliance with the AI Act. They also promote discoverability and collaboration, reduce risks, and align with best practices in AI documentation and governance.
  The templates are evaluated and refined based on user feedback to enable insights into their usability and implementability. We then validate the approach on real-world scenarios, providing examples that further guide their implementation: the data template is followed to document a skin tones dataset created to support fairness evaluations of downstream computer vision models and human-centric applications; the model template is followed to document a neural network for segmenting human silhouettes in photos. The application template is tested on a system deployed for construction site safety using real-time video analytics and sensor data. Our results show that TechOps can serve as a practical tool to enable oversight for regulatory compliance and responsible AI development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems</title>
<link>https://arxiv.org/abs/2508.08805</link>
<guid>https://arxiv.org/abs/2508.08805</guid>
<content:encoded><![CDATA[
arXiv:2508.08805v1 Announce Type: cross 
Abstract: AI systems for music generation are increasingly common and easy to use, granting people without any musical background the ability to create music. Because of this, generative-AI has been marketed and celebrated as a means of democratizing music making. However, inclusivity often functions as marketable rhetoric rather than a genuine guiding principle in these industry settings. In this paper, we look at four generative-AI music making systems available to the public as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they are rhetoricized by their developers, and received by users. Our aim is to investigate ideologies that are driving the early-stage development and adoption of generative-AI in music making, with a particular focus on democratization. A combination of autoethnography and digital ethnography is used to examine patterns and incongruities in rhetoric when positioned against product functionality. The results are then collated to develop a nuanced, contextual discussion. The shared ideology we map between producers and consumers is individualist, globalist, techno-liberal, and ethically evasive. It is a 'total ideology' which obfuscates individual responsibility, and through which the nature of music and musical practice is transfigured to suit generative outcomes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not in My Backyard! Temporal Voting Over Public Chores</title>
<link>https://arxiv.org/abs/2508.08810</link>
<guid>https://arxiv.org/abs/2508.08810</guid>
<content:encoded><![CDATA[
arXiv:2508.08810v1 Announce Type: cross 
Abstract: We study a temporal voting model where voters have dynamic preferences over a set of public chores -- projects that benefit society, but impose individual costs on those affected by their implementation. We investigate the computational complexity of optimizing utilitarian and egalitarian welfare. Our results show that while optimizing the former is computationally straightforward, minimizing the latter is computationally intractable, even in very restricted cases. Nevertheless, we identify several settings where this problem can be solved efficiently, either exactly or by an approximation algorithm. We also examine the effects of enforcing temporal fairness and its impact on social welfare, and analyze the competitive ratio of online algorithms. We then explore the strategic behavior of agents, providing insights into potential malfeasance in such decision-making environments. Finally, we discuss a range of fairness measures and their suitability for our setting.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TempOpt -- Unsupervised Alarm Relation Learning for Telecommunication Networks</title>
<link>https://arxiv.org/abs/2508.08814</link>
<guid>https://arxiv.org/abs/2508.08814</guid>
<content:encoded><![CDATA[
arXiv:2508.08814v1 Announce Type: cross 
Abstract: In a telecommunications network, fault alarms generated by network nodes are monitored in a Network Operations Centre (NOC) to ensure network availability and continuous network operations. The monitoring process comprises of tasks such as active alarms analysis, root alarm identification, and resolution of the underlying problem. Each network node potentially can generate alarms of different types, while nodes can be from multiple vendors, a network can have hundreds of nodes thus resulting in an enormous volume of alarms at any time. Since network nodes are inter-connected, a single fault in the network would trigger multiple sequences of alarms across a variety of nodes and from a monitoring point of view, it is a challenging task for a NOC engineer to be aware of relations between the various alarms, when trying to identify, for example, a root alarm on which an action needs to be taken. To effectively identify root alarms, it is essential to learn relation among the alarms for accurate and faster resolution. In this work we propose a novel unsupervised alarm relation learning technique Temporal Optimization (TempOpt) that is practical and overcomes the limitations of an existing class of alarm relational learning method-temporal dependency methods. Experiments have been carried on real-world network datasets, that demonstrate the improved quality of alarm relations learned by TempOpt as compared to temporal dependency method.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads</title>
<link>https://arxiv.org/abs/2508.08822</link>
<guid>https://arxiv.org/abs/2508.08822</guid>
<content:encoded><![CDATA[
arXiv:2508.08822v1 Announce Type: cross 
Abstract: Artificial Intelligence models are currently driven by a significant up-scaling of their complexity, with massive matrix multiplication workloads representing the major computational bottleneck. In-memory computing architectures are proposed to avoid the Von Neumann bottleneck. However, both digital/binary-based and analogue in-memory computing architectures suffer from various limitations, which significantly degrade the performance and energy efficiency gains. This work proposes OISMA, a novel in-memory computing architecture that utilizes the computational simplicity of a quasi-stochastic computing domain (Bent-Pyramid system), while keeping the same efficiency, scalability, and productivity of digital memories. OISMA converts normal memory read operations into in-situ stochastic multiplication operations with a negligible cost. An accumulation periphery then accumulates the output multiplication bitstreams, achieving the matrix multiplication functionality. Extensive matrix multiplication benchmarking was conducted to analyze the accuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4 to 512x512. The accuracy results show a significant decrease in the average relative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared to 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB capacity was implemented using a commercial 180nm technology node and in-house RRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for energy and area efficiency, respectively, occupying an effective computing area of 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a significant improvement of two orders of magnitude in energy efficiency and one order of magnitude in area efficiency, compared to dense matrix multiplication in-memory computing architectures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Mixture of Experts for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.08825</link>
<guid>https://arxiv.org/abs/2508.08825</guid>
<content:encoded><![CDATA[
arXiv:2508.08825v1 Announce Type: cross 
Abstract: The field of time series forecasting is rapidly advancing, with recent large-scale Transformers and lightweight Multilayer Perceptron (MLP) models showing strong predictive performance. However, conventional Transformer models are often hindered by their large number of parameters and their limited ability to capture non-stationary features in data through smoothing. Similarly, MLP models struggle to manage multi-channel dependencies effectively. To address these limitations, we propose a novel, lightweight time series prediction model, WaveTS-B. This model combines wavelet transforms with MLP to capture both periodic and non-stationary characteristics of data in the wavelet domain. Building on this foundation, we propose a channel clustering strategy that incorporates a Mixture of Experts (MoE) framework, utilizing a gating mechanism and expert network to handle multi-channel dependencies efficiently. We propose WaveTS-M, an advanced model tailored for multi-channel time series prediction. Empirical evaluation across eight real-world time series datasets demonstrates that our WaveTS series models achieve state-of-the-art (SOTA) performance with significantly fewer parameters. Notably, WaveTS-M shows substantial improvements on multi-channel datasets, highlighting its effectiveness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination</title>
<link>https://arxiv.org/abs/2508.08826</link>
<guid>https://arxiv.org/abs/2508.08826</guid>
<content:encoded><![CDATA[
arXiv:2508.08826v1 Announce Type: cross 
Abstract: Real-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.
  From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.
  We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v1 Announce Type: cross 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 49 % on the originals but drops by 4 percentage points on surface variants, and by 10.5 percentage points on core-step-based variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EditMF: Drawing an Invisible Fingerprint for Your Large Language Models</title>
<link>https://arxiv.org/abs/2508.08836</link>
<guid>https://arxiv.org/abs/2508.08836</guid>
<content:encoded><![CDATA[
arXiv:2508.08836v1 Announce Type: cross 
Abstract: Training large language models (LLMs) is resource-intensive and expensive, making protecting intellectual property (IP) for LLMs crucial. Recently, embedding fingerprints into LLMs has emerged as a prevalent method for establishing model ownership. However, existing back-door-based methods suffer from limited stealth and efficiency. To simultaneously address these issues, we propose EditMF, a training-free fingerprinting paradigm that achieves highly imperceptible fingerprint embedding with minimal computational overhead. Ownership bits are mapped to compact, semantically coherent triples drawn from an encrypted artificial knowledge base (e.g., virtual author-novel-protagonist facts). Causal tracing localizes the minimal set of layers influencing each triple, and a zero-space update injects the fingerprint without perturbing unrelated knowledge. Verification requires only a single black-box query and succeeds when the model returns the exact pre-embedded protagonist. Empirical results on LLaMA and Qwen families show that EditMF combines high imperceptibility with negligible model's performance loss, while delivering robustness far beyond LoRA-based fingerprinting and approaching that of SFT embeddings. Extensive experiments demonstrate that EditMF is an effective and low-overhead solution for secure LLM ownership verification.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents</title>
<link>https://arxiv.org/abs/2508.08837</link>
<guid>https://arxiv.org/abs/2508.08837</guid>
<content:encoded><![CDATA[
arXiv:2508.08837v1 Announce Type: cross 
Abstract: The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another -- US citizens' perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMs' capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Towards Fairness: Mitigating Political Bias in LLMs</title>
<link>https://arxiv.org/abs/2508.08846</link>
<guid>https://arxiv.org/abs/2508.08846</guid>
<content:encoded><![CDATA[
arXiv:2508.08846v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. In this paper, we propose a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), our method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasGym: Fantastic Biases and How to Find (and Remove) Them</title>
<link>https://arxiv.org/abs/2508.08855</link>
<guid>https://arxiv.org/abs/2508.08855</guid>
<content:encoded><![CDATA[
arXiv:2508.08855v1 Announce Type: cross 
Abstract: Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during training. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from a country being `reckless drivers') and in probing fictional associations (e.g., people from a country having `blue skin'), showing its utility for both safety interventions and interpretability research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models</title>
<link>https://arxiv.org/abs/2508.08875</link>
<guid>https://arxiv.org/abs/2508.08875</guid>
<content:encoded><![CDATA[
arXiv:2508.08875v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to utilize private, task-specific datasets for fine-tuning while preserving data privacy. However, while federated LLM frameworks effectively enable collaborative training without raw data sharing, they critically lack built-in mechanisms for regulatory compliance like GDPR's right to be forgotten. Integrating private data heightens concerns over data quality and long-term governance, yet existing distributed training frameworks offer no principled way to selectively remove specific client contributions post-training. Due to distributed data silos, stringent privacy constraints, and the intricacies of interdependent model aggregation, federated LLM unlearning is significantly more complex than centralized LLM unlearning. To address this gap, we introduce Oblivionis, a lightweight learning and unlearning framework that enables clients to selectively remove specific private data during federated LLM training, enhancing trustworthiness and regulatory compliance. By unifying FL and unlearning as a dual optimization objective, we incorporate 6 FL and 5 unlearning algorithms for comprehensive evaluation and comparative analysis, establishing a robust pipeline for federated LLM unlearning. Extensive experiments demonstrate that Oblivionis outperforms local training, achieving a robust balance between forgetting efficacy and model utility, with cross-algorithm comparisons providing clear directions for future LLM development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08879</link>
<guid>https://arxiv.org/abs/2508.08879</guid>
<content:encoded><![CDATA[
arXiv:2508.08879v1 Announce Type: cross 
Abstract: The growing deployment of large language models (LLMs) across diverse cultural contexts necessitates a better understanding of how the overgeneralization of less documented cultures within LLMs' representations impacts their cultural understanding. Prior work only performs extrinsic evaluation of LLMs' cultural competence, without accounting for how LLMs' internal mechanisms lead to cultural (mis)representation. To bridge this gap, we propose Culturescope, the first mechanistic interpretability-based method that probes the internal representations of LLMs to elicit the underlying cultural knowledge space. CultureScope utilizes a patching method to extract the cultural knowledge. We introduce a cultural flattening score as a measure of the intrinsic cultural biases. Additionally, we study how LLMs internalize Western-dominance bias and cultural flattening, which allows us to trace how cultural biases emerge within LLMs. Our experimental results reveal that LLMs encode Western-dominance bias and cultural flattening in their cultural knowledge space. We find that low-resource cultures are less susceptible to cultural biases, likely due to their limited training resources. Our work provides a foundation for future research on mitigating cultural biases and enhancing LLMs' cultural understanding. Our codes and data used for experiments are publicly available.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption</title>
<link>https://arxiv.org/abs/2508.08883</link>
<guid>https://arxiv.org/abs/2508.08883</guid>
<content:encoded><![CDATA[
arXiv:2508.08883v1 Announce Type: cross 
Abstract: Causal machine learning has the potential to revolutionize decision-making by combining the predictive power of machine learning algorithms with the theory of causal inference. However, these methods remain underutilized by the broader machine learning community, in part because current empirical evaluations do not permit assessment of their reliability and robustness, undermining their practical utility. Specifically, one of the principal criticisms made by the community is the extensive use of synthetic experiments. We argue, on the contrary, that synthetic experiments are essential and necessary to precisely assess and understand the capabilities of causal machine learning methods. To substantiate our position, we critically review the current evaluation practices, spotlight their shortcomings, and propose a set of principles for conducting rigorous empirical analyses with synthetic data. Adopting the proposed principles will enable comprehensive evaluations that build trust in causal machine learning methods, driving their broader adoption and impactful real-world use.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</title>
<link>https://arxiv.org/abs/2508.08895</link>
<guid>https://arxiv.org/abs/2508.08895</guid>
<content:encoded><![CDATA[
arXiv:2508.08895v1 Announce Type: cross 
Abstract: The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2508.08912</link>
<guid>https://arxiv.org/abs/2508.08912</guid>
<content:encoded><![CDATA[
arXiv:2508.08912v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape Completion and Real-Time Visualization in Robotic Ultrasound Spine Acquisitions</title>
<link>https://arxiv.org/abs/2508.08923</link>
<guid>https://arxiv.org/abs/2508.08923</guid>
<content:encoded><![CDATA[
arXiv:2508.08923v1 Announce Type: cross 
Abstract: Ultrasound (US) imaging is increasingly used in spinal procedures due to its real-time, radiation-free capabilities; however, its effectiveness is hindered by shadowing artifacts that obscure deeper tissue structures. Traditional approaches, such as CT-to-US registration, incorporate anatomical information from preoperative CT scans to guide interventions, but they are limited by complex registration requirements, differences in spine curvature, and the need for recent CT imaging. Recent shape completion methods can offer an alternative by reconstructing spinal structures in US data, while being pretrained on large set of publicly available CT scans. However, these approaches are typically offline and have limited reproducibility. In this work, we introduce a novel integrated system that combines robotic ultrasound with real-time shape completion to enhance spinal visualization. Our robotic platform autonomously acquires US sweeps of the lumbar spine, extracts vertebral surfaces from ultrasound, and reconstructs the complete anatomy using a deep learning-based shape completion network. This framework provides interactive, real-time visualization with the capability to autonomously repeat scans and can enable navigation to target locations. This can contribute to better consistency, reproducibility, and understanding of the underlying anatomy. We validate our approach through quantitative experiments assessing shape completion accuracy and evaluations of multiple spine acquisition protocols on a phantom setup. Additionally, we present qualitative results of the visualization on a volunteer scan.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EGGCodec: A Robust Neural Encodec Framework for EGG Reconstruction and F0 Extraction</title>
<link>https://arxiv.org/abs/2508.08924</link>
<guid>https://arxiv.org/abs/2508.08924</guid>
<content:encoded><![CDATA[
arXiv:2508.08924v1 Announce Type: cross 
Abstract: This letter introduces EGGCodec, a robust neural Encodec framework engineered for electroglottography (EGG) signal reconstruction and F0 extraction. We propose a multi-scale frequency-domain loss function to capture the nuanced relationship between original and reconstructed EGG signals, complemented by a time-domain correlation loss to improve generalization and accuracy. Unlike conventional Encodec models that extract F0 directly from features, EGGCodec leverages reconstructed EGG signals, which more closely correspond to F0. By removing the conventional GAN discriminator, we streamline EGGCodec's training process without compromising efficiency, incurring only negligible performance degradation. Trained on a widely used EGG-inclusive dataset, extensive evaluations demonstrate that EGGCodec outperforms state-of-the-art F0 extraction schemes, reducing mean absolute error (MAE) from 14.14 Hz to 13.69 Hz, and improving voicing decision error (VDE) by 38.2\%. Moreover, extensive ablation experiments validate the contribution of each component of EGGCodec.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Long, Think Short: Curriculum Learning for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2508.08940</link>
<guid>https://arxiv.org/abs/2508.08940</guid>
<content:encoded><![CDATA[
arXiv:2508.08940v1 Announce Type: cross 
Abstract: Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalising Traffic Forecasting to Regions without Traffic Observations</title>
<link>https://arxiv.org/abs/2508.08947</link>
<guid>https://arxiv.org/abs/2508.08947</guid>
<content:encoded><![CDATA[
arXiv:2508.08947v1 Announce Type: cross 
Abstract: Traffic forecasting is essential for intelligent transportation systems. Accurate forecasting relies on continuous observations collected by traffic sensors. However, due to high deployment and maintenance costs, not all regions are equipped with such sensors. This paper aims to forecast for regions without traffic sensors, where the lack of historical traffic observations challenges the generalisability of existing models. We propose a model named GenCast, the core idea of which is to exploit external knowledge to compensate for the missing observations and to enhance generalisation. We integrate physics-informed neural networks into GenCast, enabling physical principles to regularise the learning process. We introduce an external signal learning module to explore correlations between traffic states and external signals such as weather conditions, further improving model generalisability. Additionally, we design a spatial grouping module to filter localised features that hinder model generalisability. Extensive experiments show that GenCast consistently reduces forecasting errors on multiple real-world datasets.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAMRO: Quality-aware Adaptive Margin Ranking Optimization for Human-aligned Assessment of Audio Generation Systems</title>
<link>https://arxiv.org/abs/2508.08957</link>
<guid>https://arxiv.org/abs/2508.08957</guid>
<content:encoded><![CDATA[
arXiv:2508.08957v1 Announce Type: cross 
Abstract: Evaluating audio generation systems, including text-to-music (TTM), text-to-speech (TTS), and text-to-audio (TTA), remains challenging due to the subjective and multi-dimensional nature of human perception. Existing methods treat mean opinion score (MOS) prediction as a regression problem, but standard regression losses overlook the relativity of perceptual judgments. To address this limitation, we introduce QAMRO, a novel Quality-aware Adaptive Margin Ranking Optimization framework that seamlessly integrates regression objectives from different perspectives, aiming to highlight perceptual differences and prioritize accurate ratings. Our framework leverages pre-trained audio-text models such as CLAP and Audiobox-Aesthetics, and is trained exclusively on the official AudioMOS Challenge 2025 dataset. It demonstrates superior alignment with human evaluations across all dimensions, significantly outperforming robust baseline models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Role of Audio Channels in ASR Performance Degradation</title>
<link>https://arxiv.org/abs/2508.08967</link>
<guid>https://arxiv.org/abs/2508.08967</guid>
<content:encoded><![CDATA[
arXiv:2508.08967v1 Announce Type: cross 
Abstract: Pre-trained automatic speech recognition (ASR) models have demonstrated strong performance on a variety of tasks. However, their performance can degrade substantially when the input audio comes from different recording channels. While previous studies have demonstrated this phenomenon, it is often attributed to the mismatch between training and testing corpora. This study argues that variations in speech characteristics caused by different recording channels can fundamentally harm ASR performance. To address this limitation, we propose a normalization technique designed to mitigate the impact of channel variation by aligning internal feature representations in the ASR model with those derived from a clean reference channel. This approach significantly improves ASR performance on previously unseen channels and languages, highlighting its ability to generalize across channel and language differences.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban-STA4CLC: Urban Theory-Informed Spatio-Temporal Attention Model for Predicting Post-Disaster Commercial Land Use Change</title>
<link>https://arxiv.org/abs/2508.08976</link>
<guid>https://arxiv.org/abs/2508.08976</guid>
<content:encoded><![CDATA[
arXiv:2508.08976v1 Announce Type: cross 
Abstract: Natural disasters such as hurricanes and wildfires increasingly introduce unusual disturbance on economic activities, which are especially likely to reshape commercial land use pattern given their sensitive to customer visitation. However, current modeling approaches are limited in capturing such complex interplay between human activities and commercial land use change under and following disturbances. Such interactions have been more effectively captured in current resilient urban planning theories. This study designs and calibrates a Urban Theory-Informed Spatio-Temporal Attention Model for Predicting Post-Disaster Commercial Land Use Change (Urban-STA4CLC) to predict both the yearly decline and expansion of commercial land use at census block level under cumulative impact of disasters on human activities over two years. Guided by urban theories, Urban-STA4CLC integrates both spatial and temporal attention mechanisms with three theory-informed modules. Resilience theory guides a disaster-aware temporal attention module that captures visitation dynamics. Spatial economic theory informs a multi-relational spatial attention module for inter-block representation. Diffusion theory contributes a regularization term that constrains land use transitions. The model performs significantly better than non-theoretical baselines in predicting commercial land use change under the scenario of recurrent hurricanes, with around 19% improvement in F1 score (0.8763). The effectiveness of the theory-guided modules was further validated through ablation studies. The research demonstrates that embedding urban theory into commercial land use modeling models may substantially enhance the capacity to capture its gains and losses. These advances in commercial land use modeling contribute to land use research that accounts for cumulative impacts of recurrent disasters and shifts in economic activity patterns.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</title>
<link>https://arxiv.org/abs/2508.08982</link>
<guid>https://arxiv.org/abs/2508.08982</guid>
<content:encoded><![CDATA[
arXiv:2508.08982v1 Announce Type: cross 
Abstract: Exploration is crucial for enabling legged robots to learn agile locomotion behaviors that can overcome diverse obstacles. However, such exploration is inherently challenging, and we often rely on extensive reward engineering, expert demonstrations, or curriculum learning - all of which limit generalizability. In this work, we propose Skill Discovery as Exploration (SDAX), a novel learning framework that significantly reduces human engineering effort. SDAX leverages unsupervised skill discovery to autonomously acquire a diverse repertoire of skills for overcoming obstacles. To dynamically regulate the level of exploration during training, SDAX employs a bi-level optimization process that autonomously adjusts the degree of exploration. We demonstrate that SDAX enables quadrupedal robots to acquire highly agile behaviors including crawling, climbing, leaping, and executing complex maneuvers such as jumping off vertical walls. Finally, we deploy the learned policy on real hardware, validating its successful transfer to the real world.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Inverse Reasoning</title>
<link>https://arxiv.org/abs/2508.08983</link>
<guid>https://arxiv.org/abs/2508.08983</guid>
<content:encoded><![CDATA[
arXiv:2508.08983v1 Announce Type: cross 
Abstract: Humans can observe a single, imperfect demonstration and immediately generalize to very different problem settings. Robots, in contrast, often require hundreds of examples and still struggle to generalize beyond the training conditions. We argue that this limitation arises from the inability to recover the latent explanations that underpin intelligent behavior, and that these explanations can take the form of structured programs consisting of high-level goals, sub-task decomposition, and execution constraints. In this work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring these latent programs through a hierarchical generative model of behavior. RIR frames few-shot imitation as Bayesian program induction: a vision-language model iteratively proposes structured symbolic task hypotheses, while a planner-in-the-loop inference scheme scores each by the likelihood of the observed demonstration under that hypothesis. This loop yields a posterior over concise, executable programs. We evaluate RIR on a suite of continuous manipulation tasks designed to test one-shot and few-shot generalization across variations in object pose, count, geometry, and layout. With as little as one demonstration, RIR infers the intended task structure and generalizes to novel settings, outperforming state-of-the-art vision-language model baselines.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospective Sparse Attention for Efficient Long-Context Generation</title>
<link>https://arxiv.org/abs/2508.09001</link>
<guid>https://arxiv.org/abs/2508.09001</guid>
<content:encoded><![CDATA[
arXiv:2508.09001v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA</title>
<link>https://arxiv.org/abs/2508.09012</link>
<guid>https://arxiv.org/abs/2508.09012</guid>
<content:encoded><![CDATA[
arXiv:2508.09012v1 Announce Type: cross 
Abstract: This paper describes our participation in SemEval 2025 Task 8, focused on Tabular Question Answering. We developed a zero-shot pipeline that leverages an Large Language Model to generate functional code capable of extracting the relevant information from tabular data based on an input question. Our approach consists of a modular pipeline where the main code generator module is supported by additional components that identify the most relevant columns and analyze their data types to improve extraction accuracy. In the event that the generated code fails, an iterative refinement process is triggered, incorporating the error feedback into a new generation prompt to enhance robustness. Our results show that zero-shot code generation is a valid approach for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacks and Defenses Against LLM Fingerprinting</title>
<link>https://arxiv.org/abs/2508.09021</link>
<guid>https://arxiv.org/abs/2508.09021</guid>
<content:encoded><![CDATA[
arXiv:2508.09021v1 Announce Type: cross 
Abstract: As large language models are increasingly deployed in sensitive environments, fingerprinting attacks pose significant privacy and security risks. We present a study of LLM fingerprinting from both offensive and defensive perspectives. Our attack methodology uses reinforcement learning to automatically optimize query selection, achieving better fingerprinting accuracy with only 3 queries compared to randomly selecting 3 queries from the same pool. Our defensive approach employs semantic-preserving output filtering through a secondary LLM to obfuscate model identity while maintaining semantic integrity. The defensive method reduces fingerprinting accuracy across tested models while preserving output quality. These contributions show the potential to improve fingerprinting tools capabilities while providing practical mitigation strategies against fingerprinting attacks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</title>
<link>https://arxiv.org/abs/2508.09022</link>
<guid>https://arxiv.org/abs/2508.09022</guid>
<content:encoded><![CDATA[
arXiv:2508.09022v1 Announce Type: cross 
Abstract: Existing deepfake detection methods heavily depend on labeled training data. However, as AI-generated content becomes increasingly realistic, even \textbf{human annotators struggle to distinguish} between deepfakes and authentic images. This makes the labeling process both time-consuming and less reliable. Specifically, there is a growing demand for approaches that can effectively utilize large-scale unlabeled data from online social networks. Unlike typical unsupervised learning tasks, where categories are distinct, AI-generated faces closely mimic real image distributions and share strong similarities, causing performance drop in conventional strategies. In this paper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key challenges: (1) bridging the domain gap between faces from different generation models, and (2) utilizing unlabeled image samples. The method features two core modules: text-guided cross-domain alignment, which uses learnable prompts to unify visual and textual embeddings into a domain-invariant feature space, and curriculum-driven pseudo label generation, which dynamically exploit more informative unlabeled samples. To prevent catastrophic forgetting, we also facilitate bridging between domains via cross-domain knowledge distillation. Extensive experiments on \textbf{11 popular datasets}, show that DPGNet outperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectiveness in leveraging unlabeled data to address the annotation challenges posed by the increasing realism of deepfakes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency</title>
<link>https://arxiv.org/abs/2508.09023</link>
<guid>https://arxiv.org/abs/2508.09023</guid>
<content:encoded><![CDATA[
arXiv:2508.09023v1 Announce Type: cross 
Abstract: SQL query rewriting aims to reformulate a query into a more efficient form while preserving equivalence. Most existing methods rely on predefined rewrite rules. However, such rule-based approaches face fundamental limitations: (1) fixed rule sets generalize poorly to novel query patterns and struggle with complex queries; (2) a wide range of effective rewriting strategies cannot be fully captured by declarative rules. To overcome these issues, we propose using large language models (LLMs) to generate rewrites. LLMs can capture complex strategies, such as evaluation reordering and CTE rewriting. Despite this potential, directly applying LLMs often results in suboptimal or non-equivalent rewrites due to a lack of execution awareness and semantic grounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL rewriting framework that produces executable, equivalent, and efficient queries. It integrates two core components: a context construction module and a reinforcement learning framework. First, the context module leverages execution plans and retrieved demonstrations to build bottleneck-aware prompts that guide inference-time rewriting. Second, we design a reward function targeting executability, equivalence, and efficiency, evaluated via syntax checks, equivalence verification, and cost estimation. Third, to ensure stable multi-objective learning, we adopt a staged curriculum that first emphasizes executability and equivalence, then gradually incorporates efficiency. Extensive experiments show that E3-Rewrite achieves up to a 25.6\% reduction in query execution time compared to state-of-the-art methods across multiple SQL benchmarks. Moreover, it delivers up to 24.4\% more successful rewrites, expanding coverage to complex queries that previous systems failed to handle.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</title>
<link>https://arxiv.org/abs/2508.09032</link>
<guid>https://arxiv.org/abs/2508.09032</guid>
<content:encoded><![CDATA[
arXiv:2508.09032v1 Announce Type: cross 
Abstract: Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at https://ampiromax.github.io/ST-VLA.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams</title>
<link>https://arxiv.org/abs/2508.09036</link>
<guid>https://arxiv.org/abs/2508.09036</guid>
<content:encoded><![CDATA[
arXiv:2508.09036v1 Announce Type: cross 
Abstract: The rapid emergence of large language models (LLMs) has raised urgent questions across the modern workforce about this new technology's strengths, weaknesses, and capabilities. For privacy professionals, the question is whether these AI systems can provide reliable support on regulatory compliance, privacy program management, and AI governance. In this study, we evaluate ten leading open and closed LLMs, including models from OpenAI, Anthropic, Google DeepMind, Meta, and DeepSeek, by benchmarking their performance on industry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the International Association of Privacy Professionals (IAPP). Each model was tested using official sample exams in a closed-book setting and compared to IAPP's passing thresholds. Our findings show that several frontier models such as Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the standards for professional human certification - demonstrating substantial expertise in privacy law, technical controls, and AI governance. The results highlight both the strengths and domain-specific gaps of current LLMs and offer practical insights for privacy officers, compliance leads, and technologists assessing the readiness of AI tools for high-stakes data governance roles. This paper provides an overview for professionals navigating the intersection of AI advancement and regulatory risk and establishes a machine benchmark based on human-centric evaluations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring</title>
<link>https://arxiv.org/abs/2508.09085</link>
<guid>https://arxiv.org/abs/2508.09085</guid>
<content:encoded><![CDATA[
arXiv:2508.09085v1 Announce Type: cross 
Abstract: Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system</title>
<link>https://arxiv.org/abs/2508.09090</link>
<guid>https://arxiv.org/abs/2508.09090</guid>
<content:encoded><![CDATA[
arXiv:2508.09090v1 Announce Type: cross 
Abstract: Modeling multi-interests has arisen as a core problem in real-world RS. Current multi-interest retrieval methods pose three major challenges: 1) Interests, typically extracted from predefined external knowledge, are invariant. Failed to dynamically evolve with users' real-time consumption preferences. 2) Online inference typically employs an over-exploited strategy, mainly matching users' existing interests, lacking proactive exploration and discovery of novel and long-tail interests. To address these challenges, we propose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive Retrieval Model via Codebooks). Our contribution is two folds. First, the framework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to construct a discretized interest space. It achieves joint training of the RQ-VAE with the industrial large scale recommendation model, mining behavior-aware interests that can perceive user feedback and evolve dynamically. Secondly, a probabilistic interest module that predicts the probability distribution over the entire dynamic and discrete interest space. This facilitates an efficient "soft-search" strategy during online inference, revolutionizing the retrieval paradigm from "passive matching" to "proactive exploration" and thereby effectively promoting interest discovery. Online A/B tests on an industrial platform with tens of millions daily active users, have achieved substantial gains in business metrics: +0.9% increase in user view duration, +0.4% increase in user page views (PV), and a +22.7% improvement in PV500(new content reaching 500 PVs in 24 hours). Offline evaluations are conducted on open-source Amazon Product datasets. Metrics, such as Recall@K and Normalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent improvement. Both online and offline experiments validate the efficacy and practical value of the proposed method.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Neural Inference</title>
<link>https://arxiv.org/abs/2508.09100</link>
<guid>https://arxiv.org/abs/2508.09100</guid>
<content:encoded><![CDATA[
arXiv:2508.09100v1 Announce Type: cross 
Abstract: Real-world data often appears in diverse, disjoint forms -- with varying schemas, inconsistent semantics, and no fixed feature ordering -- making it challenging to build general-purpose models that can leverage information across datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant Reasoning Engine, a Universal Neural Inference model for semantic reasoning and prediction over heterogeneous structured data. ASPIRE combines a permutation-invariant, set-based Transformer with a semantic grounding module that incorporates natural language descriptions, dataset metadata, and in-context examples to learn cross-dataset feature dependencies. This architecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and support examples, align semantics across disjoint tables, and make predictions for any specified target. Once trained, ASPIRE generalizes to new inference tasks without additional tuning. In addition to delivering strong results across diverse benchmarks, ASPIRE naturally supports cost-aware active feature acquisition in an open-world setting, selecting informative features under test-time budget constraints for an arbitrary unseen dataset. These capabilities position ASPIRE as a step toward truly universal, semantics-aware inference over structured data.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</title>
<link>https://arxiv.org/abs/2508.09131</link>
<guid>https://arxiv.org/abs/2508.09131</guid>
<content:encoded><![CDATA[
arXiv:2508.09131v1 Announce Type: cross 
Abstract: Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.09138</link>
<guid>https://arxiv.org/abs/2508.09138</guid>
<content:encoded><![CDATA[
arXiv:2508.09138v1 Announce Type: cross 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System~2 Reasoning for Human--AI Alignment: Generality and Adaptivity via ARC-AGI</title>
<link>https://arxiv.org/abs/2410.07866</link>
<guid>https://arxiv.org/abs/2410.07866</guid>
<content:encoded><![CDATA[
arXiv:2410.07866v4 Announce Type: replace 
Abstract: Despite their broad applicability, transformer-based models still fall short in System~2 reasoning, lacking the generality and adaptivity needed for human--AI alignment. We examine weaknesses on ARC-AGI tasks, revealing gaps in compositional generalization and novel-rule adaptation, and argue that closing these gaps requires overhauling the reasoning pipeline and its evaluation. We propose three research axes: (1) Symbolic representation pipeline for compositional generality, (2) Interactive feedback-driven reasoning loop for adaptivity, and (3) Test-time task augmentation balancing both qualities. Finally, we demonstrate how ARC-AGI's evaluation suite can be adapted to track progress in symbolic generality, feedback-driven adaptivity, and task-level robustness, thereby guiding future work on robust human--AI alignment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</title>
<link>https://arxiv.org/abs/2412.20977</link>
<guid>https://arxiv.org/abs/2412.20977</guid>
<content:encoded><![CDATA[
arXiv:2412.20977v2 Announce Type: replace 
Abstract: We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects. UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics</title>
<link>https://arxiv.org/abs/2505.19317</link>
<guid>https://arxiv.org/abs/2505.19317</guid>
<content:encoded><![CDATA[
arXiv:2505.19317v2 Announce Type: replace 
Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning</title>
<link>https://arxiv.org/abs/2507.05011</link>
<guid>https://arxiv.org/abs/2507.05011</guid>
<content:encoded><![CDATA[
arXiv:2507.05011v2 Announce Type: replace 
Abstract: Surgical action planning requires predicting future instrument-verb-target triplets for real-time assistance. While teleoperated robotic surgery provides natural expert demonstrations for imitation learning (IL), reinforcement learning (RL) could potentially discover superior strategies through exploration. We present the first comprehensive comparison of IL versus RL for surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and 33.6% next frame prediction mAP with smooth planning degradation to 29.2% at 10-second horizons. We evaluated three RL variants: world model-based RL, direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while direct video RL achieved only 15.9%. Our analysis reveals that distribution matching on expert-annotated test sets systematically favors IL over potentially valid RL policies that differ from training demonstrations. This challenges assumptions about RL superiority in sequential decision making and provides crucial insights for surgical AI development.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Active Goal Recognition</title>
<link>https://arxiv.org/abs/2507.21846</link>
<guid>https://arxiv.org/abs/2507.21846</guid>
<content:encoded><![CDATA[
arXiv:2507.21846v2 Announce Type: replace 
Abstract: In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
arXiv:2508.00414v2 Announce Type: replace 
Abstract: General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Based Multimodal Sensor Data Fusion with Vision Language Models (VLMs) for Real-time Autonomous Vehicle Accident Avoidance</title>
<link>https://arxiv.org/abs/2508.01057</link>
<guid>https://arxiv.org/abs/2508.01057</guid>
<content:encoded><![CDATA[
arXiv:2508.01057v2 Announce Type: replace 
Abstract: Autonomous driving (AD) systems relying solely on onboard sensors may fail to detect distant or obstacle hazards, potentially causing preventable collisions; however, existing transformer-based Vehicle-to-Everything (V2X) approaches, which mitigate AD sensing limitations, either lack effective multimodal fusion and reasoning or struggle to meet real-time performance requirements under complex, high-dimensional traffic conditions. This paper proposes the Real-time Edge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated trajectory optimization framework for AD based on a fine-tuned lightweight Vision-Language Model (VLM). REACT integrates infrastructure-provided hazard alerts with onboard sensor data, capturing intricate surrounding traffic dynamics and vehicle intents through visual embeddings, interpreting precise numerical data from symbolic inputs, and employing contextual reasoning to generate optimized, safety-oriented trajectories. To ensure robust real-time deployment on edge devices, REACT innovatively employs Residual Trajectory Fusion (RTF) design and specialized edge-adaptation strategies to reduce model complexity and improve inference efficiency. Evaluated on the DeepAccident benchmark, REACT achieves state-of-the-art performance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation studies validate the contribution of each input, module, and edge adaptation strategy. These results highlight the effectiveness of lightweight VLMs in enabling real-time cooperative planning on edge platforms and underscore the potential of language-guided contextual reasoning for improving traffic safety and responsiveness.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Dynamic Mask Sparse Attention</title>
<link>https://arxiv.org/abs/2508.02124</link>
<guid>https://arxiv.org/abs/2508.02124</guid>
<content:encoded><![CDATA[
arXiv:2508.02124v2 Announce Type: replace 
Abstract: In large language models, the demand for modeling long contexts is constantly increasing, but the quadratic complexity of the standard self-attention mechanism often becomes a bottleneck. Although existing sparse attention mechanisms have improved efficiency, they may still encounter issues such as static patterns or information loss. We introduce a trainable dynamic mask sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes content-aware and position-aware sparsity. DMA achieves this through two key innovations: First, it dynamically generates content-aware sparse masks from value representations, enabling the model to identify and focus on critical information adaptively. Second, it implements position-aware sparse attention computation that effectively skips unnecessary calculation regions. This dual-sparsity design allows the model to significantly reduce the computational complexity of important information while retaining complete information, achieving an excellent balance between information fidelity and computational efficiency. We have verified the performance of DMA through comprehensive experiments. Comparative studies show that DMA outperforms multi-head attention, sliding window attention, multi-head latent attention, and native sparse attention in terms of perplexity under Chinchilla Scaling Law settings. Moreover, in challenging multi-query associative recall tasks, DMA also demonstrates superior performance and efficiency compared to these methods. Crucially, in the evaluation of a 1.7B parameter model, DMA significantly outperforms multi-head attention in both standard benchmark performance and the challenging needle-in-a-haystack task. These experimental results highlight its capability to balance model efficiency and long-context modeling ability effectively.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity</title>
<link>https://arxiv.org/abs/2204.05138</link>
<guid>https://arxiv.org/abs/2204.05138</guid>
<content:encoded><![CDATA[
arXiv:2204.05138v2 Announce Type: replace-cross 
Abstract: This article presents an artificial intelligence (AI) architecture intended to simulate the iterative updating of the human working memory system. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level representational patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). Representations held in persistent activity are recursively replaced resulting in incremental changes to the content of the working memory system. As this content gradually evolves, successive processing states overlap and are continuous with one another. The present article will explore how this architecture can lead to iterative shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like thought and cognition. Like the human brain, this AI working memory store will be linked to multiple imagery (topographic map) generation systems corresponding to various sensory modalities. As working memory is iteratively updated, the maps created in response will construct sequences of related mental imagery. Thus, neural networks emulating the prefrontal cortex and its reciprocal interactions with early sensory and motor cortex capture the imagery guidance functions of the human brain. This sensory and motor imagery creation, coupled with an iteratively updated working memory store may provide an AI system with the cognitive assets needed to achieve synthetic consciousness or artificial sentience.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BELLA: Black box model Explanations by Local Linear Approximations</title>
<link>https://arxiv.org/abs/2305.11311</link>
<guid>https://arxiv.org/abs/2305.11311</guid>
<content:encoded><![CDATA[
arXiv:2305.11311v3 Announce Type: replace-cross 
Abstract: Understanding the decision-making process of black-box models has become not just a legal requirement, but also an additional way to assess their performance. However, the state of the art post-hoc explanation approaches for regression models rely on synthetic data generation, which introduces uncertainty and can hurt the reliability of the explanations. Furthermore, they tend to produce explanations that apply to only very few data points. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. BELLA maximizes the size of the neighborhood to which the linear model applies so that the explanations are accurate, simple, general, and robust.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video Solution to Enhance Community Safety</title>
<link>https://arxiv.org/abs/2312.02078</link>
<guid>https://arxiv.org/abs/2312.02078</guid>
<content:encoded><![CDATA[
arXiv:2312.02078v3 Announce Type: replace-cross 
Abstract: This article adopts and evaluates an AI-enabled Smart Video Solution (SVS) designed to enhance safety in the real world. The system integrates with existing infrastructure camera networks, leveraging recent advancements in AI for easy adoption. Prioritizing privacy and ethical standards, pose based data is used for downstream AI tasks such as anomaly detection. Cloud-based infrastructure and mobile app are deployed, enabling real-time alerts within communities. The SVS employs innovative data representation and visualization techniques, such as the Occupancy Indicator, Statistical Anomaly Detection, Bird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance public safety. Evaluation of the SVS demonstrates its capacity to convert complex computer vision outputs into actionable insights for stakeholders, community partners, law enforcement, urban planners, and social scientists. This article presents a comprehensive real-world deployment and evaluation of the SVS, implemented in a community college environment across 16 cameras. The system integrates AI-driven visual processing, supported by statistical analysis, database management, cloud communication, and user notifications. Additionally, the article evaluates the end-to-end latency from the moment an AI algorithm detects anomalous behavior in real-time at the camera level to the time stakeholders receive a notification. The results demonstrate the system's robustness, effectively managing 16 CCTV cameras with a consistent throughput of 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end latency of 26.76 seconds between anomaly detection and alert issuance.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI Inference Workflows</title>
<link>https://arxiv.org/abs/2312.11488</link>
<guid>https://arxiv.org/abs/2312.11488</guid>
<content:encoded><![CDATA[
arXiv:2312.11488v2 Announce Type: replace-cross 
Abstract: AI inference workflows are typically structured as a pipeline or graph of AI programs triggered by events. As events occur, the AIs perform inference or classification tasks under time pressure to respond or take some action. Standard techniques that reduce latency in other streaming settings (such as caching and optimization-driven scheduling) are of limited value because AI data access patterns (models, databases) change depending on the triggering event: a significant departure from traditional streaming. In this work, we propose a novel affinity grouping mechanism that makes it easier for developers to express application-specific data access correlations, enabling coordinated management of data objects in server clusters hosting streaming inference tasks. Our proposals are thus complementary to other approaches such as caching and scheduling. Experiments confirm the limitations of standard techniques, while showing that the proposed mechanism is able to maintain significantly lower latency as workload and scale-out increase, and yet requires only minor code changes.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIOS: LLM Agent Operating System</title>
<link>https://arxiv.org/abs/2403.16971</link>
<guid>https://arxiv.org/abs/2403.16971</guid>
<content:encoded><![CDATA[
arXiv:2403.16971v5 Announce Type: replace-cross 
Abstract: LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) for runtime agents. To enhance usability, AIOS also includes an AIOS SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multidimensional Adaptive Coefficient for Inference Trajectory Optimization in Flow and Diffusion</title>
<link>https://arxiv.org/abs/2404.14161</link>
<guid>https://arxiv.org/abs/2404.14161</guid>
<content:encoded><![CDATA[
arXiv:2404.14161v4 Announce Type: replace-cross 
Abstract: Flow and diffusion models have demonstrated strong performance and training stability across various tasks but lack two critical properties of simulation-based methods: freedom of dimensionality and adaptability to different inference trajectories. To address this limitation, we propose the Multidimensional Adaptive Coefficient (MAC), a plug-in module for flow and diffusion models that extends conventional unidimensional coefficients to multidimensional ones and enables inference trajectory-wise adaptation. MAC is trained via simulation-based feedback through adversarial refinement. Empirical results across diverse frameworks and datasets demonstrate that MAC enhances generative quality with high training efficiency. Consequently, our work offers a new perspective on inference trajectory optimality, encouraging future research to move beyond vector field design and to leverage training-efficient, simulation-based optimization.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</title>
<link>https://arxiv.org/abs/2406.16258</link>
<guid>https://arxiv.org/abs/2406.16258</guid>
<content:encoded><![CDATA[
arXiv:2406.16258v3 Announce Type: replace-cross 
Abstract: Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce MEReQ (Maximum-Entropy Residual-Q Inverse Reinforcement Learning), designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion</title>
<link>https://arxiv.org/abs/2407.12899</link>
<guid>https://arxiv.org/abs/2407.12899</guid>
<content:encoded><![CDATA[
arXiv:2407.12899v3 Announce Type: replace-cross 
Abstract: Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each scene's subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules ensure appearance and semantic consistency with reference images and text, respectively. Both modules employ masking mechanisms to prevent subject blending. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at https://dream-xyz.github.io/dreamstory.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge</title>
<link>https://arxiv.org/abs/2408.02865</link>
<guid>https://arxiv.org/abs/2408.02865</guid>
<content:encoded><![CDATA[
arXiv:2408.02865v2 Announce Type: replace-cross 
Abstract: The need for improved diagnostic methods in ophthalmology is acute, especially in the underdeveloped regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational aid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and underrepresented ophthalmic conditions. VisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and understanding of disease mechanisms. The source code is at https://github.com/HUANGLIZI/VisionUnite.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OE3DIS: Open-Ended 3D Point Cloud Instance Segmentation</title>
<link>https://arxiv.org/abs/2408.11747</link>
<guid>https://arxiv.org/abs/2408.11747</guid>
<content:encoded><![CDATA[
arXiv:2408.11747v2 Announce Type: replace-cross 
Abstract: Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recently demonstrated their ability to generalize to unseen objects. However, these methods still depend on predefined class names during testing, restricting the autonomy of agents. To mitigate this constraint, we propose a novel problem termed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates the necessity for predefined class names during testing. Moreover, we contribute a comprehensive set of strong baselines, derived from OV-3DIS approaches and leveraging 2D Multimodal Large Language Models. To assess the performance of our OE-3DIS system, we introduce a novel Open-Ended score, evaluating both the semantic and geometric quality of predicted masks and their associated class names, alongside the standard AP score. Our approach demonstrates significant performance improvements over the baselines on the ScanNet200 and ScanNet++ datasets. Remarkably, our method surpasses the performance of Open3DIS, the current state-of-the-art method in OV-3DIS, even in the absence of ground-truth object class names.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Return Prediction for Mean-Variance Portfolio Selection: How Decision-Focused Learning Shapes Forecasting Models</title>
<link>https://arxiv.org/abs/2409.09684</link>
<guid>https://arxiv.org/abs/2409.09684</guid>
<content:encoded><![CDATA[
arXiv:2409.09684v2 Announce Type: replace-cross 
Abstract: Markowitz laid the foundation of portfolio theory through the mean-variance optimization (MVO) framework. However, the effectiveness of MVO is contingent on the precise estimation of expected returns, variances, and covariances of asset returns, which are typically uncertain. Machine learning models are becoming useful in estimating uncertain parameters, and such models are trained to minimize prediction errors, such as mean squared errors (MSE), which treat prediction errors uniformly across assets. Recent studies have pointed out that this approach would lead to suboptimal decisions and proposed Decision-Focused Learning (DFL) as a solution, integrating prediction and optimization to improve decision-making outcomes. While studies have shown DFL's potential to enhance portfolio performance, the detailed mechanisms of how DFL modifies prediction models for MVO remain unexplored. This study investigates how DFL adjusts stock return prediction models to optimize decisions in MVO. Theoretically, we show that DFL's gradient can be interpreted as tilting the MSE-based prediction errors by the inverse covariance matrix, effectively incorporating inter-asset correlations into the learning process, while MSE treats each asset's error independently. This tilting mechanism leads to systematic prediction biases where DFL overestimates returns for assets included in portfolios while underestimating excluded assets. Our findings reveal why DFL achieves superior portfolio performance despite higher prediction errors. The strategic biases are features, not flaws.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DFacePolicy: Audio-Driven 3D Facial Animation Based on Action Control</title>
<link>https://arxiv.org/abs/2409.10848</link>
<guid>https://arxiv.org/abs/2409.10848</guid>
<content:encoded><![CDATA[
arXiv:2409.10848v2 Announce Type: replace-cross 
Abstract: Audio-driven 3D facial animation has achieved significant progress in both research and applications. While recent baselines struggle to generate natural and continuous facial movements due to their frame-by-frame vertex generation approach, we propose 3DFacePolicy, a pioneer work that introduces a novel definition of vertex trajectory changes across consecutive frames through the concept of "action". By predicting action sequences for each vertex that encode frame-to-frame movements, we reformulate vertex generation approach into an action-based control paradigm. Specifically, we leverage a robotic control mechanism, diffusion policy, to predict action sequences conditioned on both audio and vertex states. Extensive experiments on VOCASET and BIWI datasets demonstrate that our approach significantly outperforms state-of-the-art methods and is particularly expert in dynamic, expressive and naturally smooth facial animations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning</title>
<link>https://arxiv.org/abs/2409.11676</link>
<guid>https://arxiv.org/abs/2409.11676</guid>
<content:encoded><![CDATA[
arXiv:2409.11676v2 Announce Type: replace-cross 
Abstract: The intricate nature of real-world driving environments, characterized by dynamic and diverse interactions among multiple vehicles and their possible future states, presents considerable challenges in accurately predicting the motion states of vehicles and handling the uncertainty inherent in the predictions. Addressing these challenges requires comprehensive modeling and reasoning to capture the implicit relations among vehicles and the corresponding diverse behaviors. This research introduces an integrated framework for autonomous vehicles (AVs) motion prediction to address these complexities, utilizing a novel Relational Hypergraph Interaction-informed Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational reasoning by integrating a multi-scale hypergraph neural network to model group-wise interactions among multiple vehicles and their multi-modal driving behaviors, thereby enhancing motion prediction accuracy and reliability. Experimental validation using real-world datasets demonstrates the superior performance of this framework in improving predictive accuracy and fostering socially aware automated driving in dynamic traffic scenarios. The source code is publicly available at https://github.com/keshuw95/RHINO-Hypergraph-Motion-Generation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Generalization of Vision-Based RL Without Data Augmentation</title>
<link>https://arxiv.org/abs/2410.07441</link>
<guid>https://arxiv.org/abs/2410.07441</guid>
<content:encoded><![CDATA[
arXiv:2410.07441v2 Announce Type: replace-cross 
Abstract: Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variations without relying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.17971</link>
<guid>https://arxiv.org/abs/2410.17971</guid>
<content:encoded><![CDATA[
arXiv:2410.17971v2 Announce Type: replace-cross 
Abstract: Spectrum access is an essential problem in device-to-device (D2D) communications. However, with the recent growth in the number of mobile devices, the wireless spectrum is becoming scarce, resulting in low spectral efficiency for D2D communications. To address this problem, this paper aims to integrate the ambient backscatter communication technology into D2D devices to allow them to backscatter ambient RF signals to transmit their data when the shared spectrum is occupied by mobile users. To obtain the optimal spectrum access policy, i.e., stay idle or access the shared spectrum and perform active transmissions or backscattering ambient RF signals for transmissions, to maximize the average throughput for D2D users, deep reinforcement learning (DRL) can be adopted. However, DRL-based solutions may require long training time due to the curse of dimensionality issue as well as complex deep neural network architectures. For that, we develop a novel quantum reinforcement learning (RL) algorithm that can achieve a faster convergence rate with fewer training parameters compared to DRL thanks to the quantum superposition and quantum entanglement principles. Specifically, instead of using conventional deep neural networks, the proposed quantum RL algorithm uses a parametrized quantum circuit to approximate an optimal policy. Extensive simulations then demonstrate that the proposed solution not only can significantly improve the average throughput of D2D devices when the shared spectrum is busy but also can achieve much better performance in terms of convergence rate and learning complexity compared to existing DRL-based methods.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Marmoset Vocal Patterns with a Masked Autoencoder for Robust Call Segmentation, Classification, and Caller Identification</title>
<link>https://arxiv.org/abs/2410.23279</link>
<guid>https://arxiv.org/abs/2410.23279</guid>
<content:encoded><![CDATA[
arXiv:2410.23279v4 Announce Type: replace-cross 
Abstract: The marmoset, a highly vocal primate, is a key model for studying social-communicative behavior. Unlike human speech, marmoset vocalizations are less structured, highly variable, and recorded in noisy, low-resource conditions. Learning marmoset communication requires joint call segmentation, classification, and caller identification -- challenging domain tasks. Previous CNNs handle local patterns but struggle with long-range temporal structure. We applied Transformers using self-attention for global dependencies. However, Transformers show overfitting and instability on small, noisy annotated datasets. To address this, we pretrain Transformers with MAE -- a self-supervised method reconstructing masked segments from hundreds of hours of unannotated marmoset recordings. The pretraining improved stability and generalization. Results show MAE-pretrained Transformers outperform CNNs, demonstrating modern self-supervised architectures effectively model low-resource non-human vocal communication.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health</title>
<link>https://arxiv.org/abs/2411.02594</link>
<guid>https://arxiv.org/abs/2411.02594</guid>
<content:encoded><![CDATA[
arXiv:2411.02594v2 Announce Type: replace-cross 
Abstract: Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Informed Deep Neural Networks for Power Flow Analysis</title>
<link>https://arxiv.org/abs/2412.02659</link>
<guid>https://arxiv.org/abs/2412.02659</guid>
<content:encoded><![CDATA[
arXiv:2412.02659v3 Announce Type: replace-cross 
Abstract: This study introduces PINN4PF, an end-to-end deep learning architecture for power flow (PF) analysis that effectively captures the nonlinear dynamics of large-scale modern power systems. The proposed neural network (NN) architecture consists of two important advancements in the training pipeline: (A) a double-head feed-forward NN that aligns with PF analysis, including an activation function that adjusts to the net active and reactive power injections patterns, and (B) a physics-based loss function that partially incorporates power system topology information through a novel hidden function. The effectiveness of the proposed architecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus test systems and is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). The comparison is based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities (specifically line current, line active power, and line reactive power), and (v) scalability. Results demonstrate that PINN4PF outperforms both baselines across all test systems by up to two orders of magnitude not only in terms of direct criteria, e.g., generalization ability, but also in terms of approximating derived physical quantities.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chemist-aligned retrosynthesis by ensembling diverse inductive bias models</title>
<link>https://arxiv.org/abs/2412.05269</link>
<guid>https://arxiv.org/abs/2412.05269</guid>
<content:encoded><![CDATA[
arXiv:2412.05269v2 Announce Type: replace-cross 
Abstract: Chemical synthesis remains a critical bottleneck in the discovery and manufacture of functional small molecules. AI-based synthesis planning models could be a potential remedy to find effective syntheses, and have made progress in recent years. However, they still struggle with less frequent, yet critical reactions for synthetic strategy, as well as hallucinated, incorrect predictions. This hampers multi-step search algorithms that rely on models, and leads to misalignment with chemists' expectations. Here we propose RetroChimera: a frontier retrosynthesis model, built upon two newly developed components with complementary inductive biases, which we fuse together using a new framework for integrating predictions from multiple sources via a learning-based ensembling strategy. Through experiments across several orders of magnitude in data scale and splitting strategy, we show RetroChimera outperforms all major models by a large margin, demonstrating robustness outside the training data, as well as for the first time the ability to learn from even a very small number of examples per reaction class. Moreover, industrial organic chemists prefer predictions from RetroChimera over the reactions it was trained on in terms of quality, revealing high levels of alignment. Finally, we demonstrate zero-shot transfer to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our ensembling framework unlocks, we anticipate further acceleration in the development of even more accurate models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13983</link>
<guid>https://arxiv.org/abs/2501.13983</guid>
<content:encoded><![CDATA[
arXiv:2501.13983v5 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora, the problem of data contamination is becoming increasingly serious, and there is a risk that static evaluation benchmarks overestimate the performance of LLMs. To address this, this paper proposes a dynamic data evaluation method called AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts knowledge points and main ideas from static datasets to achieve dynamic alignment with the core content of static benchmarks, and by avoiding direct reliance on static datasets, it inherently reduces the risk of data contamination from the source. It then obtains background information through online searches to generate detailed descriptions of the knowledge points. Finally, it designs questions based on Bloom's cognitive hierarchy across six dimensions-remembering, understanding, applying, analyzing, evaluating, and creating to enable multi-level cognitive assessment. Additionally, AdEval controls the complexity of dynamically generated datasets through iterative question reconstruction. Experimental results on multiple datasets show that AdEval effectively alleviates the impact of data contamination on evaluation results, solves the problems of insufficient complexity control and single-dimensional evaluation, and improves the fairness, reliability and diversity of LLMs evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding-based Regression</title>
<link>https://arxiv.org/abs/2501.19383</link>
<guid>https://arxiv.org/abs/2501.19383</guid>
<content:encoded><![CDATA[
arXiv:2501.19383v2 Announce Type: replace-cross 
Abstract: Language models have recently been shown capable of performing regression wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal sequence decoding models as numeric regression heads given any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoder-based heads are as performant as standard pointwise heads when benchmarked over standard regression tasks, while being flexible enough to capture smooth numeric distributions, such as in the task of density estimation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning</title>
<link>https://arxiv.org/abs/2502.08577</link>
<guid>https://arxiv.org/abs/2502.08577</guid>
<content:encoded><![CDATA[
arXiv:2502.08577v2 Announce Type: replace-cross 
Abstract: In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forget the Data and Fine-Tuning! Just Fold the Network to Compress</title>
<link>https://arxiv.org/abs/2502.10216</link>
<guid>https://arxiv.org/abs/2502.10216</guid>
<content:encoded><![CDATA[
arXiv:2502.10216v2 Announce Type: replace-cross 
Abstract: We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</title>
<link>https://arxiv.org/abs/2502.12207</link>
<guid>https://arxiv.org/abs/2502.12207</guid>
<content:encoded><![CDATA[
arXiv:2502.12207v3 Announce Type: replace-cross 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://github.com/LMBTough/PAR
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Emotion Annotation in Facial Images Using Large Multimodal Models: Benchmarking and Prospects for Multi-Class, Multi-Frame Approaches</title>
<link>https://arxiv.org/abs/2502.12454</link>
<guid>https://arxiv.org/abs/2502.12454</guid>
<content:encoded><![CDATA[
arXiv:2502.12454v2 Announce Type: replace-cross 
Abstract: This study investigates the feasibility and performance of using large multimodal models (LMMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad," "Surprise"), the LMM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LMMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LMMs in complex multimodal environments.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions</title>
<link>https://arxiv.org/abs/2502.13135</link>
<guid>https://arxiv.org/abs/2502.13135</guid>
<content:encoded><![CDATA[
arXiv:2502.13135v3 Announce Type: replace-cross 
Abstract: We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoP: Robust LLM Inference via Evolutionary Pruning</title>
<link>https://arxiv.org/abs/2502.14910</link>
<guid>https://arxiv.org/abs/2502.14910</guid>
<content:encoded><![CDATA[
arXiv:2502.14910v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing model pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model.
  To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing model pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization</title>
<link>https://arxiv.org/abs/2503.02303</link>
<guid>https://arxiv.org/abs/2503.02303</guid>
<content:encoded><![CDATA[
arXiv:2503.02303v3 Announce Type: replace-cross 
Abstract: Many tasks require flexibly modifying perception and behavior based on current goals. Humans can retrieve episodic memories from days to years ago, using them to contextualize and generalize behaviors across novel but structurally related situations. The brain's ability to control episodic memories based on task demands is often attributed to interactions between the prefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement learning model that incorporates a PFC-HPC interaction mechanism for goal-directed generalization. In our model, the PFC learns to generate query-key representations to encode and retrieve goal-relevant episodic memories, modulating HPC memories top-down based on current task demands. Moreover, the PFC adapts its encoding and retrieval strategies dynamically when faced with multiple goals presented in a blocked, rather than interleaved, manner. Our results show that: (1) combining working memory with selectively retrieved episodic memory allows transfer of decisions among similar environments or situations, (2) top-down control from PFC over HPC improves learning of arbitrary structural associations between events for generalization to novel environments compared to a bottom-up sensory-driven approach, and (3) the PFC encodes generalizable representations during both encoding and retrieval of goal-relevant memories, whereas the HPC exhibits event-specific representations. Together, these findings highlight the importance of goal-directed prefrontal control over hippocampal episodic memory for decision-making in novel situations and suggest a computational mechanism by which PFC-HPC interactions enable flexible behavior.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation</title>
<link>https://arxiv.org/abs/2503.07050</link>
<guid>https://arxiv.org/abs/2503.07050</guid>
<content:encoded><![CDATA[
arXiv:2503.07050v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion architectures. We propose TIDE-Temporal-aware sparse autoencoders for Interpretable Diffusion transformErs-a framework designed to extract sparse, interpretable activation features across timesteps in DiTs. TIDE effectively captures temporally-varying representations and reveals that DiTs naturally learn hierarchical semantics (e.g., 3D structure, object class, and fine-grained concepts) during large-scale pretraining. Experiments show that TIDE enhances interpretability and controllability while maintaining reasonable generation quality, enabling applications such as safe image editing and style transfer.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</title>
<link>https://arxiv.org/abs/2503.10331</link>
<guid>https://arxiv.org/abs/2503.10331</guid>
<content:encoded><![CDATA[
arXiv:2503.10331v2 Announce Type: replace-cross 
Abstract: Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opioid Named Entity Recognition (ONER-2025) from Reddit</title>
<link>https://arxiv.org/abs/2504.00027</link>
<guid>https://arxiv.org/abs/2504.00027</guid>
<content:encoded><![CDATA[
arXiv:2504.00027v4 Announce Type: replace-cross 
Abstract: The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</title>
<link>https://arxiv.org/abs/2504.00043</link>
<guid>https://arxiv.org/abs/2504.00043</guid>
<content:encoded><![CDATA[
arXiv:2504.00043v2 Announce Type: replace-cross 
Abstract: Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles -- a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats (text and image), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings highlight limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot</title>
<link>https://arxiv.org/abs/2504.04299</link>
<guid>https://arxiv.org/abs/2504.04299</guid>
<content:encoded><![CDATA[
arXiv:2504.04299v2 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Computation Pruning for the Forgetting Transformer</title>
<link>https://arxiv.org/abs/2504.06949</link>
<guid>https://arxiv.org/abs/2504.06949</guid>
<content:encoded><![CDATA[
arXiv:2504.06949v2 Announce Type: replace-cross 
Abstract: The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. In particular, our method performs provably safe pruning via a dynamically set pruning threshold that guarantees the pruned attention weights are negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs and memory accesses in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 50% to 70% reduction in attention runtime (or a 2-3$\times$ speedup) and a roughly 10% to 40% increase in end-to-end training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatBench: From Static Benchmarks to Human-AI Evaluation</title>
<link>https://arxiv.org/abs/2504.07114</link>
<guid>https://arxiv.org/abs/2504.07114</guid>
<content:encoded><![CDATA[
arXiv:2504.07114v2 Announce Type: replace-cross 
Abstract: With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning</title>
<link>https://arxiv.org/abs/2504.08713</link>
<guid>https://arxiv.org/abs/2504.08713</guid>
<content:encoded><![CDATA[
arXiv:2504.08713v5 Announce Type: replace-cross 
Abstract: Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students</title>
<link>https://arxiv.org/abs/2504.10961</link>
<guid>https://arxiv.org/abs/2504.10961</guid>
<content:encoded><![CDATA[
arXiv:2504.10961v2 Announce Type: replace-cross 
Abstract: As generative AI models, particularly large language models (LLMs), transform educational feedback practices in higher education (HE) contexts, understanding students' perceptions of different sources of feedback becomes crucial for their effective implementation and adoption. This study addresses a critical gap by comparing undergraduate students' trust in LLM, human, and human-AI co-produced feedback in their authentic HE context. More specifically, through a within-subject experimental design involving 91 participants, we investigated factors that predict students' ability to distinguish between feedback types, their perceptions of feedback quality, and potential biases related to the source of feedback. Findings revealed that when the source was blinded, students generally preferred AI and co-produced feedback over human feedback regarding perceived usefulness and objectivity. However, they presented a strong bias against AI when the source of feedback was disclosed. In addition, only AI feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational AI experience improved students' ability to identify LLM-generated feedback and increased their trust in all types of feedback. More years of students' experience using AI for general purposes were associated with lower perceived usefulness and credibility of feedback. These insights offer substantial evidence of the importance of source credibility and the need to enhance both feedback literacy and AI literacy to mitigate bias in student perceptions for AI-generated feedback to be adopted and impact education.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIE: Semantic and Structural Post-Training of Image Editing Diffusion Models with AI feedback</title>
<link>https://arxiv.org/abs/2504.12833</link>
<guid>https://arxiv.org/abs/2504.12833</guid>
<content:encoded><![CDATA[
arXiv:2504.12833v2 Announce Type: replace-cross 
Abstract: This paper presents SPIE: a novel approach for semantic and structural post-training of instruction-based image editing diffusion models, addressing key challenges in alignment with user prompts and consistency with input images. We introduce an online reinforcement learning framework that aligns the diffusion model with human preferences without relying on extensive human annotations or curating a large dataset. Our method significantly improves the alignment with instructions and realism in two ways. First, SPIE captures fine nuances in the desired edit by leveraging a visual prompt, enabling detailed control over visual edits without lengthy textual prompts. Second, it achieves precise and structurally coherent modifications in complex scenes while maintaining high fidelity in instruction-irrelevant areas. This approach simplifies users' efforts to achieve highly specific edits, requiring only 5 reference images depicting a certain concept for training. Experimental results demonstrate that SPIE can perform intricate edits in complex scenes, after just 10 training steps. Finally, we showcase the versatility of our method by applying it to robotics, where targeted image edits enhance the visual realism of simulated environments, which improves their utility as proxy for real-world settings.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation with Conflicting Evidence</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
arXiv:2504.13079v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU</title>
<link>https://arxiv.org/abs/2504.17028</link>
<guid>https://arxiv.org/abs/2504.17028</guid>
<content:encoded><![CDATA[
arXiv:2504.17028v3 Announce Type: replace-cross 
Abstract: This paper demonstrates the feasibility of democratizing AI-driven global weather forecasting models among university research groups by leveraging Graphics Processing Units (GPUs) and freely available AI models, such as NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network for weather prediction and is trained on a 73-channel subset of the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset at single levels and different pressure levels. Although the training specifications for FourCastNetv2 are not released to the public, the training documentation of the model's first generation, FourCastNet, is available to all users. The training had 64 A100 GPUs and took 16 hours to complete. Although NVIDIA's models offer significant reductions in both time and cost compared to traditional Numerical Weather Prediction (NWP), reproducing published forecasting results presents ongoing challenges for resource-constrained university research groups with limited GPU availability. We demonstrate both (i) leveraging FourCastNetv2 to create predictions through the designated application programming interface (API) and (ii) utilizing NVIDIA hardware to train the original FourCastNet model. Further, this paper demonstrates the capabilities and limitations of NVIDIA A100's for resource-limited research groups in universities. We also explore data management, training efficiency, and model validation, highlighting the advantages and challenges of using limited high-performance computing resources. Consequently, this paper and its corresponding GitHub materials may serve as an initial guide for other university research groups and courses related to machine learning, climate science, and data science to develop research and education programs on AI weather forecasting, and hence help democratize the AI NWP in the digital economy.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence</title>
<link>https://arxiv.org/abs/2504.17703</link>
<guid>https://arxiv.org/abs/2504.17703</guid>
<content:encoded><![CDATA[
arXiv:2504.17703v3 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field of distributed machine learning, enabling multiple clients such as mobile devices, edge nodes, or organizations to collaboratively train a shared global model without the need to centralize sensitive data. This decentralized approach addresses growing concerns around data privacy, security, and regulatory compliance, making it particularly attractive in domains such as healthcare, finance, and smart IoT systems. This survey provides a concise yet comprehensive overview of Federated Learning, beginning with its core architecture and communication protocol. We discuss the standard FL lifecycle, including local training, model aggregation, and global updates. A particular emphasis is placed on key technical challenges such as handling non-IID (non-independent and identically distributed) data, mitigating system and hardware heterogeneity, reducing communication overhead, and ensuring privacy through mechanisms like differential privacy and secure aggregation. Furthermore, we examine emerging trends in FL research, including personalized FL, cross-device versus cross-silo settings, and integration with other paradigms such as reinforcement learning and quantum computing. We also highlight real-world applications and summarize benchmark datasets and evaluation metrics commonly used in FL research. Finally, we outline open research problems and future directions to guide the development of scalable, efficient, and trustworthy FL systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mj\"olnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density</title>
<link>https://arxiv.org/abs/2504.19822</link>
<guid>https://arxiv.org/abs/2504.19822</guid>
<content:encoded><![CDATA[
arXiv:2504.19822v3 Announce Type: replace-cross 
Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mj\"olnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mj\"olnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs).
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
<link>https://arxiv.org/abs/2505.01821</link>
<guid>https://arxiv.org/abs/2505.01821</guid>
<content:encoded><![CDATA[
arXiv:2505.01821v3 Announce Type: replace-cross 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay</title>
<link>https://arxiv.org/abs/2505.04209</link>
<guid>https://arxiv.org/abs/2505.04209</guid>
<content:encoded><![CDATA[
arXiv:2505.04209v3 Announce Type: replace-cross 
Abstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2506.08835</link>
<guid>https://arxiv.org/abs/2506.08835</guid>
<content:encoded><![CDATA[
arXiv:2506.08835v2 Announce Type: replace-cross 
Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual content generation raises concerns about their ability to accurately represent diverse cultural contexts -- where missed cues can stereotype communities and undermine usability. In this work, we present the first study to systematically quantify the alignment of T2I models and evaluation metrics with respect to both explicit (stated) as well as implicit (unstated, implied by the prompt's cultural context) cultural expectations. To this end, we introduce CulturalFrames, a novel benchmark designed for rigorous human evaluation of cultural representation in visual generations. Spanning 10 countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts, 3637 corresponding images generated by 4 state-of-the-art T2I models, and over 10k detailed human annotations. We find that across models and countries, cultural expectations are missed an average of 44% of the time. Among these failures, explicit expectations are missed at a surprisingly high average rate of 68%, while implicit expectation failures are also significant, averaging 49%. Furthermore, we show that existing T2I evaluation metrics correlate poorly with human judgments of cultural alignment, irrespective of their internal reasoning. Collectively, our findings expose critical gaps, provide a concrete testbed, and outline actionable directions for developing culturally informed T2I models and metrics that improve global usability.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saturation Self-Organizing Map</title>
<link>https://arxiv.org/abs/2506.10680</link>
<guid>https://arxiv.org/abs/2506.10680</guid>
<content:encoded><![CDATA[
arXiv:2506.10680v3 Announce Type: replace-cross 
Abstract: Continual learning poses a fundamental challenge for neural systems, which often suffer from catastrophic forgetting when exposed to sequential tasks. Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are not immune to this issue. In this paper, we introduce Saturation Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve knowledge retention in continual learning scenarios. SatSOM incorporates a novel saturation mechanism that gradually reduces the learning rate and neighborhood radius of neurons as they accumulate information. This effectively freezes well-trained neurons and redirects learning to underutilized areas of the map.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Document and Template Clustering using Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2506.12116</link>
<guid>https://arxiv.org/abs/2506.12116</guid>
<content:encoded><![CDATA[
arXiv:2506.12116v2 Announce Type: replace-cross 
Abstract: This paper investigates a novel approach to unsupervised document clustering by leveraging multimodal embeddings as input to clustering algorithms such as $k$-Means, DBSCAN, a combination of HDBSCAN and $k$-NN, and BIRCH. Our method aims to achieve a finer-grained document understanding by not only grouping documents at the type level (e.g., invoices, purchase orders), but also distinguishing between different templates within the same document category. This is achieved by using embeddings that capture textual content, layout information, and visual features of documents. We evaluated the effectiveness of this approach using embeddings generated by several state-of-the-art pre-trained multimodal models, including SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, ColPali, Gemma3, and InternVL3. Our findings demonstrate the potential of multimodal embeddings to significantly enhance document clustering, offering benefits for various applications in intelligent document processing, document layout analysis, and unsupervised document classification. This work provides valuable insight into the advantages and limitations of different multimodal models for this task and opens new avenues for future research to understand and organize document collections.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</title>
<link>https://arxiv.org/abs/2506.14412</link>
<guid>https://arxiv.org/abs/2506.14412</guid>
<content:encoded><![CDATA[
arXiv:2506.14412v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55 in the non-human evaluation, placing it overall in third place in the SIGIR 2025 LiveRAG Challenge.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?</title>
<link>https://arxiv.org/abs/2506.14805</link>
<guid>https://arxiv.org/abs/2506.14805</guid>
<content:encoded><![CDATA[
arXiv:2506.14805v2 Announce Type: replace-cross 
Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies</title>
<link>https://arxiv.org/abs/2506.15716</link>
<guid>https://arxiv.org/abs/2506.15716</guid>
<content:encoded><![CDATA[
arXiv:2506.15716v2 Announce Type: replace-cross 
Abstract: Citizens' assemblies are an increasingly influential form of deliberative democracy, where randomly selected people discuss policy questions. The legitimacy of these assemblies hinges on their representation of the broader population, but participant dropout often leads to an unbalanced composition. In practice, dropouts are replaced by preselected alternates, but existing methods do not address how to choose these alternates. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. Our theoretical bounds provide guarantees on sample complexity (with implications for computational efficiency) and on loss due to dropout probability mis-estimation. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Completion Learning for Language Models</title>
<link>https://arxiv.org/abs/2507.20252</link>
<guid>https://arxiv.org/abs/2507.20252</guid>
<content:encoded><![CDATA[
arXiv:2507.20252v3 Announce Type: replace-cross 
Abstract: Current language model training paradigms typically terminate learning upon reaching the end-of-sequence () token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System</title>
<link>https://arxiv.org/abs/2507.23261</link>
<guid>https://arxiv.org/abs/2507.23261</guid>
<content:encoded><![CDATA[
arXiv:2507.23261v2 Announce Type: replace-cross 
Abstract: Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance. To address these limitations, we propose DynaSwarm, a dynamic framework that enhances LLM-based MAS through two key innovations: (1) an actor-critic reinforcement learning (A2C) mechanism to optimize graph structures with improved stability over prior RL methods, and (2) a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. (c) We propose to fine-tune the demonstration retriever to fully exploit the power of in-context learning (ICL). Extensive experiments on question answering, mathematical reasoning, and coding tasks demonstrate that DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones. Our findings highlight the importance of sample-aware structural flexibility in LLM MAS designs.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title>
<link>https://arxiv.org/abs/2507.23465</link>
<guid>https://arxiv.org/abs/2507.23465</guid>
<content:encoded><![CDATA[
arXiv:2507.23465v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions</title>
<link>https://arxiv.org/abs/2508.01687</link>
<guid>https://arxiv.org/abs/2508.01687</guid>
<content:encoded><![CDATA[
arXiv:2508.01687v2 Announce Type: replace-cross 
Abstract: Explaining machine learning (ML) models for time series (TS) classification remains challenging due to the difficulty of interpreting raw time series and the high dimensionality of the input space. We introduce PHAR-Post-hoc Attribution Rules-a unified framework that transforms numeric feature attributions from post-hoc, instance-wise explainers (e.g., LIME, SHAP) into structured, human-readable rules. These rules define interpretable intervals that indicate where and when key decision boundaries occur, enhancing model transparency. PHAR performs comparably to native rule-based methods, such as Anchor, while scaling more efficiently to long TS sequences and achieving broader instance coverage. A dedicated rule fusion step consolidates rule sets using strategies like weighted selection and lasso-based refinement, balancing key quality metrics: coverage, confidence, and simplicity. This fusion ensures each instance receives a concise and unambiguous rule, improving both explanation fidelity and consistency. We further introduce visualization techniques to illustrate specificity-generalization trade-offs in the derived rules. PHAR resolves conflicting and overlapping explanations-a common effect of the Rashomon phenomenon-into coherent, domain-adaptable insights. Comprehensive experiments on UCR/UEA Time Series Classification Archive demonstrate that PHAR improves interpretability, decision transparency, and practical applicability for TS classification tasks.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.04276</link>
<guid>https://arxiv.org/abs/2508.04276</guid>
<content:encoded><![CDATA[
arXiv:2508.04276v2 Announce Type: replace-cross 
Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as a promising paradigm for enhancing large language models (LLMs) by converting raw text into structured knowledge graphs, improving both accuracy and explainability. However, GraphRAG relies on LLMs to extract knowledge from raw text during graph construction, and this process can be maliciously manipulated to implant misleading information. Targeting this attack surface, we propose two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a few words in the source text can significantly change the constructed graph, poison the GraphRAG, and severely mislead downstream reasoning. The first attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate vulnerable nodes in the generated graphs and rewrites the corresponding narratives with LLMs, achieving precise control over specific question-answering (QA) outcomes with a success rate of 93.1\%, while keeping the poisoned text fluent and natural. The second attack, named Universal KPA (UKPA), exploits linguistic cues such as pronouns and dependency relations to disrupt the structural integrity of the generated graph by altering globally influential words. With fewer than 0.05\% of full text modified, the QA accuracy collapses from 95\% to 50\%. Furthermore, experiments show that state-of-the-art defense methods fail to detect these attacks, highlighting that securing GraphRAG pipelines against knowledge poisoning remains largely unexplored.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</title>
<link>https://arxiv.org/abs/2508.04349</link>
<guid>https://arxiv.org/abs/2508.04349</guid>
<content:encoded><![CDATA[
arXiv:2508.04349v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference</title>
<link>https://arxiv.org/abs/2508.04586</link>
<guid>https://arxiv.org/abs/2508.04586</guid>
<content:encoded><![CDATA[
arXiv:2508.04586v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game Reasoning Arena: A Framework and Benchmark for Assessing Reasoning Capabilites of Large Language Models via Game Play</title>
<link>https://arxiv.org/abs/2508.03368</link>
<guid>https://arxiv.org/abs/2508.03368</guid>
<content:encoded><![CDATA[
<div> Keywords: Game Reasoning Arena, large language models, board games, strategic decision making, empirical evaluation<br />
Summary: The Game Reasoning Arena library offers a framework to assess the decision-making capabilities of large language models (LLMs) using strategic board games in the Google OpenSpiel library. It allows for comparing LLM-based agents with random, heuristic, and reinforcement learning agents in various game scenarios. The framework supports different agent types and wraps multiple board and matrix games. It provides API access to models through liteLLM, local model deployment via vLLM, and distributed execution via Ray. The library structure, key features, and motivation are outlined, emphasizing its contribution to evaluating the reasoning and game theoretic behavior of LLMs. <div>
arXiv:2508.03368v2 Announce Type: replace 
Abstract: The Game Reasoning Arena library provides a framework for evaluating the decision making abilities of large language models (LLMs) through strategic board games implemented in Google OpenSpiel library. The framework enables systematic comparisons between LLM based agents and other agents (random, heuristic, reinforcement learning agents, etc.) in various game scenarios by wrapping multiple board and matrix games and supporting different agent types. It integrates API access to models via liteLLM, local model deployment via vLLM, and offers distributed execution through Ray. This paper summarises the library structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game theoretic behaviour.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectrumFM: Redefining Spectrum Cognition via Foundation Modeling</title>
<link>https://arxiv.org/abs/2508.02742</link>
<guid>https://arxiv.org/abs/2508.02742</guid>
<content:encoded><![CDATA[
<div> Keywords: spectrum cognition, SpectrumFM, convolutional neural networks, self-supervised learning, low-rank adaptation

Summary:
SpectrumFM is a new spectrum foundation model that enhances spectrum efficiency and security through advanced spectrum cognition methods. It utilizes a spectrum encoder combining convolutional neural networks and self-attention mechanisms to capture both local and global spectrum data structures effectively. Two self-supervised learning tasks, masked reconstruction and next-slot signal prediction, enable rich and transferable representations in pre-training. The model's adaptability is improved through low-rank adaptation for fine-tuning on various spectrum cognition tasks like spectrum sensing, anomaly detection, and wireless technology classification. Experimental results show significant improvements over existing methods, with increased detection probability in spectrum sensing, boosted AUC in anomaly detection, and enhanced accuracy in wireless technology classification by 9.6%. <br /><br />Summary: <div>
arXiv:2508.02742v2 Announce Type: replace-cross 
Abstract: The enhancement of spectrum efficiency and the realization of secure spectrum utilization are critically dependent on spectrum cognition. However, existing spectrum cognition methods often exhibit limited generalization and suboptimal accuracy when deployed across diverse spectrum environments and tasks. To overcome these challenges, we propose a spectrum foundation model, termed SpectrumFM, which provides a new paradigm for spectrum cognition. An innovative spectrum encoder that exploits the convolutional neural networks and the multi-head self attention mechanisms is proposed to effectively capture both fine-grained local signal structures and high-level global dependencies in the spectrum data. To enhance its adaptability, two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, are developed for pre-training SpectrumFM, enabling the model to learn rich and transferable representations. Furthermore, low-rank adaptation (LoRA) parameter-efficient fine-tuning is exploited to enable SpectrumFM to seamlessly adapt to various downstream spectrum cognition tasks, including spectrum sensing (SS), anomaly detection (AD), and wireless technology classification (WTC). Extensive experiments demonstrate the superiority of SpectrumFM over state-of-the-art methods. Specifically, it improves detection probability in the SS task by 30% at -4 dB signal-to-noise ratio (SNR), boosts the area under the curve (AUC) in the AD task by over 10%, and enhances WTC accuracy by 9.6%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation</title>
<link>https://arxiv.org/abs/2508.03104</link>
<guid>https://arxiv.org/abs/2508.03104</guid>
<content:encoded><![CDATA[
<div> Hierarchical Contrastive Learning, Text-attributed Hypergraphs, Self-supervised Learning, Semantic-aware Augmentation, Long-range Dependencies <br />
<br />
Summary: 
The paper introduces HiTeC, a two-stage hierarchical contrastive learning framework for self-supervised learning on text-attributed hypergraphs. It addresses limitations in existing methods by pre-training the text encoder with a structure-aware contrastive objective in the first stage. In the second stage, semantic-aware augmentation techniques are introduced to enhance informative view generation. A multi-scale contrastive loss is proposed to capture long-range dependencies effectively. By decoupling text encoder pretraining from hypergraph contrastive learning, HiTeC achieves scalability without compromising representation quality. Experimental results confirm the effectiveness of the proposed framework in improving self-supervised learning on text-attributed hypergraphs. <br /> <div>
arXiv:2508.03104v2 Announce Type: replace-cross 
Abstract: Contrastive learning (CL) has become a dominant paradigm for self-supervised hypergraph learning, enabling effective training without costly labels. However, node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning. Although HyperBERT pioneers CL on TAHGs, its co-training paradigm suffers from poor scalability. To fill the research gap, we introduce HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs. In the first stage, we pre-train the text encoder with a structure-aware contrastive objective to overcome the graph-agnostic nature of conventional methods. In the second stage, we introduce two semantic-aware augmentation strategies, including prompt-enhanced text augmentation and semantic-aware hyperedge drop, to facilitate informative view generation. Furthermore, we propose a multi-scale contrastive loss that extends existing objectives with an $s$-walk-based subgraph-level contrast to better capture long-range dependencies. By decoupling text encoder pretraining from hypergraph contrastive learning, this two-stage design enhances scalability without compromising representation quality. Extensive experiments confirm the effectiveness of HiTeC.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.03483</link>
<guid>https://arxiv.org/abs/2508.03483</guid>
<content:encoded><![CDATA[
<div> visual attributes, demographic bias, text-to-image generation, stereotypes, generative models
Summary:
The study investigates demographic bias in generated objects through a new framework called SODA. It compares visual attributes of objects generated with demographic cues to neutral prompts across 2,700 images from three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five categories. The analysis reveals strong associations between specific demographic groups and visual attributes, such as color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce stereotypes, along with less obvious biases. Some models produce less diverse outputs, exacerbating visual disparities compared to neutral prompts. The proposed framework helps test and uncover biases in generative models, highlighting the persistence of stereotypes in AI development. <br /><br />Summary: <div>
arXiv:2508.03483v2 Announce Type: replace-cross 
Abstract: While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., "for young people'') to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in today's generative models. We see this as an essential step toward more systematic and responsible AI development.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay</title>
<link>https://arxiv.org/abs/2508.03628</link>
<guid>https://arxiv.org/abs/2508.03628</guid>
<content:encoded><![CDATA[
<div> keywords: eBay, keyphrase recommendations, LLM, debiasing, multi-task training

Summary:
In this study, the focus is on improving keyphrase recommendations for sellers at eBay to optimize their advertising campaigns by aligning with seller and search judgments. To address biases in click-data, a two-step LLM distillation process is introduced to debias the Embedding Based Retrieval (EBR) model. The process involves distilling knowledge from an LLM teacher to a bi-encoder student through a cross-encoder assistant in a multi-task training approach. By utilizing this approach, the study demonstrates enhanced performance of the bi-encoder in retrieving relevant advertiser keyphrases. This method aims to prevent the overcrowding of search systems with irrelevant items and maintain a positive seller perception, ultimately benefiting both sellers and search users on the eBay platform. The findings suggest that incorporating knowledge distillation from LLMs in a multi-task setting can enhance the accuracy and relevance of keyphrase recommendations in online advertising campaigns. 

<br /><br />Summary: <div>
arXiv:2508.03628v2 Announce Type: replace-cross 
Abstract: Sellers at eBay are recommended keyphrases to bid on to enhance the performance of their advertising campaigns. The relevance of these keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies. This study introduces a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases. We show that integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2508.04131</link>
<guid>https://arxiv.org/abs/2508.04131</guid>
<content:encoded><![CDATA[
<div> supervised learning, medical imaging, segmentation, deep learning, uncertainty estimation 

Summary:
- The paper introduces a Detail-Semantic Deep Supervision Network (DS^2Net) for medical image segmentation.
- DS^2Net utilizes Detail Enhance Module (DEM) and Semantic Enhance Module (SEM) to supervise low-level detailed and high-level semantic features.
- The network adopts multi-view deep supervision to enhance feature learning, unlike previous single-view approaches.
- DS^2Net incorporates an uncertainty-based supervision loss to dynamically adjust the supervision strength based on feature uncertainty.
- Experimental results on six benchmarks in colonoscopy, ultrasound, and microscope images demonstrate the superior performance of DS^2Net over existing state-of-the-art methods. 

Summary: <div>
arXiv:2508.04131v2 Announce Type: replace-cross 
Abstract: Deep Supervision Networks exhibit significant efficacy for the medical imaging community. Nevertheless, existing work merely supervises either the coarse-grained semantic features or fine-grained detailed features in isolation, which compromises the fact that these two types of features hold vital relationships in medical image analysis. We advocate the powers of complementary feature supervision for medical image segmentation, by proposing a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates both low-level detailed and high-level semantic feature supervision through Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM respectively harness low-level and high-level feature maps to create detail and semantic masks for enhancing feature supervision. This is a novel shift from single-view deep supervision to multi-view deep supervision. DS$^2$Net is also equipped with a novel uncertainty-based supervision loss that adaptively assigns the supervision strength of features within distinct scales based on their uncertainty, thus circumventing the sub-optimal heuristic design that typifies previous works. Through extensive experiments on six benchmarks captured under either colonoscopy, ultrasound and microscope, we demonstrate that DS$^2$Net consistently outperforms state-of-the-art methods for medical image analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization</title>
<link>https://arxiv.org/abs/2508.06559</link>
<guid>https://arxiv.org/abs/2508.06559</guid>
<content:encoded><![CDATA[
<div> CUDA-accelerated computational framework, Pasur, Counterfactual Regret Minimization, memory management, near-Nash equilibrium<br />
Summary:<br />
This paper presents a CUDA-accelerated computational framework for simulating the fishing card game Pasur. The framework efficiently handles the game's intricate rules and large game tree size by utilizing PyTorch CUDA tensors and decomposing the game tree into actual game states and inherited scores. A round-by-round backward training strategy is employed to manage computational complexity. After computing near-Nash equilibrium strategies, a tree-based model is trained to predict these strategies for gameplay. The fair value of each deck is estimated through large-scale self-play simulations. The framework's methodology can be extended to other reinforcement learning algorithms for games with multiple rounds, such as turn-based strategy games or financial market trading decisions.<br /> <div>
arXiv:2508.06559v1 Announce Type: new 
Abstract: Pasur is a fishing card game played over six rounds and is played similarly to games such as Cassino and Scopa, and Bastra. This paper introduces a CUDA-accelerated computational framework for simulating Pasur, emphasizing efficient memory management. We use our framework to compute near-Nash equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the large size of its game tree. We handle rule complexity using PyTorch CUDA tensors and to address the memory-intensive nature of the game, we decompose the game tree into two key components: (1) actual game states, and (2) inherited scores from previous rounds. We construct the Full Game Tree by pairing card states with accumulated scores in the Unfolding Process. This design reduces memory overhead by storing only essential strategy values and node connections. To further manage computational complexity, we apply a round-by-round backward training strategy, starting from the final round and recursively propagating average utilities to earlier stages. Our approach constructs the complete game tree, which on average consists of over $10^9$ nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model to predict these strategies for use during gameplay. We then estimate the fair value of each deck through large-scale self-play between equilibrium strategies by simulating, for instance, 10,000 games per matchup, executed in parallel using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms where the action tree naturally decomposes into multiple rounds such as turn-based strategy games or sequential trading decisions in financial markets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop</title>
<link>https://arxiv.org/abs/2508.06569</link>
<guid>https://arxiv.org/abs/2508.06569</guid>
<content:encoded><![CDATA[
<div> Keywords: serendipity, materials research, artificial intelligence, autonomous laboratories, experimental observation

Summary:<br /><br />
The article introduces SciLink, an open-source AI framework designed to promote serendipity in materials research by linking experimental observation, novelty assessment, and theoretical simulations. This framework utilizes a hybrid AI approach, combining specialized machine learning models with large language models to analyze experimental data and evaluate its novelty against existing literature. It aims to enhance efficiency in materials research while also fostering an environment conducive to unexpected discoveries. SciLink demonstrates versatility in diverse research scenarios, including atomic-resolution and hyperspectral data analysis, integration of real-time expert input, and proposal of follow-up experiments. By systematically analyzing and contextualizing experimental observations, SciLink bridges the gap between automated experimentation and open-ended scientific exploration, providing a practical framework for AI-driven materials research. <div>
arXiv:2508.06569v1 Announce Type: new 
Abstract: The history of science is punctuated by serendipitous discoveries, where unexpected observations, rather than targeted hypotheses, opened new fields of inquiry. While modern autonomous laboratories excel at accelerating hypothesis testing, their optimization for efficiency risks overlooking these crucial, unplanned findings. To address this gap, we introduce SciLink, an open-source, multi-agent artificial intelligence framework designed to operationalize serendipity in materials research by creating a direct, automated link between experimental observation, novelty assessment, and theoretical simulations. The framework employs a hybrid AI strategy where specialized machine learning models perform quantitative analysis of experimental data, while large language models handle higher-level reasoning. These agents autonomously convert raw data from materials characterization techniques into falsifiable scientific claims, which are then quantitatively scored for novelty against the published literature. We demonstrate the framework's versatility across diverse research scenarios, showcasing its application to atomic-resolution and hyperspectral data, its capacity to integrate real-time human expert guidance, and its ability to close the research loop by proposing targeted follow-up experiments. By systematically analyzing all observations and contextualizing them, SciLink provides a practical framework for AI-driven materials research that not only enhances efficiency but also actively cultivates an environment ripe for serendipitous discoveries, thereby bridging the gap between automated experimentation and open-ended scientific exploration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</title>
<link>https://arxiv.org/abs/2508.06571</link>
<guid>https://arxiv.org/abs/2508.06571</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Inverse Reinforcement Learning, Vision-Language-Action, Autonomous Driving, Close-loop Training
<br />
Summary: 
The paper introduces IRL-VLA, a novel close-loop Reinforcement Learning approach for Vision-Language-Action models in autonomous driving. It addresses two key challenges by combining imitation learning with a lightweight reward world model constructed through Inverse Reinforcement Learning. The framework consists of three stages: pretraining the VLA policy, creating the reward world model, and using reinforcement learning to optimize driving behavior. By balancing safety, comfort, and efficiency, the approach achieves state-of-the-art performance in the NAVSIM v2 end-to-end driving benchmark and places as the 1st runner up in the CVPR2025 Autonomous Grand Challenge. This innovative framework aims to advance research in Vision-Language-Action models for autonomous driving. 
<br /> <div>
arXiv:2508.06571v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountQA: How Well Do MLLMs Count in the Wild?</title>
<link>https://arxiv.org/abs/2508.06585</link>
<guid>https://arxiv.org/abs/2508.06585</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, object counting, CountQA benchmark, high object density, spatial awareness

Summary:
Multimodal Large Language Models (MLLMs) excel in understanding visual scenes but struggle with object counting, a crucial cognitive skill. The CountQA benchmark, containing over 1,500 question-answer pairs with realistic scenarios, highlights this deficiency. Evaluating 15 MLLMs on CountQA shows limited accuracy in object counting, with performance decreasing as object counts increase. The top model achieves only 42.9% accuracy, indicating a significant room for improvement. By providing a comprehensive benchmark, CountQA aims to spur the development of MLLMs that are proficient in both linguistic understanding and numerical/spatial tasks. The dataset and code will be released to further research in this area.

<br /><br />Summary: <div>
arXiv:2508.06585v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in understanding visual scenes, yet they exhibit a critical lack in a fundamental cognitive skill: object counting. This blind spot severely limits their reliability in real-world applications. To date, this capability has been largely unevaluated in complex scenarios, as existing benchmarks either feature sparse object densities or are confined to specific visual domains, failing to test models under realistic conditions. Addressing this gap, we introduce CountQA, a challenging new benchmark designed to probe this deficiency. Comprising over 1,500 question-answer pairs, CountQA features real-world images with high object density, clutter, and occlusion. We investigate this weakness by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the top-performing model achieves a mere 42.9% accuracy, with performance declining as object counts rise. By providing a dedicated benchmark to diagnose and rectify this core weakness, CountQA paves the way for a new generation of MLLMs that are not only descriptively fluent but also numerically grounded and spatially aware. We will open-source the dataset and code upon paper acceptance to foster further research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis</title>
<link>https://arxiv.org/abs/2508.06668</link>
<guid>https://arxiv.org/abs/2508.06668</guid>
<content:encoded><![CDATA[
<div> Formal Concept Analysis, hierarchical clustering, knowledge representation, variability extraction, conceptual structures <br />
Summary: <br />
Formal Concept Analysis (FCA) is a mathematical framework used for knowledge representation and discovery. It organizes objects based on shared attributes, creating conceptual structures that reveal commonalities and variabilities among similar objects. FCA is particularly suited for variability extraction and analysis tasks. This paper aims to highlight essential properties of FCA for variability analysis and how they can be utilized to interpret variability information within conceptual structures. By bridging the gap between FCA's mathematical foundation and practical applications, the paper provides insights on leveraging FCA for variability-related tasks. <div>
arXiv:2508.06668v1 Announce Type: new 
Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge representation and discovery. It performs a hierarchical clustering over a set of objects described by attributes, resulting in conceptual structures in which objects are organized depending on the attributes they share. These conceptual structures naturally highlight commonalities and variabilities among similar objects by categorizing them into groups which are then arranged by similarity, making it particularly appropriate for variability extraction and analysis. Despite the potential of FCA, determining which of its properties can be leveraged for variability-related tasks (and how) is not always straightforward, partly due to the mathematical orientation of its foundational literature. This paper attempts to bridge part of this gap by gathering a selection of properties of the framework which are essential to variability analysis, and how they can be used to interpret diverse variability information within the resulting conceptual structures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Cellular Trajectory Map Matching</title>
<link>https://arxiv.org/abs/2508.06674</link>
<guid>https://arxiv.org/abs/2508.06674</guid>
<content:encoded><![CDATA[
<div> transferable geospatial knowledge, pixel-based trajectory calibration assistant, zero-shot CTMM, sequential features, spatial-temporal awareness module<br />
Summary:<br />
The paper introduces a novel approach for Cellular Trajectory Map-Matching (CTMM) called Zero-shot CTMM. It addresses the limitations of current methods by incorporating transferable geospatial knowledge and pixel-based trajectory calibration to improve accuracy in aligning cellular location sequences to road networks. The model utilizes a Gaussian mixture model in VAE for knowledge sharing across similar regions and a spatial-temporal awareness module to capture sequential features and location uncertainty. A constrained path-finding algorithm is used to reconstruct the road ID sequence, ensuring topological validity while optimizing for the shortest feasible path. Experimental results show that the proposed model outperforms existing methods in zero-shot CTMM by 16.8%. <div>
arXiv:2508.06674v1 Announce Type: new 
Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location sequences to road networks, which is a necessary preprocessing in location-based services on web platforms like Google Maps, including navigation and route optimization. Current approaches mainly rely on ID-based features and region-specific data to learn correlations between cell towers and roads, limiting their adaptability to unexplored areas. To enable high-accuracy CTMM without additional training in target regions, Zero-shot CTMM requires to extract not only region-adaptive features, but also sequential and location uncertainty to alleviate positioning errors in cellular data. In this paper, we propose a pixel-based trajectory calibration assistant for zero-shot CTMM, which takes advantage of transferable geospatial knowledge to calibrate pixelated trajectory, and then guide the path-finding process at the road network level. To enhance knowledge sharing across similar regions, a Gaussian mixture model is incorporated into VAE, enabling the identification of scenario-adaptive experts through soft clustering. To mitigate high positioning errors, a spatial-temporal awareness module is designed to capture sequential features and location uncertainty, thereby facilitating the inference of approximate user positions. Finally, a constrained path-finding algorithm is employed to reconstruct the road ID sequence, ensuring topological validity within the road network. This process is guided by the calibrated trajectory while optimizing for the shortest feasible path, thus minimizing unnecessary detours. Extensive experiments demonstrate that our model outperforms existing methods in zero-shot CTMM by 16.8\%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets</title>
<link>https://arxiv.org/abs/2508.06706</link>
<guid>https://arxiv.org/abs/2508.06706</guid>
<content:encoded><![CDATA[
<div> Keywords: rule-based methods, knowledge graph completion, probabilistic circuits, probabilistic logic, inference procedure <br />
Summary: 
The study introduces an approach for knowledge graph completion using rule-based methods that aim to reduce the number of rules needed for competitive performance. By identifying meaningful rule contexts and using probabilistic circuits to model their relationships, the researchers achieved a significant reduction in the number of rules while still outperforming existing baseline methods. The proposed framework, grounded in probabilistic logic, does not rely on independence assumptions and offers tractable inference procedures. Empirical studies on benchmark datasets demonstrated the effectiveness of the approach, showing promising results with significantly fewer rules compared to state-of-the-art methods. This work opens up possibilities for applying probabilistic reasoning to learned sets of rules, providing a more efficient and explainable approach to knowledge graph completion. <br /><br />Summary: <div>
arXiv:2508.06706v1 Announce Type: new 
Abstract: Rule-based methods for knowledge graph completion provide explainable results but often require a significantly large number of rules to achieve competitive performance. This can hinder explainability due to overwhelmingly large rule sets. We discover rule contexts (meaningful subsets of rules that work together) from training data and use learned probability distribution (i.e. probabilistic circuits) over these rule contexts to more rapidly achieve performance of the full rule set. Our approach achieves a 70-96% reduction in number of rules used while outperforming baseline by up to 31$\times$ when using equivalent minimal number of rules and preserves 91% of peak baseline performance even when comparing our minimal rule sets against baseline's full rule sets. We show that our framework is grounded in well-known semantics of probabilistic logic, does not require independence assumptions, and that our tractable inference procedure provides both approximate lower bounds and exact probability of a given query. The efficacy of our method is validated by empirical studies on 8 standard benchmark datasets where we show competitive performance by using only a fraction of the rules required by AnyBURL's standard inference method, the current state-of-the-art for rule-based knowledge graph completion. This work may have further implications for general probabilistic reasoning over learned sets of rules.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning</title>
<link>https://arxiv.org/abs/2508.06716</link>
<guid>https://arxiv.org/abs/2508.06716</guid>
<content:encoded><![CDATA[
<div> Keywords: differentiable inductive logic programming, rule learning, knowledge graph completion, symbolic solvers, deep neural networks 

Summary: 
GLIDR is a differentiable rule learning method that introduces more expressive syntax for modeling logic rules, allowing for branches and cycles. It uses a differentiable message passing inference algorithm and a simple rule search space parameterized by a limit on free variables. GLIDR outperforms existing methods on knowledge graph completion tasks and can compete with embedding methods. Rules extracted from GLIDR maintain predictive performance and show high robustness to training data noise. GLIDR can be combined with deep neural networks for end-to-end optimization for rule learning on various data modalities. The method allows for the extraction of explicit logic rules for use with symbolic solvers. These findings highlight the effectiveness and versatility of GLIDR in improving rule learning performance on knowledge graphs. 

<br /><br />Summary: <div>
arXiv:2508.06716v1 Announce Type: new 
Abstract: Differentiable inductive logic programming (ILP) techniques have proven effective at finding approximate rule-based solutions to link prediction and node classification problems on knowledge graphs; however, the common assumption of chain-like rule structure can hamper the performance and interpretability of existing approaches. We introduce GLIDR, a differentiable rule learning method that models the inference of logic rules with more expressive syntax than previous methods. GLIDR uses a differentiable message passing inference algorithm that generalizes previous chain-like rule learning methods to allow rules with features like branches and cycles. GLIDR has a simple and expressive rule search space which is parameterized by a limit on the maximum number of free variables that may be included in a rule. Explicit logic rules can be extracted from the weights of a GLIDR model for use with symbolic solvers. We demonstrate that GLIDR can significantly outperform existing rule learning methods on knowledge graph completion tasks and even compete with embedding methods despite the inherent disadvantage of being a structure-only prediction method. We show that rules extracted from GLIDR retain significant predictive performance, and that GLIDR is highly robust to training data noise. Finally, we demonstrate that GLIDR can be chained with deep neural networks and optimized end-to-end for rule learning on arbitrary data modalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search</title>
<link>https://arxiv.org/abs/2508.06736</link>
<guid>https://arxiv.org/abs/2508.06736</guid>
<content:encoded><![CDATA[
<div> bandits, large neighborhood search, mixed-integer programming, parallelization, computational resources

Summary: 
The paper explores the parallelization capabilities of Balans, a multi-armed bandits-based adaptive large neighborhood search algorithm for Mixed-Integer Programming (MIP) problems. An extension called ParBalans is introduced to leverage solver-level and algorithmic-level parallelism, enhancing performance on challenging MIP instances. The modular architecture of Balans allows for parallel exploration of diverse parameter configurations. Experimental results show that ParBalans performs competitively with the commercial solver Gurobi, especially on difficult optimization challenges. This demonstrates the potential of leveraging parallelization techniques to improve the scalability and performance of solving MIP problems. <div>
arXiv:2508.06736v1 Announce Type: new 
Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial computational resources due to their combinatorial nature. Parallelization has emerged as a critical strategy to accelerate solution times and enhance scalability to tackle large, complex instances. This paper investigates the parallelization capabilities of Balans, a recently proposed multi-armed bandits-based adaptive large neighborhood search for MIPs. While Balans's modular architecture inherently supports parallel exploration of diverse parameter configurations, this potential has not been thoroughly examined. To address this gap, we introduce ParBalans, an extension that leverages both solver-level and algorithmic-level parallelism to improve performance on challenging MIP instances. Our experimental results demonstrate that ParBalans exhibits competitive performance compared to the state-of-the-art commercial solver Gurobi, particularly on hard optimization benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism</title>
<link>https://arxiv.org/abs/2508.06746</link>
<guid>https://arxiv.org/abs/2508.06746</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV networks, Graph Diffusion-based Policy Optimization, Stackelberg Game, incentive mechanism, covert communication

Summary: 
The paper introduces a framework for self-organizing UAV networks in sensitive applications, addressing challenges posed by dynamic mobility and exposure risks. Combining Graph Diffusion-based Policy Optimization (GDPO) and a Stackelberg Game (SG)-based incentive mechanism, the framework dynamically generates sparse but well-connected topologies to adapt to changing node distributions and Ground User demands. The SG-based incentive mechanism guides self-interested UAVs towards relay behaviors and neighbor links that promote cooperation and enhance covert communication. Extensive experiments demonstrate the effectiveness of the framework in terms of model convergence, topology generation quality, and covert communication performance. The proposed approach offers a promising solution for ensuring reliable connectivity and covert communication in UAV networks for applications such as urban monitoring, emergency response, and secure sensing.<br /><br />Summary: <div>
arXiv:2508.06746v1 Announce Type: new 
Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in sensitive applications, such as urban monitoring, emergency response, and secure sensing, ensuring reliable connectivity and covert communication has become increasingly vital. However, dynamic mobility and exposure risks pose significant challenges. To tackle these challenges, this paper proposes a self-organizing UAV network framework combining Graph Diffusion-based Policy Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The GDPO method uses generative AI to dynamically generate sparse but well-connected topologies, enabling flexible adaptation to changing node distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game (SG)-based incentive mechanism guides self-interested UAVs to choose relay behaviors and neighbor links that support cooperation and enhance covert communication. Extensive experiments are conducted to validate the effectiveness of the proposed framework in terms of model convergence, topology generation quality, and enhancement of covert communication performance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Envelope of LLM Inference on AI-PC</title>
<link>https://arxiv.org/abs/2508.06753</link>
<guid>https://arxiv.org/abs/2508.06753</guid>
<content:encoded><![CDATA[
<div> ultra-low-bit LLM models, resource-constrained environments, computational efficiency, microkernels, PyTorch-TPP <br />
Summary:<br />
The article introduces ultra-low-bit LLM models that match the performance of full-precision models in resource-constrained environments. It highlights the need for efficient inference runtimes and presents optimized 1-bit and 2-bit microkernels for modern CPUs. These microkernels are integrated into the PyTorch-TPP framework, resulting in up to 2.2x improvement over the current SOTA runtime bitnet.cpp. The optimized runtime achieves a 7x speedup compared to 16-bit model inference, enhancing LLM inference on AI PCs and edge devices. This advancement enables the efficient deployment of ultra-low-bit LLM models, paving the way for cost-effective solutions in terms of latency, memory, throughput, and energy consumption. <br /> <div>
arXiv:2508.06753v1 Announce Type: new 
Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the perplexity and end-task performance of their full-precision counterparts using the same model size, is ushering in a new era of LLM inference for resource-constrained environments such as edge devices and AI PCs. While these quantization advances promise models that are more cost-effective in terms of latency, memory, throughput, and energy consumption, the computational efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp) used to deploy them remains underexplored. In this work, we take a bottom-up approach: we first design and implement 1-bit and 2-bit microkernels optimized for modern CPUs, achieving peak computational efficiency across a variety of CPU platforms. We integrate these microkernels into a state-of-the-art LLM inference framework, namely PyTorch-TPP, and present end-to-end inference results with 2-bit models that outperform the current SOTA runtime bitnet.cpp by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model inference. Our optimized runtime advances the state of LLM inference on AI PCs and edge devices, paving the way for efficient deployment of ultra-low-bit LLM models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks</title>
<link>https://arxiv.org/abs/2508.06754</link>
<guid>https://arxiv.org/abs/2508.06754</guid>
<content:encoded><![CDATA[
<div> Keywords: prompting framework, large language models, adaptive behavior, intelligent tutoring, procedural content generation <br />
<br />
Summary: 
The article introduces a modular prompting framework designed to improve the use of large language models (LLMs) in dynamic tasks. Based on human learning theory, specifically the Zone of Proximal Development (ZPD), the framework combines a natural language prompt with a control schema to enable LLMs to adjust behavior based on user feedback without the need for fine-tuning. In simulated intelligent tutoring scenarios, the framework enhances adaptivity, scaffolding quality, and instructional alignment across various models, surpassing standard prompting methods. The evaluation, performed using rubric-based LLM graders at scale, highlights the efficacy of the framework. While initially developed for educational purposes, the framework shows promise in other interaction-heavy fields like procedural content generation for games. Overall, this framework provides a safe and reusable approach for structuring goal-aligned LLM behavior in uncertain or changing environments. <br /><br /> <div>
arXiv:2508.06754v1 Announce Type: new 
Abstract: We introduce a modular prompting framework that supports safer and more adaptive use of large language models (LLMs) across dynamic, user-centered tasks. Grounded in human learning theory, particularly the Zone of Proximal Development (ZPD), our method combines a natural language boundary prompt with a control schema encoded with fuzzy scaffolding logic and adaptation rules. This architecture enables LLMs to modulate behavior in response to user state without requiring fine-tuning or external orchestration. In a simulated intelligent tutoring setting, the framework improves scaffolding quality, adaptivity, and instructional alignment across multiple models, outperforming standard prompting baselines. Evaluation is conducted using rubric-based LLM graders at scale. While initially developed for education, the framework has shown promise in other interaction-heavy domains, such as procedural content generation for games. Designed for safe deployment, it provides a reusable methodology for structuring interpretable, goal-aligned LLM behavior in uncertain or evolving contexts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation</title>
<link>https://arxiv.org/abs/2508.06823</link>
<guid>https://arxiv.org/abs/2508.06823</guid>
<content:encoded><![CDATA[
<div> Keywords: volumetric data, natural language interaction, CLIP Score, reinforcement learning, viewpoint selection

Summary:
This paper introduces a novel framework that utilizes natural language interaction to improve the exploration of volumetric data. By encoding volumetric blocks and incorporating a CLIP Score mechanism, the framework provides semantic information to guide navigation. A reinforcement learning framework is then employed to efficiently search for and identify desired viewpoints based on user intent. The selected viewpoints are evaluated using CLIP Score to ensure alignment with user queries. This automated viewpoint selection process enhances the efficiency of volumetric data navigation and improves the interpretability of complex scientific phenomena.<br /><br />Summary: <div>
arXiv:2508.06823v1 Announce Type: new 
Abstract: Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user's intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges</title>
<link>https://arxiv.org/abs/2508.06832</link>
<guid>https://arxiv.org/abs/2508.06832</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, language-centered interpretation, Large Language Models (LLMs), cognitive framework, Global Workspace Theory (GWT)

Summary: 
This review discusses the limitations of vision-centered models in remote sensing and proposes a language-centered framework for interpretation using Large Language Models (LLMs) as the central cognitive component. The proposed framework integrates perceptual, task, knowledge, and action spaces to enable unified understanding, reasoning, and decision-making. Technical challenges such as unified multimodal representation, knowledge association, and reasoning are highlighted. A global workspace-driven interpretation mechanism is constructed to address these challenges. Future research directions include adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to lay the conceptual foundation for the next generation of remote sensing interpretation systems and advance cognition-driven intelligent geospatial analysis.

<br /><br />Summary: <div>
arXiv:2508.06832v1 Announce Type: new 
Abstract: The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06836</link>
<guid>https://arxiv.org/abs/2508.06836</guid>
<content:encoded><![CDATA[
<div> Cooperative multi-agent reinforcement learning, credit assignment, MARL, multiple levels, advantage formulation<br />
<br />
Summary: <br />
Cooperative multi-agent reinforcement learning (MARL) involves coordinating multiple agents towards a common goal. A key challenge in MARL is credit assignment, which assesses each agent's contribution to the shared reward. This study introduces the concept of credit assignment levels, focusing on the number of agents collaborating to achieve a reward. The Multi-level Advantage Credit Assignment (MACA) method addresses scenarios with multiple coexisting levels by incorporating explicit counterfactual reasoning to infer credits across different levels. MACA integrates advantage functions for individual, joint, and correlated actions, utilizing an attention-based framework to identify correlated agent relationships. Through experiments on Starcraft tasks, MACA shows superior performance in complex credit assignment scenarios, highlighting its efficacy in handling diverse agent contributions. <br /> <div>
arXiv:2508.06836v1 Announce Type: new 
Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple agents to achieve a common goal. A key challenge in MARL is credit assignment, which involves assessing each agent's contribution to the shared reward. Given the diversity of tasks, agents may perform different types of coordination, with rewards attributed to diverse and often overlapping agent subsets. In this work, we formalize the credit assignment level as the number of agents cooperating to obtain a reward, and address scenarios with multiple coexisting levels. We introduce a multi-level advantage formulation that performs explicit counterfactual reasoning to infer credits across distinct levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures agent contributions at multiple levels by integrating advantage functions that reason about individual, joint, and correlated actions. Utilizing an attention-based framework, MACA identifies correlated agent relationships and constructs multi-level advantages to guide policy learning. Comprehensive experiments on challenging Starcraft v1\&amp;v2 tasks demonstrate MACA's superior performance, underscoring its efficacy in complex credit assignment scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams</title>
<link>https://arxiv.org/abs/2508.06851</link>
<guid>https://arxiv.org/abs/2508.06851</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, MDK12-Bench, dynamic evaluation framework, knowledge-driven reasoning, AI-assisted education

Summary:
MDK12-Bench introduces a large-scale benchmark for evaluating multimodal large language models (MLLMs) using real-world K-12 exams across six disciplines. The benchmark includes 141K instances and 6,225 knowledge points organized in a taxonomy. A novel dynamic evaluation framework challenges MLLMs with unfamiliar shifts in visual, textual, and question formats to improve model generalization and objectivity. The benchmark enables comprehensive evaluation over difficulty levels, temporal shifts, contextual shifts, and knowledge-driven reasoning. Evaluating knowledge-point reference-augmented generation (KP-RAG) highlights the role of knowledge in problem-solving. The findings reveal limitations in current MLLMs and provide insights for enhancing model robustness, interpretability, and AI-assisted education.

<br /><br />Summary: <div>
arXiv:2508.06851v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs), which integrate language and visual cues for problem-solving, are crucial for advancing artificial general intelligence (AGI). However, current benchmarks for measuring the intelligence of MLLMs suffer from limited scale, narrow coverage, and unstructured knowledge, offering only static and undifferentiated evaluations. To bridge this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark built from real-world K-12 exams spanning six disciplines with 141K instances and 6,225 knowledge points organized in a six-layer taxonomy. Covering five question formats with difficulty and year annotations, it enables comprehensive evaluation to capture the extent to which MLLMs perform over four dimensions: 1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts, and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation framework that introduces unfamiliar visual, textual, and question form shifts to challenge model generalization while improving benchmark objectivity and longevity by mitigating data contamination. We further evaluate knowledge-point reference-augmented generation (KP-RAG) to examine the role of knowledge in problem-solving. Key findings reveal limitations in current MLLMs in multiple aspects and provide guidance for enhancing model robustness, interpretability, and AI-assisted education.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction</title>
<link>https://arxiv.org/abs/2508.06859</link>
<guid>https://arxiv.org/abs/2508.06859</guid>
<content:encoded><![CDATA[
<div> Keywords: severe weather warning, AI weather station, MP-Bench, MMLM, meteorological data<br />
Summary:<br />
Timely and accurate severe weather warnings are crucial for disaster mitigation, but current forecasting systems rely heavily on manual interpretation. The emergence of AI technologies has led to the development of end-to-end AI weather stations for predicting severe weather events. The challenges faced include a lack of severe weather event samples, misalignment between meteorological data and textual warnings, and limitations in existing multimodal language models. To address these challenges, the authors introduce MP-Bench, a large-scale multimodal dataset for severe weather event prediction. They develop the Meteorology Multimodal Large Model (MMLM) that can directly process 4D meteorological inputs and incorporates adaptive fusion modules for feature extraction and integration. The experiments show that MMLM performs well across multiple tasks, demonstrating its effectiveness in severe weather forecasting and advancing towards automated AI-driven weather prediction systems. Both the source code and dataset will be made publicly available. <br /><br />Summary: <div>
arXiv:2508.06859v1 Announce Type: new 
Abstract: Timely and accurate severe weather warnings are critical for disaster mitigation. However, current forecasting systems remain heavily reliant on manual expert interpretation, introducing subjectivity and significant operational burdens. With the rapid development of AI technologies, the end-to-end "AI weather station" is gradually emerging as a new trend in predicting severe weather events. Three core challenges impede the development of end-to-end AI severe weather system: (1) scarcity of severe weather event samples; (2) imperfect alignment between high-dimensional meteorological data and textual warnings; (3) existing multimodal language models are unable to handle high-dimensional meteorological data and struggle to fully capture the complex dependencies across temporal sequences, vertical pressure levels, and spatial dimensions. To address these challenges, we introduce MP-Bench, the first large-scale temporal multimodal dataset for severe weather events prediction, comprising 421,363 pairs of raw multi-year meteorological data and corresponding text caption, covering a wide range of severe weather scenarios across China. On top of this dataset, we develop a meteorology multimodal large model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is designed to accommodate the unique characteristics of 4D meteorological data flow, incorporating three plug-and-play adaptive fusion modules that enable dynamic feature extraction and integration across temporal sequences, vertical pressure layers, and spatial dimensions. Extensive experiments on MP-Bench demonstrate that MMLM performs exceptionally well across multiple tasks, highlighting its effectiveness in severe weather understanding and marking a key step toward realizing automated, AI-driven weather forecasting systems. Our source code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushdown Reward Machines for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06894</link>
<guid>https://arxiv.org/abs/2508.06894</guid>
<content:encoded><![CDATA[
<div> reward machines, reinforcement learning, pushdown reward machines, deterministic context-free languages, expressive power

Summary:<br />
This article introduces pushdown reward machines (pdRMs), an extension of reward machines used in reinforcement learning. pdRMs can recognize and reward temporally extended behaviors represented in deterministic context-free languages, making them more expressive. Two variants of pdRM-based policies are presented, one with access to the entire stack and another with access limited to the top k symbols of the stack. The article establishes theoretical results on the expressive power of pdRMs and provides space complexity results for the proposed learning problems. Experimental results demonstrate that agents can be trained to complete tasks representable in deterministic context-free languages using pdRMs. <div>
arXiv:2508.06894v1 Announce Type: new 
Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian) reward functions for reinforcement learning (RL). RMs can reward any behaviour representable in regular languages and, when paired with RL algorithms that exploit RM structure, have been shown to significantly improve sample efficiency in many domains. In this work, we present pushdown reward machines (pdRMs), an extension of reward machines based on deterministic pushdown automata. pdRMs can recognize and reward temporally extended behaviours representable in deterministic context-free languages, making them more expressive than reward machines. We introduce two variants of pdRM-based policies, one which has access to the entire stack of the pdRM, and one which can only access the top $k$ symbols (for a given constant $k$) of the stack. We propose a procedure to check when the two kinds of policies (for a given environment, pdRM, and constant $k$) achieve the same optimal expected reward. We then provide theoretical results establishing the expressive power of pdRMs, and space complexity results about the proposed learning problems. Finally, we provide experimental results showing how agents can be trained to perform tasks representable in deterministic context-free languages using pdRMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization</title>
<link>https://arxiv.org/abs/2508.06899</link>
<guid>https://arxiv.org/abs/2508.06899</guid>
<content:encoded><![CDATA[
<div> Keywords: Local search, Distributed Constraint Optimization Problems, GDBA, premature convergence, penalty updates

Summary:
Local search algorithms are widely used for solving Distributed Constraint Optimization Problems (DCOPs) but often converge to poor solutions. The Guided Dynamic Branch and Bound Algorithm (GDBA) aims to escape premature convergence, but its performance is limited on general-valued problems. This work identifies three factors leading to GDBA's inferior performance: overly aggressive constraint violation conditions, unbounded penalty accumulation, and uncoordinated penalty updates. To address these issues, a new Distributed Guided Local Search (DGLS) framework is proposed, incorporating adaptive violation conditions, penalty evaporation mechanism, and synchronized penalty updates. Theoretical analysis shows bounded penalty values and agents play a potential game in DGLS. Empirical results across various benchmarks demonstrate the superior performance of DGLS compared to existing baselines, showing competitive results on general-valued problems and significant improvements on structured problems in terms of anytime results. 

<br /><br />Summary: <div>
arXiv:2508.06899v1 Announce Type: new 
Abstract: Local search is an important class of incomplete algorithms for solving Distributed Constraint Optimization Problems (DCOPs) but it often converges to poor local optima. While GDBA provides a comprehensive rule set to escape premature convergence, its empirical benefits remain marginal on general-valued problems. In this work, we systematically examine GDBA and identify three factors that potentially lead to its inferior performance, i.e., over-aggressive constraint violation conditions, unbounded penalty accumulation, and uncoordinated penalty updates. To address these issues, we propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs that incorporates an adaptive violation condition to selectively penalize constraints with high cost, a penalty evaporation mechanism to control the magnitude of penalization, and a synchronization scheme for coordinated penalty updates. We theoretically show that the penalty values are bounded, and agents play a potential game in our DGLS. Our extensive empirical results on various standard benchmarks demonstrate the great superiority of DGLS over state-of-the-art baselines. Particularly, compared to Damped Max-sum with high damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance on general-valued problems, and outperforms it by significant margins (\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Formalization via Conceptual Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2508.06931</link>
<guid>https://arxiv.org/abs/2508.06931</guid>
<content:encoded><![CDATA[
<div> formalization, autoformalization, retrieval-augmented generation, concept-definition knowledge base, mathematical concepts<br />
Summary:<br />
The article introduces CRAMF, a Concept-driven Retrieval-Augmented Mathematical Formalization framework aimed at enhancing autoformalization in Interactive theorem provers (ITPs). It addresses challenges such as model hallucination and the semantic gap by retrieving formal definitions of core mathematical concepts. CRAMF constructs a concept-definition knowledge base from Mathlib4, a standard mathematical library for Lean 4, with over 26,000 formal definitions and 1,000+ core mathematical concepts. To handle conceptual polymorphism, it utilizes contextual query augmentation and a dual-channel hybrid retrieval strategy with reranking. Experimental results on various benchmarks demonstrate significant improvements in translation accuracy, with up to 62.1% and an average of 29.9% relative improvement. <div>
arXiv:2508.06931v1 Announce Type: new 
Abstract: Interactive theorem provers (ITPs) require manual formalization, which is labor-intensive and demands expert knowledge. While automated formalization offers a potential solution, it faces two major challenges: model hallucination (e.g., undefined predicates, symbol misuse, and version incompatibility) and the semantic gap caused by ambiguous or missing premises in natural language descriptions. To address these issues, we propose CRAMF, a Concept-driven Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances LLM-based autoformalization by retrieving formal definitions of core mathematical concepts, providing contextual grounding during code generation. However, applying retrieval-augmented generation (RAG) in this setting is non-trivial due to the lack of structured knowledge bases, the polymorphic nature of mathematical concepts, and the high precision required in formal retrieval. We introduce a framework for automatically constructing a concept-definition knowledge base from Mathlib4, the standard mathematical library for the Lean 4 theorem prover, indexing over 26,000 formal definitions and 1,000+ core mathematical concepts. To address conceptual polymorphism, we propose contextual query augmentation with domain- and application-level signals. In addition, we design a dual-channel hybrid retrieval strategy with reranking to ensure accurate and relevant definition retrieval. Experiments on miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding consistent improvements in translation accuracy, achieving up to 62.1% and an average of 29.9% relative improvement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction</title>
<link>https://arxiv.org/abs/2508.06939</link>
<guid>https://arxiv.org/abs/2508.06939</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, Multimodal learning, Crop yield prediction, Feature attributions, Modality attributions<br />
<br />
Summary: 
This study focuses on explaining multimodal learning networks using Transformer-based models for crop yield prediction at the subfield level. The study covers various crops, regions, and years, using multispectral satellite, weather time series, terrain elevation maps, and soil properties as input modalities. The Transformer-based models outperform convolutional and recurrent networks, achieving higher R2 scores at both subfield and field levels. The Attention Rollout (AR) method proves to be more reliable for temporal attributions compared to Generic Attention (GA) and Shapley Value Sampling (SVS). The Weighted Modality Activation (WMA) method is proposed to assess modality attributions. Information about crop phenology stages is leveraged to interpret explanation results in alignment with agronomic knowledge. The modality attributions reveal varying patterns between the two methods compared. <div>
arXiv:2508.06939v1 Announce Type: new 
Abstract: Multimodal learning enables various machine learning tasks to benefit from diverse data sources, effectively mimicking the interplay of different factors in real-world applications, particularly in agriculture. While the heterogeneous nature of involved data modalities may necessitate the design of complex architectures, the model interpretability is often overlooked. In this study, we leverage the intrinsic explainability of Transformer-based models to explain multimodal learning networks, focusing on the task of crop yield prediction at the subfield level. The large datasets used cover various crops, regions, and years, and include four different input modalities: multispectral satellite and weather time series, terrain elevation maps and soil properties. Based on the self-attention mechanism, we estimate feature attributions using two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and evaluate their performance against Shapley-based model-agnostic estimations, Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality Activation (WMA) method to assess modality attributions and compare it with SVS attributions. Our findings indicate that Transformer-based models outperform other architectures, specifically convolutional and recurrent networks, achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field levels, respectively. AR is shown to provide more robust and reliable temporal attributions, as confirmed through qualitative and quantitative evaluation, compared to GA and SVS values. Information about crop phenology stages was leveraged to interpret the explanation results in the light of established agronomic knowledge. Furthermore, modality attributions revealed varying patterns across the two methods compared.[...]
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Do Not Simulate Human Psychology</title>
<link>https://arxiv.org/abs/2508.06950</link>
<guid>https://arxiv.org/abs/2508.06950</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human psychology, reliability, validation, psychological research 
<br />
Summary: 
Large Language Models (LLMs), such as ChatGPT, are being used in various research applications, including psychological studies. However, there is a caution against viewing LLMs as simulators of human psychology due to conceptual arguments and empirical evidence. The article highlights that minor wording changes can significantly alter LLMs' responses, leading to discrepancies with human responses. Even models specifically fine-tuned on psychological data, like CENTAUR, show limitations in accurately simulating human psychology. Variability in responses among different LLMs further emphasizes their lack of reliability. As a result, it is recommended that psychological researchers treat LLMs as useful tools but recognize their inherent unreliability. Validation against human responses is necessary for each new application involving LLMs in psychological research.
<br /><br />Summary: <div>
arXiv:2508.06950v1 Announce Type: new 
Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in research, ranging from simple writing assistance to complex data annotation tasks. Recently, some research has suggested that LLMs may even be able to simulate human psychology and can, hence, replace human participants in psychological studies. We caution against this approach. We provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs' and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</title>
<link>https://arxiv.org/abs/2508.06960</link>
<guid>https://arxiv.org/abs/2508.06960</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data availability, AI agents, dataset discovery, benchmark

Summary:<br /><br />
The article discusses the shift in AI development from computational power to data availability, emphasizing the importance of discovering valuable datasets hidden in various repositories. It introduces DatasetResearch, a benchmark evaluating AI agents' ability to discover and synthesize datasets based on user requirements across knowledge-intensive and reasoning-intensive tasks. The evaluation framework highlights that current deep research systems only achieve a 22% score on challenging dataset discovery tasks. The analysis reveals that search agents excel at knowledge tasks through retrieval breadth, while synthesis agents perform better on reasoning challenges through structured generation. However, both types of agents fail on "corner cases" outside existing distributions. The findings establish a baseline for dataset discovery agents and pave the way for AI systems capable of autonomously finding any dataset in the digital universe. The benchmark and analysis are publicly available for further research and development. 

Summary: <div>
arXiv:2508.06960v1 Announce Type: new 
Abstract: The rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specialized repositories, research appendices, and domain platforms. As reasoning capabilities and deep research methodologies continue to evolve, a critical question emerges: can AI agents transcend conventional search to systematically discover any dataset that meets specific user requirements, enabling truly autonomous demand-driven data curation? We introduce DatasetResearch, the first comprehensive benchmark evaluating AI agents' ability to discover and synthesize datasets from 208 real-world demands across knowledge-intensive and reasoning-intensive tasks. Our tri-dimensional evaluation framework reveals a stark reality: even advanced deep research systems achieve only 22% score on our challenging DatasetResearch-pro subset, exposing the vast gap between current capabilities and perfect dataset discovery. Our analysis uncovers a fundamental dichotomy-search agents excel at knowledge tasks through retrieval breadth, while synthesis agents dominate reasoning challenges via structured generation-yet both catastrophically fail on "corner cases" outside existing distributions. These findings establish the first rigorous baseline for dataset discovery agents and illuminate the path toward AI systems capable of finding any dataset in the digital universe. Our benchmark and comprehensive analysis provide the foundation for the next generation of self-improving AI systems and are publicly available at https://github.com/GAIR-NLP/DatasetResearch.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair</title>
<link>https://arxiv.org/abs/2508.06963</link>
<guid>https://arxiv.org/abs/2508.06963</guid>
<content:encoded><![CDATA[
<div> Trustworthiness Repair, Large Language Models, Representation Engineering, MASteer, Automated<br />
<br />
Summary: 
The article introduces MASteer, a framework for trustworthiness repair in Large Language Models (LLMs) based on representation engineering. It integrates AutoTester, a system that generates diverse steer samples, and AutoRepairer, which constructs adaptive steering strategies with anchor vectors for context-aware strategy selection during inference. Experiments show MASteer outperforms baselines on trustworthiness tasks, improving metrics on LLaMA-3.1-8B-Chat and Qwen-3-8B-Chat. It demonstrates robustness, generalization, and practical value for scalable, efficient trustworthiness repair in LLMs. <div>
arXiv:2508.06963v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness issues, motivating developers to seek automated and flexible repair methods that enable convenient deployment across diverse scenarios. Existing repair methods like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) are costly and slow, while prompt engineering lacks robustness and scalability. Representation engineering, which steers model behavior by injecting targeted concept vectors during inference, offers a lightweight, training-free alternative. However, current approaches depend on manually crafted samples and fixed steering strategies, limiting automation and adaptability. To overcome these challenges, we propose MASteer, the first end-to-end framework for trustworthiness repair in LLMs based on representation engineering. MASteer integrates two core components: AutoTester, a multi-agent system that generates diverse, high-quality steer samples tailored to developer needs; and AutoRepairer, which constructs adaptive steering strategies with anchor vectors for automated, context-aware strategy selection during inference. Experiments on standard and customized trustworthiness tasks show MASteer consistently outperforms baselines, improving metrics by 15.36% on LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model capabilities. MASteer demonstrates strong robustness, generalization, and practical value for scalable, efficient trustworthiness repair.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning</title>
<link>https://arxiv.org/abs/2508.06972</link>
<guid>https://arxiv.org/abs/2508.06972</guid>
<content:encoded><![CDATA[
<div> framework, distributed machine learning, cryptographic verification, zero-knowledge, inference  
Summary:  
DSperse is a modular framework designed for distributed machine learning inference with strategic cryptographic verification. It operates using distributed zero-knowledge machine learning principles, allowing targeted verification of specific subcomputations without the need for full-model circuitization. These verifiable slices within the inference pipeline can be strategically chosen to cover partial or complete components, ensuring global consistency through audit, replication, or economic incentives. By localizing zero-knowledge proofs to areas of highest value, DSperse enables trust minimization. The framework supports various proving systems and offers empirical insights on memory usage, runtime, and circuit behavior in sliced and unsliced configurations. DSperse's flexibility allows proof boundaries to align with the logical structure of the model, facilitating scalable and targeted verification strategies tailored to different deployment requirements. <div>
arXiv:2508.06972v1 Announce Type: new 
Abstract: DSperse is a modular framework for distributed machine learning inference with strategic cryptographic verification. Operating within the emerging paradigm of distributed zero-knowledge machine learning, DSperse avoids the high cost and rigidity of full-model circuitization by enabling targeted verification of strategically chosen subcomputations. These verifiable segments, or "slices", may cover part or all of the inference pipeline, with global consistency enforced through audit, replication, or economic incentives. This architecture supports a pragmatic form of trust minimization, localizing zero-knowledge proofs to the components where they provide the greatest value. We evaluate DSperse using multiple proving systems and report empirical results on memory usage, runtime, and circuit behavior under sliced and unsliced configurations. By allowing proof boundaries to align flexibly with the model's logical structure, DSperse supports scalable, targeted verification strategies suited to diverse deployment needs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model</title>
<link>https://arxiv.org/abs/2508.06980</link>
<guid>https://arxiv.org/abs/2508.06980</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, autonomous agents, active inference, decision-making, biological neuronal networks

Summary: 
This study explores the use of biological neuronal networks to model decision-making in autonomous agents. By employing the framework of active inference, a theory of behavior, decision-making processes in embodied agents were simulated in a game-play environment. The experiment-informed generative models facilitated learning in these agents, shedding light on the significance of memory-based learning and predictive planning in intelligent decision-making. The research showcases the potential of biologically based systems in developing explainable and biologically plausible models for artificial intelligence. This study contributes to the field by offering a scalable approach grounded in biology to understand purposeful behavior in autonomous agents. <div>
arXiv:2508.06980v1 Announce Type: new 
Abstract: With recent and rapid advancements in artificial intelligence (AI), understanding the foundation of purposeful behaviour in autonomous agents is crucial for developing safe and efficient systems. While artificial neural networks have dominated the path to AI, recent studies are exploring the potential of biologically based systems, such as networks of living biological neuronal networks. Along with promises of high power and data efficiency, these systems may also inform more explainable and biologically plausible models. In this work, we propose a framework rooted in active inference, a general theory of behaviour, to model decision-making in embodied agents. Using experiment-informed generative models, we simulate decision-making processes in a simulated game-play environment, mirroring experimental setups that use biological neurons. Our results demonstrate learning in these agents, providing insights into the role of memory-based learning and predictive planning in intelligent decision-making. This work contributes to the growing field of explainable AI by offering a biologically grounded and scalable approach to understanding purposeful behaviour in agents.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach</title>
<link>https://arxiv.org/abs/2508.07015</link>
<guid>https://arxiv.org/abs/2508.07015</guid>
<content:encoded><![CDATA[
<div> Optimization, Combinatorial, Hitting Set, Pseudo-Boolean, Stochastic Local Search <br />
Summary: <br />
The study explores alternative algorithms for hitting set optimization within the implicit hitting set (IHS) approach for combinatorial optimization problems. It investigates the use of pseudo-Boolean reasoning and stochastic local search as alternatives to integer programming (IP). While IP solvers are effective, they can lead to correctness issues due to numerical instability. The study shows that exact hitting set computations using pseudo-Boolean reasoning can be competitive with numerically exact IP solvers. Using pseudo-Boolean reasoning allows for obtaining certificates of correctness for IHS computations, applicable to any instantiation where reasoning can be represented in the PB-based proof format utilized. The research highlights a trade-off between efficiency and reliability in selecting the most appropriate method for hitting set optimization within the IHS framework. <div>
arXiv:2508.07015v1 Announce Type: new 
Abstract: The implicit hitting set (IHS) approach offers a general framework for solving computationally hard combinatorial optimization problems declaratively. IHS iterates between a decision oracle used for extracting sources of inconsistency and an optimizer for computing so-called hitting sets (HSs) over the accumulated sources of inconsistency. While the decision oracle is language-specific, the optimizers is usually instantiated through integer programming.
  We explore alternative algorithmic techniques for hitting set optimization based on different ways of employing pseudo-Boolean (PB) reasoning as well as stochastic local search. We extensively evaluate the practical feasibility of the alternatives in particular in the context of pseudo-Boolean (0-1 IP) optimization as one of the most recent instantiations of IHS. Highlighting a trade-off between efficiency and reliability, while a commercial IP solver turns out to remain the most effective way to instantiate HS computations, it can cause correctness issues due to numerical instability; in fact, we show that exact HS computations instantiated via PB reasoning can be made competitive with a numerically exact IP solver. Furthermore, the use of PB reasoning as a basis for HS computations allows for obtaining certificates for the correctness of IHS computations, generally applicable to any IHS instantiation in which reasoning in the declarative language at hand can be captured in the PB-based proof format we employ.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA</title>
<link>https://arxiv.org/abs/2508.07022</link>
<guid>https://arxiv.org/abs/2508.07022</guid>
<content:encoded><![CDATA[
<div> Benchmark, Knowledge Editing, Multimodal, Clinical, Medical <br />
Summary: <br />
Knowledge editing (KE) is crucial for updating factual knowledge in large language models without full retraining. The study introduces MultiMedEdit, a benchmark tailored for evaluating KE in clinical multimodal tasks. It addresses the need for integrating updated knowledge with visual reasoning to support clinical decisions. The framework spans understanding and reasoning tasks, defines a three-dimensional metric suite, and enables cross-paradigm comparisons. Results from experiments under single-editing and lifelong-editing settings highlight challenges in generalization and long-tail reasoning in complex clinical workflows. An efficiency analysis reveals practical trade-offs in real-world deployment across KE paradigms. Overall, MultiMedEdit reveals current limitations and lays the foundation for developing clinically robust knowledge editing techniques in the future. <div>
arXiv:2508.07022v1 Announce Type: new 
Abstract: Knowledge editing (KE) provides a scalable approach for updating factual knowledge in large language models without full retraining. While previous studies have demonstrated effectiveness in general domains and medical QA tasks, little attention has been paid to KE in multimodal medical scenarios. Unlike text-only settings, medical KE demands integrating updated knowledge with visual reasoning to support safe and interpretable clinical decisions. To address this gap, we propose MultiMedEdit, the first benchmark tailored to evaluating KE in clinical multimodal tasks. Our framework spans both understanding and reasoning task types, defines a three-dimensional metric suite (reliability, generality, and locality), and supports cross-paradigm comparisons across general and domain-specific models. We conduct extensive experiments under single-editing and lifelong-editing settings. Results suggest that current methods struggle with generalization and long-tail reasoning, particularly in complex clinical workflows. We further present an efficiency analysis (e.g., edit latency, memory footprint), revealing practical trade-offs in real-world deployment across KE paradigms. Overall, MultiMedEdit not only reveals the limitations of current approaches but also provides a solid foundation for developing clinically robust knowledge editing techniques in the future.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Dense Analyst: Towards Fully Automated Scientific Analysis</title>
<link>https://arxiv.org/abs/2508.07043</link>
<guid>https://arxiv.org/abs/2508.07043</guid>
<content:encoded><![CDATA[
<div> Keywords: bioinformatics analysis, large language models, K-Dense Analyst, autonomous scientific reasoning, computational biologists<br />
Summary:<br />
The article introduces K-Dense Analyst, a hierarchical multi-agent system designed to autonomously conduct bioinformatics analysis. This system utilizes a dual-loop architecture to decompose complex objectives into verifiable tasks within secure computational environments. On a benchmark test, K-Dense Analyst outperforms the best language model GPT-5 by 6.3 percentage points, a significant improvement of nearly 27%. This performance is achieved using Gemini 2.5 Pro, which has lower accuracy when used directly, showcasing the effectiveness of the architectural innovations. The study highlights the importance of purpose-built systems in bridging the gap between high-level scientific objectives and low-level computational execution. By advancing autonomous scientific reasoning beyond language models, K-Dense Analyst represents a stride towards fully autonomous computational biologists capable of accelerating discovery in the life sciences.<br /><br />Summary: <div>
arXiv:2508.07043v1 Announce Type: new 
Abstract: The complexity of modern bioinformatics analysis has created a critical gap between data generation and developing scientific insights. While large language models (LLMs) have shown promise in scientific reasoning, they remain fundamentally limited when dealing with real-world analytical workflows that demand iterative computation, tool integration and rigorous validation. We introduce K-Dense Analyst, a hierarchical multi-agent system that achieves autonomous bioinformatics analysis through a dual-loop architecture. K-Dense Analyst, part of the broader K-Dense platform, couples planning with validated execution using specialized agents to decompose complex objectives into executable, verifiable tasks within secure computational environments. On BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense Analyst achieves 29.2% accuracy, surpassing the best-performing language model (GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what is widely considered the most powerful LLM available. Remarkably, K-Dense Analyst achieves this performance using Gemini 2.5 Pro, which attains only 18.3% accuracy when used directly, demonstrating that our architectural innovations unlock capabilities far beyond the underlying model's baseline performance. Our insights demonstrate that autonomous scientific reasoning requires more than enhanced language models, it demands purpose-built systems that can bridge the gap between high-level scientific objectives and low-level computational execution. These results represent a significant advance toward fully autonomous computational biologists capable of accelerating discovery across the life sciences.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach</title>
<link>https://arxiv.org/abs/2508.07063</link>
<guid>https://arxiv.org/abs/2508.07063</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, Large Language Models, human emotions, offensive behaviors, bias detection

Summary: 
The study discusses the need for safer and more reliable moderation in AI systems due to their integration into daily life. Large Language Models (LLMs) have shown remarkable capabilities but struggle with detecting implicit hate, offensive language, and biases. The study presents an experimental framework using state-of-the-art models to assess human emotions and offensive behaviors. A unified benchmark dataset covering various categories related to emotions, offensive text, and biases was introduced. SafePhi, a QLoRA fine-tuned version of Phi-4, outperformed benchmark moderators with a Macro F1 score of 0.89. The research highlights areas where LLM moderators consistently underperformed and emphasizes the importance of incorporating diverse and representative data with human-in-the-loop for better model robustness and explainability.<br /><br />Summary: <div>
arXiv:2508.07063v1 Announce Type: new 
Abstract: As AI systems become more integrated into daily life, the need for safer and more reliable moderation has never been greater. Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing earlier models in complexity and performance. Their evaluation across diverse tasks has consistently showcased their potential, enabling the development of adaptive and personalized agents. However, despite these advancements, LLMs remain prone to errors, particularly in areas requiring nuanced moral reasoning. They struggle with detecting implicit hate, offensive language, and gender biases due to the subjective and context-dependent nature of these issues. Moreover, their reliance on training data can inadvertently reinforce societal biases, leading to inconsistencies and ethical concerns in their outputs. To explore the limitations of LLMs in this role, we developed an experimental framework based on state-of-the-art (SOTA) models to assess human emotions and offensive behaviors. The framework introduces a unified benchmark dataset encompassing 49 distinct categories spanning the wide spectrum of human emotions, offensive and hateful text, and gender and racial biases. Furthermore, we introduced SafePhi, a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This research also highlights the critical domains where LLM moderators consistently underperformed, pressing the need to incorporate more heterogeneous and representative data with human-in-the-loop, for better model robustness and explainability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention</title>
<link>https://arxiv.org/abs/2508.07107</link>
<guid>https://arxiv.org/abs/2508.07107</guid>
<content:encoded><![CDATA[
<div> Adaptive, Feedback-Driven, Decision Support System, LightGBM, SHAP<br />
<br />
Summary:<br />
A new approach for accurate student performance prediction in education is proposed, utilizing a Feedback-Driven Decision Support System. The system incorporates a LightGBM-based regressor with incremental retraining, allowing continuous model refinement based on updated student results. This adaptive mechanism improves prediction accuracy by learning from real-world academic progress. A Flask-based web interface enables real-time interaction, while SHAP provides transparency and explainability. Experimental results demonstrate a 10.7% reduction in RMSE after retraining, with consistent improvements in predicted scores for intervened students. By transforming static predictors into self-improving systems, this approach advances educational analytics towards more human-centered, data-driven, and responsive AI. The framework is designed for integration into Learning Management Systems and institutional dashboards.<br /> <div>
arXiv:2508.07107v1 Announce Type: new 
Abstract: Accurate prediction of student performance is essential for timely academic intervention. However, most machine learning models in education are static and cannot adapt when new data, such as post-intervention outcomes, become available. To address this limitation, we propose a Feedback-Driven Decision Support System (DSS) with a closed-loop architecture that enables continuous model refinement. The system integrates a LightGBM-based regressor with incremental retraining, allowing educators to input updated student results, which automatically trigger model updates. This adaptive mechanism improves prediction accuracy by learning from real-world academic progress. The platform features a Flask-based web interface for real-time interaction and incorporates SHAP for explainability, ensuring transparency. Experimental results show a 10.7\% reduction in RMSE after retraining, with consistent upward adjustments in predicted scores for intervened students. By transforming static predictors into self-improving systems, our approach advances educational analytics toward human-centered, data-driven, and responsive AI. The framework is designed for integration into LMS and institutional dashboards.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables</title>
<link>https://arxiv.org/abs/2508.07186</link>
<guid>https://arxiv.org/abs/2508.07186</guid>
<content:encoded><![CDATA[
<div> Keywords: structured data, enterprise, language model, summarization, multi-dimensional

Summary: 
The article proposes a new framework for summarizing structured enterprise data using large language model (LLM)-based agents. Traditional table-to-text models struggle to reason across hierarchical structures and context-aware deltas, essential in business reporting tasks. The new method introduces a multi-agent pipeline for extracting, analyzing, and summarizing multi-dimensional data through agents for slicing, variance detection, context construction, and LLM-based generation. The results show that the framework outperforms traditional approaches, achieving 83% faithfulness to data, superior coverage of significant changes, and high relevance scores for decision-critical insights. It particularly excels in capturing subtle trade-offs, such as revenue increase due to price changes amidst declining unit volumes, often overlooked or addressed vaguely by other methods. Evaluation on Kaggle datasets indicates significant improvements in faithfulness, relevance, and insight quality compared to baseline approaches.<br /><br />Summary: <div>
arXiv:2508.07186v1 Announce Type: new 
Abstract: We propose a novel framework for summarizing structured enterprise data across multiple dimensions using large language model (LLM)-based agents. Traditional table-to-text models often lack the capacity to reason across hierarchical structures and context-aware deltas, which are essential in business reporting tasks. Our method introduces a multi-agent pipeline that extracts, analyzes, and summarizes multi-dimensional data using agents for slicing, variance detection, context construction, and LLM-based generation. Our results show that the proposed framework outperforms traditional approaches, achieving 83\% faithfulness to underlying data, superior coverage of significant changes, and high relevance scores (4.4/5) for decision-critical insights. The improvements are especially pronounced in categories involving subtle trade-offs, such as increased revenue due to price changes amid declining unit volumes, which competing methods either overlook or address with limited specificity. We evaluate the framework on Kaggle datasets and demonstrate significant improvements in faithfulness, relevance, and insight quality over baseline table summarization approaches.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning</title>
<link>https://arxiv.org/abs/2508.07292</link>
<guid>https://arxiv.org/abs/2508.07292</guid>
<content:encoded><![CDATA[
<div> Memory-guided agent, endoscopic analysis, adaptive tool selection, collaboration, reasoning acuity <br />
Summary: 
The article introduces EndoAgent, a novel AI system for endoscopic image diagnosis that integrates iterative reasoning with adaptive tool selection and collaboration. It utilizes a dual-memory design to track short-term actions and enhance long-term reasoning acuity. EndoAgent incorporates expert-designed tools within a unified reasoning loop to support various clinical tasks. Additionally, EndoAgentBench, a benchmark testing visual understanding and language generation capabilities, consists of 5,709 question-answer pairs. Through extensive experiments, EndoAgent demonstrates superior performance compared to general and medical multimodal models, showcasing its flexibility and strong reasoning abilities. <br /><br /> <div>
arXiv:2508.07292v1 Announce Type: new 
Abstract: Developing general artificial intelligence (AI) systems to support endoscopic image diagnosis is an emerging research priority. Existing methods based on large-scale pretraining often lack unified coordination across tasks and struggle to handle the multi-step processes required in complex clinical workflows. While AI agents have shown promise in flexible instruction parsing and tool integration across domains, their potential in endoscopy remains underexplored. To address this gap, we propose EndoAgent, the first memory-guided agent for vision-to-decision endoscopic analysis that integrates iterative reasoning with adaptive tool selection and collaboration. Built on a dual-memory design, it enables sophisticated decision-making by ensuring logical coherence through short-term action tracking and progressively enhancing reasoning acuity through long-term experiential learning. To support diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools within a unified reasoning loop. We further introduce EndoAgentBench, a benchmark of 5,709 visual question-answer pairs that assess visual understanding and language generation capabilities in realistic scenarios. Extensive experiments show that EndoAgent consistently outperforms both general and medical multimodal models, exhibiting its strong flexibility and reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape</title>
<link>https://arxiv.org/abs/2508.07334</link>
<guid>https://arxiv.org/abs/2508.07334</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, illusions, probabilistic Turing machine, Retrieval Enhanced Generations, continuous learning<br />
Summary:<br />
This article delves into the illusion phenomenon of large language models (LLMs) and formalizes them as probabilistic Turing machines. It introduces a "computational necessity hierarchy" and proves the inevitability of illusions on various boundaries. Two "escape routes" are proposed to address the issue: one involves modeling Retrieval Enhanced Generations (RAGs) as oracle machines, providing a formal theory for their effectiveness; the second path formalizes continuous learning through an "internalized oracle" mechanism using a novel neural game theory framework. These approaches offer potential solutions to the challenges posed by illusions in LLMs, highlighting the importance of understanding and addressing these issues in the deployment of such models. The article provides insights into the theoretical foundations of LLMs and offers strategies for overcoming the limitations and risks associated with their use.<br /> <div>
arXiv:2508.07334v1 Announce Type: new 
Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle to their reliable deployment. This article formalizes the large language model as a probabilistic Turing machine by constructing a "computational necessity hierarchy", and for the first time proves the illusions are inevitable on diagonalization, incomputability, and information theory boundaries supported by the new "learner pump lemma". However, we propose two "escape routes": one is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving their absolute escape through "computational jumps", providing the first formal theory for the effectiveness of RAGs; The second is to formalize continuous learning as an "internalized oracle" mechanism and implement this path through a novel neural game theory framework.Finally, this article proposes a
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach</title>
<link>https://arxiv.org/abs/2508.07353</link>
<guid>https://arxiv.org/abs/2508.07353</guid>
<content:encoded><![CDATA[
<div> scaling law, benchmark construction, large language models, domain-specific abilities, corpus design

Summary:<br /><br />In this paper, the authors emphasize the importance of effective benchmark construction for evaluating large language models' domain-specific abilities. They propose the Comp-Comp framework, which prioritizes comprehensiveness and compactness in benchmark design, focusing on semantic recall and precision in domain-specific tasks. The framework was validated through the creation of XUBench, a comprehensive closed-domain benchmark based on a case study in an academic setting. The study challenges the traditional scaling law approach and underscores the significance of corpus and QA set design in improving precision and recall for domain-specific benchmarks. The Comp-Comp framework offers valuable insights for benchmark construction across diverse domains, beyond academia. <div>
arXiv:2508.07353v1 Announce Type: new 
Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities of large language models (LLMs), highlighting the need for effective and efficient benchmark construction. Existing domain-specific benchmarks primarily focus on the scaling law, relying on massive corpora for supervised fine-tuning or generating extensive question sets for broad coverage. However, the impact of corpus and question-answer (QA) set design on the precision and recall of domain-specific LLMs remains unexplored. In this paper, we address this gap and demonstrate that the scaling law is not always the optimal principle for benchmark construction in specific domains. Instead, we propose Comp-Comp, an iterative benchmarking framework based on a comprehensiveness-compactness principle. Here, comprehensiveness ensures semantic recall of the domain, while compactness enhances precision, guiding both corpus and QA set construction. To validate our framework, we conducted a case study in a well-renowned university, resulting in the creation of XUBench, a large-scale and comprehensive closed-domain benchmark. Although we use the academic domain as the case in this work, our Comp-Comp framework is designed to be extensible beyond academia, providing valuable insights for benchmark construction across various domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07382</link>
<guid>https://arxiv.org/abs/2508.07382</guid>
<content:encoded><![CDATA[
<div> Dataset, Reinforcement Learning, Large Language Models, Cybersecurity, Penetration Testing

Summary:
The article introduces Pentest-R1, a novel framework designed to enhance Large Language Models (LLMs) for penetration testing. The framework utilizes a two-stage reinforcement learning pipeline, first using a dataset of real-world walkthroughs for offline RL to establish attack logic, and then fine-tuning in a Capture The Flag environment for adaptive strategies. Pentest-R1 achieves a 24.2% success rate on AutoPenBench, ranking second only to Gemini 2.5 Flash, and a 15.0% success rate on Cybench in unguided tasks, matching top proprietary models. Ablation studies highlight the importance of both training stages for its success. The framework addresses limitations in LLM reasoning, such as error handling and task automation, effectively enhancing cybersecurity through automated penetration testing. 

<br /><br />Summary: <div>
arXiv:2508.07382v1 Announce Type: new 
Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the framework's effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0\% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding</title>
<link>https://arxiv.org/abs/2508.07388</link>
<guid>https://arxiv.org/abs/2508.07388</guid>
<content:encoded><![CDATA[
<div> temporal video grounding, localization accuracy, action understanding, inversion tasks, semantic understanding  
Summary:  
Invert4TVG introduces a framework for Temporal Video Grounding (TVG) that improves both localization accuracy and action understanding without additional data. By leveraging inversion tasks derived from existing TVG annotations, including Verb Completion, Action Recognition, and Video Description, the approach ensures balanced optimization of localization and semantics. Through a reinforcement learning framework with well-designed reward functions, the method outperforms existing approaches, achieving a 7.1% improvement in R1@0.7 on Charades-STA for a 3B model compared to Time-R1. By deriving query-related actions from video segments, the approach strengthens semantic understanding and significantly raises the ceiling of localization accuracy.  
<br /><br />Summary: <div>
arXiv:2508.07388v1 Announce Type: new 
Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a given textual query. Current methods, while optimizing for high temporal Intersection-over-Union (IoU), often overfit to this metric, compromising semantic action understanding in the video and query, a critical factor for robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG), a novel framework that enhances both localization accuracy and action understanding without additional data. Our approach leverages three inversion tasks derived from existing TVG annotations: (1) Verb Completion, predicting masked action verbs in queries from video segments; (2) Action Recognition, identifying query-described actions; and (3) Video Description, generating descriptions of video segments that explicitly embed query-relevant actions. These tasks, integrated with TVG via a reinforcement learning framework with well-designed reward functions, ensure balanced optimization of localization and semantics. Experiments show our method outperforms state-of-the-art approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B model compared to Time-R1. By inverting TVG to derive query-related actions from segments, our approach strengthens semantic understanding, significantly raising the ceiling of localization accuracy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Strategic Plan Development</title>
<link>https://arxiv.org/abs/2508.07405</link>
<guid>https://arxiv.org/abs/2508.07405</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, Large Language Models, Strategic Plans, Topic Modeling, Government Organizations

Summary:
This paper explores the use of Generative Artificial Intelligence (GAI) in developing strategic plans for large government organizations. The study evaluates the performance of BERTopic and Non-negative Matrix Factorization (NMF) in generating themes related to Vision Elements within a strategic plan by analyzing reports from the Government Accountability Office (GAO). The results show that both techniques are effective, with BERTopic outperforming NMF in generating correlated topics. The ability to use GAI for strategic planning has significant implications for the public sector, as it can help organizations overcome regulatory challenges and improve efficiency. Future research will focus on implementing this concept and exploring the viability of other modules in the proposed model for GAI-generated strategic plans.<br /><br />Summary: <div>
arXiv:2508.07405v1 Announce Type: new 
Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and Large Language Models (LLMs), more and more professional services are being augmented through Artificial Intelligence (AI), which once seemed impossible to automate. This paper presents a modular model for leveraging GAI in developing strategic plans for large scale government organizations and evaluates leading machine learning techniques in their application towards one of the identified modules. Specifically, the performance of BERTopic and Non-negative Matrix Factorization (NMF) are evaluated in their ability to use topic modeling to generate themes representative of Vision Elements within a strategic plan. To accomplish this, BERTopic and NMF models are trained using a large volume of reports from the Government Accountability Office (GAO). The generated topics from each model are then scored for similarity against the Vision Elements of a published strategic plan and the results are compared. Our results show that these techniques are capable of generating themes similar to 100% of the elements being evaluated against. Further, we conclude that BERTopic performs best in this application with more than half of its correlated topics achieving a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan development impacts a multi-billion dollar industry and assists the federal government in overcoming regulatory requirements which are crucial to the public good. Further work will focus on the operationalization of the concept proven in this study as well as viability of the remaining modules in the proposed model for GAI-generated strategic plans.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</title>
<link>https://arxiv.org/abs/2508.07407</link>
<guid>https://arxiv.org/abs/2508.07407</guid>
<content:encoded><![CDATA[
<div> Evolutionary algorithms, agentic systems, self-evolving techniques, domain-specific strategies, evaluation<br />
<br />
Summary:<br />
Recent advancements in large language models have led to an interest in AI agents capable of solving complex tasks. However, existing agent systems often lack adaptability to dynamic environments. Research on agent evolution aims to enhance systems based on interaction data and feedback, creating self-evolving AI agents. A conceptual framework highlights key components in designing self-evolving systems. Techniques targeting different agent system components are reviewed, including domain-specific strategies in areas like biomedicine and finance. Evaluation, safety, and ethical considerations are also discussed to ensure effectiveness and reliability of self-evolving agents. This survey aims to provide a comprehensive understanding for researchers and practitioners in developing more adaptive and lifelong agentic systems. <div>
arXiv:2508.07407v1 Announce Type: new 
Abstract: Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</title>
<link>https://arxiv.org/abs/2508.07466</link>
<guid>https://arxiv.org/abs/2508.07466</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, multi-agent decision-making, prompt engineering, memory architectures, multi-modal information processing

Summary: 
In this work, the focus is on enhancing large language models (LLMs) by integrating them with multi-agent decision-making algorithms. The aim is to improve communication and coordination among agents by developing a framework for multi-agentic LLMs. Key integration practices include advanced prompt engineering, effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. Ablation studies are conducted on classic game settings with social dilemmas and game-theoretic considerations to evaluate these design choices. The results suggest that integrating LLMs with multi-agent decision-making algorithms can enhance language-based interactions and reasoning among agents, leading to improved coordination and strategies.<br /><br />Summary: <div>
arXiv:2508.07466v1 Announce Type: new 
Abstract: Language is a ubiquitous tool that is foundational to reasoning and collaboration, ranging from everyday interactions to sophisticated problem-solving tasks. The establishment of a common language can serve as a powerful asset in ensuring clear communication and understanding amongst agents, facilitating desired coordination and strategies. In this work, we extend the capabilities of large language models (LLMs) by integrating them with advancements in multi-agent decision-making algorithms. We propose a systematic framework for the design of multi-agentic large language models (LLMs), focusing on key integration practices. These include advanced prompt engineering techniques, the development of effective memory architectures, multi-modal information processing, and alignment strategies through fine-tuning algorithms. We evaluate these design choices through extensive ablation studies on classic game settings with significant underlying social dilemmas and game-theoretic considerations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Agent: Agentic Constraint Programming</title>
<link>https://arxiv.org/abs/2508.07468</link>
<guid>https://arxiv.org/abs/2508.07468</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language, constraint modeling, Python, ReAct principle, CP-Bench

Summary: 
The article introduces a new approach for translating natural language problem descriptions into formal constraint models using a pure agentic strategy. The approach involves a Python coding agent based on the ReAct principle, which utilizes a persistent IPython kernel for stateful code execution and iterative development. Instead of embedding constraint programming logic, domain-specific expertise is injected through a project prompt. The agent combines this prompt-encoded knowledge with access to file operations and code execution tools to dynamically test hypotheses, debug failures, and verify solutions. Implemented in a few hundred lines of code, the architecture successfully solves all 101 problems of the CP-Bench constraint programming benchmark set. The results suggest that constraint modeling tasks benefit from a combination of general coding tools and domain expertise encoded in prompts, rather than specialized agent architectures or predefined workflows. 

<br /><br />Summary: <div>
arXiv:2508.07468v1 Announce Type: new 
Abstract: Translating natural language problem descriptions into formal constraint models remains a fundamental challenge in constraint programming, requiring deep expertise in both the problem domain and modeling frameworks. Previous approaches to automating this translation have employed fixed workflows with predetermined modeling steps, failing on a significant number of benchmark problems. We present a new approach using a pure agentic strategy without any fixed pipeline. We developed a general-purpose Python coding agent based on the ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for stateful code execution and iterative development. Rather than embedding constraint programming logic into the agent architecture, domain-specific expertise is injected solely through a carefully crafted project prompt. The agent combines this prompt-encoded knowledge with access to file operations and code execution tools, enabling it to test hypotheses, debug failures, and verify solutions dynamically. Implemented in just a few hundred lines of code, this architecture successfully solves all 101 problems of the CP-Bench constraint programming benchmark set. The results suggest that constraint modeling tasks require the combination of general coding tools and domain expertise encoded in prompts, rather than specialized agent architectures or predefined workflows.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy</title>
<link>https://arxiv.org/abs/2508.07485</link>
<guid>https://arxiv.org/abs/2508.07485</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Diplomacy, game state, strategy reasoning, data-driven iteration 
Summary:<br /><br />We present an evaluation harness that allows Large Language Models (LLMs) to play Diplomacy without fine-tuning. Through data-driven iteration, we optimized a textual game state representation for a 24B model to play matches reliably. Our tooling facilitates hypothesis testing and statistical analysis, enabling insights into persuasion, aggressive playstyles, and model performance. Experiments show that larger LLMs outperform smaller ones, but all models play adequately. We introduce Critical State Analysis for in-depth game moment analysis. Our harness enables studying strategic reasoning in LLMs without the need for fine-tuning and sheds light on how these capabilities naturally emerge from widely used models. The code is available in the supplement and will be open-sourced. <div>
arXiv:2508.07485v1 Announce Type: new 
Abstract: We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark</title>
<link>https://arxiv.org/abs/2508.07575</link>
<guid>https://arxiv.org/abs/2508.07575</guid>
<content:encoded><![CDATA[
<div> MCP, LLMs, function calls, evaluation, benchmark<br />
Summary:<br />
The article discusses the integration of various data sources or API results into LLMs' context window using function calls, facilitated by the Model Context Protocol (MCP). However, evaluating LLMs' performance on calling MCP tools faces challenges due to a lack of comprehensive datasets and varying success rates of real-world MCP servers. To address this, the MCPToolBench++ benchmark was proposed, utilizing over 4k MCP servers from various categories to evaluate SOTA LLMs with agentic abilities. The benchmark includes single-step and multi-step tool calls across different domains, aiming to provide a standardized method for assessing LLMs' capabilities in utilizing MCP tools. <div>
arXiv:2508.07575v1 Announce Type: new 
Abstract: LLMs' capabilities are enhanced by using function calls to integrate various data sources or API results into the context window. Typical tools include search, web crawlers, maps, financial data, file systems, and browser usage, etc. Integrating these data sources or functions requires a standardized method. The Model Context Protocol (MCP) provides a standardized way to supply context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use abilities suffer from several issues. First, there's a lack of comprehensive datasets or benchmarks to evaluate various MCP tools. Second, the diverse formats of response from MCP tool call execution further increase the difficulty of evaluation. Additionally, unlike existing tool-use benchmarks with high success rates in functions like programming and math functions, the success rate of real-world MCP tool is not guaranteed and varies across different MCP servers. Furthermore, the LLMs' context window also limits the number of available tools that can be called in a single run, because the textual descriptions of tool and the parameters have long token length for an LLM to process all at once. To help address the challenges of evaluating LLMs' performance on calling MCP tools, we propose MCPToolBench++, a large-scale, multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is build upon marketplace of over 4k MCP servers from more than 40 categories, collected from the MCP marketplaces and GitHub communities. The datasets consist of both single-step and multi-step tool calls across different categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and reported the results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method</title>
<link>https://arxiv.org/abs/2508.07586</link>
<guid>https://arxiv.org/abs/2508.07586</guid>
<content:encoded><![CDATA[
<div> semantic communication, covert communication, friendly jammer, reinforcement learning, privacy protection 

Summary:
The paper introduces a novel covert semantic communication framework where a server transmits semantic information to a user over multiple time slots while a potential attacker tries to intercept the information. To protect the data, a friendly jammer transmits interference signals to thwart eavesdropping attempts. The server strategically selects time slots for transmission without knowledge of the jammer's transmit power. To address this, a prioritised sampling assisted twin delayed deep deterministic policy gradient algorithm is proposed to optimize semantic information transmission and transmit power per time slot. This approach, which incorporates an additional Q network for action selection, improves privacy and semantic information transmission quality significantly. Simulation results demonstrate up to 77.8% enhancement in privacy and 14.3% improvement in information transmission quality compared to traditional reinforcement learning methods. 

<br /><br />Summary: <div>
arXiv:2508.07586v1 Announce Type: new 
Abstract: In this paper, a novel covert semantic communication framework is investigated. Within this framework, a server extracts and transmits the semantic information, i.e., the meaning of image data, to a user over several time slots. An attacker seeks to detect and eavesdrop the semantic transmission to acquire details of the original image. To avoid data meaning being eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming signals to interfere the attacker so as to hide the transmitted semantic information. Meanwhile, the server will strategically select time slots for semantic information transmission. Due to limited energy, the jammer will not communicate with the server and hence the server does not know the transmit power of the jammer. Therefore, the server must jointly optimize the semantic information transmitted at each time slot and the corresponding transmit power to maximize the privacy and the semantic information transmission quality of the user. To solve this problem, we propose a prioritised sampling assisted twin delayed deep deterministic policy gradient algorithm to jointly determine the transmitted semantic information and the transmit power per time slot without the communications between the server and the jammer. Compared to standard reinforcement learning methods, the propose method uses an additional Q network to estimate Q values such that the agent can select the action with a lower Q value from the two Q networks thus avoiding local optimal action selection and estimation bias of Q values. Simulation results show that the proposed algorithm can improve the privacy and the semantic information transmission quality by up to 77.8% and 14.3% compared to the traditional reinforcement learning methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol</title>
<link>https://arxiv.org/abs/2508.07602</link>
<guid>https://arxiv.org/abs/2508.07602</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Tool selection, Hierarchical Gaussian Mixture, Scalability<br />
<br />
The article introduces the Hierarchical Gaussian Mixture Framework (HGMF) as a method to enhance the selection of external tools for Large Language Models (LLMs). HGMF maps user queries and tool descriptions into a unified semantic space and utilizes a two-stage process involving Gaussian Mixture Model (GMM) clustering to filter servers and tools based on query relevance. This hierarchical approach results in a more compact and relevant candidate set, improving tool selection accuracy and reducing inference latency. Experimental results on a public dataset demonstrate the efficacy of HGMF in enhancing tool selection in large tool libraries, showcasing its scalability and effectiveness for improving real-world task performance for LLMs.<br /><br />Summary: <div>
arXiv:2508.07602v1 Announce Type: new 
Abstract: Invoking external tools enables Large Language Models (LLMs) to perform complex, real-world tasks, yet selecting the correct tool from large, hierarchically-structured libraries remains a significant challenge. The limited context windows of LLMs and noise from irrelevant options often lead to low selection accuracy and high computational costs. To address this, we propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic pruning method for scalable tool invocation. HGMF first maps the user query and all tool descriptions into a unified semantic space. The framework then operates in two stages: it clusters servers using a Gaussian Mixture Model (GMM) and filters them based on the query's likelihood. Subsequently, it applies the same GMM-based clustering and filtering to the tools associated with the selected servers. This hierarchical process produces a compact, high-relevance candidate set, simplifying the final selection task for the LLM. Experiments on a public dataset show that HGMF significantly improves tool selection accuracy while reducing inference latency, confirming the framework's scalability and effectiveness for large-scale tool libraries.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkTuning: Instilling Cognitive Reflections without Distillation</title>
<link>https://arxiv.org/abs/2508.07616</link>
<guid>https://arxiv.org/abs/2508.07616</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time scaling, thinking LLMs, multi-step reasoning, ThinkTuning, implicit supervision

Summary:
Recent advancements in test-time scaling have led to the development of thinking Large Language Models (LLMs) capable of self-reflective behaviors and multi-step reasoning. However, it has been found that reinforcement learning (RL) alone may not inherently instill these new reasoning abilities in models but rather amplify existing behaviors. To address this, a new approach called ThinkTuning is proposed, which employs a Generative Rollout Policy Optimization (GRPO) based interactive training method. This approach mimics the feedback loop between a teacher and student in a classroom setting, where the teacher guides the student towards the correct solution through corrective feedback. By incorporating feedback from a similarly sized teacher model during training, the student model's reasoning capabilities are enhanced. ThinkTuning demonstrates an average improvement of 3.85% over zero-shot baselines across various benchmarks, with notable improvements on specific datasets such as MATH-500, AIME, and GPQA-Diamond. The source code for ThinkTuning is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2508.07616v1 Announce Type: new 
Abstract: Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization</title>
<link>https://arxiv.org/abs/2508.07628</link>
<guid>https://arxiv.org/abs/2508.07628</guid>
<content:encoded><![CDATA[
<div> Keywords: poultry production, welfare monitoring, Artificial Intelligence, multimodal sensing, data-driven systems <br />
<br />
Summary: 
The article discusses the necessity for a paradigm shift in poultry production towards data-driven, intelligent monitoring ecosystems to improve welfare assessment. It highlights the limitations of traditional welfare checks and the potential of Multimodal Artificial Intelligence (AI) in capturing the complex nature of laying hen welfare. The research demonstrates that intermediate fusion strategies are optimal for balancing robustness and performance in real-world farm conditions. Adoption barriers such as sensor fragility and high deployment costs are addressed through novel evaluation tools like the Domain Transfer Score (DTS) and the Data Reliability Index (DRI). A modular deployment framework is proposed for laying hen environments to enable practical integration of multimodal sensing. By transitioning from reactive, unimodal monitoring to proactive, precision-driven welfare systems, the industry aims to achieve a harmonious balance between productivity and ethical animal care. <div>
arXiv:2508.07628v1 Announce Type: new 
Abstract: The future of poultry production depends on a paradigm shift replacing subjective, labor-intensive welfare checks with data-driven, intelligent monitoring ecosystems. Traditional welfare assessments-limited by human observation and single-sensor data-cannot fully capture the complex, multidimensional nature of laying hen welfare in modern farms. Multimodal Artificial Intelligence (AI) offers a breakthrough, integrating visual, acoustic, environmental, and physiological data streams to reveal deeper insights into avian welfare dynamics. This investigation highlights multimodal As transformative potential, showing that intermediate (feature-level) fusion strategies achieve the best balance between robustness and performance under real-world poultry conditions, and offer greater scalability than early or late fusion approaches. Key adoption barriers include sensor fragility in harsh farm environments, high deployment costs, inconsistent behavioral definitions, and limited cross-farm generalizability. To address these, we introduce two novel evaluation tools - the Domain Transfer Score (DTS) to measure model adaptability across diverse farm settings, and the Data Reliability Index (DRI) to assess sensor data quality under operational constraints. We also propose a modular, context-aware deployment framework designed for laying hen environments, enabling scalable and practical integration of multimodal sensing. This work lays the foundation for a transition from reactive, unimodal monitoring to proactive, precision-driven welfare systems that unite productivity with ethical, science based animal care.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</title>
<link>https://arxiv.org/abs/2508.07642</link>
<guid>https://arxiv.org/abs/2508.07642</guid>
<content:encoded><![CDATA[
<div> skill-based reasoning, Vision-and-Language Navigation, Transformer-based agents, zero-shot Vision-Language Model, state-of-the-art performance

Summary:
SkillNav introduces structured, skill-based reasoning into Transformer-based Vision-and-Language Navigation (VLN) agents to improve their ability to interpret natural language instructions and navigate 3D environments effectively. Navigation is broken down into interpretable atomic skills, such as Vertical Movement and Area Identification, each managed by a specialized agent. A novel zero-shot Vision-Language Model (VLM)-based router is introduced to dynamically select the most suitable agent based on visual observations and historical actions. The approach outperforms existing methods on the R2R benchmark and demonstrates strong generalization to novel instruction styles and unseen environments in the GSA-R2R benchmark. SkillNav's modular framework enhances the agents' spatial and temporal reasoning capabilities, leading to improved performance and generalization in VLN tasks. 

<br /><br />Summary: <div>
arXiv:2508.07642v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. We then introduce a novel zero-shot Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav achieves a new state-of-the-art performance on the R2R benchmark and demonstrates strong generalization to the GSA-R2R benchmark that includes novel instruction styles and unseen environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</title>
<link>https://arxiv.org/abs/2508.07649</link>
<guid>https://arxiv.org/abs/2508.07649</guid>
<content:encoded><![CDATA[
<div> Keywords: Point-of-Interest recommendation, spatial-temporal transitions, social relationships, disentangled representation learning, multiplex graph

Summary:
DiMuST is a new model for Point-of-Interest (POI) recommendation that integrates spatial-temporal transitions and social relationships. It addresses the issue of misalignment in existing models by using a Disentangled variational multiplex graph Auto-Encoder (DAE) to separate shared and private distributions. The model then fuses shared features through a Product of Experts (PoE) mechanism and denoises private features with contrastive constraints. By effectively capturing spatial-temporal transition representations of POIs while preserving their relationships, DiMuST outperforms existing methods on challenging datasets. The approach enhances interpretability and reduces model uncertainty, leading to more accurate and reliable recommendations for users. <br /><br />Summary: <div>
arXiv:2508.07649v1 Announce Type: new 
Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business intelligence, where users' spatial-temporal transitions and social relationships play key roles. However, most existing works model spatial and temporal transitions separately, leading to misaligned representations of the same spatial-temporal key nodes. This misalignment introduces redundant information during fusion, increasing model uncertainty and reducing interpretability. To address this issue, we propose DiMuST, a socially enhanced POI recommendation model based on disentangled representation learning over multiplex spatial-temporal transition graphs. The model employs a novel Disentangled variational multiplex graph Auto-Encoder (DAE), which first disentangles shared and private distributions using a multiplex spatial-temporal graph strategy. It then fuses the shared features via a Product of Experts (PoE) mechanism and denoises the private features through contrastive constraints. The model effectively captures the spatial-temporal transition representations of POIs while preserving the intrinsic correlation of their spatial-temporal relationships. Experiments on two challenging datasets demonstrate that our DiMuST significantly outperforms existing methods across multiple metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning</title>
<link>https://arxiv.org/abs/2508.07667</link>
<guid>https://arxiv.org/abs/2508.07667</guid>
<content:encoded><![CDATA[
<div> framework, contextual privacy, multi-agent, information-flow, privacy leakage
Summary: 
The study introduces a multi-agent framework to tackle contextual privacy concerns in interactive settings involving large language models (LLMs). The framework breaks down privacy reasoning into specialized tasks like extraction and classification, reducing the burden on individual agents and enabling iterative validation for better adherence to privacy norms. Through systematic ablation experiments on different information-flow topologies, the research identifies the emergence and propagation of privacy errors in LLMs. The results show that the multi-agent configuration significantly decreases private information leakage by 18% and 19% on ConfAIde and PrivacyLens benchmarks, respectively, when using GPT-4o. This outperforms single-agent baselines and emphasizes the importance of well-designed information flow in multi-agent systems for enhancing contextual privacy in LLMs. <br /><br /> <div>
arXiv:2508.07667v1 Announce Type: new 
Abstract: Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning into specialized subtasks (extraction, classification), reducing the information load on any single agent while enabling iterative validation and more reliable adherence to contextual privacy norms. To understand how privacy errors emerge and propagate, we conduct a systematic ablation over information-flow topologies, revealing when and why upstream detection mistakes cascade into downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with several open-source and closed-sourced LLMs demonstrate that our best multi-agent configuration substantially reduces private information leakage (\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while preserving the fidelity of public content, outperforming single-agent baselines. These results highlight the promise of principled information-flow design in multi-agent systems for contextual privacy with LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration</title>
<link>https://arxiv.org/abs/2508.07671</link>
<guid>https://arxiv.org/abs/2508.07671</guid>
<content:encoded><![CDATA[
arXiv:2508.07671v1 Announce Type: new 
Abstract: Current AI approaches to refugee integration optimize narrow objectives such as employment and fail to capture the cultural, emotional, and ethical dimensions critical for long-term success. We introduce EMPATHIA (Enriched Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance), a multi-agent framework addressing the central Creative AI question: how do we preserve human dignity when machines participate in life-altering decisions? Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes integration into three modules: SEED (Socio-cultural Entry and Embedding Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency Engine) for early independence, and THRIVE (Transcultural Harmony and Resilience through Integrated Values and Engagement) for sustained outcomes. SEED employs a selector-validator architecture with three specialized agents - emotional, cultural, and ethical - that deliberate transparently to produce interpretable recommendations. Experiments on the UN Kakuma dataset (15,026 individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic variables achieved 87.4% validation convergence and explainable assessments across five host countries. EMPATHIA's weighted integration of cultural, emotional, and ethical factors balances competing value systems while supporting practitioner-AI collaboration. By augmenting rather than replacing human expertise, EMPATHIA provides a generalizable framework for AI-driven allocation tasks where multiple values must be reconciled.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethics2vec: aligning automatic agents and human preferences</title>
<link>https://arxiv.org/abs/2508.07673</link>
<guid>https://arxiv.org/abs/2508.07673</guid>
<content:encoded><![CDATA[
arXiv:2508.07673v1 Announce Type: new 
Abstract: Though intelligent agents are supposed to improve human experience (or make it more efficient), it is hard from a human perspective to grasp the ethical values which are explicitly or implicitly embedded in an agent behaviour. This is the well-known problem of alignment, which refers to the challenge of designing AI systems that align with human values, goals and preferences. This problem is particularly challenging since most human ethical considerations refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable) values and criteria. Consider, for instance, a medical agent prescribing a treatment to a cancerous patient. How could it take into account (and/or weigh) incommensurable aspects like the value of a human life and the cost of the treatment? Now, the alignment between human and artificial values is possible only if we define a common space where a metric can be defined and used. This paper proposes to extend to ethics the conventional Anything2vec approach, which has been successful in plenty of similar and hard-to-quantify domains (ranging from natural language processing to recommendation systems and graph analysis). This paper proposes a way to map an automatic agent decision-making (or control law) strategy to a multivariate vector representation, which can be used to compare and assess the alignment with human values. The Ethics2Vec method is first introduced in the case of an automatic agent performing binary decision-making. Then, a vectorisation of an automatic control law (like in the case of a self-driving car) is discussed to show how the approach can be extended to automatic control settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Aware Transformer Training for Automated Planning</title>
<link>https://arxiv.org/abs/2508.07743</link>
<guid>https://arxiv.org/abs/2508.07743</guid>
<content:encoded><![CDATA[
arXiv:2508.07743v1 Announce Type: new 
Abstract: While transformers excel in many settings, their application in the field of automated planning is limited. Prior work like PlanGPT, a state-of-the-art decoder-only transformer, struggles with extrapolation from easy to hard planning problems. This in turn stems from problem symmetries: planning tasks can be represented with arbitrary variable names that carry no meaning beyond being identifiers. This causes a combinatorial explosion of equivalent representations that pure transformers cannot efficiently learn from. We propose a novel contrastive learning objective to make transformers symmetry-aware and thereby compensate for their lack of inductive bias. Combining this with architectural improvements, we show that transformers can be efficiently trained for either plan-generation or heuristic-prediction. Our results across multiple planning domains demonstrate that our symmetry-aware training effectively and efficiently addresses the limitations of PlanGPT.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best-Effort Policies for Robust Markov Decision Processes</title>
<link>https://arxiv.org/abs/2508.07790</link>
<guid>https://arxiv.org/abs/2508.07790</guid>
<content:encoded><![CDATA[
arXiv:2508.07790v1 Announce Type: new 
Abstract: We study the common generalization of Markov decision processes (MDPs) with sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal in RMDPs is to compute a policy that maximizes the expected return under an adversarial choice of the transition probabilities. If the uncertainty in the probabilities is independent between the states, known as s-rectangularity, such optimal robust policies can be computed efficiently using robust value iteration. However, there might still be multiple optimal robust policies, which, while equivalent with respect to the worst-case, reflect different expected returns under non-adversarial choices of the transition probabilities. Hence, we propose a refined policy selection criterion for RMDPs, drawing inspiration from the notions of dominance and best-effort in game theory. Instead of seeking a policy that only maximizes the worst-case expected return, we additionally require the policy to achieve a maximal expected return under different (i.e., not fully adversarial) transition probabilities. We call such a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE policies always exist, characterize their structure, and present an algorithm to compute them with a small overhead compared to standard robust value iteration. ORBE policies offer a principled tie-breaker among optimal robust policies. Numerical experiments show the feasibility of our approach.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations</title>
<link>https://arxiv.org/abs/2508.07834</link>
<guid>https://arxiv.org/abs/2508.07834</guid>
<content:encoded><![CDATA[
arXiv:2508.07834v1 Announce Type: new 
Abstract: Over the years, the need for rescue operations throughout the world has increased rapidly. Demographic changes and the resulting risk of injury or health disorders form the basis for emergency calls. In such scenarios, first responders are in a rush to reach the patient in need, provide first aid, and save lives. In these situations, they must be able to provide personalized and optimized healthcare in the shortest possible time and estimate the patients condition with the help of freshly recorded vital data in an emergency situation. However, in such a timedependent situation, first responders and medical experts cannot fully grasp their knowledge and need assistance and recommendation for further medical treatments. To achieve this, on the spot calculated, evaluated, and processed knowledge must be made available to improve treatments by first responders. The Knowledge Graph presented in this article as a central knowledge representation provides first responders with an innovative knowledge management that enables intelligent treatment recommendations with an artificial intelligence-based pre-recognition of the situation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\(X\)-evolve: Solution space evolution powered by large language models</title>
<link>https://arxiv.org/abs/2508.07932</link>
<guid>https://arxiv.org/abs/2508.07932</guid>
<content:encoded><![CDATA[
arXiv:2508.07932v1 Announce Type: new 
Abstract: While combining large language models (LLMs) with evolutionary algorithms (EAs) shows promise for solving complex optimization problems, current approaches typically evolve individual solutions, often incurring high LLM call costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead evolves solution spaces \(X\) (sets of individual solutions) - subsets of the overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs wherein certain code snippets, designated as parameters, define a tunable solution space. A score-based search algorithm then efficiently explores this parametrically defined space, guided by feedback from objective function scores. This strategy enables broader and more efficient exploration, which can potentially accelerate convergence at a much lower search cost, requiring up to two orders of magnitude fewer LLM calls than prior leading methods. We demonstrate \(X\)-evolve's efficacy across three distinct hard optimization problems. For the cap set problem, we discover a larger partial admissible set, establishing a new tighter asymptotic lower bound for the cap set constant (\(C \ge 2.2203\)). In information theory, we uncover a larger independent set for the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946), thereby raising the known lower bound on its Shannon capacity. Furthermore, for the NP-hard online bin packing problem, we generate heuristics that consistently outperform standard strategies across established benchmarks. By evolving solution spaces, our method considerably improves search effectiveness, making it possible to tackle high-dimensional problems that were previously computationally prohibitive.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots</title>
<link>https://arxiv.org/abs/2508.07941</link>
<guid>https://arxiv.org/abs/2508.07941</guid>
<content:encoded><![CDATA[
arXiv:2508.07941v1 Announce Type: new 
Abstract: This article proposes a collision risk anticipation method based on short-term prediction of the agents position. A Long Short-Term Memory (LSTM) model, trained on past trajectories, is used to estimate the next position of each robot. This prediction allows us to define an anticipated collision risk by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent. The approach is tested in a constrained environment, where two robots move without communication or identifiers. Despite a limited sampling frequency (1 Hz), the results show a significant decrease of the collisions number and a stability improvement. The proposed method, which is computationally inexpensive, appears particularly attractive for implementation on embedded systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</title>
<link>https://arxiv.org/abs/2508.07950</link>
<guid>https://arxiv.org/abs/2508.07950</guid>
<content:encoded><![CDATA[
arXiv:2508.07950v1 Announce Type: new 
Abstract: Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</title>
<link>https://arxiv.org/abs/2508.08001</link>
<guid>https://arxiv.org/abs/2508.08001</guid>
<content:encoded><![CDATA[
arXiv:2508.08001v1 Announce Type: new 
Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a communication tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an LLM-based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty decoding module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fitting Description Logic Ontologies to ABox and Query Examples</title>
<link>https://arxiv.org/abs/2508.08007</link>
<guid>https://arxiv.org/abs/2508.08007</guid>
<content:encoded><![CDATA[
arXiv:2508.08007v1 Announce Type: new 
Abstract: We study a fitting problem inspired by ontology-mediated querying: given a collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof (UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be ${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</title>
<link>https://arxiv.org/abs/2508.08053</link>
<guid>https://arxiv.org/abs/2508.08053</guid>
<content:encoded><![CDATA[
arXiv:2508.08053v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence</title>
<link>https://arxiv.org/abs/2508.08075</link>
<guid>https://arxiv.org/abs/2508.08075</guid>
<content:encoded><![CDATA[
arXiv:2508.08075v1 Announce Type: new 
Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field of information fusion under uncertainty. Most existing research focuses on combining evidence within the same frame of discernment. However, in real-world scenarios, trained algorithms or data often originate from different regions or organizations, where data silos are prevalent. As a result, using different data sources or models to generate basic probability assignments may lead to heterogeneous frames, for which traditional fusion methods often yield unsatisfactory results. To address this challenge, this study proposes an open-world information fusion method, termed Full Negation Belief Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a criterion is introduced to determine whether a given fusion task belongs to the open-world setting. Then, by extending the frames, the method can accommodate elements from heterogeneous frames. Finally, a full negation mechanism is employed to transform the mass functions, so that existing combination rules can be applied to the transformed mass functions for such information fusion. Theoretically, the proposed method satisfies three desirable properties, which are formally proven: mass function invariance, heritability, and essential conflict elimination. Empirically, FNBT demonstrates superior performance in pattern classification tasks on real-world datasets and successfully resolves Zadeh's counterexample, thereby validating its practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</title>
<link>https://arxiv.org/abs/2508.08115</link>
<guid>https://arxiv.org/abs/2508.08115</guid>
<content:encoded><![CDATA[
arXiv:2508.08115v1 Announce Type: new 
Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically integrates evidence-based teamwork components from human-human collaboration into medical decision-making with large language models (LLMs). Our approach validates an organizational psychology teamwork model from human collaboration to computational multi-agent medical systems by operationalizing six core teamwork components derived from Salas et al.'s "Big Five" model: team leadership, mutual performance monitoring, team orientation, shared mental models, closed-loop communication, and mutual trust. We implement and evaluate these components as modular, configurable mechanisms within an adaptive collaboration architecture while assessing the effect of the number of agents involved based on the task's requirements and domain. Systematic evaluation of computational implementations of teamwork behaviors across eight medical benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets, Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8 evaluated datasets. Controlled ablation studies conducted on 50 questions per configuration across 3 independent runs provide mechanistic insights into individual component contributions, revealing optimal teamwork configurations that vary by reasoning task complexity and domain-specific requirements. Our ablation analyses reveal dataset-specific optimal teamwork configurations, indicating that different medical reasoning modalities benefit from distinct collaborative patterns. TeamMedAgents represents an advancement in collaborative AI by providing a systematic translation of established teamwork theories from human collaboration into agentic collaboration, establishing a foundation for evidence-based multi-agent system design in critical decision-making domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</title>
<link>https://arxiv.org/abs/2508.08127</link>
<guid>https://arxiv.org/abs/2508.08127</guid>
<content:encoded><![CDATA[
arXiv:2508.08127v1 Announce Type: new 
Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various communication patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework</title>
<link>https://arxiv.org/abs/2508.08147</link>
<guid>https://arxiv.org/abs/2508.08147</guid>
<content:encoded><![CDATA[
arXiv:2508.08147v1 Announce Type: new 
Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent that automatically converts natural-language descriptions of power system optimization scenarios into compact, solver-ready formulations and generates corresponding solutions. In contrast to approaches that rely solely on LLM to produce solutions directly, the proposed method focuses on discovering a mathematically compatible formulation that can be efficiently solved by off-the-shelf optimization solvers. Directly using LLMs to produce solutions often leads to infeasible or suboptimal results, as these models lack the numerical precision and constraint-handling capabilities of established optimization solvers. The pipeline integrates a domain-aware prompt and schema with an LLM, enforces feasibility through systematic validation and iterative repair, and returns both solver-ready models and user-facing results. Using the unit commitment problem as a representative case study, the agent produces optimal or near-optimal schedules along with the associated objective costs. Results demonstrate that coupling the solver with task-specific validation significantly enhances solution reliability. This work shows that combining AI with established optimization frameworks bridges high-level problem descriptions and executable mathematical models, enabling more efficient decision-making in energy systems
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPP: Unified Path Planner with Adaptive Safety and Optimality</title>
<link>https://arxiv.org/abs/2505.23197</link>
<guid>https://arxiv.org/abs/2505.23197</guid>
<content:encoded><![CDATA[
arXiv:2505.23197v1 Announce Type: cross 
Abstract: We are surrounded by robots helping us perform complex tasks. Robots have a wide range of applications, from industrial automation to personalized assistance. However, with great technological innovation come significant challenges. One of the major challenges in robotics is path planning. Despite advancements such as graph search, sampling, and potential field methods, most path planning algorithms focus either on optimality or on safety. Very little research addresses both simultaneously. We propose a Unified Path Planner (UPP) that uses modified heuristics and a dynamic safety cost function to balance safety and optimality. The level of safety can be adjusted via tunable parameters, trading off against computational complexity. We demonstrate the planner's performance in simulations, showing how parameter variation affects results. UPP is compared with various traditional and safe-optimal planning algorithms across different scenarios. We also validate it on a TurtleBot, where the robot successfully finds safe and sub-optimal paths.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers</title>
<link>https://arxiv.org/abs/2508.05691</link>
<guid>https://arxiv.org/abs/2508.05691</guid>
<content:encoded><![CDATA[
arXiv:2508.05691v1 Announce Type: cross 
Abstract: Generative models are increasingly adopted in high-stakes domains, yet current deployments offer no mechanisms to verify the origin of model outputs. We address this gap by extending model fingerprinting techniques beyond the traditional collaborative setting to one where the model provider may act adversarially. To our knowledge, this is the first work to evaluate fingerprinting for provenance attribution under such a threat model. The methods rely on a trusted verifier that extracts secret fingerprints from the model's output space, unknown to the provider, and trains a model to predict and verify them. Our empirical evaluation shows that our methods achieve near-zero FPR@95%TPR for instances of GAN and diffusion models, even when tested on small modifications to the original architecture and training data. Moreover, the methods remain robust against adversarial attacks that actively modify the outputs to bypass detection. Source codes are available at https://github.com/PSMLab/authprint.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction</title>
<link>https://arxiv.org/abs/2508.06495</link>
<guid>https://arxiv.org/abs/2508.06495</guid>
<content:encoded><![CDATA[
arXiv:2508.06495v1 Announce Type: cross 
Abstract: The accelerated dissemination of disinformation often outpaces the capacity for manual fact-checking, highlighting the urgent need for Semi-Automated Fact-Checking (SAFC) systems. Within the Portuguese language context, there is a noted scarcity of publicly available datasets that integrate external evidence, an essential component for developing robust AFC systems, as many existing resources focus solely on classification based on intrinsic text features. This dissertation addresses this gap by developing, applying, and analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) with external evidence. The approach simulates a user's verification process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash) to extract the main claim from texts and search engine APIs (Google Search API, Google FactCheck Claims Search API) to retrieve relevant external documents (evidence). Additionally, a data validation and preprocessing framework, including near-duplicate detection, is introduced to enhance the quality of the base corpora.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Commodity Price Shocks Using Temporal and Semantic Fusion of Prices Signals and Agentic Generative AI Extracted Economic News</title>
<link>https://arxiv.org/abs/2508.06497</link>
<guid>https://arxiv.org/abs/2508.06497</guid>
<content:encoded><![CDATA[
arXiv:2508.06497v1 Announce Type: cross 
Abstract: Accurate forecasting of commodity price spikes is vital for countries with limited economic buffers, where sudden increases can strain national budgets, disrupt import-reliant sectors, and undermine food and energy security. This paper introduces a hybrid forecasting framework that combines historical commodity price data with semantic signals derived from global economic news, using an agentic generative AI pipeline. The architecture integrates dual-stream Long Short-Term Memory (LSTM) networks with attention mechanisms to fuse structured time-series inputs with semantically embedded, fact-checked news summaries collected from 1960 to 2023. The model is evaluated on a 64-year dataset comprising normalized commodity price series and temporally aligned news embeddings. Results show that the proposed approach achieves a mean AUC of 0.94 and an overall accuracy of 0.91 substantially outperforming traditional baselines such as logistic regression (AUC = 0.34), random forest (AUC = 0.57), and support vector machines (AUC = 0.47). Additional ablation studies reveal that the removal of attention or dimensionality reduction leads to moderate declines in performance, while eliminating the news component causes a steep drop in AUC to 0.46, underscoring the critical value of incorporating real-world context through unstructured text. These findings demonstrate that integrating agentic generative AI with deep learning can meaningfully improve early detection of commodity price shocks, offering a practical tool for economic planning and risk mitigation in volatile market environments while saving the very high costs of operating a full generative AI agents pipeline.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-Specific Models for Multimodal Brain Response Prediction</title>
<link>https://arxiv.org/abs/2508.06499</link>
<guid>https://arxiv.org/abs/2508.06499</guid>
<content:encoded><![CDATA[
arXiv:2508.06499v1 Announce Type: cross 
Abstract: In this work, we present a network-specific approach for predicting brain responses to complex multimodal movies, leveraging the Yeo 7-network parcellation of the Schaefer atlas. Rather than treating the brain as a homogeneous system, we grouped the seven functional networks into four clusters and trained separate multi-subject, multi-layer perceptron (MLP) models for each. This architecture supports cluster-specific optimization and adaptive memory modeling, allowing each model to adjust temporal dynamics and modality weighting based on the functional role of its target network. Our results demonstrate that this clustered strategy significantly enhances prediction accuracy across the 1,000 cortical regions of the Schaefer atlas. The final model achieved an eighth-place ranking in the Algonauts Project 2025 Challenge, with out-of-distribution (OOD) correlation scores nearly double those of the baseline model used in the selection phase. Code is available at https://github.com/Corsi01/algo2025.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing with Canonical Microcircuits</title>
<link>https://arxiv.org/abs/2508.06501</link>
<guid>https://arxiv.org/abs/2508.06501</guid>
<content:encoded><![CDATA[
arXiv:2508.06501v1 Announce Type: cross 
Abstract: The human brain represents the only known example of general intelligence that naturally aligns with human values. On a mere 20-watt power budget, the brain achieves robust learning and adaptive decision-making in ways that continue to elude advanced AI systems. Inspired by the brain, we present a computational architecture based on canonical microcircuits (CMCs) - stereotyped patterns of neurons found ubiquitously throughout the cortex. We implement these circuits as neural ODEs comprising spiny stellate, inhibitory, and pyramidal neurons, forming an 8-dimensional dynamical system with biologically plausible recurrent connections. Our experiments show that even a single CMC node achieves 97.8 percent accuracy on MNIST, while hierarchical configurations - with learnable inter-regional connectivity and recurrent connections - yield improved performance on more complex image benchmarks. Notably, our approach achieves competitive results using substantially fewer parameters than conventional deep learning models. Phase space analysis revealed distinct dynamical trajectories for different input classes, highlighting interpretable, emergent behaviors observed in biological systems. These findings suggest that neuromorphic computing approaches can improve both efficiency and interpretability in artificial neural networks, offering new directions for parameter-efficient architectures grounded in the computational principles of the human brain.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Human Limits in Pattern Recognition: A Computational Model of Sequential Reasoning in Rock, Paper, Scissors</title>
<link>https://arxiv.org/abs/2508.06503</link>
<guid>https://arxiv.org/abs/2508.06503</guid>
<content:encoded><![CDATA[
arXiv:2508.06503v1 Announce Type: cross 
Abstract: How do we predict others from patterns in their behavior and what are the computational constraints that limit this ability? We investigate these questions by modeling human behavior over repeated games of rock, paper, scissors from Brockbank & Vul (2024). Against algorithmic opponents that varied in strategic sophistication, people readily exploit simple transition patterns (e.g., consistently playing rock after paper) but struggle to detect more complex sequential dependencies. To understand the cognitive mechanisms underlying these abilities and their limitations, we deploy Hypothetical Minds (HM), a large language model-based agent that generates and tests hypotheses about opponent strategies, as a cognitive model of this behavior (Cross et al., 2024). We show that when applied to the same experimental conditions, HM closely mirrors human performance patterns, succeeding and failing in similar ways. To better understand the source of HM's failures and whether people might face similar cognitive bottlenecks in this context, we performed a series of ablations and augmentations targeting different components of the system. When provided with natural language descriptions of the opponents' strategies, HM successfully exploited 6/7 bot opponents with win rates >80% suggesting that accurate hypothesis generation is the primary cognitive bottleneck in this task. Further, by systematically manipulating the model's hypotheses through pedagogically-inspired interventions, we find that the model substantially updates its causal understanding of opponent behavior, revealing how model-based analyses can produce testable hypotheses about human cognition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models</title>
<link>https://arxiv.org/abs/2508.06504</link>
<guid>https://arxiv.org/abs/2508.06504</guid>
<content:encoded><![CDATA[
arXiv:2508.06504v1 Announce Type: cross 
Abstract: Biomedical named entity recognition (NER) is a high-utility natural language processing (NLP) task, and large language models (LLMs) show promise particularly in few-shot settings (i.e., limited training data). In this article, we address the performance challenges of LLMs for few-shot biomedical NER by investigating a dynamic prompting strategy involving retrieval-augmented generation (RAG). In our approach, the annotated in-context learning examples are selected based on their similarities with the input texts, and the prompt is dynamically updated for each instance during inference. We implemented and optimized static and dynamic prompt engineering techniques and evaluated them on five biomedical NER datasets. Static prompting with structured components increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA 3-70B, relative to basic static prompting. Dynamic prompting further improved performance, with TF-IDF and SBERT retrieval methods yielding the best results, improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively. These findings highlight the utility of contextually adaptive prompts via RAG for biomedical NER.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06524</link>
<guid>https://arxiv.org/abs/2508.06524</guid>
<content:encoded><![CDATA[
arXiv:2508.06524v1 Announce Type: cross 
Abstract: Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiKV: KV Cache Management System for Mixture of Experts</title>
<link>https://arxiv.org/abs/2508.06526</link>
<guid>https://arxiv.org/abs/2508.06526</guid>
<content:encoded><![CDATA[
arXiv:2508.06526v1 Announce Type: cross 
Abstract: As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library: \href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition</title>
<link>https://arxiv.org/abs/2508.06528</link>
<guid>https://arxiv.org/abs/2508.06528</guid>
<content:encoded><![CDATA[
arXiv:2508.06528v1 Announce Type: cross 
Abstract: Video-based behavior recognition is essential in fields such as public safety, intelligent surveillance, and human-computer interaction. Traditional 3D Convolutional Neural Network (3D CNN) effectively capture local spatiotemporal features but struggle with modeling long-range dependencies. Conversely, Transformers excel at learning global contextual information but face challenges with high computational costs. To address these limitations, we propose a hybrid framework combining 3D CNN and Transformer architectures. The 3D CNN module extracts low-level spatiotemporal features, while the Transformer module captures long-range temporal dependencies, with a fusion mechanism integrating both representations. Evaluated on benchmark datasets, the proposed model outperforms traditional 3D CNN and standalone Transformers, achieving higher recognition accuracy with manageable complexity. Ablation studies further validate the complementary strengths of the two modules. This hybrid framework offers an effective and scalable solution for video-based behavior recognition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Breaking Words: Rethinking Multilingual Tokenizer Design</title>
<link>https://arxiv.org/abs/2508.06533</link>
<guid>https://arxiv.org/abs/2508.06533</guid>
<content:encoded><![CDATA[
arXiv:2508.06533v1 Announce Type: cross 
Abstract: While model architecture and training objectives are well-studied, tokenization, particularly in multilingual contexts, remains a relatively neglected aspect of Large Language Model (LLM) development. Existing tokenizers often exhibit high token-to-word ratios, inefficient use of context length, and slower inference. We present a systematic study that links vocabulary size, pre-tokenization rules, and training-corpus composition to both token-to-word efficiency and model quality. To ground our analysis in a linguistically diverse context, we conduct extensive experiments on Indic scripts, which present unique challenges due to their high script diversity and orthographic complexity. Drawing on the insights from these analyses, we propose a novel algorithm for data composition that balances multilingual data for tokenizer training. Our observations on pretokenization strategies significantly improve model performance, and our data composition algorithm reduces the average token-to-word ratio by approximately 6% with respect to the conventional data randomization approach. Our tokenizer achieves more than 40% improvement on average token-to-word ratio against stateof-the-art multilingual Indic models. This improvement yields measurable gains in both model performance and inference speed. This highlights tokenization alongside architecture and training objectives as a critical lever for building efficient, scalable multilingual LLMs
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.06534</link>
<guid>https://arxiv.org/abs/2508.06534</guid>
<content:encoded><![CDATA[
arXiv:2508.06534v1 Announce Type: cross 
Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD) systems is a critical and unresolved challenge. This paper introduces MetAdv, a novel adversarial testing platform that enables realistic, dynamic, and interactive evaluation by tightly integrating virtual simulation with physical vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical sandbox, within which we design a three-layer closed-loop testing environment with dynamic adversarial test evolution. This architecture facilitates end-to-end adversarial evaluation, ranging from high-level unified adversarial generation, through mid-level simulation-based interaction, to low-level execution on physical vehicles. Additionally, MetAdv supports a broad spectrum of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines, end-to-end learning, vision-language models). It supports flexible 3D vehicle modeling and seamless transitions between simulated and physical environments, with built-in compatibility for commercial platforms such as Apollo and Tesla. A key feature of MetAdv is its human-in-the-loop capability: besides flexible environmental configuration for more customized evaluation, it enables real-time capture of physiological signals and behavioral feedback from drivers, offering new insights into human-machine trust under adversarial conditions. We believe MetAdv can offer a scalable and unified framework for adversarial assessment, paving the way for safer AD.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots</title>
<link>https://arxiv.org/abs/2508.06538</link>
<guid>https://arxiv.org/abs/2508.06538</guid>
<content:encoded><![CDATA[
arXiv:2508.06538v1 Announce Type: cross 
Abstract: Reduced-order models are essential for motion planning and control of quadruped robots, as they simplify complex dynamics while preserving critical behaviors. This paper introduces a novel methodology for deriving such interpretable dynamic models, specifically for jumping. We capture the high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space by proposing a learning architecture combining Sparse Identification of Nonlinear Dynamics (SINDy) with physical structural priors on the jump dynamics. Our approach demonstrates superior accuracy to the traditional actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through simulation and hardware experiments across different jumping strategies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factor Augmented Supervised Learning with Text Embeddings</title>
<link>https://arxiv.org/abs/2508.06548</link>
<guid>https://arxiv.org/abs/2508.06548</guid>
<content:encoded><![CDATA[
arXiv:2508.06548v1 Announce Type: cross 
Abstract: Large language models (LLMs) generate text embeddings from text data, producing vector representations that capture the semantic meaning and contextual relationships of words. However, the high dimensionality of these embeddings often impedes efficiency and drives up computational cost in downstream tasks. To address this, we propose AutoEncoder-Augmented Learning with Text (AEALT), a supervised, factor-augmented framework that incorporates dimension reduction directly into pre-trained LLM workflows. First, we extract embeddings from text documents; next, we pass them through a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors. By modeling the nonlinear structure of complex embeddings, AEALT outperforms conventional deep-learning approaches that rely on raw embeddings. We validate its broad applicability with extensive experiments on classification, anomaly detection, and prediction tasks using multiple real-world public datasets. Numerical results demonstrate that AEALT yields substantial gains over both vanilla embeddings and several standard dimension reduction methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features</title>
<link>https://arxiv.org/abs/2508.06566</link>
<guid>https://arxiv.org/abs/2508.06566</guid>
<content:encoded><![CDATA[
arXiv:2508.06566v1 Announce Type: cross 
Abstract: Surface material recognition is a key component in robotic perception and physical interaction, particularly when leveraging both tactile and visual sensory inputs. In this work, we propose Surformer v1, a transformer-based architecture designed for surface classification using structured tactile features and PCA-reduced visual embeddings extracted via ResNet-50. The model integrates modality-specific encoders with cross-modal attention layers, enabling rich interactions between vision and touch. Currently, state-of-the-art deep learning models for vision tasks have achieved remarkable performance. With this in mind, our first set of experiments focused exclusively on tactile-only surface classification. Using feature engineering, we trained and evaluated multiple machine learning models, assessing their accuracy and inference time. We then implemented an encoder-only Transformer model tailored for tactile features. This model not only achieved the highest accuracy but also demonstrated significantly faster inference time compared to other evaluated models, highlighting its potential for real-time applications. To extend this investigation, we introduced a multimodal fusion setup by combining vision and tactile inputs. We trained both Surformer v1 (using structured features) and Multimodal CNN (using raw images) to examine the impact of feature-based versus image-based multimodal learning on classification accuracy and computational efficiency. The results showed that Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while the Multimodal CNN achieved slightly higher accuracy but required significantly more inference time. These findings suggest Surformer v1 offers a compelling balance between accuracy, efficiency, and computational cost for surface material recognition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Introduction to Programming in the times of AI: A case study of a course re-design</title>
<link>https://arxiv.org/abs/2508.06572</link>
<guid>https://arxiv.org/abs/2508.06572</guid>
<content:encoded><![CDATA[
arXiv:2508.06572v1 Announce Type: cross 
Abstract: The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios</title>
<link>https://arxiv.org/abs/2508.06575</link>
<guid>https://arxiv.org/abs/2508.06575</guid>
<content:encoded><![CDATA[
arXiv:2508.06575v1 Announce Type: cross 
Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their development and deployment. Safety-critical scenarios pose more severe challenges, necessitating efficient testing methods to validate AVs safety. This study focuses on designing an accelerated testing algorithm for AVs in safety-critical scenarios, enabling swift recognition of their driving capabilities. First, typical logical scenarios were extracted from real-world crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA) database, obtaining pre-crash features through reconstruction. Second, Baidu Apollo, an advanced black-box automated driving system (ADS) is integrated to control the behavior of the ego vehicle. Third, we proposed an adaptive large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to expedite the testing process. Experimental results demonstrate a significant enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an 84.00% coverage of safety-critical scenarios, with crash scenario coverage of 96.83% and near-crash scenario coverage of 92.07%. Compared to genetic algorithm (GA), adaptive large neighborhood-simulated annealing algorithm (ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage in safety-critical scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting</title>
<link>https://arxiv.org/abs/2508.06577</link>
<guid>https://arxiv.org/abs/2508.06577</guid>
<content:encoded><![CDATA[
arXiv:2508.06577v1 Announce Type: cross 
Abstract: Participatory Budgeting (PB) empowers citizens to propose and vote on public investment projects. Yet, despite its democratic potential, PB initiatives often suffer from low participation rates, limiting their visibility and perceived legitimacy. In this work, we aim to strengthen PB elections in two key ways: by supporting project proposers in crafting better proposals, and by helping PB organizers manage large volumes of submissions in a transparent manner. We propose a privacy-preserving approach to predict which PB proposals are likely to be funded, using only their textual descriptions and anonymous historical voting records -- without relying on voter demographics or personally identifiable information. We evaluate the performance of GPT 4 Turbo in forecasting proposal outcomes across varying contextual scenarios, observing that the LLM's prior knowledge needs to be complemented by past voting data to obtain predictions reflecting real-world PB voting behavior. Our findings highlight the potential of AI-driven tools to support PB processes by improving transparency, planning efficiency, and civic engagement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs</title>
<link>https://arxiv.org/abs/2508.06583</link>
<guid>https://arxiv.org/abs/2508.06583</guid>
<content:encoded><![CDATA[
arXiv:2508.06583v1 Announce Type: cross 
Abstract: The conversational capabilities of large language models hold significant promise for enabling scalable and interactive tutoring. While prior research has primarily examined their capacity for Socratic questioning, it often overlooks a critical dimension: adaptively guiding learners based on their cognitive states. This study shifts focus from mere question generation to the broader instructional guidance capability. We ask: Can LLMs emulate expert tutors who dynamically adjust strategies in response to learners' understanding? To investigate this, we propose GuideEval, a benchmark grounded in authentic educational dialogues that evaluates pedagogical guidance through a three-phase behavioral framework: (1) Perception, inferring learner states; (2) Orchestration, adapting instructional strategies; and (3) Elicitation, stimulating proper reflections. Empirical findings reveal that existing LLMs frequently fail to provide effective adaptive scaffolding when learners exhibit confusion or require redirection. Furthermore, we introduce a behavior-guided finetuning strategy that leverages behavior-prompted instructional dialogues, significantly enhancing guidance performance. By shifting the focus from isolated content evaluation to learner-centered interaction, our work advocates a more dialogic paradigm for evaluating Socratic LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution</title>
<link>https://arxiv.org/abs/2508.06584</link>
<guid>https://arxiv.org/abs/2508.06584</guid>
<content:encoded><![CDATA[
arXiv:2508.06584v1 Announce Type: cross 
Abstract: The development, integration, and maintenance of geospatial databases rely heavily on efficient and accurate matching procedures of Geospatial Entity Resolution (ER). While resolution of points-of-interest (POIs) has been widely addressed, resolution of entities with diverse geometries has been largely overlooked. This is partly due to the lack of a uniform technique for embedding heterogeneous geometries seamlessly into a neural network framework. Existing neural approaches simplify complex geometries to a single point, resulting in significant loss of spatial information. To address this limitation, we propose Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is capable of embedding point, line, polyline, polygon, and multi-polygon geometries, enabling the model to capture the complex geospatial intricacies of the places being compared. Furthermore, Omni leverages transformer-based pre-trained language models over individual textual attributes of place records in an Attribute Affinity mechanism. The model is rigorously tested on existing point-only datasets and a new diverse-geometry geospatial ER dataset. Omni produces up to 12% (F1) improvement over existing methods.
  Furthermore, we test the potential of Large Language Models (LLMs) to conduct geospatial ER, experimenting with prompting strategies and learning scenarios, comparing the results of pre-trained language model-based methods with LLMs. Results indicate that LLMs show competitive results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2508.06588</link>
<guid>https://arxiv.org/abs/2508.06588</guid>
<content:encoded><![CDATA[
arXiv:2508.06588v1 Announce Type: cross 
Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for learning discrete representations of graph-structured data. However, a fundamental challenge, i.e., codebook collapse, remains underexplored in the graph domain, significantly limiting the expressiveness and generalization of graph tokens.In this paper, we present the first empirical study showing that codebook collapse consistently occurs when applying VQ to graph data, even with mitigation strategies proposed in vision or language domains. To understand why graph VQ is particularly vulnerable to collapse, we provide a theoretical analysis and identify two key factors: early assignment imbalances caused by redundancy in graph features and structural patterns, and self-reinforcing optimization loops in deterministic VQ. To address these issues, we propose RGVQ, a novel framework that integrates graph topology and feature similarity as explicit regularization signals to enhance codebook utilization and promote token diversity. RGVQ introduces soft assignments via Gumbel-Softmax reparameterization, ensuring that all codewords receive gradient updates. In addition, RGVQ incorporates a structure-aware contrastive regularization to penalize the token co-assignments among similar node pairs. Extensive experiments demonstrate that RGVQ substantially improves codebook utilization and consistently boosts the performance of state-of-the-art graph VQ backbones across multiple downstream tasks, enabling more expressive and transferable graph token representations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis</title>
<link>https://arxiv.org/abs/2508.06589</link>
<guid>https://arxiv.org/abs/2508.06589</guid>
<content:encoded><![CDATA[
arXiv:2508.06589v1 Announce Type: cross 
Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing neuroimaging data for neurological and psychiatric disorders. However, small-sample studies suffer from low reproducibility, while large-scale datasets introduce confounding heterogeneity due to multiple disease subtypes being labeled under a single category. To address these challenges, we propose a novel federated learning framework tailored for neuroimaging CAD systems. Our approach includes a dynamic navigation module that routes samples to the most suitable local models based on latent subtype representations, and a meta-integration module that combines predictions from heterogeneous local models into a unified diagnostic output. We evaluated our framework using a comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100 healthy controls across multiple study cohorts. Experimental results demonstrate significant improvements in diagnostic accuracy and robustness compared to traditional methods. Specifically, our framework achieved an average accuracy of 74.06\% across all tested sites, showcasing its effectiveness in handling subtype heterogeneity and enhancing model generalizability. Ablation studies further confirmed the importance of both the dynamic navigation and meta-integration modules in improving performance. By addressing data heterogeneity and subtype confounding, our framework advances reliable and reproducible neuroimaging CAD systems, offering significant potential for personalized medicine and clinical decision-making in neurology and psychiatry.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials</title>
<link>https://arxiv.org/abs/2508.06591</link>
<guid>https://arxiv.org/abs/2508.06591</guid>
<content:encoded><![CDATA[
arXiv:2508.06591v1 Announce Type: cross 
Abstract: Large language models (LLMs) have reshaped the research landscape by enabling new approaches to knowledge retrieval and creative ideation. Yet their application in discipline-specific experimental science, particularly in highly multi-disciplinary domains like materials science, remains limited. We present a first-of-its-kind framework that integrates generative AI with literature from hitherto-unconnected fields such as plant science, biomimetics, and materials engineering to extract insights and design experiments for materials. We focus on humidity-responsive systems such as pollen-based materials and Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and adaptive performance. Using a suite of AI tools, including a fine-tuned model (BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a Hierarchical Sampling strategy, we extract structure-property relationships and translate them into new classes of bioinspired materials. Structured inference protocols generate and evaluate hundreds of hypotheses from a single query, surfacing novel and experimentally tractable ideas. We validate our approach through real-world implementation: LLM-generated procedures, materials designs, and mechanical predictions were tested in the laboratory, culminating in the fabrication of a novel pollen-based adhesive with tunable morphology and measured shear strength, establishing a foundation for future plant-derived adhesive design. This work demonstrates how AI-assisted ideation can drive real-world materials design and enable effective human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Integrated Alignment</title>
<link>https://arxiv.org/abs/2508.06592</link>
<guid>https://arxiv.org/abs/2508.06592</guid>
<content:encoded><![CDATA[
arXiv:2508.06592v1 Announce Type: cross 
Abstract: As AI adoption expands across human society, the problem of aligning AI models to match human preferences remains a grand challenge. Currently, the AI alignment field is deeply divided between behavioral and representational approaches, resulting in narrowly aligned models that are more vulnerable to increasingly deceptive misalignment threats. In the face of this fragmentation, we propose an integrated vision for the future of the field. Drawing on related lessons from immunology and cybersecurity, we lay out a set of design principles for the development of Integrated Alignment frameworks that combine the complementary strengths of diverse alignment approaches through deep integration and adaptive coevolution. We highlight the importance of strategic diversity - deploying orthogonal alignment and misalignment detection approaches to avoid homogeneous pipelines that may be "doomed to success". We also recommend steps for greater unification of the AI alignment research field itself, through cross-collaboration, open model weights and shared community resources.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Without an Expert Curated Dataset</title>
<link>https://arxiv.org/abs/2508.06595</link>
<guid>https://arxiv.org/abs/2508.06595</guid>
<content:encoded><![CDATA[
arXiv:2508.06595v1 Announce Type: cross 
Abstract: Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
arXiv:2508.06601v1 Announce Type: cross 
Abstract: Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach</title>
<link>https://arxiv.org/abs/2508.06616</link>
<guid>https://arxiv.org/abs/2508.06616</guid>
<content:encoded><![CDATA[
arXiv:2508.06616v1 Announce Type: cross 
Abstract: With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions to enable adaptive, intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a hierarchical learning-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. Unlike most existing approaches that apply GenAI in the form of LLMs for intent processing only, we propose a hierarchical framework that introduces GenAI across all three stages of IDN. To demonstrate the effectiveness of the proposed IDN management architecture, we present a case study based on the latest GenAI architecture named Mamba. The case study shows how the proposed GenAI-driven architecture enhances network performance through intelligent automation, surpassing the performance of the conventional IDN architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Scaling Laws for Dense and Sparse Large Language Models</title>
<link>https://arxiv.org/abs/2508.06617</link>
<guid>https://arxiv.org/abs/2508.06617</guid>
<content:encoded><![CDATA[
arXiv:2508.06617v1 Announce Type: cross 
Abstract: Over the past few years, the size of language models has grown exponentially, as has the computational cost to train these large models. This rapid growth has motivated researchers to develop new techniques aimed at enhancing the efficiency of the training process. Despite these advancements, optimally predicting the model size or allocating optimal resources remains a challenge. Several efforts have addressed the challenge by proposing different scaling laws, but almost all of them are architecture-specific (dense or sparse). In this work we revisit existing scaling laws and propose a generalized scaling law to provide a unified framework that is applicable to both dense and sparse large language models. We evaluate and compare our proposed scaling law with existing scaling laws to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record</title>
<link>https://arxiv.org/abs/2508.06627</link>
<guid>https://arxiv.org/abs/2508.06627</guid>
<content:encoded><![CDATA[
arXiv:2508.06627v1 Announce Type: cross 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at https://github.com/MosbahAouad/EarlyPDAC-MML.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition</title>
<link>https://arxiv.org/abs/2508.06632</link>
<guid>https://arxiv.org/abs/2508.06632</guid>
<content:encoded><![CDATA[
arXiv:2508.06632v1 Announce Type: cross 
Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view synthesis, but challenges remain in rendering scenes with complex specular reflections and highlights. Existing approaches may produce blurry reflections due to entanglement between lighting and material properties, or encounter optimization instability when relying on physically-based inverse rendering. In this work, we present a neural rendering framework based on dynamic coefficient decomposition, aiming to improve the modeling of view-dependent appearance. Our approach decomposes complex appearance into a shared, static neural basis that encodes intrinsic material properties, and a set of dynamic coefficients generated by a Coefficient Network conditioned on view and illumination. A Dynamic Radiance Integrator then combines these components to synthesize the final radiance. Experimental results on several challenging benchmarks suggest that our method can produce sharper and more realistic specular highlights compared to existing techniques. We hope that this decomposition paradigm can provide a flexible and effective direction for modeling complex appearance in neural scene representations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Imperfect Synthetic Data in Downstream Inference Tasks</title>
<link>https://arxiv.org/abs/2508.06635</link>
<guid>https://arxiv.org/abs/2508.06635</guid>
<content:encoded><![CDATA[
arXiv:2508.06635v1 Announce Type: cross 
Abstract: Predictions and generations from large language models are increasingly being explored as an aid to computational social science and human subject research in limited data regimes. While previous technical work has explored the potential to use model-predicted labels for unlabeled data in a principled manner, there is increasing interest in using large language models to generate entirely new synthetic samples (also termed as synthetic simulations), such as in responses to surveys. However, it is not immediately clear by what means practitioners can combine such data with real data and yet produce statistically valid conclusions upon them. In this work, we introduce a new estimator based on generalized method of moments, providing a hyperparameter-free solution with strong theoretical guarantees to address the challenge at hand. Surprisingly, we find that interactions between the moment residuals of synthetic data and those of real data can improve estimates of the target parameter. We empirically validate the finite-sample performance of our estimator across different regression tasks in computational social science applications, demonstrating large empirical gains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series</title>
<link>https://arxiv.org/abs/2508.06638</link>
<guid>https://arxiv.org/abs/2508.06638</guid>
<content:encoded><![CDATA[
arXiv:2508.06638v1 Announce Type: cross 
Abstract: As time series data become increasingly prevalent in domains such as manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt to nonstationary environments where statistical properties shift over time. Traditional static thresholds are easily rendered obsolete by regime shifts, concept drift, or multi-scale changes. To address these challenges, we introduce and empirically evaluate two novel adaptive thresholding frameworks: Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence Segments (MACS). Both leverage statistical online learning and segmentation principles for local, contextually sensitive adaptation, maintaining guarantees on false alarm rates even under evolving distributions. Our experiments across Wafer Manufacturing benchmark datasets show significant F1-score improvement compared to traditional percentile and rolling quantile approaches. This work demonstrates that robust, statistically principled adaptive thresholds enable reliable, interpretable, and timely detection of diverse real-world anomalies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractal Language Modelling by Universal Sequence Maps (USM)</title>
<link>https://arxiv.org/abs/2508.06641</link>
<guid>https://arxiv.org/abs/2508.06641</guid>
<content:encoded><![CDATA[
arXiv:2508.06641v1 Announce Type: cross 
Abstract: Motivation: With the advent of Language Models using Transformers, popularized by ChatGPT, there is a renewed interest in exploring encoding procedures that numerically represent symbolic sequences at multiple scales and embedding dimensions. The challenge that encoding addresses is the need for mechanisms that uniquely retain contextual information about the succession of individual symbols, which can then be modeled by nonlinear formulations such as neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively encode symbolic sequences onto embedded numerical spaces. USM is composed of two Chaos Game Representations (CGR), iterated forwardly and backwardly, that can be projected into the frequency domain (FCGR). The corresponding USM coordinates can be used to compute a Chebyshev distance metric as well as k-mer frequencies, without having to recompute the embedded numeric coordinates, and, paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal Sequence Maps (USM) by resolving seeding biases affecting the iterated process. The resolution had two results, the first expected, the second an intriguing outcome: 1) full reconciliation of numeric positioning with sequence identity; and 2) uncovering the nature of USM as an efficient numeric process converging towards a steady state sequence embedding solution. We illustrate these results for genomic sequences because of the convenience of a planar representation defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless, the application to alphabet of arbitrary cardinality was found to be straightforward.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Reinforcement Learning via Communicative World Models</title>
<link>https://arxiv.org/abs/2508.06659</link>
<guid>https://arxiv.org/abs/2508.06659</guid>
<content:encoded><![CDATA[
arXiv:2508.06659v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks and contexts without updating their parameters, mainly because their learned representations and policies are overfit to the specifics of their training environments. To boost agents' in-context RL (ICRL) ability, this work formulates ICRL as a two-agent emergent communication problem and introduces CORAL (Communicative Representation for Adaptive RL), a framework that learns a transferable communicative context by decoupling latent representation learning from control. In CORAL, an Information Agent (IA) is pre-trained as a world model on a diverse distribution of tasks. Its objective is not to maximize task reward, but to build a world model and distill its understanding into concise messages. The emergent communication protocol is shaped by a novel Causal Influence Loss, which measures the effect that the message has on the next action. During deployment, the previously trained IA serves as a fixed contextualizer for a new Control Agent (CA), which learns to solve tasks by interpreting the provided communicative context. Our experiments demonstrate that this approach enables the CA to achieve significant gains in sample efficiency and successfully perform zero-shot adaptation with the help of pre-trained IA in entirely unseen sparse-reward environments, validating the efficacy of learning a transferable communicative representation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Biased Models Have Biased Thoughts?</title>
<link>https://arxiv.org/abs/2508.06671</link>
<guid>https://arxiv.org/abs/2508.06671</guid>
<content:encoded><![CDATA[
arXiv:2508.06671v1 Announce Type: cross 
Abstract: The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: \textit{Do biased models have biased thoughts}? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMFformer: Multimodal Fusion Transformer Network for Depression Detection</title>
<link>https://arxiv.org/abs/2508.06701</link>
<guid>https://arxiv.org/abs/2508.06701</guid>
<content:encoded><![CDATA[
arXiv:2508.06701v1 Announce Type: cross 
Abstract: Depression is a serious mental health illness that significantly affects an individual's well-being and quality of life, making early detection crucial for adequate care and treatment. Detecting depression is often difficult, as it is based primarily on subjective evaluations during clinical interviews. Hence, the early diagnosis of depression, thanks to the content of social networks, has become a prominent research area. The extensive and diverse nature of user-generated information poses a significant challenge, limiting the accurate extraction of relevant temporal information and the effective fusion of data across multiple modalities. This paper introduces MMFformer, a multimodal depression detection network designed to retrieve depressive spatio-temporal high-level patterns from multimodal social media information. The transformer network with residual connections captures spatial features from videos, and a transformer encoder is exploited to design important temporal dynamics in audio. Moreover, the fusion architecture fused the extracted features through late and intermediate fusion strategies to find out the most relevant intermodal correlations among them. Finally, the proposed network is assessed on two large-scale depression detection datasets, and the results clearly reveal that it surpasses existing state-of-the-art approaches, improving the F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is made available publicly at https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2508.06709</link>
<guid>https://arxiv.org/abs/2508.06709</guid>
<content:encoded><![CDATA[
arXiv:2508.06709v1 Announce Type: cross 
Abstract: Large language models (LLMs) can serve as judges that offer rapid and reliable assessments of other LLM outputs. However, models may systematically assign overly favorable ratings to their own outputs, a phenomenon known as self-bias, which can distort evaluations of true model performance. Previous studies often conflate genuine differences in model quality with bias or incorrectly assume that evaluations from LLMs and humans follow the same rating distributions. In this work, we present a statistical framework that explicitly formalizes assumptions under which self-bias can be identified and estimated. Our method models the difference in the scoring distribution that LLM-as-a-judge assigns to its own completions compared to other models, while accounting for the underlying quality of the completions provided by an independent, third-party judge (e.g., humans). Our method reliably isolates and quantifies self-bias, even when models vary in ability, ensuring that genuine performance differences are not mistaken for self-bias. We conduct an empirical analysis of self-bias on a large dataset (>5000 prompt-completion pairs) consisting of expert human annotations and judgments from nine different LLM judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet, systematically assign higher scores to their own outputs. These models also display family-bias; systematically assigning higher ratings to outputs produced by other models of the same family. Our findings highlight potential pitfalls of using LLM judges and offer practical guidance to mitigate biases when interpreting automated evaluations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.06729</link>
<guid>https://arxiv.org/abs/2508.06729</guid>
<content:encoded><![CDATA[
arXiv:2508.06729v1 Announce Type: cross 
Abstract: Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Causal Structure Distributions for Robust Planning</title>
<link>https://arxiv.org/abs/2508.06742</link>
<guid>https://arxiv.org/abs/2508.06742</guid>
<content:encoded><![CDATA[
arXiv:2508.06742v1 Announce Type: cross 
Abstract: Structural causal models describe how the components of a robotic system interact. They provide both structural and functional information about the relationships that are present in the system. The structural information outlines the variables among which there is interaction. The functional information describes how such interactions work, via equations or learned models. In this paper we find that learning the functional relationships while accounting for the uncertainty about the structural information leads to more robust dynamics models which improves downstream planning, while using significantly lower computational resources. This in contrast with common model-learning methods that ignore the causal structure and fail to leverage the sparsity of interactions in robotic systems. We achieve this by estimating a causal structure distribution that is used to sample causal graphs that inform the latent-space representations in an encoder-multidecoder probabilistic model. We show that our model can be used to learn the dynamics of a robot, which together with a sampling-based planner can be used to perform new tasks in novel environments, provided an objective function for the new requirement is available. We validate our method using manipulators and mobile robots in both simulation and the real-world. Additionally, we validate the learned dynamics' adaptability and increased robustness to corrupted inputs and changes in the environment, which is highly desirable in challenging real-world robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Schedule-Free Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2508.06743</link>
<guid>https://arxiv.org/abs/2508.06743</guid>
<content:encoded><![CDATA[
arXiv:2508.06743v1 Announce Type: cross 
Abstract: First-order methods underpin most large-scale learning algorithms, yet their classical convergence guarantees hinge on carefully scheduled step-sizes that depend on the total horizon $T$, which is rarely known in advance. The Schedule-Free (SF) method promises optimal performance with hyperparameters that are independent of $T$ by interpolating between Polyak--Ruppert averaging and momentum, but nonconvex analysis of SF has been limited or reliant on strong global assumptions. We introduce a robust Lyapunov framework that, under only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step descent inequality. This yields horizon-agnostic bounds in the nonconvex setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for polynomial averaging. We complement these proofs with Performance Estimation Problem (PEP) experiments that numerically validate our rates and suggest that our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to $O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex optimization and charts future directions for optimal nonconvex rates.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many-Turn Jailbreaking</title>
<link>https://arxiv.org/abs/2508.06755</link>
<guid>https://arxiv.org/abs/2508.06755</guid>
<content:encoded><![CDATA[
arXiv:2508.06755v1 Announce Type: cross 
Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit unsafe outputs from given prompts. However, it only focuses on single-turn jailbreaking targeting one specific query. On the contrary, the advanced LLMs are designed to handle extremely long contexts and can thus conduct multi-turn conversations. So, we propose exploring multi-turn jailbreaking, in which the jailbroken LLMs are continuously tested on more than the first-turn conversation or a single target query. This is an even more serious threat because 1) it is common for users to continue asking relevant follow-up questions to clarify certain jailbroken details, and 2) it is also possible that the initial round of jailbreaking causes the LLMs to respond to additional irrelevant questions consistently. As the first step (First draft done at June 2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and closed-source models and provide novel insights into this new safety threat. By revealing this new vulnerability, we aim to call for community efforts to build safer LLMs and pave the way for a more in-depth understanding of jailbreaking LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI</title>
<link>https://arxiv.org/abs/2508.06756</link>
<guid>https://arxiv.org/abs/2508.06756</guid>
<content:encoded><![CDATA[
arXiv:2508.06756v1 Announce Type: cross 
Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is essential for effective glioma management. Traditional methods rely on invasive tissue sampling, which may fail to capture a tumor's spatial heterogeneity. While deep learning models have shown promise in molecular profiling, their performance is often limited by scarce annotated data. In contrast, foundation deep learning models offer a more generalizable approach for glioma imaging biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch signals associated with IDH mutation. The model was trained and validated on a diverse, multi-center cohort of 1705 glioma patients from six public datasets. Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE and CMD modules are essential for improving predictive accuracy. By integrating large-scale pretraining and task-specific fine-tuning, FoundBioNet enables generalizable glioma characterization. This approach enhances diagnostic accuracy and interpretability, with the potential to enable more personalized patient care.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</title>
<link>https://arxiv.org/abs/2508.06763</link>
<guid>https://arxiv.org/abs/2508.06763</guid>
<content:encoded><![CDATA[
arXiv:2508.06763v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress across a range of vision-language tasks and demonstrate strong potential for traffic accident understanding. However, existing MLLMs in this domain primarily focus on coarse-grained image-level or video-level comprehension and often struggle to handle fine-grained visual details or localized scene components, limiting their applicability in complex accident scenarios. To address these limitations, we propose SafePLUG, a novel framework that empowers MLLMs with both Pixel-Level Understanding and temporal Grounding for comprehensive traffic accident analysis. SafePLUG supports both arbitrary-shaped visual prompts for region-aware question answering and pixel-level segmentation based on language instructions, while also enabling the recognition of temporally anchored events in traffic accident scenarios. To advance the development of MLLMs for traffic accident understanding, we curate a new dataset containing multimodal question-answer pairs centered on diverse accident scenarios, with detailed pixel-level annotations and temporal event boundaries. Experimental results show that SafePLUG achieves strong performance on multiple tasks, including region-based question answering, pixel-level segmentation, temporal event localization, and accident event understanding. These capabilities lay a foundation for fine-grained understanding of complex traffic scenes, with the potential to improve driving safety and enhance situational awareness in smart transportation systems. The code, dataset, and model checkpoints will be made publicly available at: https://zihaosheng.github.io/SafePLUG
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</title>
<link>https://arxiv.org/abs/2508.06767</link>
<guid>https://arxiv.org/abs/2508.06767</guid>
<content:encoded><![CDATA[
arXiv:2508.06767v1 Announce Type: cross 
Abstract: Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift</title>
<link>https://arxiv.org/abs/2508.06776</link>
<guid>https://arxiv.org/abs/2508.06776</guid>
<content:encoded><![CDATA[
arXiv:2508.06776v1 Announce Type: cross 
Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for detecting model drift from null directions of transformer activations without task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound for low-rank updates, and (iv) a logarithmic-regret guarantee for online null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with non-asymptotic tail bounds and a concentration inequality, yielding a-priori thresholds for drift under a Gaussian null model. These results show that monitoring right/left null spaces of layer activations and their Fisher geometry provides concrete, testable guarantees on representational change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</title>
<link>https://arxiv.org/abs/2508.06781</link>
<guid>https://arxiv.org/abs/2508.06781</guid>
<content:encoded><![CDATA[
arXiv:2508.06781v1 Announce Type: cross 
Abstract: Neural sentence embedding models for dense retrieval typically rely on binary relevance labels, treating query-document pairs as either relevant or irrelevant. However, real-world relevance often exists on a continuum, and recent advances in large language models (LLMs) have made it feasible to scale the generation of fine-grained graded relevance labels. In this work, we propose BiXSE, a simple and effective pointwise training method that optimizes binary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSE interprets these scores as probabilistic targets, enabling granular supervision from a single labeled query-document pair per query. Unlike pairwise or listwise losses that require multiple annotated comparisons per query, BiXSE achieves strong performance with reduced annotation and compute costs by leveraging in-batch negatives. Extensive experiments across sentence embedding (MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistently outperforms softmax-based contrastive learning (InfoNCE), and matches or exceeds strong pairwise ranking baselines when trained on LLM-supervised data. BiXSE offers a robust, scalable alternative for training dense retrieval models as graded relevance supervision becomes increasingly accessible.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROPS: Progressively Private Self-alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.06783</link>
<guid>https://arxiv.org/abs/2508.06783</guid>
<content:encoded><![CDATA[
arXiv:2508.06783v1 Announce Type: cross 
Abstract: Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</title>
<link>https://arxiv.org/abs/2508.06784</link>
<guid>https://arxiv.org/abs/2508.06784</guid>
<content:encoded><![CDATA[
arXiv:2508.06784v1 Announce Type: cross 
Abstract: High-dimensional data, particularly in the form of high-order tensors, presents a major challenge in self-supervised learning. While MLP-based autoencoders (AE) are commonly employed, their dependence on flattening operations exacerbates the curse of dimensionality, leading to excessively large model sizes, high computational overhead, and challenging optimization for deep structural feature capture. Although existing tensor networks alleviate computational burdens through tensor decomposition techniques, most exhibit limited capability in learning non-linear relationships. To overcome these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear framework and employs a Pick-and-Unfold strategy, facilitating flexible per-mode encoding of high-order tensors via recursive unfold-encode-fold operations, effectively integrating tensor structural priors. Notably, MA-NTAE exhibits linear growth in computational complexity with tensor order and proportional growth with mode dimensions. Extensive experiments demonstrate MA-NTAE's performance advantages over standard AE and current tensor networks in compression and clustering tasks, which become increasingly pronounced for higher-order, higher-dimensional tensors.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Spiking Graph Neural Network</title>
<link>https://arxiv.org/abs/2508.06793</link>
<guid>https://arxiv.org/abs/2508.06793</guid>
<content:encoded><![CDATA[
arXiv:2508.06793v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive capabilities in modeling graph-structured data, while Spiking Neural Networks (SNNs) offer high energy efficiency through sparse, event-driven computation. However, existing spiking GNNs predominantly operate in Euclidean space and rely on fixed geometric assumptions, limiting their capacity to model complex graph structures such as hierarchies and cycles. To overcome these limitations, we propose \method{}, a novel Geometry-Aware Spiking Graph Neural Network that unifies spike-based neural dynamics with adaptive representation learning on Riemannian manifolds. \method{} features three key components: a Riemannian Embedding Layer that projects node features into a pool of constant-curvature manifolds, capturing non-Euclidean structures; a Manifold Spiking Layer that models membrane potential evolution and spiking behavior in curved spaces via geometry-consistent neighbor aggregation and curvature-based attention; and a Manifold Learning Objective that enables instance-wise geometry adaptation through jointly optimized classification and link prediction losses defined over geodesic distances. All modules are trained using Riemannian SGD, eliminating the need for backpropagation through time. Extensive experiments on multiple benchmarks show that GSG achieves superior accuracy, robustness, and energy efficiency compared to both Euclidean SNNs and manifold-based GNNs, establishing a new paradigm for curvature-aware, energy-efficient graph learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</title>
<link>https://arxiv.org/abs/2508.06799</link>
<guid>https://arxiv.org/abs/2508.06799</guid>
<content:encoded><![CDATA[
arXiv:2508.06799v1 Announce Type: cross 
Abstract: Digital Twins (DTs) offer powerful tools for managing complex infrastructure systems, but their effectiveness is often limited by challenges in integrating unstructured knowledge. Recent advances in Large Language Models (LLMs) bring new potential to address this gap, with strong abilities in extracting and organizing diverse textual information. We therefore propose LSDTs (LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract planning knowledge from unstructured documents like environmental regulations and technical guidelines, and organize it into a formal ontology. This ontology forms a semantic layer that powers a digital twin-a virtual model of the physical system-allowing it to simulate realistic, regulation-aware planning scenarios. We evaluate LSDTs through a case study of offshore wind farm planning in Maryland, including its application during Hurricane Sandy. Results demonstrate that LSDTs support interpretable, regulation-aware layout optimization, enable high-fidelity simulation, and enhance adaptability in infrastructure planning. This work shows the potential of combining generative AI with digital twins to support complex, knowledge-driven planning tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</title>
<link>https://arxiv.org/abs/2508.06800</link>
<guid>https://arxiv.org/abs/2508.06800</guid>
<content:encoded><![CDATA[
arXiv:2508.06800v1 Announce Type: cross 
Abstract: Missing modalities have recently emerged as a critical research direction in multimodal emotion recognition (MER). Conventional approaches typically address this issue through missing modality reconstruction. However, these methods fail to account for variations in reconstruction difficulty across different samples, consequently limiting the model's ability to handle hard samples effectively. To overcome this limitation, we propose a novel Hardness-Aware Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates in two key stages: first, it estimates the hardness level of each sample, and second, it strategically emphasizes hard samples during training to enhance model performance on these challenging instances. Specifically, we first introduce a Multi-view Hardness Evaluation mechanism that quantifies reconstruction difficulty by considering both Direct Hardness (modality reconstruction errors) and Indirect Hardness (cross-modal mutual information). Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy that dynamically adjusts the training curriculum by retrieving samples with similar semantic information and balancing the learning focus between easy and hard instances. Extensive experiments on benchmark datasets demonstrate that HARDY-MER consistently outperforms existing methods in missing-modality scenarios. Our code will be made publicly available at https://github.com/HARDY-MER/HARDY-MER.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation</title>
<link>https://arxiv.org/abs/2508.06806</link>
<guid>https://arxiv.org/abs/2508.06806</guid>
<content:encoded><![CDATA[
arXiv:2508.06806v1 Announce Type: cross 
Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online fine-tuning on an offline pre-trained policy to minimize costly online interactions. Existing work used offline datasets to generate data that conform to the online data distribution for data augmentation. However, generated data still exhibits a gap with the online data, limiting overall performance. To address this, we propose a new data augmentation approach, Classifier-Free Diffusion Generation (CFDG). Without introducing additional classifier training overhead, CFDG leverages classifier-free guidance diffusion to significantly enhance the generation quality of offline and online data with different distributions. Additionally, it employs a reweighting method to enable more generated data to align with the online data, enhancing performance while maintaining the agent's stability. Experimental results show that CFDG outperforms replaying the two data types or using a standard diffusion model to generate new data. Our method is versatile and can be integrated with existing offline-to-online RL algorithms. By implementing CFDG to popular methods IQL, PEX and APL, we achieve a notable 15% average improvement in empirical performance on the D4RL benchmark such as MuJoCo and AntMaze.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face</title>
<link>https://arxiv.org/abs/2508.06811</link>
<guid>https://arxiv.org/abs/2508.06811</guid>
<content:encoded><![CDATA[
arXiv:2508.06811v1 Announce Type: cross 
Abstract: Many have observed that the development and deployment of generative machine learning (ML) and artificial intelligence (AI) models follow a distinctive pattern in which pre-trained models are adapted and fine-tuned for specific downstream tasks. However, there is limited empirical work that examines the structure of these interactions. This paper analyzes 1.86 million models on Hugging Face, a leading peer production platform for model development. Our study of model family trees -- networks that connect fine-tuned models to their base or parent -- reveals sprawling fine-tuning lineages that vary widely in size and structure. Using an evolutionary biology lens to study ML models, we use model metadata and model cards to measure the genetic similarity and mutation of traits over model families. We find that models tend to exhibit a family resemblance, meaning their genetic markers and traits exhibit more overlap when they belong to the same model family. However, these similarities depart in certain ways from standard models of asexual reproduction, because mutations are fast and directed, such that two `sibling' models tend to exhibit more similarity than parent/child pairs. Further analysis of the directional drifts of these mutations reveals qualitative insights about the open machine learning ecosystem: Licenses counter-intuitively drift from restrictive, commercial licenses towards permissive or copyleft licenses, often in violation of upstream license's terms; models evolve from multi-lingual compatibility towards english-only compatibility; and model cards reduce in length and standardize by turning, more often, to templates and automatically generated text. Overall, this work takes a step toward an empirically grounded understanding of model fine-tuning and suggests that ecological models and methods can yield novel scientific insights.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's the Evil Twin? Differential Auditing for Undesired Behavior</title>
<link>https://arxiv.org/abs/2508.06827</link>
<guid>https://arxiv.org/abs/2508.06827</guid>
<content:encoded><![CDATA[
arXiv:2508.06827v1 Announce Type: cross 
Abstract: Detecting hidden behaviors in neural networks poses a significant challenge due to minimal prior knowledge and potential adversarial obfuscation. We explore this problem by framing detection as an adversarial game between two teams: the red team trains two similar models, one trained solely on benign data and the other trained on data containing hidden harmful behavior, with the performance of both being nearly indistinguishable on the benign dataset. The blue team, with limited to no information about the harmful behaviour, tries to identify the compromised model. We experiment using CNNs and try various blue team strategies, including Gaussian noise analysis, model diffing, integrated gradients, and adversarial attacks under different levels of hints provided by the red team. Results show high accuracy for adversarial-attack-based methods (100\% correct prediction, using hints), which is very promising, whilst the other techniques yield more varied performance. During our LLM-focused rounds, we find that there are not many parallel methods that we could apply from our study with CNNs. Instead, we find that effective LLM auditing methods require some hints about the undesired distribution, which can then used in standard black-box and open-weight methods to probe the models further and reveal their misalignment. We open-source our auditing games (with the model and data) and hope that our findings contribute to designing better audits.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators</title>
<link>https://arxiv.org/abs/2508.06846</link>
<guid>https://arxiv.org/abs/2508.06846</guid>
<content:encoded><![CDATA[
arXiv:2508.06846v1 Announce Type: cross 
Abstract: Large language models (LLMs) are susceptible to generating inaccurate or false information, often referred to as "hallucinations" or "confabulations." While several technical advancements have been made to detect hallucinated content by assessing the factuality of the model's responses, there is still limited research on how to effectively communicate this information to users. To address this gap, we conducted two scenario-based experiments with a total of 208 participants to systematically compare the effects of various design strategies for communicating factuality scores by assessing participants' ratings of trust, ease in validating response accuracy, and preference. Our findings reveal that participants preferred and trusted a design in which all phrases within a response were color-coded based on factuality scores. Participants also found it easier to validate accuracy of the response in this style compared to a baseline with no style applied. Our study offers practical design guidelines for LLM application developers and designers, aimed at calibrating user trust, aligning with user preferences, and enhancing users' ability to scrutinize LLM outputs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development</title>
<link>https://arxiv.org/abs/2508.06849</link>
<guid>https://arxiv.org/abs/2508.06849</guid>
<content:encoded><![CDATA[
arXiv:2508.06849v1 Announce Type: cross 
Abstract: Lived experiences fundamentally shape how individuals interact with AI systems, influencing perceptions of safety, trust, and usability. While prior research has focused on developing techniques to emulate human preferences, and proposed taxonomies to categorize risks (such as psychological harms and algorithmic biases), these efforts have provided limited systematic understanding of lived human experiences or actionable strategies for embedding them meaningfully into the AI development lifecycle. This work proposes a framework for meaningfully integrating lived experience into the design and evaluation of AI systems. We synthesize interdisciplinary literature across lived experience philosophy, human-centered design, and human-AI interaction, arguing that centering lived experience can lead to models that more accurately reflect the retrospective, emotional, and contextual dimensions of human cognition. Drawing from a wide body of work across psychology, education, healthcare, and social policy, we present a targeted taxonomy of lived experiences with specific applicability to AI systems. To ground our framework, we examine three application domains (i) education, (ii) healthcare, and (iii) cultural alignment, illustrating how lived experience informs user goals, system expectations, and ethical considerations in each context. We further incorporate insights from AI system operators and human-AI partnerships to highlight challenges in responsibility allocation, mental model calibration, and long-term system adaptation. We conclude with actionable recommendations for developing experience-centered AI systems that are not only technically robust but also empathetic, context-aware, and aligned with human realities. This work offers a foundation for future research that bridges technical development with the lived experiences of those impacted by AI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGIC: Attention-Guided Image Captioning to Improve Caption Relevance</title>
<link>https://arxiv.org/abs/2508.06853</link>
<guid>https://arxiv.org/abs/2508.06853</guid>
<content:encoded><![CDATA[
arXiv:2508.06853v1 Announce Type: cross 
Abstract: Despite significant progress in image captioning, generating accurate and descriptive captions remains a long-standing challenge. In this study, we propose Attention-Guided Image Captioning (AGIC), which amplifies salient visual regions directly in the feature space to guide caption generation. We further introduce a hybrid decoding strategy that combines deterministic and probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we conduct extensive experiments on the Flickr8k and Flickr30k datasets. The results show that AGIC matches or surpasses several state-of-the-art models while achieving faster inference. Moreover, AGIC demonstrates strong performance across multiple evaluation metrics, offering a scalable and interpretable solution for image captioning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.06869</link>
<guid>https://arxiv.org/abs/2508.06869</guid>
<content:encoded><![CDATA[
arXiv:2508.06869v1 Announce Type: cross 
Abstract: Long video understanding presents a significant challenge to multimodal large language models (MLLMs) primarily due to the immense data scale. A critical and widely adopted strategy for making this task computationally tractable is keyframe retrieval, which seeks to identify a sparse set of video frames that are most salient to a given textual query. However, the efficacy of this approach is hindered by weak multimodal alignment between textual queries and visual content and fails to capture the complex temporal semantic information required for precise reasoning. To address this, we propose Visual-Subtitle Integeration(VSI), a multimodal keyframe search method that integrates subtitles, timestamps, and scene boundaries into a unified multimodal search process. The proposed method captures the visual information of video frames as well as the complementary textual information through a dual-stream search mechanism by Video Search Stream as well as Subtitle Match Stream, respectively, and improves the keyframe search accuracy through the interaction of the two search streams. Experimental results show that VSI achieve 40.00% key frame localization accuracy on the text-relevant subset of LongVideoBench and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive baselines by 20.35% and 15.79%, respectively. Furthermore, on the LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA tasks, demonstrating the robustness and generalizability of the proposed multimodal search strategy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06871</link>
<guid>https://arxiv.org/abs/2508.06871</guid>
<content:encoded><![CDATA[
arXiv:2508.06871v1 Announce Type: cross 
Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a critical challenge in deep reinforcement learning. We examine this issue in multi-task reinforcement learning (MTRL), where higher representational flexibility is crucial for managing diverse and potentially conflicting task demands. We systematically explore how sparsification methods, particularly Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance plasticity and consequently improve performance in MTRL agents. We evaluate these approaches across distinct MTRL architectures (shared backbone, Mixture of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks, comparing against dense baselines, and a comprehensive range of alternative plasticity-inducing or regularization methods. Our results demonstrate that both GMP and SET effectively mitigate key indicators of plasticity degradation, such as neuron dormancy and representational collapse. These plasticity improvements often correlate with enhanced multi-task performance, with sparse agents frequently outperforming dense counterparts and achieving competitive results against explicit plasticity interventions. Our findings offer insights into the interplay between plasticity, network sparsity, and MTRL designs, highlighting dynamic sparsification as a robust but context-sensitive tool for developing more adaptable MTRL systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESNERA: Empirical and semantic named entity alignment for named entity dataset merging</title>
<link>https://arxiv.org/abs/2508.06877</link>
<guid>https://arxiv.org/abs/2508.06877</guid>
<content:encoded><![CDATA[
arXiv:2508.06877v1 Announce Type: cross 
Abstract: Named Entity Recognition (NER) is a fundamental task in natural language processing. It remains a research hotspot due to its wide applicability across domains. Although recent advances in deep learning have significantly improved NER performance, they rely heavily on large, high-quality annotated datasets. However, building these datasets is expensive and time-consuming, posing a major bottleneck for further research. Current dataset merging approaches mainly focus on strategies like manual label mapping or constructing label graphs, which lack interpretability and scalability. To address this, we propose an automatic label alignment method based on label similarity. The method combines empirical and semantic similarities, using a greedy pairwise merging strategy to unify label spaces across different datasets. Experiments are conducted in two stages: first, merging three existing NER datasets into a unified corpus with minimal impact on NER performance; second, integrating this corpus with a small-scale, self-built dataset in the financial domain. The results show that our method enables effective dataset merging and enhances NER performance in the low-resource financial domain. This study presents an efficient, interpretable, and scalable solution for integrating multi-source NER corpora.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective</title>
<link>https://arxiv.org/abs/2508.06878</link>
<guid>https://arxiv.org/abs/2508.06878</guid>
<content:encoded><![CDATA[
arXiv:2508.06878v1 Announce Type: cross 
Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet challenging task in defense and civilian applications, owing to the dim, shapeless appearance of targets and severe background clutter. Recent CNN-based methods have achieved promising target perception results, but they only focus on enhancing feature representation to offset the impact of noise, which results in the increased false alarms problem. In this paper, through analyzing the problem from the frequency domain, we pioneer in improving performance from noise suppression perspective and propose a novel noise-suppression feature pyramid network (NS-FPN), which integrates a low-frequency guided feature purification (LFP) module and a spiral-aware feature sampling (SFS) module into the original FPN structure. The LFP module suppresses the noise features by purifying high-frequency components to achieve feature enhancement devoid of noise interference, while the SFS module further adopts spiral sampling to fuse target-relevant features in feature fusion process. Our NS-FPN is designed to be lightweight yet effective and can be easily plugged into existing IRSTDS frameworks. Extensive experiments on the public IRSTDS datasets demonstrate that our method significantly reduces false alarms and achieves superior performance on IRSTDS tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody</title>
<link>https://arxiv.org/abs/2508.06890</link>
<guid>https://arxiv.org/abs/2508.06890</guid>
<content:encoded><![CDATA[
arXiv:2508.06890v1 Announce Type: cross 
Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech while preserving its linguistic content. In practical EVC, controllability, the ability to independently control speaker identity and emotional style using distinct references, is crucial. However, existing methods often struggle to fully disentangle these attributes and lack the ability to model fine-grained emotional expressions such as temporal dynamics. We propose Maestro-EVC, a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references. We further introduce a temporal emotion representation and an explicit prosody modeling with prosody augmentation to robustly capture and transfer the temporal dynamics of the target emotion, even under prosody-mismatched conditions. Experimental results confirm that Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.06895</link>
<guid>https://arxiv.org/abs/2508.06895</guid>
<content:encoded><![CDATA[
arXiv:2508.06895v1 Announce Type: cross 
Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual understanding by using a vision projector to bridge well-pretrained vision encoders and large language models (LLMs). The inherent gap between visual and textual modalities makes the embeddings from the vision projector critical for visual comprehension. However, current alignment approaches treat visual embeddings as contextual cues and merely apply auto-regressive supervision to textual outputs, neglecting the necessity of introducing equivalent direct visual supervision, which hinders the potential finer alignment of visual embeddings. In this paper, based on our analysis of the refinement process of visual embeddings in the LLM's shallow layers, we propose BASIC, a method that utilizes refined visual embeddings within the LLM as supervision to directly guide the projector in generating initial visual embeddings. Specifically, the guidance is conducted from two perspectives: (i) optimizing embedding directions by reducing angles between initial and supervisory embeddings in semantic space; (ii) improving semantic matching by minimizing disparities between the logit distributions of both visual embeddings. Without additional supervisory models or artificial annotations, BASIC significantly improves the performance of MLLMs across a wide range of benchmarks, demonstrating the effectiveness of our introduced direct visual supervision.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Chinese font generation since deep learning era: A survey</title>
<link>https://arxiv.org/abs/2508.06900</link>
<guid>https://arxiv.org/abs/2508.06900</guid>
<content:encoded><![CDATA[
arXiv:2508.06900v1 Announce Type: cross 
Abstract: Chinese font generation aims to create a new Chinese font library based on some reference samples. It is a topic of great concern to many font designers and typographers. Over the past years, with the rapid development of deep learning algorithms, various new techniques have achieved flourishing and thriving progress. Nevertheless, how to improve the overall quality of generated Chinese character images remains a tough issue. In this paper, we conduct a holistic survey of the recent Chinese font generation approaches based on deep learning. To be specific, we first illustrate the research background of the task. Then, we outline our literature selection and analysis methodology, and review a series of related fundamentals, including classical deep learning architectures, font representation formats, public datasets, and frequently-used evaluation metrics. After that, relying on the number of reference samples required to generate a new font, we categorize the existing methods into two major groups: many-shot font generation and few-shot font generation methods. Within each category, representative approaches are summarized, and their strengths and limitations are also discussed in detail. Finally, we conclude our paper with the challenges and future directions, with the expectation to provide some valuable illuminations for the researchers in this field.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</title>
<link>https://arxiv.org/abs/2508.06908</link>
<guid>https://arxiv.org/abs/2508.06908</guid>
<content:encoded><![CDATA[
arXiv:2508.06908v1 Announce Type: cross 
Abstract: Person re-identification (ReID) aims to retrieve the images of an interested person in the gallery images, with wide applications in medical rehabilitation, abnormal behavior detection, and public security. However, traditional person ReID models suffer from uni-modal capability, leading to poor generalization ability in multi-modal data, such as RGB, thermal, infrared, sketch images, textual descriptions, etc. Recently, the emergence of multi-modal large language models (MLLMs) shows a promising avenue for addressing this problem. Despite this potential, existing methods merely regard MLLMs as feature extractors or caption generators, which do not fully unleash their reasoning, instruction-following, and cross-modal understanding capabilities. To bridge this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark specifically designed for person ReID. The MMReID-Bench includes 20,710 multi-modal queries and gallery images covering 10 different person ReID tasks. Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in delivering effective and versatile person ReID. Nevertheless, they also have limitations in handling a few modalities, particularly thermal and infrared data. We hope MMReID-Bench can facilitate the community to develop more robust and generalizable multimodal foundation models for person ReID.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROP: Integrating Topological and Spatial Structures via Cross-View Prefixes for Molecular LLMs</title>
<link>https://arxiv.org/abs/2508.06917</link>
<guid>https://arxiv.org/abs/2508.06917</guid>
<content:encoded><![CDATA[
arXiv:2508.06917v1 Announce Type: cross 
Abstract: Recent advances in molecular science have been propelled significantly by large language models (LLMs). However, their effectiveness is limited when relying solely on molecular sequences, which fail to capture the complex structures of molecules. Beyond sequence representation, molecules exhibit two complementary structural views: the first focuses on the topological relationships between atoms, as exemplified by the graph view; and the second emphasizes the spatial configuration of molecules, as represented by the image view. The two types of views provide unique insights into molecular structures. To leverage these views collaboratively, we propose the CROss-view Prefixes (CROP) to enhance LLMs' molecular understanding through efficient multi-view integration. CROP possesses two advantages: (i) efficiency: by jointly resampling multiple structural views into fixed-length prefixes, it avoids excessive consumption of the LLM's limited context length and allows easy expansion to more views; (ii) effectiveness: by utilizing the LLM's self-encoded molecular sequences to guide the resampling process, it boosts the quality of the generated prefixes. Specifically, our framework features a carefully designed SMILES Guided Resampler for view resampling, and a Structural Embedding Gate for converting the resulting embeddings into LLM's prefixes. Extensive experiments demonstrate the superiority of CROP in tasks including molecule captioning, IUPAC name prediction and molecule property prediction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing</title>
<link>https://arxiv.org/abs/2508.06937</link>
<guid>https://arxiv.org/abs/2508.06937</guid>
<content:encoded><![CDATA[
arXiv:2508.06937v1 Announce Type: cross 
Abstract: Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAP: Coreference-Linked Augmentation for Passage Retrieval</title>
<link>https://arxiv.org/abs/2508.06941</link>
<guid>https://arxiv.org/abs/2508.06941</guid>
<content:encoded><![CDATA[
arXiv:2508.06941v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based passage expansion has shown promise for enhancing first-stage retrieval, but often underperforms with dense retrievers due to semantic drift and misalignment with their pretrained semantic space. Beyond this, only a portion of a passage is typically relevant to a query, while the rest introduces noise--an issue compounded by chunking techniques that break coreference continuity. We propose Coreference-Linked Augmentation for Passage Retrieval (CLAP), a lightweight LLM-based expansion framework that segments passages into coherent chunks, resolves coreference chains, and generates localized pseudo-queries aligned with dense retriever representations. A simple fusion of global topical signals and fine-grained subtopic signals achieves robust performance across domains. CLAP yields consistent gains even as retriever strength increases, enabling dense retrievers to match or surpass second-stage rankers such as BM25 + MonoT5-3B, with up to 20.68% absolute nDCG@10 improvement. These improvements are especially notable in out-of-domain settings, where conventional LLM-based expansion methods relying on domain knowledge often falter. CLAP instead adopts a logic-centric pipeline that enables robust, domain-agnostic generalization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction</title>
<link>https://arxiv.org/abs/2508.06942</link>
<guid>https://arxiv.org/abs/2508.06942</guid>
<content:encoded><![CDATA[
arXiv:2508.06942v1 Announce Type: cross 
Abstract: With the growing capabilities of large language models (LLMs), they are increasingly applied in areas like intelligent customer service, code generation, and knowledge management. Natural language (NL) prompts act as the ``APIs'' for human-LLM interaction. To improve prompt quality, best practices for prompt engineering (PE) have been developed, including writing guidelines and templates. Building on this, we propose Controlled NL for Prompt (CNL-P), which not only incorporates PE best practices but also draws on key principles from software engineering (SE). CNL-P introduces precise grammar structures and strict semantic norms, further eliminating NL's ambiguity, allowing for a declarative but structured and accurate expression of user intent. This helps LLMs better interpret and execute the prompts, leading to more consistent and higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on LLMs, enabling users to write prompts in NL, which are then transformed into CNL-P format, thus lowering the learning curve of CNL-P. In particular, we develop a linting tool that checks CNL-P prompts for syntactic and semantic accuracy, applying static analysis techniques to NL for the first time. Extensive experiments demonstrate that CNL-P enhances the quality of LLM responses through the novel and organic synergy of PE and SE. We believe that CNL-P can bridge the gap between emerging PE and traditional SE, laying the foundation for a new programming paradigm centered around NL.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Unbiasing for Generalization in Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.06943</link>
<guid>https://arxiv.org/abs/2508.06943</guid>
<content:encoded><![CDATA[
arXiv:2508.06943v1 Announce Type: cross 
Abstract: Medical diagnosis might fail due to bias. In this work, we identified class-feature bias, which refers to models' potential reliance on features that are strongly correlated with only a subset of classes, leading to biased performance and poor generalization on other classes. We aim to train a class-unbiased model (Cls-unbias) that mitigates both class imbalance and class-feature bias simultaneously. Specifically, we propose a class-wise inequality loss which promotes equal contributions of classification loss from positive-class and negative-class samples. We propose to optimize a class-wise group distributionally robust optimization objective-a class-weighted training objective that upweights underperforming classes-to enhance the effectiveness of the inequality loss under class imbalance. Through synthetic and real-world datasets, we empirically demonstrate that class-feature bias can negatively impact model performance. Our proposed method effectively mitigates both class-feature bias and class imbalance, thereby improving the model's generalization ability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[
arXiv:2508.06944v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Beam Field for Spatial Beam RSRP Prediction</title>
<link>https://arxiv.org/abs/2508.06956</link>
<guid>https://arxiv.org/abs/2508.06956</guid>
<content:encoded><![CDATA[
arXiv:2508.06956v1 Announce Type: cross 
Abstract: Accurately predicting beam-level reference signal received power (RSRP) is essential for beam management in dense multi-user wireless networks, yet challenging due to high measurement overhead and fast channel variations. This paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for efficient and interpretable spatial beam RSRP prediction. Central to our approach is the introduction of the Multi-path Conditional Power Profile (MCPP), which bridges site-specific multipath propagation with antenna/beam configurations via closed-form analytical modeling. We adopt a decoupled ``blackbox-whitebox" design: a Transformer-based deep neural network (DNN) learns the MCPP from sparse user measurements and positions, while a physics-inspired module analytically infers beam RSRP statistics. To improve convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC) strategy that leverages ray-tracing priors and on-site calibration using RSRP data. Extensive simulations results demonstrate that NBF significantly outperforms conventional table-based channel knowledge maps (CKMs) and pure blackbox DNNs in prediction accuracy, training efficiency, and generalization, while maintaining a compact model size. The proposed framework offers a scalable and physically grounded solution for intelligent beam management in next-generation dense wireless networks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification</title>
<link>https://arxiv.org/abs/2508.06959</link>
<guid>https://arxiv.org/abs/2508.06959</guid>
<content:encoded><![CDATA[
arXiv:2508.06959v1 Announce Type: cross 
Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in capturing discriminative and class-specific cues that correspond to subtle visual characteristics. Recently, frequency decomposition/transform based approaches have attracted considerable interests since its appearing discriminative cue mining ability. However, the frequency-domain methods are based on fixed basis functions, lacking adaptability to image content and unable to dynamically adjust feature extraction according to the discriminative requirements of different images. To address this, we propose a novel method for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively enhances the representational capability of low-level details and high-level semantics in the spatial domain, breaking through the limitations of fixed scales in the frequency domain and improving the flexibility of multi-scale fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor (SDE), which dynamically enhances subtle details such as edges and textures from shallow features, and the Salient Semantic Refiner (SSR), which learns semantically coherent and structure-aware refinement features from the high-level features guided by the enhanced shallow features. The SDE and SSR are cascaded stage-by-stage to progressively combine local details with global semantics. Extensive experiments demonstrate that our method achieves new state-of-the-art on four popular fine-grained image classification benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multitask Learning Enhance Model Explainability?</title>
<link>https://arxiv.org/abs/2508.06966</link>
<guid>https://arxiv.org/abs/2508.06966</guid>
<content:encoded><![CDATA[
arXiv:2508.06966v1 Announce Type: cross 
Abstract: Remote sensing provides satellite data in diverse types and formats. The usage of multimodal learning networks exploits this diversity to improve model performance, except that the complexity of such networks comes at the expense of their interpretability. In this study, we explore how modalities can be leveraged through multitask learning to intrinsically explain model behavior. In particular, instead of additional inputs, we use certain modalities as additional targets to be predicted along with the main task. The success of this approach relies on the rich information content of satellite data, which remains as input modalities. We show how this modeling context provides numerous benefits: (1) in case of data scarcity, the additional modalities do not need to be collected for model inference at deployment, (2) the model performance remains comparable to the multimodal baseline performance, and in some cases achieves better scores, (3) prediction errors in the main task can be explained via the model behavior in the auxiliary task(s). We demonstrate the efficiency of our approach on three datasets, including segmentation, classification, and regression tasks. Code available at git.opendfki.de/hiba.najjar/mtl_explainability/.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v1 Announce Type: cross 
Abstract: Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Set-based Human-AI Complementarity with Multiple Experts</title>
<link>https://arxiv.org/abs/2508.06997</link>
<guid>https://arxiv.org/abs/2508.06997</guid>
<content:encoded><![CDATA[
arXiv:2508.06997v1 Announce Type: cross 
Abstract: Decision support systems are designed to assist human experts in classification tasks by providing conformal prediction sets derived from a pre-trained model. This human-AI collaboration has demonstrated enhanced classification performance compared to using either the model or the expert independently. In this study, we focus on the selection of instance-specific experts from a pool of multiple human experts, contrasting it with existing research that typically focuses on single-expert scenarios. We characterize the conditions under which multiple experts can benefit from the conformal sets. With the insight that only certain experts may be relevant for each instance, we explore the problem of subset selection and introduce a greedy algorithm that utilizes conformal sets to identify the subset of expert predictions that will be used in classifying an instance. This approach is shown to yield better performance compared to naive methods for human subset selection. Based on real expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation study indicates that our proposed greedy algorithm achieves near-optimal subsets, resulting in improved classification performance among multiple experts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</title>
<link>https://arxiv.org/abs/2508.07001</link>
<guid>https://arxiv.org/abs/2508.07001</guid>
<content:encoded><![CDATA[
arXiv:2508.07001v1 Announce Type: cross 
Abstract: With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems</title>
<link>https://arxiv.org/abs/2508.07009</link>
<guid>https://arxiv.org/abs/2508.07009</guid>
<content:encoded><![CDATA[
arXiv:2508.07009v1 Announce Type: cross 
Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant performance gains in next-generation wireless networks but face key challenges, notably severe double-pathloss and complex multi-user scheduling due to hardware constraints. Active IRSs partially address pathloss but still require efficient scheduling in cell-level multi-IRS multi-user systems, whereby the overhead/delay of channel state acquisition and the scheduling complexity both rise dramatically as the user density and channel dimensions increase. Motivated by these challenges, this paper proposes a novel scheduling framework based on neural Channel Knowledge Map (CKM), designing Transformer-based deep neural networks (DNNs) to predict ergodic spectral efficiency (SE) from historical channel/throughput measurements tagged with user positions. Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to predict link power statistics (LPS) and ergodic SE accurately. We further propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling algorithm. Numerical evaluations verify that the proposed neural CKM significantly enhances prediction accuracy and computational efficiency, while the SM-IB algorithm effectively achieves near-optimal max-min throughput with greatly reduced complexity.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree</title>
<link>https://arxiv.org/abs/2508.07014</link>
<guid>https://arxiv.org/abs/2508.07014</guid>
<content:encoded><![CDATA[
arXiv:2508.07014v1 Announce Type: cross 
Abstract: Recognizing specific key phrases is an essential task for contextualized Automatic Speech Recognition (ASR). However, most existing context-biasing approaches have limitations associated with the necessity of additional model training, significantly slow down the decoding process, or constrain the choice of the ASR system type. This paper proposes a universal ASR context-biasing framework that supports all major types: CTC, Transducers, and Attention Encoder-Decoder models. The framework is based on a GPU-accelerated word boosting tree, which enables it to be used in shallow fusion mode for greedy and beam search decoding without noticeable speed degradation, even with a vast number of key phrases (up to 20K items). The obtained results showed high efficiency of the proposed method, surpassing the considered open-source context-biasing approaches in accuracy and decoding speed. Our context-biasing framework is open-sourced as a part of the NeMo toolkit.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Effective Decisions: Machine Learning and the Ecogame in 1970</title>
<link>https://arxiv.org/abs/2508.07027</link>
<guid>https://arxiv.org/abs/2508.07027</guid>
<content:encoded><![CDATA[
arXiv:2508.07027v1 Announce Type: cross 
Abstract: This paper considers Ecogame, an innovative art project of 1970, whose creators believed in a positive vision of a technological future; an understanding, posited on cybernetics, of a future that could be participatory via digital means, and therefore more democratised. Using simulation and early machine learning techniques over a live network, Ecogame combined the power of visual art with cybernetic concepts of adaptation, feedback, and control to propose that behaviour had implications for the total system. It provides an historical precedent for contemporary AI-driven art about using AI in a more human-centred way.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.07029</link>
<guid>https://arxiv.org/abs/2508.07029</guid>
<content:encoded><![CDATA[
arXiv:2508.07029v1 Announce Type: cross 
Abstract: Learning robust driving policies from large-scale, real-world datasets is a central challenge in autonomous driving, as online data collection is often unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward approach to imitation learning, policies trained with BC are notoriously brittle and suffer from compounding errors in closed-loop execution. This work presents a comprehensive pipeline and a comparative study to address this limitation. We first develop a series of increasingly sophisticated BC baselines, culminating in a Transformer-based model that operates on a structured, entity-centric state representation. While this model achieves low imitation loss, we show that it still fails in long-horizon simulations. We then demonstrate that by applying a state-of-the-art Offline Reinforcement Learning algorithm, Conservative Q-Learning (CQL), to the same data and architecture, we can learn a significantly more robust policy. Using a carefully engineered reward function, the CQL agent learns a conservative value function that enables it to recover from minor errors and avoid out-of-distribution states. In a large-scale evaluation on 1,000 unseen scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a 3.2x higher success rate and a 7.4x lower collision rate than the strongest BC baseline, proving that an offline RL approach is critical for learning robust, long-horizon driving policies from static expert data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities</title>
<link>https://arxiv.org/abs/2508.07031</link>
<guid>https://arxiv.org/abs/2508.07031</guid>
<content:encoded><![CDATA[
arXiv:2508.07031v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging tasks, including image interpretation and synthetic image generation. However, these models often produce hallucinations, which are confident but incorrect outputs that can mislead clinical decisions. This study examines hallucinations in two directions: image to text, where LLMs generate reports from X-ray, CT, or MRI scans, and text to image, where models create medical images from clinical prompts. We analyze errors such as factual inconsistencies and anatomical inaccuracies, evaluating outputs using expert informed criteria across imaging modalities. Our findings reveal common patterns of hallucination in both interpretive and generative tasks, with implications for clinical reliability. We also discuss factors contributing to these failures, including model architecture and training data. By systematically studying both image understanding and generation, this work provides insights into improving the safety and trustworthiness of LLM driven medical imaging systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</title>
<link>https://arxiv.org/abs/2508.07044</link>
<guid>https://arxiv.org/abs/2508.07044</guid>
<content:encoded><![CDATA[
arXiv:2508.07044v1 Announce Type: cross 
Abstract: In the era of generative AI, ensuring the privacy of music data presents unique challenges: unlike static artworks such as images, music data is inherently temporal and multimodal, and it is sampled, transformed, and remixed at an unprecedented scale. These characteristics make its core vector embeddings, i.e, the numerical representations of the music, highly susceptible to being learned, misused, or even stolen by models without accessing the original audio files. Traditional methods like copyright licensing and digital watermarking offer limited protection for these abstract mathematical representations, thus necessitating a stronger, e.g., cryptographic, approach to safeguarding the embeddings themselves. Standard encryption schemes, such as AES, render data unintelligible for computation, making such searches impossible. While Fully Homomorphic Encryption (FHE) provides a plausible solution by allowing arbitrary computations on ciphertexts, its substantial performance overhead remains impractical for large-scale vector similarity searches. Given this trade-off, we propose a more practical approach using Additive Homomorphic Encryption (AHE) for vector similarity search. The primary contributions of this paper are threefold: we analyze threat models unique to music information retrieval systems; we provide a theoretical analysis and propose an efficient AHE-based solution through inner products of music embeddings to deliver privacy-preserving similarity search; and finally, we demonstrate the efficiency and practicality of the proposed approach through empirical evaluation and comparison to FHE schemes on real-world MP3 files.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</title>
<link>https://arxiv.org/abs/2508.07048</link>
<guid>https://arxiv.org/abs/2508.07048</guid>
<content:encoded><![CDATA[
arXiv:2508.07048v1 Announce Type: cross 
Abstract: Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive applications such as real-time captioning and meeting transcription. However, truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders and the context limitations of non-autoregressive (NAR) methods. While modern ASR encoders can process up to 30 seconds of audio at once, AR decoders still generate tokens sequentially, creating a latency bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained Whisper encoder with a text diffusion decoder. This NAR architecture resolves the AR latency bottleneck by processing the entire acoustic context in parallel at every decoding step. A lightweight cross-attention adapter trained via parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also introduce a batch-parallel, multi-step decoding strategy that improves accuracy by increasing the number of candidates with minimal impact on speed. Fine-tuned solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny (8.3% vs. 9.7%), and offers comparable latency on short audio. For longer utterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a new, efficient operating point for long-form ASR. The implementation and training scripts are available at https://github.com/taeyoun811/Whisfusion.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
arXiv:2508.07050v1 Announce Type: cross 
Abstract: Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership and Memorization in LLM Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.07054</link>
<guid>https://arxiv.org/abs/2508.07054</guid>
<content:encoded><![CDATA[
arXiv:2508.07054v1 Announce Type: cross 
Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high computational demands of Large Language Models (LLMs) by transferring knowledge from a large ''teacher'' to a smaller ''student'' model. However, students may inherit the teacher's privacy when the teacher is trained on private data. In this work, we systematically characterize and investigate membership and memorization privacy risks inherent in six LLM KD techniques. Using instruction-tuning settings that span seven NLP tasks, together with three teacher model families (GPT-2, LLAMA-2, and OPT), and various size student models, we demonstrate that all existing LLM KD approaches carry membership and memorization privacy risks from the teacher to its students. However, the extent of privacy risks varies across different KD techniques. We systematically analyse how key LLM KD components (KD objective functions, student training data and NLP tasks) impact such privacy risks. We also demonstrate a significant disagreement between memorization and membership privacy risks of LLM KD techniques. Finally, we characterize per-block privacy risk and demonstrate that the privacy risk varies across different blocks by a large margin.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages</title>
<link>https://arxiv.org/abs/2508.07069</link>
<guid>https://arxiv.org/abs/2508.07069</guid>
<content:encoded><![CDATA[
arXiv:2508.07069v1 Announce Type: cross 
Abstract: Although numerous datasets have been developed to support dialogue systems, most existing chit-chat datasets overlook the cultural nuances inherent in natural human conversations. To address this gap, we introduce SEADialogues, a culturally grounded dialogue dataset centered on Southeast Asia, a region with over 700 million people and immense cultural diversity. Our dataset features dialogues in eight languages from six Southeast Asian countries, many of which are low-resource despite having sizable speaker populations. To enhance cultural relevance and personalization, each dialogue includes persona attributes and two culturally grounded topics that reflect everyday life in the respective communities. Furthermore, we release a multi-turn dialogue dataset to advance research on culturally aware and human-centric large language models, including conversational dialogue agents.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation</title>
<link>https://arxiv.org/abs/2508.07075</link>
<guid>https://arxiv.org/abs/2508.07075</guid>
<content:encoded><![CDATA[
arXiv:2508.07075v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates, especially when new information conflicts with deeply embedded facts. Such conflicting factual edits often lead to two critical issues: resistance to adopting the new fact and severe catastrophic forgetting of unrelated knowledge. This paper introduces and evaluates a novel "unlearn-then-learn" strategy for precise knowledge editing in LLMs, leveraging the parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach is powered by an initial circuit localization phase that identifies and targets the specific internal components responsible for encoding the conflicting fact. Through a rigorous experimental methodology on microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically informed two-stage approach achieves near-perfect accuracy (98.50%) for the new, modulated fact while simultaneously effectively suppressing the original conflicting fact (96.00% forget rate). Critically, our strategy exhibits unprecedented localization (72.00% F_control accuracy), dramatically mitigating catastrophic forgetting observed in direct fine-tuning approaches (which showed as low as ~20% F_control accuracy), a direct benefit of our targeted interpretability-guided intervention. Furthermore, qualitative analysis reveals a nuanced mechanism of "soft forgetting," where original knowledge is suppressed from default retrieval but remains latent and conditionally accessible, enhancing model safety and control. These findings represent a significant advancement towards precise, localized, and safe knowledge management in compact LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.07079</link>
<guid>https://arxiv.org/abs/2508.07079</guid>
<content:encoded><![CDATA[
arXiv:2508.07079v1 Announce Type: cross 
Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for autonomous robots. This work evaluates the integration of a deep learning-based Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive Control (MPC) framework on the physical Continental Corriere robot. Tested across varied pedestrian densities, the SI-MPC system is compared to a traditional Constant Velocity (CV) model in both open-loop prediction and closed-loop navigation. Results show that SI improves trajectory prediction - reducing errors by up to 76% in low-density settings - and enhances safety and motion smoothness in crowded scenes. Moreover, real-world deployment reveals discrepancies between open-loop metrics and closed-loop performance, as the SI model yields broader, more cautious predictions. These findings emphasize the importance of system-level evaluation and highlight the SI-MPC framework's promise for safer, more adaptive navigation in dynamic, human-populated environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.07080</link>
<guid>https://arxiv.org/abs/2508.07080</guid>
<content:encoded><![CDATA[
arXiv:2508.07080v1 Announce Type: cross 
Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs), since they have to proactively interact with surrounding vehicles to enter the main road safely within limited time. However, existing decision-making algorithms fail to adequately address dynamic complexities and social acceptance of AVs, leading to suboptimal or unsafe merging decisions. To address this, we propose an evolutionary game-theoretic (EGT) merging decision-making framework, grounded in the bounded rationality of human drivers, which dynamically balances the benefits of both AVs and main-road vehicles (MVs). We formulate the cut-in decision-making process as an EGT problem with a multi-objective payoff function that reflects human-like driving preferences. By solving the replicator dynamic equation for the evolutionarily stable strategy (ESS), the optimal cut-in timing is derived, balancing efficiency, comfort, and safety for both AVs and MVs. A real-time driving style estimation algorithm is proposed to adjust the game payoff function online by observing the immediate reactions of MVs. Empirical results demonstrate that we improve the efficiency, comfort and safety of both AVs and MVs compared with existing game-theoretic and traditional planning approaches across multi-object metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQL-Exchange: Transforming SQL Queries Across Domains</title>
<link>https://arxiv.org/abs/2508.07087</link>
<guid>https://arxiv.org/abs/2508.07087</guid>
<content:encoded><![CDATA[
arXiv:2508.07087v1 Announce Type: cross 
Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across different database schemas by preserving the source query structure while adapting domain-specific elements to align with the target schema. We investigate the conditions under which such mappings are feasible and beneficial, and examine their impact on enhancing the in-context learning performance of text-to-SQL systems as a downstream task. Our comprehensive evaluation across multiple model families and benchmark datasets--assessing structural alignment with source queries, execution validity on target databases, and semantic correctness--demonstrates that SQL-Exchange is effective across a wide range of schemas and query types. Our results further show that using mapped queries as in-context examples consistently improves text-to-SQL performance over using queries from the source schema.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust</title>
<link>https://arxiv.org/abs/2508.07095</link>
<guid>https://arxiv.org/abs/2508.07095</guid>
<content:encoded><![CDATA[
arXiv:2508.07095v1 Announce Type: cross 
Abstract: Large language models are known to produce outputs that are plausible but factually incorrect. To prevent people from making erroneous decisions by blindly trusting AI, researchers have explored various ways of communicating factuality estimates in AI-generated outputs to end-users. However, little is known about whether revealing content estimated to be factually incorrect influences users' trust when compared to hiding it altogether. We tested four different ways of disclosing an AI-generated output with factuality assessments: transparent (highlights less factual content), attention (highlights factual content), opaque (removes less factual content), ambiguity (makes less factual content vague), and compared them with a baseline response without factuality information. We conducted a human subjects research (N = 148) using the strategies in question-answering scenarios. We found that the opaque and ambiguity strategies led to higher trust while maintaining perceived answer quality, compared to the other strategies. We discuss the efficacy of hiding presumably less factual content to build end-user trust.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2508.07101</link>
<guid>https://arxiv.org/abs/2508.07101</guid>
<content:encoded><![CDATA[
arXiv:2508.07101v1 Announce Type: cross 
Abstract: Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a $1.1\times$ average decoding speed-up compared to full attention. Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss, achieving a $1.13\times$ end-to-end speed-up compared to existing sparse attention methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria</title>
<link>https://arxiv.org/abs/2508.07102</link>
<guid>https://arxiv.org/abs/2508.07102</guid>
<content:encoded><![CDATA[
arXiv:2508.07102v1 Announce Type: cross 
Abstract: Generative modelling has seen significant advances through simulation-free paradigms such as Flow Matching, and in particular, the MeanFlow framework, which replaces instantaneous velocity fields with average velocities to enable efficient single-step sampling. In this work, we introduce a theoretical study on Second-Order MeanFlow, a novel extension that incorporates average acceleration fields into the MeanFlow objective. We first establish the feasibility of our approach by proving that the average acceleration satisfies a generalized consistency condition analogous to first-order MeanFlow, thereby supporting stable, one-step sampling and tractable loss functions. We then characterize its expressivity via circuit complexity analysis, showing that under mild assumptions, the Second-Order MeanFlow sampling process can be implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class. Finally, we derive provably efficient criteria for scalable implementation by leveraging fast approximate attention computations: we prove that attention operations within the Second-Order MeanFlow architecture can be approximated to within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results lay the theoretical foundation for high-order flow matching models that combine rich dynamics with practical sampling efficiency.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution</title>
<link>https://arxiv.org/abs/2508.07111</link>
<guid>https://arxiv.org/abs/2508.07111</guid>
<content:encoded><![CDATA[
arXiv:2508.07111v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called Coreference Confidence Disparity which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers, indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning</title>
<link>https://arxiv.org/abs/2508.07126</link>
<guid>https://arxiv.org/abs/2508.07126</guid>
<content:encoded><![CDATA[
arXiv:2508.07126v1 Announce Type: cross 
Abstract: Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE offers a scalable and principled approach for harnessing human input in online reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</title>
<link>https://arxiv.org/abs/2508.07128</link>
<guid>https://arxiv.org/abs/2508.07128</guid>
<content:encoded><![CDATA[
arXiv:2508.07128v1 Announce Type: cross 
Abstract: Generative image models have achieved remarkable progress in both natural and medical imaging. In the medical context, these techniques offer a potential solution to data scarcity-especially for low-prevalence anomalies that impair the performance of AI-driven diagnostic and segmentation tools. However, questions remain regarding the fidelity and clinical utility of synthetic images, since poor generation quality can undermine model generalizability and trust. In this study, we evaluate the effectiveness of state-of-the-art generative models-Generative Adversarial Networks (GANs) and Diffusion Models (DMs)-for synthesizing chest X-rays conditioned on four abnormalities: Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged Cardiac Silhouette (ECS). Using a benchmark composed of real images from the MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a reader study with three radiologists of varied experience. Participants were asked to distinguish real from synthetic images and assess the consistency between visual features and the target abnormality. Our results show that while DMs generate more visually realistic images overall, GANs can report better accuracy for specific conditions, such as absence of ECS. We further identify visual cues radiologists use to detect synthetic images, offering insights into the perceptual gaps in current models. These findings underscore the complementary strengths of GANs and DMs and point to the need for further refinement to ensure generative models can reliably augment training datasets for AI diagnostic systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers</title>
<link>https://arxiv.org/abs/2508.07129</link>
<guid>https://arxiv.org/abs/2508.07129</guid>
<content:encoded><![CDATA[
arXiv:2508.07129v1 Announce Type: cross 
Abstract: Artificial intelligence researchers have proposed various data-driven algorithms to improve the processes that match individuals experiencing homelessness to scarce housing resources. It remains unclear whether and how these algorithms are received or adopted by practitioners and what their corresponding consequences are. Through semi-structured interviews with 13 policymakers in homeless services in Los Angeles, we investigate whether such change-makers are open to the idea of integrating AI into the housing resource matching process, identifying where they see potential gains and drawbacks from such a system in issues of efficiency, fairness, and transparency. Our qualitative analysis indicates that, even when aware of various complicating factors, policymakers welcome the idea of an AI matching tool if thoughtfully designed and used in tandem with human decision-makers. Though there is no consensus as to the exact design of such an AI system, insights from policymakers raise open questions and design considerations that can be enlightening for future researchers and practitioners who aim to build responsible algorithmic systems to support decision-making in low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Draw me a curator" Examining the visual stereotyping of a cultural services profession by generative AI</title>
<link>https://arxiv.org/abs/2508.07132</link>
<guid>https://arxiv.org/abs/2508.07132</guid>
<content:encoded><![CDATA[
arXiv:2508.07132v1 Announce Type: cross 
Abstract: Based on 230 visualisations, this paper examines the depiction of museum curators by the popular generative Artificial Intelligence (AI) model, ChatGPT4o. While the AI-generated representations do not reiterate popular stereotypes of curators as nerdy, conservative in dress and stuck in time rummaging through collections, they contrast sharply with real-world demographics. AI-generated imagery extremely underrepresents women (3.5% vs 49% to 72% in reality) and disregards ethnic communities other than Caucasian (0% vs 18% to 36%). It only over-represents young curators (79% vs approx. 27%) but also renders curators to resemble yuppie professionals or people featuring in fashion advertising. Stereotypical attributes are prevalent, with curators widely depicted as wearing beards and holding clipboards or digital tablets. The findings highlight biases in the generative AI image creation dataset, which is poised to shape an inaccurate portrayal of museum professionals if the images were to be taken uncritically at face value.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stable and Principled Loss Function for Direct Language Model Alignment</title>
<link>https://arxiv.org/abs/2508.07137</link>
<guid>https://arxiv.org/abs/2508.07137</guid>
<content:encoded><![CDATA[
arXiv:2508.07137v1 Announce Type: cross 
Abstract: The alignment of large language models (LLMs) with human preferences is commonly achieved through Reinforcement Learning from Human Feedback (RLHF). Direct Preference Optimization (DPO) simplified this paradigm by establishing a direct mapping between the optimal policy and a reward function, eliminating the need for an explicit reward model. However, we argue that the DPO loss function is theoretically misaligned with its own derivation, as it promotes the indefinite maximization of a logits difference, which can lead to training instability and reward hacking. In this paper, we propose a novel loss function derived directly from the RLHF optimality condition. Our proposed loss targets a specific, finite value for the logits difference, which is dictated by the underlying reward, rather than its maximization. We provide a theoretical analysis, including a gradient-based comparison, to demonstrate that our method avoids the large gradients that plague DPO when the probability of dispreferred responses approaches zero. This inherent stability prevents reward hacking and leads to more effective alignment. We validate our approach by fine-tuning a Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO baseline and achieving competitive performance against larger models like Llama-3.1-8B.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection</title>
<link>https://arxiv.org/abs/2508.07139</link>
<guid>https://arxiv.org/abs/2508.07139</guid>
<content:encoded><![CDATA[
arXiv:2508.07139v1 Announce Type: cross 
Abstract: Ensuring LLM alignment is critical to information security as AI models become increasingly widespread and integrated in society. Unfortunately, many defenses against adversarial attacks and jailbreaking on LLMs cannot adapt quickly to new attacks, degrade model responses to benign prompts, or introduce significant barriers to scalable implementation. To mitigate these challenges, we introduce a real-time, self-tuning (RTST) moderator framework to defend against adversarial attacks while maintaining a lightweight training footprint. We empirically evaluate its effectiveness using Google's Gemini models against modern, effective jailbreaks. Our results demonstrate the advantages of an adaptive, minimally intrusive framework for jailbreak defense over traditional fine-tuning or classifier models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGD Convergence under Stepsize Shrinkage in Low-Precision Training</title>
<link>https://arxiv.org/abs/2508.07142</link>
<guid>https://arxiv.org/abs/2508.07142</guid>
<content:encoded><![CDATA[
arXiv:2508.07142v1 Announce Type: cross 
Abstract: Low-precision training has become essential for reducing the computational and memory costs of large-scale deep learning. However, quantization of gradients introduces both magnitude shrinkage and additive noise, which can alter the convergence behavior of stochastic gradient descent (SGD). In this work, we study the convergence of SGD under a gradient shrinkage model, where each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by zero-mean quantization noise. We show that this shrinkage is equivalent to replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$, which slows convergence when $q_{\min} < 1$. Under standard smoothness and bounded-variance assumptions, we prove that low-precision SGD still converges, but at a reduced rate determined by $q_{\min}$, and with an increased asymptotic error floor due to quantization noise. We theoretically analyze how reduced numerical precision slows down training by modeling it as gradient shrinkage in the standard SGD convergence framework.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens</title>
<link>https://arxiv.org/abs/2508.07143</link>
<guid>https://arxiv.org/abs/2508.07143</guid>
<content:encoded><![CDATA[
arXiv:2508.07143v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation -- it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate1) and harmful discrimination (discriminate2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties ("temporal taxation"), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal/cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.07146</link>
<guid>https://arxiv.org/abs/2508.07146</guid>
<content:encoded><![CDATA[
arXiv:2508.07146v1 Announce Type: cross 
Abstract: Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2508.07163</link>
<guid>https://arxiv.org/abs/2508.07163</guid>
<content:encoded><![CDATA[
arXiv:2508.07163v1 Announce Type: cross 
Abstract: Neurosymbolic AI combines neural network adaptability with symbolic reasoning, promising an approach to address the complex regulatory, operational, and safety challenges in Advanced Air Mobility (AAM). This survey reviews its applications across key AAM domains such as demand forecasting, aircraft design, and real-time air traffic management. Our analysis reveals a fragmented research landscape where methodologies, including Neurosymbolic Reinforcement Learning, have shown potential for dynamic optimization but still face hurdles in scalability, robustness, and compliance with aviation standards. We classify current advancements, present relevant case studies, and outline future research directions aimed at integrating these approaches into reliable, transparent AAM systems. By linking advanced AI techniques with AAM's operational demands, this work provides a concise roadmap for researchers and practitioners developing next-generation air mobility solutions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</title>
<link>https://arxiv.org/abs/2508.07165</link>
<guid>https://arxiv.org/abs/2508.07165</guid>
<content:encoded><![CDATA[
arXiv:2508.07165v1 Announce Type: cross 
Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable versatility, enabling the distinct visualization of different tissue types. Nevertheless, the inherent heterogeneity among MRI sequences poses significant challenges to the generalization capability of deep learning models. These challenges undermine model performance when faced with varying acquisition parameters, thereby severely restricting their clinical utility. In this study, we present PRISM, a foundation model PRe-trained with large-scale multI-Sequence MRI. We collected a total of 64 datasets from both public and private sources, encompassing a wide range of whole-body anatomical structures, with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI scans from 34 datasets (8 public and 26 private) were curated to construct the largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a novel pretraining paradigm that disentangles anatomically invariant features from sequence-specific variations in MRI, while preserving high-level semantic representations. We established a benchmark comprising 44 downstream tasks, including disease diagnosis, image segmentation, registration, progression prediction, and report generation. These tasks were evaluated on 32 public datasets and 5 private cohorts. PRISM consistently outperformed both non-pretrained models and existing foundation models, achieving first-rank results in 39 out of 44 downstream benchmarks with statistical significance improvements. These results underscore its ability to learn robust and generalizable representations across unseen data acquired under diverse MRI protocols. PRISM provides a scalable framework for multi-sequence MRI analysis, thereby enhancing the translational potential of AI in radiology. It delivers consistent performance across diverse imaging protocols, reinforcing its clinical applicability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection</title>
<link>https://arxiv.org/abs/2508.07170</link>
<guid>https://arxiv.org/abs/2508.07170</guid>
<content:encoded><![CDATA[
arXiv:2508.07170v1 Announce Type: cross 
Abstract: In the domain of computer vision, multi-scale feature extraction is vital for tasks such as salient object detection. However, achieving this capability in lightweight networks remains challenging due to the trade-off between efficiency and performance. This paper proposes a novel lightweight multi-scale feature extraction layer, termed the LMF layer, which employs depthwise separable dilated convolutions in a fully connected structure. By integrating multiple LMF layers, we develop LMFNet, a lightweight network tailored for salient object detection. Our approach significantly reduces the number of parameters while maintaining competitive performance. Here, we show that LMFNet achieves state-of-the-art or comparable results on five benchmark datasets with only 0.81M parameters, outperforming several traditional and lightweight models in terms of both efficiency and accuracy. Our work not only addresses the challenge of multi-scale learning in lightweight networks but also demonstrates the potential for broader applications in image processing tasks. The related code files are available at https://github.com/Shi-Yun-peng/LMFNet
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</title>
<link>https://arxiv.org/abs/2508.07178</link>
<guid>https://arxiv.org/abs/2508.07178</guid>
<content:encoded><![CDATA[
arXiv:2508.07178v1 Announce Type: cross 
Abstract: Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks</title>
<link>https://arxiv.org/abs/2508.07179</link>
<guid>https://arxiv.org/abs/2508.07179</guid>
<content:encoded><![CDATA[
arXiv:2508.07179v1 Announce Type: cross 
Abstract: Enterprise data pipelines, characterized by complex transformations across multiple programming languages, often cause a semantic disconnect between original metadata and downstream data. This "semantic drift" compromises data reproducibility and governance, and impairs the utility of services like retrieval-augmented generation (RAG) and text-to-SQL systems. To address this, a novel framework is proposed for the automated extraction of fine-grained schema lineage from multilingual enterprise pipeline scripts. This method identifies four key components: source schemas, source tables, transformation logic, and aggregation operations, creating a standardized representation of data transformations. For the rigorous evaluation of lineage quality, this paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that assesses both structural correctness and semantic fidelity. A new benchmark is also presented, comprising 1,700 manually annotated lineages from real-world industrial scripts. Experiments were conducted with 12 language models, from 1.3B to 32B small language models (SLMs) to large language models (LLMs) like GPT-4o and GPT-4.1. The results demonstrate that the performance of schema lineage extraction scales with model size and the sophistication of prompting techniques. Specially, a 32B open-source model, using a single reasoning trace, can achieve performance comparable to the GPT series under standard prompting. This finding suggests a scalable and economical approach for deploying schema-aware agents in practical applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes</title>
<link>https://arxiv.org/abs/2508.07180</link>
<guid>https://arxiv.org/abs/2508.07180</guid>
<content:encoded><![CDATA[
arXiv:2508.07180v1 Announce Type: cross 
Abstract: As large language models LLMs) become increasingly integrated into software development workflows, rigorously evaluating their performance on complex, real-world code generation tasks has become essential. However, existing benchmarks often suffer from data contamination and limited test rigor, constraining their ability to reveal model failures effectively. To address these, we present CODE2BENCH, a end-to-end pipeline for dynamically constructing robust and contamination-resistant benchmarks from real-world GitHub repositories. Specifically, CODE2BENCH introduces three key innovations: (1) Automated Dynamism, achieved through periodic ingestion of recent code to minimize training data contamination; (2) Scope Graph-based dependency analysis, which enables structured classification of functions into benchmark instances with controlled dependency levels (distinguishing between Self-Contained (SC) tasks for cross-language evaluation and Weakly Self-Contained (WSC) tasks involving permitted library usage); and (3) Property-Based Testing (PBT) for the automated synthesis of rigorous test suites to enable thorough functional verification. Using this pipeline, we construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python projects spanning diverse domains, comprising 1,163 code generation tasks with 100% average branch coverage on ground-truth implementations. Extensive evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently struggle with SC tasks requiring complex, non-standard logic and cross-language transfer, while showing relatively stronger performance on WSC tasks in Python. Our work introduces a contamination-resistant, language-agnostic methodology for dynamic benchmark construction, offering a principled foundation for the comprehensive and realistic evaluation of LLMs on real-world software development tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI</title>
<link>https://arxiv.org/abs/2508.07183</link>
<guid>https://arxiv.org/abs/2508.07183</guid>
<content:encoded><![CDATA[
arXiv:2508.07183v1 Announce Type: cross 
Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to support artistic engagement, modifiability, and sustained practice. While curated datasets and training human-scale models can offer artists greater agency and control, large-scale generative models like text-to-image diffusion systems often obscure these possibilities. We suggest that even large models can be treated as creative materials if their internal structure is exposed and manipulable. We propose a craft-based approach to explainability rooted in long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and demonstrate its application through a model-bending and inspection plugin integrated into the node-based interface of ComfyUI. We demonstrate that by interactively manipulating different parts of a generative model, artists can develop an intuition about how each component influences the output.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</title>
<link>https://arxiv.org/abs/2508.07185</link>
<guid>https://arxiv.org/abs/2508.07185</guid>
<content:encoded><![CDATA[
arXiv:2508.07185v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</title>
<link>https://arxiv.org/abs/2508.07195</link>
<guid>https://arxiv.org/abs/2508.07195</guid>
<content:encoded><![CDATA[
arXiv:2508.07195v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Smaller Large Language Models Evaluate Research Quality?</title>
<link>https://arxiv.org/abs/2508.07196</link>
<guid>https://arxiv.org/abs/2508.07196</guid>
<content:encoded><![CDATA[
arXiv:2508.07196v1 Announce Type: cross 
Abstract: Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give research quality evaluation scores that correlate positively with expert scores in nearly all fields, and more strongly that citations in most, it is not known whether this is true for smaller Large Language Models (LLMs). In response, this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The results for 104,187 articles show that Gemma-3-27b-it scores correlate positively with an expert research quality score proxy for all 34 Units of Assessment (broad fields) from the UK Research Excellence Framework 2021. The Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7% of the strength of ChatGPT 4o-mini correlations. Differently from the two larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when the scores are averaged across five repetitions, its scores tend to be lower, and its reports are relatively uniform in style. Overall, the results show that research quality score estimation can be conducted by offline LLMs, so this capability is not an emergent property of the largest LLMs. Moreover, score improvement through repetition is not a universal feature of LLMs. In conclusion, although the largest LLMs still have the highest research evaluation score estimation capability, smaller ones can also be used for this task, and this can be helpful for cost saving or when secure offline processing is needed.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.07201</link>
<guid>https://arxiv.org/abs/2508.07201</guid>
<content:encoded><![CDATA[
arXiv:2508.07201v1 Announce Type: cross 
Abstract: Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Presburger Functional Synthesis: Complexity and Tractable Normal Forms</title>
<link>https://arxiv.org/abs/2508.07207</link>
<guid>https://arxiv.org/abs/2508.07207</guid>
<content:encoded><![CDATA[
arXiv:2508.07207v1 Announce Type: cross 
Abstract: Given a relational specification between inputs and outputs as a logic formula, the problem of functional synthesis is to automatically synthesize a function from inputs to outputs satisfying the relation. Recently, a rich line of work has emerged tackling this problem for specifications in different theories, from Boolean to general first-order logic. In this paper, we launch an investigation of this problem for the theory of Presburger Arithmetic, that we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved in EXPTIME and provide a matching exponential lower bound. This is unlike the case for Boolean functional synthesis (BFnS), where only conditional exponential lower bounds are known. Further, we show that PFnS for one input and one output variable is as hard as BFnS in general. We then identify a special normal form, called PSyNF, for the specification formula that guarantees poly-time and poly-size solvability of PFnS. We prove several properties of PSyNF, including how to check and compile to this form, and conditions under which any other form that guarantees poly-time solvability of PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic normal form that is easier to check but is exponentially less succinct than PSyNF.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains</title>
<link>https://arxiv.org/abs/2508.07208</link>
<guid>https://arxiv.org/abs/2508.07208</guid>
<content:encoded><![CDATA[
arXiv:2508.07208v1 Announce Type: cross 
Abstract: In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Bridge Processes</title>
<link>https://arxiv.org/abs/2508.07220</link>
<guid>https://arxiv.org/abs/2508.07220</guid>
<content:encoded><![CDATA[
arXiv:2508.07220v1 Announce Type: cross 
Abstract: Learning stochastic functions from partially observed context-target pairs is a fundamental problem in probabilistic modeling. Traditional models like Gaussian Processes (GPs) face scalability issues with large datasets and assume Gaussianity, limiting their applicability. While Neural Processes (NPs) offer more flexibility, they struggle with capturing complex, multi-modal target distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a learned diffusion process but rely solely on conditional signals in the denoising network, resulting in weak input coupling from an unconditional forward process and semantic mismatch at the diffusion endpoint. In this work, we propose Neural Bridge Processes (NBPs), a novel method for modeling stochastic functions where inputs x act as dynamic anchors for the entire diffusion trajectory. By reformulating the forward kernel to explicitly depend on x, NBP enforces a constrained path that strictly terminates at the supervised target. This approach not only provides stronger gradient signals but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG signal regression and image regression tasks, achieving substantial improvements over baselines. These results underscore the effectiveness of DDPM-style bridge sampling in enhancing both performance and theoretical consistency for structured prediction tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</title>
<link>https://arxiv.org/abs/2508.07221</link>
<guid>https://arxiv.org/abs/2508.07221</guid>
<content:encoded><![CDATA[
arXiv:2508.07221v1 Announce Type: cross 
Abstract: Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selection and Exploitation of High-Quality Knowledge from Large Language Models for Recommendation</title>
<link>https://arxiv.org/abs/2508.07223</link>
<guid>https://arxiv.org/abs/2508.07223</guid>
<content:encoded><![CDATA[
arXiv:2508.07223v1 Announce Type: cross 
Abstract: In recent years, there has been growing interest in leveraging the impressive generalization capabilities and reasoning ability of large language models (LLMs) to improve the performance of recommenders. With this operation, recommenders can access and learn the additional world knowledge and reasoning information via LLMs. However, in general, for different users and items, the world knowledge derived from LLMs suffers from issues of hallucination, content redundant, and information homogenization. Directly feeding the generated response embeddings into the recommendation model can lead to unavoidable performance deterioration. To address these challenges, we propose a Knowledge Selection \& Exploitation Recommendation (KSER) framework, which effectively select and extracts the high-quality knowledge from LLMs. The framework consists of two key components: a knowledge filtering module and a embedding spaces alignment module. In the knowledge filtering module, a Embedding Selection Filter Network (ESFNet) is designed to assign adaptive weights to different knowledge chunks in different knowledge fields. In the space alignment module, an attention-based architecture is proposed to align the semantic embeddings from LLMs with the feature space used to train the recommendation models. In addition, two training strategies--\textbf{all-parameters training} and \textbf{extractor-only training}--are proposed to flexibly adapt to different downstream tasks and application scenarios, where the extractor-only training strategy offers a novel perspective on knowledge-augmented recommendation. Experimental results validate the necessity and effectiveness of both the knowledge filtering and alignment modules, and further demonstrate the efficiency and effectiveness of the extractor-only training strategy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning</title>
<link>https://arxiv.org/abs/2508.07224</link>
<guid>https://arxiv.org/abs/2508.07224</guid>
<content:encoded><![CDATA[
arXiv:2508.07224v1 Announce Type: cross 
Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning framework composed of four stages: Evaluate (ability and state estimation), Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics (misconception discovery from distractor patterns and response latencies), contrastive item generation (minimal perturbations that invalidate learner shortcuts while pre-serving psychometric validity), and principled scheduling (a restless bandit approximation to spaced retrieval). We formalize a composite readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity, and derive an index policy that is near-optimal under mild assumptions on forgetting and learning gains. We further establish conditions under which counterfactual items provably reduce the posterior probability of a targeted misconception faster than standard practice. The paper focuses on theory and implementable pseudocode; empirical study is left to future work.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocRipple: A Two-Stage Framework for Cold-Start Video Recommendations</title>
<link>https://arxiv.org/abs/2508.07241</link>
<guid>https://arxiv.org/abs/2508.07241</guid>
<content:encoded><![CDATA[
arXiv:2508.07241v1 Announce Type: cross 
Abstract: Most industry scale recommender systems face critical cold start challenges new items lack interaction history, making it difficult to distribute them in a personalized manner. Standard collaborative filtering models underperform due to sparse engagement signals, while content only approaches lack user specific relevance. We propose SocRipple, a novel two stage retrieval framework tailored for coldstart item distribution in social graph based platforms. Stage 1 leverages the creators social connections for targeted initial exposure. Stage 2 builds on early engagement signals and stable user embeddings learned from historical interactions to "ripple" outwards via K Nearest Neighbor (KNN) search. Large scale experiments on a major video platform show that SocRipple boosts cold start item distribution by +36% while maintaining user engagement rate on cold start items, effectively balancing new item exposure with personalized recommendations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation</title>
<link>https://arxiv.org/abs/2508.07243</link>
<guid>https://arxiv.org/abs/2508.07243</guid>
<content:encoded><![CDATA[
arXiv:2508.07243v1 Announce Type: cross 
Abstract: Heuristic negative sampling enhances recommendation performance by selecting negative samples of varying hardness levels from predefined candidate pools to guide the model toward learning more accurate decision boundaries. However, our empirical and theoretical analyses reveal that unobserved environmental confounders (e.g., exposure or popularity biases) in candidate pools may cause heuristic sampling methods to introduce false hard negatives (FHNS). These misleading samples can encourage the model to learn spurious correlations induced by such confounders, ultimately compromising its generalization ability under distribution shifts. To address this issue, we propose a novel method named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing negative samples in the latent space via a conditional diffusion process, CNSDiff avoids the bias introduced by predefined candidate pools and thus reduces the likelihood of generating FHNS. Moreover, it incorporates a causal regularization term to explicitly mitigate the influence of environmental confounders during the negative sampling process, leading to robust negatives that promote out-of-distribution (OOD) generalization. Comprehensive experiments under four representative distribution shift scenarios demonstrate that CNSDiff achieves an average improvement of 13.96% across all evaluation metrics compared to state-of-the-art baselines, verifying its effectiveness and robustness in OOD recommendation tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHAIV: A Framework Towards Practical Open-World Learning</title>
<link>https://arxiv.org/abs/2508.07270</link>
<guid>https://arxiv.org/abs/2508.07270</guid>
<content:encoded><![CDATA[
arXiv:2508.07270v1 Announce Type: cross 
Abstract: Substantial progress has been made in various techniques for open-world recognition. Out-of-distribution (OOD) detection methods can effectively distinguish between known and unknown classes in the data, while incremental learning enables continuous model knowledge updates. However, in open-world scenarios, these approaches still face limitations. Relying solely on OOD detection does not facilitate knowledge updates in the model, and incremental fine-tuning typically requires supervised conditions, which significantly deviate from open-world settings. To address these challenges, this paper proposes OpenHAIV, a novel framework that integrates OOD detection, new class discovery, and incremental continual fine-tuning into a unified pipeline. This framework allows models to autonomously acquire and update knowledge in open-world environments. The proposed framework is available at https://haiv-lab.github.io/openhaiv .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models</title>
<link>https://arxiv.org/abs/2508.07273</link>
<guid>https://arxiv.org/abs/2508.07273</guid>
<content:encoded><![CDATA[
arXiv:2508.07273v1 Announce Type: cross 
Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations in empathetic reasoning, primarily due to the absence of training datasets that integrate both contextual content and paralinguistic cues. In this work, we propose two approaches to incorporate contextual paralinguistic information into model training: (1) an explicit method that provides paralinguistic metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit method that automatically generates novel training question-answer (QA) pairs using both categorical and dimensional emotion annotations alongside speech transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41% on a human-annotated QA benchmark, reaching 46.02% when combined with the explicit approach, showing effectiveness in contextual paralinguistic understanding. We also validate the LLM judge by demonstrating its correlation with classification metrics, providing support for its reliability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</title>
<link>https://arxiv.org/abs/2508.07279</link>
<guid>https://arxiv.org/abs/2508.07279</guid>
<content:encoded><![CDATA[
arXiv:2508.07279v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Understanding via Activation Maximization</title>
<link>https://arxiv.org/abs/2508.07281</link>
<guid>https://arxiv.org/abs/2508.07281</guid>
<content:encoded><![CDATA[
arXiv:2508.07281v1 Announce Type: cross 
Abstract: Understanding internal feature representations of deep neural networks (DNNs) is a fundamental step toward model interpretability. Inspired by neuroscience methods that probe biological neurons using visual stimuli, recent deep learning studies have employed Activation Maximization (AM) to synthesize inputs that elicit strong responses from artificial neurons. In this work, we propose a unified feature visualization framework applicable to both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike prior efforts that predominantly focus on the last output-layer neurons in CNNs, we extend feature visualization to intermediate layers as well, offering deeper insights into the hierarchical structure of learned feature representations. Furthermore, we investigate how activation maximization can be leveraged to generate adversarial examples, revealing potential vulnerabilities and decision boundaries of DNNs. Our experiments demonstrate the effectiveness of our approach in both traditional CNNs and modern ViT, highlighting its generalizability and interpretive value.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment</title>
<link>https://arxiv.org/abs/2508.07283</link>
<guid>https://arxiv.org/abs/2508.07283</guid>
<content:encoded><![CDATA[
arXiv:2508.07283v1 Announce Type: cross 
Abstract: This study explores the intersection of electroencephalography (EEG) microstates and Large Language Models (LLMs) to enhance the assessment of cognitive load states. By utilizing EEG microstate features, the research aims to fine-tune LLMs for improved predictions of distinct cognitive states, specifically 'Rest' and 'Load'. The experimental design is delineated in four comprehensive stages: dataset collection and preprocessing, microstate segmentation and EEG backfitting, feature extraction paired with prompt engineering, and meticulous LLM model selection and refinement. Employing a supervised learning paradigm, the LLM is trained to identify cognitive load states based on EEG microstate features integrated into prompts, producing accurate discrimination of cognitive load. A curated dataset, linking EEG features to specified cognitive load conditions, underpins the experimental framework. The results indicate a significant improvement in model performance following the proposed fine-tuning, showcasing the potential of EEG-informed LLMs in cognitive neuroscience and cognitive AI applications. This approach not only contributes to the understanding of brain dynamics but also paves the way for advancements in machine learning techniques applicable to cognitive load and cognitive AI research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas</title>
<link>https://arxiv.org/abs/2508.07284</link>
<guid>https://arxiv.org/abs/2508.07284</guid>
<content:encoded><![CDATA[
arXiv:2508.07284v1 Announce Type: cross 
Abstract: As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably, "sweet zones" emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Data Attribution for Influence Functions</title>
<link>https://arxiv.org/abs/2508.07297</link>
<guid>https://arxiv.org/abs/2508.07297</guid>
<content:encoded><![CDATA[
arXiv:2508.07297v1 Announce Type: cross 
Abstract: The goal of data attribution is to trace the model's predictions through the learning algorithm and back to its training data. thereby identifying the most influential training samples and understanding how the model's behavior leads to particular predictions. Understanding how individual training examples influence a model's predictions is fundamental for machine learning interpretability, data debugging, and model accountability. Influence functions, originating from robust statistics, offer an efficient, first-order approximation to estimate the impact of marginally upweighting or removing a data point on a model's learned parameters and its subsequent predictions, without the need for expensive retraining. This paper comprehensively reviews the data attribution capability of influence functions in deep learning. We discuss their theoretical foundations, recent algorithmic advances for efficient inverse-Hessian-vector product estimation, and evaluate their effectiveness for data attribution and mislabel detection. Finally, highlighting current challenges and promising directions for unleashing the huge potential of influence functions in large-scale, real-world deep learning scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective</title>
<link>https://arxiv.org/abs/2508.07299</link>
<guid>https://arxiv.org/abs/2508.07299</guid>
<content:encoded><![CDATA[
arXiv:2508.07299v1 Announce Type: cross 
Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models by enabling them to satisfy knowledge, while semi/self-supervised learning (SSL) improves the target task performance by designing unsupervised pretext tasks for unlabeled data to make models satisfy corresponding assumptions. We extend the Nesy theory based on reliable knowledge to the scenario of unreliable knowledge (i.e., assumptions), thereby unifying the theoretical frameworks of SSL and Nesy. Through rigorous theoretical analysis, we demonstrate that, in theory, the impact of pretext tasks on target performance hinges on three factors: knowledge learnability with respect to the model, knowledge reliability with respect to the data, and knowledge completeness with respect to the target. We further propose schemes to operationalize these theoretical metrics, and thereby develop a method that can predict the effectiveness of pretext tasks in advance. This will change the current status quo in practical applications, where the selections of unsupervised tasks are heuristic-based rather than theory-based, and it is difficult to evaluate the rationality of unsupervised pretext task selection before testing the model on the target task. In experiments, we verify a high correlation between the predicted performance-estimated using minimal data-and the actual performance achieved after large-scale semi-supervised or self-supervised learning, thus confirming the validity of the theory and the effectiveness of the evaluation method.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses</title>
<link>https://arxiv.org/abs/2508.07304</link>
<guid>https://arxiv.org/abs/2508.07304</guid>
<content:encoded><![CDATA[
arXiv:2508.07304v1 Announce Type: cross 
Abstract: This paper introduces a new family of cognitive modal logics designed to formalize conjectural reasoning: a modal system in which cognitive contexts extend known facts with hypothetical assumptions to explore their consequences. Unlike traditional doxastic and epistemic systems, conjectural logics rely on a principle, called Axiom C ($\varphi \rightarrow \Box\varphi$), that ensures that all established facts are preserved across hypothetical layers. While Axiom C was dismissed in the past due to its association with modal collapse, we show that the collapse only arises under classical and bivalent assumptions, and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a paracomplete semantic framework, grounded in Weak Kleene logic or Description Logic, where undefined propositions coexist with modal assertions. This prevents the modal collapse and guarantees a layering to distinguish between factual and conjectural statements. Under this framework we define new modal systems, e.g., KC and KDC, and show that they are complete, decidable, and robust under partial knowledge. Finally, we introduce a dynamic operation, $\mathsf{settle}(\varphi)$, which formalizes the transition from conjecture to accepted fact, capturing the event of the update of a world's cognitive state through the resolution of uncertainty.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices</title>
<link>https://arxiv.org/abs/2508.07306</link>
<guid>https://arxiv.org/abs/2508.07306</guid>
<content:encoded><![CDATA[
arXiv:2508.07306v1 Announce Type: cross 
Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has experienced rising global demand due to its affordability and local availability. As dragon fruit cultivation expands, efficient pre- and post-harvest quality inspection has become essential for improving agricultural productivity and minimizing post-harvest losses. This study presents DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN) optimized for real-time quality assessment of dragon fruits on mobile devices. We curated a diverse dataset of 13,789 images, integrating self-collected samples with public datasets (dataset from Mendeley Data), and classified them into four categories: fresh, immature, mature, and defective fruits to ensure robust model training. The proposed model achieves an impressive 93.98% accuracy, outperforming existing methods in fruit quality classification. To facilitate practical adoption, we embedded the model into an intuitive mobile application, enabling farmers and agricultural stakeholders to conduct on-device, real-time quality inspections. This research provides an accurate, efficient, and scalable AI-driven solution for dragon fruit quality control, supporting digital agriculture and empowering smallholder farmers with accessible technology. By bridging the gap between research and real-world application, our work advances post-harvest management and promotes sustainable farming practices.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</title>
<link>https://arxiv.org/abs/2508.07307</link>
<guid>https://arxiv.org/abs/2508.07307</guid>
<content:encoded><![CDATA[
arXiv:2508.07307v1 Announce Type: cross 
Abstract: Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways</title>
<link>https://arxiv.org/abs/2508.07308</link>
<guid>https://arxiv.org/abs/2508.07308</guid>
<content:encoded><![CDATA[
arXiv:2508.07308v1 Announce Type: cross 
Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&amp;A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&amp;A. Its structured design enables robust evaluation of LLMs' multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities</title>
<link>https://arxiv.org/abs/2508.07315</link>
<guid>https://arxiv.org/abs/2508.07315</guid>
<content:encoded><![CDATA[
arXiv:2508.07315v1 Announce Type: cross 
Abstract: While beam search improves speech recognition quality over greedy decoding, standard implementations are slow, often sequential, and CPU-bound. To fully leverage modern hardware capabilities, we present a novel open-source FlexCTC toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal Classification (CTC) models. Developed entirely in Python and PyTorch, it offers a fast, user-friendly, and extensible alternative to traditional C++, CUDA, or WFST-based decoders. The toolkit features a high-performance, fully batched GPU implementation with eliminated CPU-GPU synchronization and minimized kernel launch overhead via CUDA Graphs. It also supports advanced contextualization techniques, including GPU-powered N-gram language model fusion and phrase-level boosting. These features enable accurate and efficient decoding, making them suitable for both research and production use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering</title>
<link>https://arxiv.org/abs/2508.07321</link>
<guid>https://arxiv.org/abs/2508.07321</guid>
<content:encoded><![CDATA[
arXiv:2508.07321v1 Announce Type: cross 
Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategies of Code-switching in Human-Machine Dialogs</title>
<link>https://arxiv.org/abs/2508.07325</link>
<guid>https://arxiv.org/abs/2508.07325</guid>
<content:encoded><![CDATA[
arXiv:2508.07325v1 Announce Type: cross 
Abstract: Most people are multilingual, and most multilinguals code-switch, yet the characteristics of code-switched language are not fully understood. We developed a chatbot capable of completing a Map Task with human participants using code-switched Spanish and English. In two experiments, we prompted the bot to code-switch according to different strategies, examining (1) the feasibility of such experiments for investigating bilingual language use, and (2) whether participants would be sensitive to variations in discourse and grammatical patterns. Participants generally enjoyed code-switching with our bot as long as it produced predictable code-switching behavior; when code-switching was random or ungrammatical (as when producing unattested incongruent mixed-language noun phrases, such as `la fork'), participants enjoyed the task less and were less successful at completing it. These results underscore the potential downsides of deploying insufficiently developed multilingual language technology, while also illustrating the promise of such technology for conducting research on bilingual language use.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</title>
<link>https://arxiv.org/abs/2508.07329</link>
<guid>https://arxiv.org/abs/2508.07329</guid>
<content:encoded><![CDATA[
arXiv:2508.07329v1 Announce Type: cross 
Abstract: With the breakthrough progress of large language models (LLMs) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through sparse activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</title>
<link>https://arxiv.org/abs/2508.07345</link>
<guid>https://arxiv.org/abs/2508.07345</guid>
<content:encoded><![CDATA[
arXiv:2508.07345v1 Announce Type: cross 
Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is essential for genomic studies due to their crucial role as structural elements in bacteriophages. Computational tools, particularly machine learning, have emerged for annotating phage protein sequences from high-throughput sequencing. However, effective annotation requires specialized sequence encodings. Our paper introduces ProteoKnight, a new image-based encoding method that addresses spatial constraints in existing techniques, yielding competitive performance in PVP classification using pre-trained convolutional neural networks. Additionally, our study evaluates prediction uncertainty in binary PVP classification through Monte Carlo Dropout (MCD). \textbf{Methods:} ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences were classified using multiple pre-trained CNNs. Variance and entropy measures assessed prediction uncertainty across proteins of various classes and lengths. \textbf{Results:} Our experiments achieved 90.8% accuracy in binary classification, comparable to state-of-the-art methods. Multi-class classification accuracy remains suboptimal. Our uncertainty analysis unveils variability in prediction confidence influenced by protein class and sequence length. \textbf{Conclusions:} Our study surpasses frequency chaos game representation (FCGR) by introducing novel image encoding that mitigates spatial information loss limitations. Our classification technique yields accurate and robust PVP predictions while identifying low-confidence predictions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</title>
<link>https://arxiv.org/abs/2508.07371</link>
<guid>https://arxiv.org/abs/2508.07371</guid>
<content:encoded><![CDATA[
arXiv:2508.07371v1 Announce Type: cross 
Abstract: As the complexity of software systems continues to increase, the demand for automated testing and maintenance tools is growing exponentially. To meet this urgent need, we propose a new assertion generation method based on Hardware Description Language (HDL). This method combines a lightweight, parameter-adjustable large language model (LLM) with the Unsloth platform to automatically generate test cases, thereby significantly reducing training costs without sacrificing accuracy or generalization performance. Empirical evaluation shows that our method can efficiently generate assertions that strictly conform to the hardware logic. This framework provides a robust and flexible solution to modern software testing and maintenance challenges. https://github.com/liusu-orange/AutoAssert-1 and https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics</title>
<link>https://arxiv.org/abs/2508.07390</link>
<guid>https://arxiv.org/abs/2508.07390</guid>
<content:encoded><![CDATA[
arXiv:2508.07390v1 Announce Type: cross 
Abstract: With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spin Glass Characterization of Neural Networks</title>
<link>https://arxiv.org/abs/2508.07397</link>
<guid>https://arxiv.org/abs/2508.07397</guid>
<content:encoded><![CDATA[
arXiv:2508.07397v1 Announce Type: cross 
Abstract: This work presents a statistical mechanics characterization of neural networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin glasses. A Hopfield-type spin glass model is constructed from a given feedforward neural network (FNN). Overlaps between simulated replica samples serve as a characteristic descriptor of the FNN. The connection between the spin-glass description and commonly studied properties of the FNN -- such as data fitting, capacity, generalization, and robustness -- has been investigated and empirically demonstrated. Unlike prior analytical studies that focus on model ensembles, this method provides a computable descriptor for individual network instances, which reveals nontrivial structural properties that are not captured by conventional metrics such as loss or accuracy. Preliminary results suggests its potential for practical applications such as model inspection, safety verification, and detection of hidden vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriVLN: Vision-and-Language Navigation for Agricultural Robots</title>
<link>https://arxiv.org/abs/2508.07406</link>
<guid>https://arxiv.org/abs/2508.07406</guid>
<content:encoded><![CDATA[
arXiv:2508.07406v1 Announce Type: cross 
Abstract: Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging GNN to Enhance MEF Method in Predicting ENSO</title>
<link>https://arxiv.org/abs/2508.07410</link>
<guid>https://arxiv.org/abs/2508.07410</guid>
<content:encoded><![CDATA[
arXiv:2508.07410v1 Announce Type: cross 
Abstract: Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO) remains a long-standing challenge in climate science. The previously developed Multimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by two independent deep learning modules: a 3D Convolutional Neural Network (3D-CNN) and a time-series module. In their approach, outputs of the two modules are combined using a weighting strategy wherein one is prioritized over the other as a function of global performance. Separate weighting or testing of individual ensemble members did not occur, however, which may have limited the model to optimize the use of high-performing but spread-out forecasts. In this study, we propose a better framework that employs graph-based analysis to directly model similarity between all 80 members of the ensemble. By constructing an undirected graph whose vertices are ensemble outputs and whose weights on edges measure similarity (via RMSE and correlation), we identify and cluster structurally similar and accurate predictions. From which we obtain an optimized subset of 20 members using community detection methods. The final prediction is then obtained by averaging this optimized subset. This method improves the forecast skill through noise removal and emphasis on ensemble coherence. Interestingly, our graph-based selection shows robust statistical characteristics among top performers, offering new ensemble behavior insights. In addition, we observe that while the GNN-based approach does not always outperform the baseline MEF under every scenario, it produces more stable and consistent outputs, particularly in compound long-lead situations. The approach is model-agnostic too, suggesting that it can be applied directly to other forecasting models with gargantuan ensemble outputs, such as statistical, physical, or hybrid models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v1 Announce Type: cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightning Prediction under Uncertainty: DeepLight with Hazy Loss</title>
<link>https://arxiv.org/abs/2508.07428</link>
<guid>https://arxiv.org/abs/2508.07428</guid>
<content:encoded><![CDATA[
arXiv:2508.07428v1 Announce Type: cross 
Abstract: Lightning, a common feature of severe meteorological conditions, poses significant risks, from direct human injuries to substantial economic losses. These risks are further exacerbated by climate change. Early and accurate prediction of lightning would enable preventive measures to safeguard people, protect property, and minimize economic losses. In this paper, we present DeepLight, a novel deep learning architecture for predicting lightning occurrences. Existing prediction models face several critical limitations: they often struggle to capture the dynamic spatial context and inherent uncertainty of lightning events, underutilize key observational data, such as radar reflectivity and cloud properties, and rely heavily on Numerical Weather Prediction (NWP) systems, which are both computationally expensive and highly sensitive to parameter settings. To overcome these challenges, DeepLight leverages multi-source meteorological data, including radar reflectivity, cloud properties, and historical lightning occurrences through a dual-encoder architecture. By employing multi-branch convolution techniques, it dynamically captures spatial correlations across varying extents. Furthermore, its novel Hazy Loss function explicitly addresses the spatio-temporal uncertainty of lightning by penalizing deviations based on proximity to true events, enabling the model to better learn patterns amidst randomness. Extensive experiments show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over state-of-the-art methods, establishing it as a robust solution for lightning prediction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freeze and Reveal: Exposing Modality Bias in Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.07432</link>
<guid>https://arxiv.org/abs/2508.07432</guid>
<content:encoded><![CDATA[
arXiv:2508.07432v1 Announce Type: cross 
Abstract: Vision Language Models achieve impressive multi-modal performance but often inherit gender biases from their training data. This bias might be coming from both the vision and text modalities. In this work, we dissect the contributions of vision and text backbones to these biases by applying targeted debiasing using Counterfactual Data Augmentation and Task Vector methods. Inspired by data-efficient approaches in hate-speech classification, we introduce a novel metric, Degree of Stereotypicality and a corresponding debiasing method, Data Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with minimal computational cost. We curate a gender annotated dataset and evaluate all methods on VisoGender benchmark to quantify improvements and identify dominant source of bias. Our results show that CDA reduces the gender gap by 6% and DAUDoS by 3% but using only one-third of the data. Both methods also improve the model's ability to correctly identify gender in images by 3%, with DAUDoS achieving this improvement using only almost one-third of training data. From our experiment's, we observed that CLIP's vision encoder is more biased whereas PaliGemma2's text encoder is more biased. By identifying whether bias stems more from vision or text encoders, our work enables more targeted and effective bias mitigation strategies in future multi-modal systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search</title>
<link>https://arxiv.org/abs/2508.07446</link>
<guid>https://arxiv.org/abs/2508.07446</guid>
<content:encoded><![CDATA[
arXiv:2508.07446v1 Announce Type: cross 
Abstract: In redistricting litigation, effective enforcement of the Voting Rights Act has often involved providing the court with districting plans that display a larger number of majority-minority districts than the current proposal (as was true, for example, in what followed Allen v. Milligan concerning the congressional districting plan for Alabama in 2023). Recent work by Cannon et al. proposed a heuristic algorithm for generating plans to optimize majority-minority districts, which they called short bursts; that algorithm relies on a sophisticated random walk over the space of all plans, transitioning in bursts, where the initial plan for each burst is the most successful plan from the previous burst. We propose a method based on integer programming, where we build upon another previous work, the stochastic hierarchical partitioning algorithm, which heuristically generates a robust set of potential districts (viewed as columns in a standard set partitioning formulation); that approach was designed to optimize a different notion of fairness across a statewide plan. We design a new column generation algorithm to find plans via integer programming that outperforms short bursts on multiple data sets in generating statewide plans with significantly more majority-minority districts. These results also rely on a new local re-optimization algorithm to iteratively improve on any baseline solution, as well as an algorithm to increase the compactness of districts in plans generated (without impacting the number of majority-minority districts).
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07452</link>
<guid>https://arxiv.org/abs/2508.07452</guid>
<content:encoded><![CDATA[
arXiv:2508.07452v1 Announce Type: cross 
Abstract: Integrated, end-to-end learning of representations and policies remains a cornerstone of deep reinforcement learning (RL). However, to address the challenge of learning effective features from a sparse reward signal, recent trends have shifted towards adding complex auxiliary objectives or fully decoupling the two processes, often at the cost of increased design complexity. This work proposes an alternative to both decoupling and naive end-to-end learning, arguing that performance can be significantly improved by structuring the interaction between distinct perception and control networks with a principled, game-theoretic dynamic. We formalize this dynamic by introducing the Stackelberg Coupled Representation and Reinforcement Learning (SCORER) framework, which models the interaction between perception and control as a Stackelberg game. The perception network (leader) strategically learns features to benefit the control network (follower), whose own objective is to minimize its Bellman error. We approximate the game's equilibrium with a practical two-timescale algorithm. Applied to standard DQN variants on benchmark tasks, SCORER improves sample efficiency and final performance. Our results show that performance gains can be achieved through principled algorithmic design of the perception-control dynamic, without requiring complex auxiliary objectives or architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Aware Generative Microscopic Traffic Simulation</title>
<link>https://arxiv.org/abs/2508.07453</link>
<guid>https://arxiv.org/abs/2508.07453</guid>
<content:encoded><![CDATA[
arXiv:2508.07453v1 Announce Type: cross 
Abstract: Accurately modeling individual vehicle behavior in microscopic traffic simulation remains a key challenge in intelligent transportation systems, as it requires vehicles to realistically generate and respond to complex traffic phenomena such as phantom traffic jams. While traditional human driver simulation models offer computational tractability, they do so by abstracting away the very complexity that defines human driving. On the other hand, recent advances in infrastructure-mounted camera-based roadway sensing have enabled the extraction of vehicle trajectory data, presenting an opportunity to shift toward generative, agent-based models. Yet, a major bottleneck remains: most existing datasets are either overly sanitized or lack standardization, failing to reflect the noisy, imperfect nature of real-world sensing. Unlike data from vehicle-mounted sensors-which can mitigate sensing artifacts like occlusion through overlapping fields of view and sensor fusion-infrastructure-based sensors surface a messier, more practical view of challenges that traffic engineers encounter. To this end, we present the I-24 MOTION Scenario Dataset (I24-MSD)-a standardized, curated dataset designed to preserve a realistic level of sensor imperfection, embracing these errors as part of the learning problem rather than an obstacle to overcome purely from preprocessing. Drawing from noise-aware learning strategies in computer vision, we further adapt existing generative models in the autonomous driving community for I24-MSD with noise-aware loss functions. Our results show that such models not only outperform traditional baselines in realism but also benefit from explicitly engaging with, rather than suppressing, data imperfection. We view I24-MSD as a stepping stone toward a new generation of microscopic traffic simulation that embraces the real-world challenges and is better aligned with practical needs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models</title>
<link>https://arxiv.org/abs/2508.07484</link>
<guid>https://arxiv.org/abs/2508.07484</guid>
<content:encoded><![CDATA[
arXiv:2508.07484v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering</title>
<link>https://arxiv.org/abs/2508.07486</link>
<guid>https://arxiv.org/abs/2508.07486</guid>
<content:encoded><![CDATA[
arXiv:2508.07486v1 Announce Type: cross 
Abstract: Modern software systems are increasingly shifting from monolithic architectures to microservices to enhance scalability, maintainability, and deployment flexibility. Existing microservice extraction methods typically rely on hard clustering, assigning each software component to a single microservice. This approach often increases inter-service coupling and reduces intra-service cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a framework that formulates microservice extraction as a soft clustering problem, allowing components to belong probabilistically to multiple microservices. This approach is inspired by expert-driven decompositions, where practitioners intentionally replicate certain software components across services to reduce communication overhead. Mo2oM combines deep semantic embeddings with structural dependencies extracted from methodcall graphs to capture both functional and architectural relationships. A graph neural network-based soft clustering algorithm then generates the final set of microservices. We evaluate Mo2oM on four open-source monolithic benchmarks and compare it against eight state-of-the-art baselines. Our results demonstrate that Mo2oM achieves improvements of up to 40.97% in structural modularity (balancing cohesion and coupling), 58% in inter-service call percentage (communication overhead), 26.16% in interface number (modularity and decoupling), and 38.96% in non-extreme distribution (service size balance) across all benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Product Hilbert Spaces to the Generalized Koopman Operator and the Nonlinear Fundamental Lemma</title>
<link>https://arxiv.org/abs/2508.07494</link>
<guid>https://arxiv.org/abs/2508.07494</guid>
<content:encoded><![CDATA[
arXiv:2508.07494v1 Announce Type: cross 
Abstract: The generalization of the Koopman operator to systems with control input and the derivation of a nonlinear fundamental lemma are two open problems that play a key role in the development of data-driven control methods for nonlinear systems. Both problems hinge on the construction of observable or basis functions and their corresponding Hilbert space that enable an infinite-dimensional, linear system representation. In this paper we derive a novel solution to these problems based on orthonormal expansion in a product Hilbert space constructed as the tensor product between the Hilbert spaces of the state and input observable functions, respectively. We prove that there exists an infinite-dimensional linear operator, i.e. the generalized Koopman operator, from the constructed product Hilbert space to the Hilbert space corresponding to the lifted state propagated forward in time. A scalable data-driven method for computing finite-dimensional approximations of generalized Koopman operators and several choices of observable functions are also presented. Moreover, we derive a nonlinear fundamental lemma by exploiting the bilinear structure of the infinite-dimensional generalized Koopman model. The effectiveness of the developed generalized Koopman embedding is illustrated on the Van der Pol oscillator.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design</title>
<link>https://arxiv.org/abs/2508.07497</link>
<guid>https://arxiv.org/abs/2508.07497</guid>
<content:encoded><![CDATA[
arXiv:2508.07497v1 Announce Type: cross 
Abstract: Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectoral Knowledge in AI and Urban Studies: A Framework for Transdisciplinary Research</title>
<link>https://arxiv.org/abs/2508.07507</link>
<guid>https://arxiv.org/abs/2508.07507</guid>
<content:encoded><![CDATA[
arXiv:2508.07507v1 Announce Type: cross 
Abstract: Transdisciplinary approaches are increasingly essential for addressing grand societal challenges, particularly in complex domains such as Artificial Intelligence (AI), urban planning, and social sciences. However, effectively validating and integrating knowledge across distinct epistemic and ontological perspectives poses significant difficulties. This article proposes a six-dimensional framework for assessing and strengthening transdisciplinary knowledge validity in AI and city studies, based on an extensive analysis of the most cited research (2014--2024). Specifically, the framework classifies research orientations according to ontological, epistemological, methodological, teleological, axiological, and valorization dimensions. Our findings show a predominance of perspectives aligned with critical realism (ontological), positivism (epistemological), analytical methods (methodological), consequentialism (teleological), epistemic values (axiological), and social/economic valorization. Less common stances, such as idealism, mixed methods, and cultural valorization, are also examined for their potential to enrich knowledge production. We highlight how early career researchers and transdisciplinary teams can leverage this framework to reconcile divergent disciplinary viewpoints and promote socially accountable outcomes.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials</title>
<link>https://arxiv.org/abs/2508.07514</link>
<guid>https://arxiv.org/abs/2508.07514</guid>
<content:encoded><![CDATA[
arXiv:2508.07514v1 Announce Type: cross 
Abstract: Field trials are vital in herbicide research and development to assess effects on crops and weeds under varied conditions. Traditionally, evaluations rely on manual visual assessments, which are time-consuming, labor-intensive, and subjective. Automating species and damage identification is challenging due to subtle visual differences, but it can greatly enhance efficiency and consistency.
  We present an improved segmentation model combining a general-purpose self-supervised visual model with hierarchical inference based on botanical taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain using digital and mobile cameras, the model was tested on digital camera data (year 2023) and drone imagery from the United States, Germany, and Spain (year 2024) to evaluate robustness under domain shift. This cross-device evaluation marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to 0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to 0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone images), it maintained strong performance with moderate degradation (species: F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where earlier models failed.
  These results confirm the model's robustness and real-world applicability. It is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated crop and weed monitoring across diverse geographies.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews</title>
<link>https://arxiv.org/abs/2508.07517</link>
<guid>https://arxiv.org/abs/2508.07517</guid>
<content:encoded><![CDATA[
arXiv:2508.07517v1 Announce Type: cross 
Abstract: Word clouds are a common way to summarize qualitative interviews, yet traditional frequency-based methods often fail in conversational contexts: they surface filler words, ignore paraphrase, and fragment semantically related ideas. This limits their usefulness in early-stage analysis, when researchers need fast, interpretable overviews of what participant actually said. We introduce ThemeClouds, an open-source visualization tool that uses large language models (LLMs) to generate thematic, participant-weighted word clouds from dialogue transcripts. The system prompts an LLM to identify concept-level themes across a corpus and then counts how many unique participants mention each topic, yielding a visualization grounded in breadth of mention rather than raw term frequency. Researchers can customize prompts and visualization parameters, providing transparency and control. Using interviews from a user study comparing five recording-device configurations (31 participants; 155 transcripts, Whisper ASR), our approach surfaces more actionable device concerns than frequency clouds and topic-modeling baselines (e.g., LDA, BERTopic). We discuss design trade-offs for integrating LLM assistance into qualitative workflows, implications for interpretability and researcher agency, and opportunities for interactive analyses such as per-condition contrasts (``diff clouds'').
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI</title>
<link>https://arxiv.org/abs/2508.07520</link>
<guid>https://arxiv.org/abs/2508.07520</guid>
<content:encoded><![CDATA[
arXiv:2508.07520v1 Announce Type: cross 
Abstract: What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A DICOM Image De-identification Algorithm in the MIDI-B Challenge</title>
<link>https://arxiv.org/abs/2508.07538</link>
<guid>https://arxiv.org/abs/2508.07538</guid>
<content:encoded><![CDATA[
arXiv:2508.07538v1 Announce Type: cross 
Abstract: Image de-identification is essential for the public sharing of medical images, particularly in the widely used Digital Imaging and Communications in Medicine (DICOM) format as required by various regulations and standards, including Health Insurance Portability and Accountability Act (HIPAA) privacy rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B) Challenge at the 27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024) was organized to evaluate rule-based DICOM image de-identification algorithms with a large dataset of clinical DICOM images. In this report, we explore the critical challenges of de-identifying DICOM images, emphasize the importance of removing personally identifiable information (PII) to protect patient privacy while ensuring the continued utility of medical data for research, diagnostics, and treatment, and provide a comprehensive overview of the standards and regulations that govern this process. Additionally, we detail the de-identification methods we applied - such as pixel masking, date shifting, date hashing, text recognition, text replacement, and text removal - to process datasets during the test phase in strict compliance with these standards. According to the final leaderboard of the MIDI-B challenge, the latest version of our solution algorithm correctly executed 99.92% of the required actions and ranked 2nd out of 10 teams that completed the challenge (from a total of 22 registered teams). Finally, we conducted a thorough analysis of the resulting statistics and discussed the limitations of current approaches and potential avenues for future improvement.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
<link>https://arxiv.org/abs/2508.07556</link>
<guid>https://arxiv.org/abs/2508.07556</guid>
<content:encoded><![CDATA[
arXiv:2508.07556v1 Announce Type: cross 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Small-footprint Acoustic Echo Cancellation Solution for Mobile Full-Duplex Speech Interactions</title>
<link>https://arxiv.org/abs/2508.07561</link>
<guid>https://arxiv.org/abs/2508.07561</guid>
<content:encoded><![CDATA[
arXiv:2508.07561v1 Announce Type: cross 
Abstract: In full-duplex speech interaction systems, effective Acoustic Echo Cancellation (AEC) is crucial for recovering echo-contaminated speech. This paper presents a neural network-based AEC solution to address challenges in mobile scenarios with varying hardware, nonlinear distortions and long latency. We first incorporate diverse data augmentation strategies to enhance the model's robustness across various environments. Moreover, progressive learning is employed to incrementally improve AEC effectiveness, resulting in a considerable improvement in speech quality. To further optimize AEC's downstream applications, we introduce a novel post-processing strategy employing tailored parameters designed specifically for tasks such as Voice Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing their overall efficacy. Finally, our method employs a small-footprint model with streaming inference, enabling seamless deployment on mobile devices. Empirical results demonstrate effectiveness of the proposed method in Echo Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside significant improvements in both VAD and ASR results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation</title>
<link>https://arxiv.org/abs/2508.07569</link>
<guid>https://arxiv.org/abs/2508.07569</guid>
<content:encoded><![CDATA[
arXiv:2508.07569v1 Announce Type: cross 
Abstract: Drafting a Statement of Work (SOW) is a vital part of business and legal projects. It outlines key details like deliverables, timelines, responsibilities, and legal terms. However, creating these documents is often a slow and complex process. It usually involves multiple people, takes several days, and leaves room for errors or outdated content. This paper introduces a new AI-driven automation system that makes the entire SOW drafting process faster, easier, and more accurate. Instead of relying completely on humans, the system uses three intelligent components or 'agents' that each handle a part of the job. One agent writes the first draft, another checks if everything is legally correct, and the third agent formats the document and ensures everything is in order. Unlike basic online tools that just fill in templates, this system understands the meaning behind the content and customizes the SOW to match the needs of the project. It also checks legal compliance and formatting so that users can trust the result. The system was tested using real business examples. It was able to create a full SOW in under three minutes, compared to several hours or days using manual methods. It also performed well in accuracy and quality, showing that it can reduce legal risks and save a lot of time. This solution shows how artificial intelligence can be used to support legal and business professionals by taking care of routine work and helping them focus on more important decisions. It's a step toward making legal processes smarter, faster, and more reliable.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression</title>
<link>https://arxiv.org/abs/2508.07571</link>
<guid>https://arxiv.org/abs/2508.07571</guid>
<content:encoded><![CDATA[
arXiv:2508.07571v1 Announce Type: cross 
Abstract: Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical transformer analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model decoding through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IBPS: Indian Bail Prediction System</title>
<link>https://arxiv.org/abs/2508.07592</link>
<guid>https://arxiv.org/abs/2508.07592</guid>
<content:encoded><![CDATA[
arXiv:2508.07592v1 Announce Type: cross 
Abstract: Bail decisions are among the most frequently adjudicated matters in Indian courts, yet they remain plagued by subjectivity, delays, and inconsistencies. With over 75% of India's prison population comprising undertrial prisoners, many from socioeconomically disadvantaged backgrounds, the lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog. In this paper, we present the Indian Bail Prediction System (IBPS), an AI-powered framework designed to assist in bail decision-making by predicting outcomes and generating legally sound rationales based solely on factual case attributes and statutory provisions. We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations such as age, health, criminal history, crime category, custody duration, statutes, and judicial reasoning. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG. Our results demonstrate that models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts. IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShoulderShot: Generating Over-the-Shoulder Dialogue Videos</title>
<link>https://arxiv.org/abs/2508.07597</link>
<guid>https://arxiv.org/abs/2508.07597</guid>
<content:encoded><![CDATA[
arXiv:2508.07597v1 Announce Type: cross 
Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at https://shouldershot.github.io.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making</title>
<link>https://arxiv.org/abs/2508.07617</link>
<guid>https://arxiv.org/abs/2508.07617</guid>
<content:encoded><![CDATA[
arXiv:2508.07617v1 Announce Type: cross 
Abstract: AI has the potential to augment human decision making. However, even high-performing models can produce inaccurate predictions when deployed. These inaccuracies, combined with automation bias, where humans overrely on AI predictions, can result in worse decisions. Selective prediction, in which potentially unreliable model predictions are hidden from users, has been proposed as a solution. This approach assumes that when AI abstains and informs the user so, humans make decisions as they would without AI involvement. To test this assumption, we study the effects of selective prediction on human decisions in a clinical context. We conducted a user study of 259 clinicians tasked with diagnosing and treating hospitalized patients. We compared their baseline performance without any AI involvement to their AI-assisted accuracy with and without selective prediction. Our findings indicate that selective prediction mitigates the negative effects of inaccurate AI in terms of decision accuracy. Compared to no AI assistance, clinician accuracy declined when shown inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]), but recovered under selective prediction (64% [95% CI: 54%-73%]). However, while selective prediction nearly maintains overall accuracy, our results suggest that it alters patterns of mistakes: when informed the AI abstains, clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35% increase in missed treatments) compared to no AI input at all. Our findings underscore the importance of empirically validating assumptions about how humans engage with AI within human-AI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation</title>
<link>https://arxiv.org/abs/2508.07621</link>
<guid>https://arxiv.org/abs/2508.07621</guid>
<content:encoded><![CDATA[
arXiv:2508.07621v1 Announce Type: cross 
Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with catheter ablation procedures, but procedural outcomes are highly variable. Evaluating and improving ablation efficacy is challenging due to the complex interaction between patient-specific tissue and procedural factors. This paper asks two questions: Can AF recurrence be predicted by simulating the effects of procedural parameters? How should we ablate to reduce AF recurrence? We propose SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel deep-learning framework that addresses these questions. SOFA first simulates the outcome of an ablation strategy by generating a post-ablation image depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and the specific procedural parameters used (e.g., ablation locations, duration, temperature, power, and force). During this simulation, it predicts AF recurrence risk. Critically, SOFA then introduces an optimization scheme that refines these procedural parameters to minimize the predicted risk. Our method leverages a multi-modal, multi-view generator that processes 2.5D representations of the atrium. Quantitative evaluations show that SOFA accurately synthesizes post-ablation images and that our optimization scheme leads to a 22.18\% reduction in the model-predicted recurrence risk. To the best of our knowledge, SOFA is the first framework to integrate the simulation of procedural effects, recurrence prediction, and parameter optimization, offering a novel tool for personalizing AF ablation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2508.07629</link>
<guid>https://arxiv.org/abs/2508.07629</guid>
<content:encoded><![CDATA[
arXiv:2508.07629v1 Announce Type: cross 
Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\% on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information</title>
<link>https://arxiv.org/abs/2508.07630</link>
<guid>https://arxiv.org/abs/2508.07630</guid>
<content:encoded><![CDATA[
arXiv:2508.07630v1 Announce Type: cross 
Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo</title>
<link>https://arxiv.org/abs/2508.07631</link>
<guid>https://arxiv.org/abs/2508.07631</guid>
<content:encoded><![CDATA[
arXiv:2508.07631v1 Announce Type: cross 
Abstract: We study the problem of posterior sampling in the context of score based generative models. We have a trained score network for a prior $p(x)$, a measurement model $p(y|x)$, and are tasked with sampling from the posterior $p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case) under well-accepted computational hardness assumptions. Despite this, popular algorithms for tasks such as image super-resolution, stylization, and reconstruction enjoy empirical success. Rather than establishing distributional assumptions or restricted settings under which exact posterior sampling is tractable, we view this as a more general "tilting" problem of biasing a distribution towards a measurement. Under minimal assumptions, we show that one can tractably sample from a distribution that is simultaneously close to the posterior of a noised prior in KL divergence and the true posterior in Fisher divergence. Intuitively, this combination ensures that the resulting sample is consistent with both the measurement and the prior. To the best of our knowledge these are the first formal results for (approximate) posterior sampling in polynomial time.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution Explanations for Deep Neural Networks: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2508.07636</link>
<guid>https://arxiv.org/abs/2508.07636</guid>
<content:encoded><![CDATA[
arXiv:2508.07636v1 Announce Type: cross 
Abstract: Attribution explanation is a typical approach for explaining deep neural networks (DNNs), inferring an importance or contribution score for each input variable to the final output. In recent years, numerous attribution methods have been developed to explain DNNs. However, a persistent concern remains unresolved, i.e., whether and which attribution methods faithfully reflect the actual contribution of input variables to the decision-making process. The faithfulness issue undermines the reliability and practical utility of attribution explanations. We argue that these concerns stem from three core challenges. First, difficulties arise in comparing attribution methods due to their unstructured heterogeneity, differences in heuristics, formulations, and implementations that lack a unified organization. Second, most methods lack solid theoretical underpinnings, with their rationales remaining absent, ambiguous, or unverified. Third, empirically evaluating faithfulness is challenging without ground truth. Recent theoretical advances provide a promising way to tackle these challenges, attracting increasing attention. We summarize these developments, with emphasis on three key directions: (i) Theoretical unification, which uncovers commonalities and differences among methods, enabling systematic comparisons; (ii) Theoretical rationale, clarifying the foundations of existing methods; (iii) Theoretical evaluation, rigorously proving whether methods satisfy faithfulness principles. Beyond a comprehensive review, we provide insights into how these studies help deepen theoretical understanding, inform method selection, and inspire new attribution methods. We conclude with a discussion of promising open problems for further work.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp-HGN: Grasping the Unexpected</title>
<link>https://arxiv.org/abs/2508.07648</link>
<guid>https://arxiv.org/abs/2508.07648</guid>
<content:encoded><![CDATA[
arXiv:2508.07648v1 Announce Type: cross 
Abstract: For transradial amputees, robotic prosthetic hands promise to regain the capability to perform daily living activities. To advance next-generation prosthetic hand control design, it is crucial to address current shortcomings in robustness to out of lab artifacts, and generalizability to new environments. Due to the fixed number of object to interact with in existing datasets, contrasted with the virtually infinite variety of objects encountered in the real world, current grasp models perform poorly on unseen objects, negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to generalize to unseen object types and show that conventional models like YOLO, despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to infer the suitable grasp type estimate based on the object's physical characteristics resulting in a significant 50.2% accuracy over unseen object types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp estimation on edge and accurate cloud inference as a fail-safe, effectively expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC) enables dynamic switching between edge and cloud models, improving semantic projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object types. Over a real-world sample mix, it reaches 86% average accuracy (12.2% gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning</title>
<link>https://arxiv.org/abs/2508.07659</link>
<guid>https://arxiv.org/abs/2508.07659</guid>
<content:encoded><![CDATA[
arXiv:2508.07659v1 Announce Type: cross 
Abstract: This study aims to discover spatial correlations between Earth observations and atmospheric states to improve the forecasting accuracy of global atmospheric state estimation, which are usually conducted using conventional numerical weather prediction (NWP) systems and is the beginning of weather forecasting. NWP systems predict future atmospheric states at fixed locations, which are called NWP grid points, by analyzing previous atmospheric states and newly acquired Earth observations without fixed locations. Thus, surrounding meteorological context and the changing locations of the observations make spatial correlations between atmospheric states and observations over time. To handle complicated spatial correlations, which change dynamically, we employ spatiotemporal graph neural networks (STGNNs) with structure learning. However, structure learning has an inherent limitation that this can cause structural information loss and over-smoothing problem by generating excessive edges. To solve this problem, we regulate edge sampling by adaptively determining node degrees and considering the spatial distances between NWP grid points and observations. We validated the effectiveness of the proposed method by using real-world atmospheric state and observation data from East Asia. Even in areas with high atmospheric variability, the proposed method outperformed existing STGNN models with and without structure learning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiClass: Generalist Lightweight Model for Sequence Classification Tasks</title>
<link>https://arxiv.org/abs/2508.07662</link>
<guid>https://arxiv.org/abs/2508.07662</guid>
<content:encoded><![CDATA[
arXiv:2508.07662v1 Announce Type: cross 
Abstract: Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting</title>
<link>https://arxiv.org/abs/2508.07668</link>
<guid>https://arxiv.org/abs/2508.07668</guid>
<content:encoded><![CDATA[
arXiv:2508.07668v1 Announce Type: cross 
Abstract: With the increase in maritime traffic and the mandatory implementation of the Automatic Identification System (AIS), the importance and diversity of maritime traffic analysis tasks based on AIS data, such as vessel trajectory prediction, anomaly detection, and collision risk assessment, is rapidly growing. However, existing approaches tend to address these tasks individually, making it difficult to holistically consider complex maritime situations. To address this limitation, we propose a novel framework, AIS-LLM, which integrates time-series AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a Cross-Modality Alignment Module for semantic alignment between time-series data and textual prompts, and an LLM-based Multi-Task Decoder. This architecture enables the simultaneous execution of three key tasks: trajectory prediction, anomaly detection, and risk assessment of vessel collisions within a single end-to-end system. Experimental results demonstrate that AIS-LLM outperforms existing methods across individual tasks, validating its effectiveness. Furthermore, by integratively analyzing task outputs to generate situation summaries and briefings, AIS-LLM presents the potential for more intelligent and efficient maritime traffic management.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation</title>
<link>https://arxiv.org/abs/2508.07681</link>
<guid>https://arxiv.org/abs/2508.07681</guid>
<content:encoded><![CDATA[
arXiv:2508.07681v1 Announce Type: cross 
Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ dysfunction, making early detection and optimal management critical. Previous reinforcement learning (RL) approaches to sepsis management rely primarily on structured data, such as lab results or vital signs, and on a dearth of a comprehensive understanding of the patient's condition. In this work, we propose a Multimodal Offline REinforcement learning for Clinical notes Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis control in intensive care units. MORE-CLEAR employs pre-trained large-scale language models (LLMs) to facilitate the extraction of rich semantic representations from clinical notes, preserving clinical context and improving patient state representation. Gated fusion and cross-modal attention allow dynamic weight adjustment in the context of time and the effective integration of multimodal data. Extensive cross-validation using two public (MIMIC-III and MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly improves estimated survival rate and policy performance compared to single-modal RL approaches. To our knowledge, this is the first to leverage LLM capabilities within a multimodal offline RL for better state representation in medical applications. This approach can potentially expedite the treatment and management of sepsis by enabling reinforcement learning models to propose enhanced actions based on a more comprehensive understanding of patient conditions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</title>
<link>https://arxiv.org/abs/2508.07683</link>
<guid>https://arxiv.org/abs/2508.07683</guid>
<content:encoded><![CDATA[
arXiv:2508.07683v1 Announce Type: cross 
Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments corresponding to natural language queries, which is a critical capability for long-form video understanding. Although existing reinforcement learning approaches encourage models to generate reasoning chains before predictions, they fail to explicitly constrain the reasoning process to ensure the quality of the final temporal predictions. To address this limitation, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG), a novel framework that introduces timestamp anchors within the reasoning process to enforce explicit supervision to the thought content. These anchors serve as intermediate verification points. More importantly, we require each reasoning step to produce increasingly accurate temporal estimations, thereby ensuring that the reasoning process contributes meaningfully to the final prediction. To address the challenge of low-probability anchor generation in models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation training strategy: (1) initial GRPO training to collect 30K high-quality reasoning traces containing multiple timestamp anchors, (2) supervised fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the SFT-enhanced model. This three-stage training strategy enables robust anchor generation while maintaining reasoning quality. Experiments show that our model achieves state-of-the-art performance while producing interpretable, verifiable reasoning chains with progressively refined temporal estimations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval</title>
<link>https://arxiv.org/abs/2508.07690</link>
<guid>https://arxiv.org/abs/2508.07690</guid>
<content:encoded><![CDATA[
arXiv:2508.07690v1 Announce Type: cross 
Abstract: Tool learning has emerged as a promising paradigm for large language models (LLMs) to solve many real-world tasks. Nonetheless, with the tool repository rapidly expanding, it is impractical to contain all tools within the limited input length of LLMs. To alleviate these issues, researchers have explored incorporating a tool retrieval module to select the most relevant tools or represent tools as unique tokens within LLM parameters. However, most state-of-the-art methods are under transductive settings, assuming all tools have been observed during training. Such a setting deviates from reality as the real-world tool repository is evolving and incorporates new tools frequently. When dealing with these unseen tools, which refer to tools not encountered during the training phase, these methods are limited by two key issues, including the large distribution shift and the vulnerability of similarity-based retrieval. To this end, inspired by human cognitive processes of mastering unseen tools through discovering and applying the logical information from prior experience, we introduce a novel Logic-Guided Semantic Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to mine and transfer latent logical information for inductive tool retrieval without costly retraining. Specifically, LoSemB contains a logic-based embedding alignment module to mitigate distribution shifts and implements a relational augmented retrieval mechanism to reduce the vulnerability of similarity-based retrieval. Extensive experiments demonstrate that LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Consumption in Parallel Neural Network Training</title>
<link>https://arxiv.org/abs/2508.07706</link>
<guid>https://arxiv.org/abs/2508.07706</guid>
<content:encoded><![CDATA[
arXiv:2508.07706v1 Announce Type: cross 
Abstract: The increasing demand for computational resources of training neural networks leads to a concerning growth in energy consumption. While parallelization has enabled upscaling model and dataset sizes and accelerated training, its impact on energy consumption is often overlooked. To close this research gap, we conducted scaling experiments for data-parallel training of two models, ResNet50 and FourCastNet, and evaluated the impact of parallelization parameters, i.e., GPU count, global batch size, and local batch size, on predictive performance, training time, and energy consumption. We show that energy consumption scales approximately linearly with the consumed resources, i.e., GPU hours; however, the respective scaling factor differs substantially between distinct model trainings and hardware, and is systematically influenced by the number of samples and gradient updates per GPU hour. Our results shed light on the complex interplay of scaling up neural network training and can inform future developments towards more sustainable AI research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</title>
<link>https://arxiv.org/abs/2508.07710</link>
<guid>https://arxiv.org/abs/2508.07710</guid>
<content:encoded><![CDATA[
arXiv:2508.07710v1 Announce Type: cross 
Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for constructing energy-efficient Transformer architectures. Compared to directly trained Spiking Transformers, ANN-to-SNN conversion methods bypass the high training costs. However, existing methods still suffer from notable limitations, failing to effectively handle nonlinear operations in Transformer architectures and requiring additional fine-tuning processes for pre-trained ANNs. To address these issues, we propose a high-performance and training-free ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron, which employs an exponential decay strategy and multi-basis encoding method to efficiently approximate various nonlinear operations. It removes the requirement for weight modifications in pre-trained ANNs. Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</title>
<link>https://arxiv.org/abs/2508.07714</link>
<guid>https://arxiv.org/abs/2508.07714</guid>
<content:encoded><![CDATA[
arXiv:2508.07714v1 Announce Type: cross 
Abstract: Accurate detection and classification of diverse door types in floor plans drawings is critical for multiple applications, such as building compliance checking, and indoor scene understanding. Despite their importance, publicly available datasets specifically designed for fine-grained multi-class door detection remain scarce. In this work, we present a semi-automated pipeline that leverages a state-of-the-art object detector and a large language model (LLM) to construct a multi-class door detection dataset with minimal manual effort. Doors are first detected as a unified category using a deep object detection model. Next, an LLM classifies each detected instance based on its visual and contextual features. Finally, a human-in-the-loop stage ensures high-quality labels and bounding boxes. Our method significantly reduces annotation cost while producing a dataset suitable for benchmarking neural models in floor plan analysis. This work demonstrates the potential of combining deep learning and multimodal reasoning for efficient dataset construction in complex real-world domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning</title>
<link>https://arxiv.org/abs/2508.07731</link>
<guid>https://arxiv.org/abs/2508.07731</guid>
<content:encoded><![CDATA[
arXiv:2508.07731v1 Announce Type: cross 
Abstract: Efficient control of prosthetic limbs via non-invasive brain-computer interfaces (BCIs) requires advanced EEG processing, including pre-filtering, feature extraction, and action prediction, performed in real time on edge AI hardware. Achieving this on resource-constrained devices presents challenges in balancing model complexity, computational efficiency, and latency. We present CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on embedded AI hardware, achieving real-time operation without compromising accuracy. The system integrates BrainFlow, an open-source library for EEG data acquisition and streaming, with optimized deep learning (DL) models for precise brain signal classification. Using evolutionary search, we identify Pareto-optimal DL configurations through hyperparameter tuning, optimizer analysis, and window selection, analyzed individually and in ensemble configurations. We apply model compression techniques such as pruning and quantization to optimize models for embedded deployment, balancing efficiency and accuracy. We collected an EEG dataset and designed an annotation pipeline enabling precise labeling of brain signals corresponding to specific intended actions, forming the basis for training our optimized DL models. CognitiveArm also supports voice commands for seamless mode switching, enabling control of the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded hardware, it ensures low latency and real-time responsiveness. A full-scale prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying three core actions (left, right, idle). Voice integration enables multiplexed, variable movement for everyday tasks (e.g., handshake, cup picking), enhancing real-world performance and demonstrating CognitiveArm's potential for advanced prosthetic control.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases</title>
<link>https://arxiv.org/abs/2508.07742</link>
<guid>https://arxiv.org/abs/2508.07742</guid>
<content:encoded><![CDATA[
arXiv:2508.07742v1 Announce Type: cross 
Abstract: Repair-based semantics have been extensively studied as a means of obtaining meaningful answers to queries posed over inconsistent knowledge bases (KBs). While several works have considered how to exploit a priority relation between facts to select optimal repairs, the question of how to specify such preferences remains largely unaddressed. This motivates us to introduce a declarative rule-based framework for specifying and computing a priority relation between conflicting facts. As the expressed preferences may contain undesirable cycles, we consider the problem of determining when a set of preference rules always yields an acyclic relation, and we also explore a pragmatic approach that extracts an acyclic relation by applying various cycle removal techniques. Towards an end-to-end system for querying inconsistent KBs, we present a preliminary implementation and experimental evaluation of the framework, which employs answer set programming to evaluate the preference rules, apply the desired cycle resolution techniques to obtain a priority relation, and answer queries under prioritized-repair semantics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title>
<link>https://arxiv.org/abs/2508.07745</link>
<guid>https://arxiv.org/abs/2508.07745</guid>
<content:encoded><![CDATA[
arXiv:2508.07745v1 Announce Type: cross 
Abstract: Insider threats, which can lead to severe losses, remain a major security concern. While machine learning-based insider threat detection (ITD) methods have shown promising results, their progress is hindered by the scarcity of high-quality data. Enterprise data is sensitive and rarely accessible, while publicly available datasets, when limited in scale due to cost, lack sufficient real-world coverage; and when purely synthetic, they fail to capture rich semantics and realistic user behavior. To address this, we propose Chimera, the first large language model (LLM)-based multi-agent framework that automatically simulates both benign and malicious insider activities and collects diverse logs across diverse enterprise environments. Chimera models each employee with agents that have role-specific behavior and integrates modules for group meetings, pairwise interactions, and autonomous scheduling, capturing realistic organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP theft, system sabotage) and has been deployed to simulate activities in three sensitive domains: technology company, finance corporation, and medical institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via human studies and quantitative analysis, confirming its diversity, realism, and presence of explainable threat patterns. Evaluations of existing ITD methods show an average F1-score of 0.83, which is significantly lower than 0.99 on the CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for advancing ITD research.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment</title>
<link>https://arxiv.org/abs/2508.07750</link>
<guid>https://arxiv.org/abs/2508.07750</guid>
<content:encoded><![CDATA[
arXiv:2508.07750v1 Announce Type: cross 
Abstract: Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Probabilistic Graph Circuits</title>
<link>https://arxiv.org/abs/2508.07763</link>
<guid>https://arxiv.org/abs/2508.07763</guid>
<content:encoded><![CDATA[
arXiv:2508.07763v1 Announce Type: cross 
Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive power thanks to very efficient and scalable neural networks. However, these networks contain non-linearities that prevent analytical computation of many standard probabilistic inference queries, i.e., these DGMs are considered \emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs) address this issue by enabling \emph{tractable} probabilistic inference, they operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue, we introduce Sparse PGCs, a new class of tractable generative models that operate directly on sparse graph representation, reducing the complexity to $\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the context of de novo drug design, we empirically demonstrate that SPGCs retain exact inference capabilities, improve memory efficiency and inference speed, and match the performance of intractable DGMs in key metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.07766</link>
<guid>https://arxiv.org/abs/2508.07766</guid>
<content:encoded><![CDATA[
arXiv:2508.07766v1 Announce Type: cross 
Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when scaled, frequently employed in computer vision and artistic design in the representation of SVG code. In this era of proliferating AI-powered systems, enabling AI to understand and generate SVG has become increasingly urgent. However, AI-driven SVG understanding and generation (U&amp;G) remain significant challenges. SVG code, equivalent to a set of curves and lines controlled by floating-point parameters, demands high precision in SVG U&amp;G. Besides, SVG generation operates under diverse conditional constraints, including textual prompts and visual references, which requires powerful multi-modal processing for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal Large Language Models (MLLMs) have demonstrated capabilities to process multi-modal inputs and generate complex vector controlling parameters, suggesting the potential to address SVG U&amp;G tasks within a unified model. To unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset called UniSVG, comprising 525k data items, tailored for MLLM training and evaluation. To our best knowledge, it is the first comprehensive dataset designed for unified SVG generation (from textual prompts and images) and SVG understanding (color, category, usage, etc.). As expected, learning on the proposed dataset boosts open-source MLLMs' performance on various SVG U&amp;G tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset, benchmark, weights, codes and experiment details on https://ryanlijinke.github.io/.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto Multi-Objective Alignment for Language Models</title>
<link>https://arxiv.org/abs/2508.07768</link>
<guid>https://arxiv.org/abs/2508.07768</guid>
<content:encoded><![CDATA[
arXiv:2508.07768v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in real-world applications that require careful balancing of multiple, often conflicting, objectives, such as informativeness versus conciseness, or helpfulness versus creativity. However, current alignment methods, primarily based on RLHF, optimize LLMs toward a single reward function, resulting in rigid behavior that fails to capture the complexity and diversity of human preferences. This limitation hinders the adaptability of LLMs to practical scenarios, making multi-objective alignment (MOA) a critical yet underexplored area. To bridge this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and computationally efficient algorithm designed explicitly for MOA in LLMs. In contrast to computationally prohibitive multi-objective optimization (MOO) methods, PAMA transforms multi-objective RLHF into a convex optimization with a closed-form solution, significantly enhancing scalability. Traditional MOO approaches suffer from prohibitive O(n^2*d) complexity, where d represents the number of model parameters, typically in the billions for LLMs, rendering direct optimization infeasible. PAMA reduces this complexity to O(n) where n is the number of objectives, enabling optimization to be completed within milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto stationary point, where no objective can be improved without degrading at least one other. Extensive experiments across language models ranging from 125M to 7B parameters demonstrate PAMA's robust and effective MOA capabilities, aligning with its theoretical advantages. PAMA provides a highly efficient solution to the MOA problem that was previously considered intractable, offering a practical and theoretically grounded approach to aligning LLMs with diverse human values, paving the way for versatile and adaptable real-world AI deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography</title>
<link>https://arxiv.org/abs/2508.07773</link>
<guid>https://arxiv.org/abs/2508.07773</guid>
<content:encoded><![CDATA[
arXiv:2508.07773v1 Announce Type: cross 
Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive testing (NDT) technique for detecting subsurface anomalies in industrial components. Due to the high dimensionality of AIRT data, current approaches employ non-linear autoencoders (AEs) for dimensionality reduction. However, the latent space learned by AIRT AEs lacks structure, limiting their effectiveness in downstream defect characterization tasks. To address this limitation, this paper proposes a principal component analysis guided (PCA-guided) autoencoding framework for structured dimensionality reduction to capture intricate, non-linear features in thermographic signals while enforcing a structured latent space. A novel loss function, PCA distillation loss, is introduced to guide AIRT AEs to align the latent representation with structured PCA components while capturing the intricate, non-linear patterns in thermographic signals. To evaluate the utility of the learned, structured latent space, we propose a neural network-based evaluation metric that assesses its suitability for defect characterization. Experimental results show that the proposed PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR), and neural network-based metrics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer</title>
<link>https://arxiv.org/abs/2508.07817</link>
<guid>https://arxiv.org/abs/2508.07817</guid>
<content:encoded><![CDATA[
arXiv:2508.07817v1 Announce Type: cross 
Abstract: The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title>
<link>https://arxiv.org/abs/2508.07819</link>
<guid>https://arxiv.org/abs/2508.07819</guid>
<content:encoded><![CDATA[
arXiv:2508.07819v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auditory Intelligence: Understanding the World Through Sound</title>
<link>https://arxiv.org/abs/2508.07829</link>
<guid>https://arxiv.org/abs/2508.07829</guid>
<content:encoded><![CDATA[
arXiv:2508.07829v1 Announce Type: cross 
Abstract: Recent progress in auditory intelligence has yielded high-performing systems for sound event detection (SED), acoustic scene classification (ASC), automated audio captioning (AAC), and audio question answering (AQA). Yet these tasks remain largely constrained to surface-level recognition-capturing what happened but not why, what it implies, or how it unfolds in context. I propose a conceptual reframing of auditory intelligence as a layered, situated process that encompasses perception, reasoning, and interaction. To instantiate this view, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX, and AUGMENT-those structure auditory understanding across time-frequency pattern captioning, hierarchical event/scene description, causal explanation, and goal-driven interpretation, respectively. Together, these paradigms provide a roadmap toward more generalizable, explainable, and human-aligned auditory intelligence, and are intended to catalyze a broader discussion of what it means for machines to understand sound.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</title>
<link>https://arxiv.org/abs/2508.07842</link>
<guid>https://arxiv.org/abs/2508.07842</guid>
<content:encoded><![CDATA[
arXiv:2508.07842v1 Announce Type: cross 
Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex multi-step tasks that require continuous planning, sequential decision-making, and extended execution across domains to achieve the final goal. However, existing methods heavily rely on skill chaining by concatenating pre-trained subtasks, with environment observations and self-state tightly coupled, lacking the ability to generalize to new combinations of environments and skills, failing to complete various LH tasks across domains. To solve this problem, this paper presents DETACH, a cross-domain learning framework for LH tasks via biologically inspired dual-stream disentanglement. Inspired by the brain's "where-what" dual pathway mechanism, DETACH comprises two core modules: i) an environment learning module for spatial understanding, which captures object functions, spatial relationships, and scene semantics, achieving cross-domain transfer through complete environment-self disentanglement; ii) a skill learning module for task execution, which processes self-state information including joint degrees of freedom and motor patterns, enabling cross-skill transfer through independent motor pattern encoding. We conducted extensive experiments on various LH tasks in HSI scenes. Compared with existing methods, DETACH can achieve an average subtasks success rate improvement of 23% and average execution efficiency improvement of 29%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images</title>
<link>https://arxiv.org/abs/2508.07847</link>
<guid>https://arxiv.org/abs/2508.07847</guid>
<content:encoded><![CDATA[
arXiv:2508.07847v1 Announce Type: cross 
Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential disruptions to critical infrastructure, while predicting solar flares remains a significant challenge. Existing methods based on heuristic physical features often lack representation learning from solar images. On the other hand, end-to-end learning approaches struggle to model long-range temporal dependencies in solar images. In this study, we propose Deep Space Weather Model (Deep SWM), which is based on multiple deep state space models for handling both ten-channel solar images and long-range spatio-temporal dependencies. Deep SWM also features a sparse masked autoencoder, a novel pretraining strategy that employs a two-phase masking approach to preserve crucial regions such as sunspots while compressing spatial information. Furthermore, we built FlareBench, a new public benchmark for solar flare prediction covering a full 11-year solar activity cycle, to validate our method. Our method outperformed baseline methods and even human expert performance on standard metrics in terms of performance and reliability. The project page can be found at https://keio-smilab25.github.io/DeepSWM.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vertex Features for Neural Global Illumination</title>
<link>https://arxiv.org/abs/2508.07852</link>
<guid>https://arxiv.org/abs/2508.07852</guid>
<content:encoded><![CDATA[
arXiv:2508.07852v1 Announce Type: cross 
Abstract: Recent research on learnable neural representations has been widely adopted in the field of 3D scene reconstruction and neural rendering applications. However, traditional feature grid representations often suffer from substantial memory footprint, posing a significant bottleneck for modern parallel computing hardware. In this paper, we present neural vertex features, a generalized formulation of learnable representation for neural rendering tasks involving explicit mesh surfaces. Instead of uniformly distributing neural features throughout 3D space, our method stores learnable features directly at mesh vertices, leveraging the underlying geometry as a compact and structured representation for neural processing. This not only optimizes memory efficiency, but also improves feature representation by aligning compactly with the surface using task-specific geometric priors. We validate our neural representation across diverse neural rendering tasks, with a specific emphasis on neural radiosity. Experimental results demonstrate that our method reduces memory consumption to only one-fifth (or even less) of grid-based representations, while maintaining comparable rendering quality and lowering inference overhead.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images</title>
<link>https://arxiv.org/abs/2508.07875</link>
<guid>https://arxiv.org/abs/2508.07875</guid>
<content:encoded><![CDATA[
arXiv:2508.07875v1 Announce Type: cross 
Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer, and early, accurate diagnosis is critical to improving patient survival rates by guiding treatment decisions. Combining medical expertise with artificial intelligence (AI) holds significant promise for enhancing the precision and efficiency of IDC detection. In this work, we propose a human-in-the-loop (HITL) deep learning system designed to detect IDC in histopathology images. The system begins with an initial diagnosis provided by a high-performance EfficientNetV2S model, offering feedback from AI to the human expert. Medical professionals then review the AI-generated results, correct any misclassified images, and integrate the revised labels into the training dataset, forming a feedback loop from the human back to the AI. This iterative process refines the model's performance over time. The EfficientNetV2S model itself achieves state-of-the-art performance compared to existing methods in the literature, with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system further improves the model's accuracy using four experimental groups with misclassified images. These results demonstrate the potential of this collaborative approach to enhance AI performance in diagnostic systems. This work contributes to advancing automated, efficient, and highly accurate methods for IDC detection through human-AI collaboration, offering a promising direction for future AI-assisted medical diagnostics.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Contrastive Learning for Weakly Supervised Affordance Grounding</title>
<link>https://arxiv.org/abs/2508.07877</link>
<guid>https://arxiv.org/abs/2508.07877</guid>
<content:encoded><![CDATA[
arXiv:2508.07877v1 Announce Type: cross 
Abstract: Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</title>
<link>https://arxiv.org/abs/2508.07885</link>
<guid>https://arxiv.org/abs/2508.07885</guid>
<content:encoded><![CDATA[
arXiv:2508.07885v1 Announce Type: cross 
Abstract: This paper introduces an advanced AI-driven perception system for autonomous quadcopter navigation in GPS-denied indoor environments. The proposed framework leverages cloud computing to offload computationally intensive tasks and incorporates a custom-designed printed circuit board (PCB) for efficient sensor data acquisition, enabling robust navigation in confined spaces. The system integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for context-aware decision-making. A virtual safety envelope, enforced by calibrated sensor offsets, ensures collision avoidance, while a multithreaded architecture achieves low-latency processing. Enhanced spatial awareness is facilitated by 3D bounding box estimation with Kalman filtering. Experimental results in an indoor testbed demonstrate strong performance, with object detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42 trials over approximately 11 minutes, and end-to-end system latency below 1 second. This cloud-supported, high-intelligence framework serves as an auxiliary perception and navigation system, complementing state-of-the-art drone autonomy for GPS-denied confined spaces.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</title>
<link>https://arxiv.org/abs/2508.07887</link>
<guid>https://arxiv.org/abs/2508.07887</guid>
<content:encoded><![CDATA[
arXiv:2508.07887v1 Announce Type: cross 
Abstract: Simulators have revolutionized scientific practice across the natural sciences. By generating data that reliably approximate real-world phenomena, they enable scientists to accelerate hypothesis testing and optimize experimental designs. This is perhaps best illustrated by AlphaFold, a Nobel-prize winning simulator in chemistry that predicts protein structures from amino acid sequences, enabling rapid prototyping of molecular interactions, drug targets, and protein functions. In the behavioral sciences, a reliable participant simulator - a system capable of producing human-like behavior across cognitive tasks - would represent a similarly transformative advance. Recently, Binz et al. introduced Centaur, a large language model (LLM) fine-tuned on human data from 160 experiments, proposing its use not only as a model of cognition but also as a participant simulator for "in silico prototyping of experimental studies", e.g., to advance automated cognitive science. Here, we review the core criteria for a participant simulator and assess how well Centaur meets them. Although Centaur demonstrates strong predictive accuracy, its generative behavior - a critical criterion for a participant simulator - systematically diverges from human data. This suggests that, while Centaur is a significant step toward predicting human behavior, it does not yet meet the standards of a reliable participant simulator or an accurate model of cognition.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction</title>
<link>https://arxiv.org/abs/2508.07897</link>
<guid>https://arxiv.org/abs/2508.07897</guid>
<content:encoded><![CDATA[
arXiv:2508.07897v1 Announce Type: cross 
Abstract: Computer vision-based technologies significantly enhance surgical automation by advancing tool tracking, detection, and localization. However, Current data-driven approaches are data-voracious, requiring large, high-quality labeled image datasets, which limits their application in surgical data science. Our Work introduces a novel dynamic Gaussian Splatting technique to address the data scarcity in surgical image datasets. We propose a dynamic Gaussian model to represent dynamic surgical scenes, enabling the rendering of surgical instruments from unseen viewpoints and deformations with real tissue backgrounds. We utilize a dynamic training adjustment strategy to address challenges posed by poorly calibrated camera poses from real-world scenarios. Additionally, we propose a method based on dynamic Gaussians for automatically generating annotations for our synthetic data. For evaluation, we constructed a new dataset featuring seven scenes with 14,000 frames of tool and camera motion and tool jaw articulation, with a background of an ex-vivo porcine model. Using this dataset, we synthetically replicate the scene deformation from the ground truth data, allowing direct comparisons of synthetic image quality. Experimental results illustrate that our method generates photo-realistic labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio (29.87). We further evaluate the performance of medical-specific neural networks trained on real and synthetic images using an unseen real-world image dataset. Our results show that the performance of models trained on synthetic images generated by the proposed method outperforms those trained with state-of-the-art standard data augmentation by 10%, leading to an overall improvement in model performances by nearly 15%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2508.07903</link>
<guid>https://arxiv.org/abs/2508.07903</guid>
<content:encoded><![CDATA[
arXiv:2508.07903v1 Announce Type: cross 
Abstract: Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias Analysis</title>
<link>https://arxiv.org/abs/2508.07944</link>
<guid>https://arxiv.org/abs/2508.07944</guid>
<content:encoded><![CDATA[
arXiv:2508.07944v1 Announce Type: cross 
Abstract: Despite growing attention to deepfake speech detection, the aspects of bias and fairness remain underexplored in the speech domain. To address this gap, we introduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly annotated resource enabling systematic evaluation of demographic biases in deepfake speech detection. SCDF contains over 237,000 utterances in a balanced representation of both male and female speakers spanning five languages and a wide age range. We evaluate several state-of-the-art detectors and show that speaker characteristics significantly influence detection performance, revealing disparities across sex, language, age, and synthesizer type. These findings highlight the need for bias-aware development and provide a foundation for building non-discriminatory deepfake detection systems aligned with ethical and regulatory standards.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Challenges and Opportunities of AI-assisted Codebase Generation</title>
<link>https://arxiv.org/abs/2508.07966</link>
<guid>https://arxiv.org/abs/2508.07966</guid>
<content:encoded><![CDATA[
arXiv:2508.07966v1 Announce Type: cross 
Abstract: Recent AI code assistants have significantly improved their ability to process more complex contexts and generate entire codebases based on a textual description, compared to the popular snippet-level generation. These codebase AI assistants (CBAs) can also extend or adapt codebases, allowing users to focus on higher-level design and deployment decisions. While prior work has extensively studied the impact of snippet-level code generation, this new class of codebase generation models is relatively unexplored. Despite initial anecdotal reports of excitement about these agents, they remain less frequently adopted compared to snippet-level code assistants. To utilize CBAs better, we need to understand how developers interact with CBAs, and how and why CBAs fall short of developers' needs. In this paper, we explored these gaps through a counterbalanced user study and interview with (n = 16) students and developers working on coding tasks with CBAs. We found that participants varied the information in their prompts, like problem description (48% of prompts), required functionality (98% of prompts), code structure (48% of prompts), and their prompt writing process. Despite various strategies, the overall satisfaction score with generated codebases remained low (mean = 2.8, median = 3, on a scale of one to five). Participants mentioned functionality as the most common factor for dissatisfaction (77% of instances), alongside poor code quality (42% of instances) and communication issues (25% of instances). We delve deeper into participants' dissatisfaction to identify six underlying challenges that participants faced when using CBAs, and extracted five barriers to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial CBAs to compare their capabilities with participant challenges and present design opportunities for more efficient and useful CBAs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer</title>
<link>https://arxiv.org/abs/2508.07970</link>
<guid>https://arxiv.org/abs/2508.07970</guid>
<content:encoded><![CDATA[
arXiv:2508.07970v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite notable advances enabled by existing RLHF training frameworks, significant challenges remain in scaling to complex multimodal workflows and adapting to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating the bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across a range of experimental scenarios, demonstrating that it achieves substantial improvements in throughput compared to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models supporting WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
arXiv:2508.07976v1 Announce Type: cross 
Abstract: Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
arXiv:2508.07981v1 Announce Type: cross 
Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval</title>
<link>https://arxiv.org/abs/2508.07995</link>
<guid>https://arxiv.org/abs/2508.07995</guid>
<content:encoded><![CDATA[
arXiv:2508.07995v1 Announce Type: cross 
Abstract: Retrieval-augmented generation has achieved strong performance on knowledge-intensive tasks where query-document relevance can be identified through direct lexical or semantic matches. However, many real-world queries involve abstract reasoning, analogical thinking, or multi-step inference, which existing retrievers often struggle to capture. To address this challenge, we present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive information retrieval. DIVER consists of four components: document processing to improve input quality, LLM-driven query expansion via iterative document interaction, a reasoning-enhanced retriever fine-tuned on synthetic multi-domain data with hard negatives, and a pointwise reranker that combines LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original queries, consistently outperforming competitive reasoning-aware models. These results demonstrate the effectiveness of reasoning-aware retrieval strategies in complex real-world tasks. Our code and retrieval model will be released soon.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[
arXiv:2508.08005v1 Announce Type: cross 
Abstract: Extensive experiments and prior studies show that no single maximum clique algorithm consistently performs best across all instances, highlighting the importance of selecting suitable algorithms based on instance features. Through an extensive analysis of relevant studies, it is found that there is a lack of research work concerning algorithm selection oriented toward the Maximum Clique Problem (MCP). In this work, we propose a learning-based framework that integrates both traditional machine learning and graph neural networks to address this gap. We construct a labeled dataset by running four exact MCP algorithms on a diverse collection of graph instances, accompanied by structural and global statistical features extracted from each graph. We first evaluate four conventional classifiers: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple dataset variants. Experimental results show that RF consistently shows strong performance across metrics and dataset variants, making it a reliable baseline. In addition, feature importance analysis indicates that connectivity and topological structure are strong predictors of algorithm performance. Building on these findings, we develop a dual-channel model named GAT-MLP, which combines a Graph Attention Network (GAT) for local structural encoding with a Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model shows strong and consistent performance across all metrics. Our results highlight the effectiveness of dual-channel architectures and the promise of graph neural networks in combinatorial algorithm selection.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Knowledge Tracing by Exploring Follow-up Performance Trends</title>
<link>https://arxiv.org/abs/2508.08019</link>
<guid>https://arxiv.org/abs/2508.08019</guid>
<content:encoded><![CDATA[
arXiv:2508.08019v1 Announce Type: cross 
Abstract: Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses, offer new opportunities for human learning. At the core of such systems, knowledge tracing (KT) predicts students' future performance by analyzing their historical learning activities, enabling an accurate evaluation of students' knowledge states over time. We show that existing KT methods often encounter correlation conflicts when analyzing the relationships between historical learning sequences and future performance. To address such conflicts, we propose to extract so-called Follow-up Performance Trends (FPTs) from historical ITS data and to incorporate them into KT. We propose a method called Forward-Looking Knowledge Tracing (FINER) that combines historical learning sequences with FPTs to enhance student performance prediction accuracy. FINER constructs learning patterns that facilitate the retrieval of FPTs from historical ITS data in linear time; FINER includes a novel similarity-aware attention mechanism that aggregates FPTs based on both frequency and contextual similarity; and FINER offers means of combining FPTs and historical learning sequences to enable more accurate prediction of student future performance. Experiments on six real-world datasets show that FINER can outperform ten state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches</title>
<link>https://arxiv.org/abs/2508.08027</link>
<guid>https://arxiv.org/abs/2508.08027</guid>
<content:encoded><![CDATA[
arXiv:2508.08027v1 Announce Type: cross 
Abstract: Speech Recognition (ASR) due to phoneme distortions and high variability. While self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown promise, their effectiveness in dysarthric speech remains unclear. This study systematically benchmarks these models with different decoding strategies, including CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our contributions include (1) benchmarking ASR architectures for dysarthric speech, (2) introducing LLM-based decoding to improve intelligibility, (3) analyzing generalization across datasets, and (4) providing insights into recognition errors across severity levels. Findings highlight that LLM-enhanced decoding improves dysarthric ASR by leveraging linguistic constraints for phoneme restoration and grammatical correction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning</title>
<link>https://arxiv.org/abs/2508.08030</link>
<guid>https://arxiv.org/abs/2508.08030</guid>
<content:encoded><![CDATA[
arXiv:2508.08030v1 Announce Type: cross 
Abstract: Radiation response in cancer is shaped by complex, patient specific biology, yet current treatment strategies often rely on uniform dose prescriptions without accounting for tumor heterogeneity. In this study, we introduce a meta learning framework for one-shot prediction of radiosensitivity measured by SF2 using cell line level gene expression data. Unlike the widely used Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene signature, our proposed meta-learned model allows the importance of each gene to vary by sample through fine tuning. This flexibility addresses key limitations of static models like RSI, which assume uniform gene contributions across tumor types and discard expression magnitude and gene gene interactions. Our results show that meta learning offers robust generalization to unseen samples and performs well in tumor subgroups with high radiosensitivity variability, such as adenocarcinoma and large cell carcinoma. By learning transferable structure across tasks while preserving sample specific adaptability, our approach enables rapid adaptation to individual samples, improving predictive accuracy across diverse tumor subtypes while uncovering context dependent patterns of gene influence that may inform personalized therapy.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title>
<link>https://arxiv.org/abs/2508.08040</link>
<guid>https://arxiv.org/abs/2508.08040</guid>
<content:encoded><![CDATA[
arXiv:2508.08040v1 Announce Type: cross 
Abstract: Prompt-based tuning has emerged as a lightweight alternative to full fine-tuning in large vision-language models, enabling efficient adaptation via learned contextual prompts. This paradigm has recently been extended to federated learning settings (e.g., PromptFL), where clients collaboratively train prompts under data privacy constraints. However, the security implications of prompt-based aggregation in federated multimodal learning remain largely unexplored, leaving a critical attack surface unaddressed. In this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack targeting prompt-based federated learning in multimodal contrastive models. In BadPromptFL, compromised clients jointly optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into the global aggregation process. These prompts are then propagated to benign clients, enabling universal backdoor activation at inference without modifying model parameters. Leveraging the contextual learning behavior of CLIP-style architectures, BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal visibility and limited client participation. Extensive experiments across multiple datasets and aggregation protocols validate the effectiveness, stealth, and generalizability of our attack, raising critical concerns about the robustness of prompt-based federated learning in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</title>
<link>https://arxiv.org/abs/2508.08042</link>
<guid>https://arxiv.org/abs/2508.08042</guid>
<content:encoded><![CDATA[
arXiv:2508.08042v1 Announce Type: cross 
Abstract: Recommendation systems have faced significant challenges in cold-start scenarios, where new items with a limited history of interaction need to be effectively recommended to users. Though multimodal data (e.g., images, text, audio, etc.) offer rich information to address this issue, existing approaches often employ simplistic integration methods such as concatenation, average pooling, or fixed weighting schemes, which fail to capture the complex relationships between modalities. Our study proposes a novel Mixture of Experts (MoE) framework for multimodal cold-start recommendation, named MAMEX, which dynamically leverages latent representation from different modalities. MAMEX utilizes modality-specific expert networks and introduces a learnable gating mechanism that adaptively weights the contribution of each modality based on its content characteristics. This approach enables MAMEX to emphasize the most informative modalities for each item while maintaining robustness when certain modalities are less relevant or missing. Extensive experiments on benchmark datasets show that MAMEX outperforms state-of-the-art methods in cold-start scenarios, with superior accuracy and adaptability. For reproducibility, the code has been made available on Github https://github.com/L2R-UET/MAMEX.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Self-Replication: Detecting Distributed Selfhood in the Outlier Cellular Automaton</title>
<link>https://arxiv.org/abs/2508.08047</link>
<guid>https://arxiv.org/abs/2508.08047</guid>
<content:encoded><![CDATA[
arXiv:2508.08047v1 Announce Type: cross 
Abstract: Spontaneous self-replication in cellular automata has long been considered rare, with most known examples requiring careful design or artificial initialization. In this paper, we present formal, causal evidence that such replication can emerge unassisted -- and that it can do so in a distributed, multi-component form. Building on prior work identifying complex dynamics in the Outlier rule, we introduce a data-driven framework that reconstructs the full causal ancestry of patterns in a deterministic cellular automaton. This allows us to rigorously identify self-replicating structures via explicit causal lineages. Our results show definitively that self-replicators in the Outlier CA are not only spontaneous and robust, but are also often composed of multiple disjoint clusters working in coordination, raising questions about some conventional notions of individuality and replication in artificial life systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
<link>https://arxiv.org/abs/2508.08052</link>
<guid>https://arxiv.org/abs/2508.08052</guid>
<content:encoded><![CDATA[
arXiv:2508.08052v1 Announce Type: cross 
Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN) capacity-its ability to represent tasks-is a fundamental challenge in continual learning (CL). Within this context, we introduce CL's effective model capacity (CLEMC) that characterizes the dynamic behavior of the stability-plasticity balance point. We develop a difference equation to model the evolution of the interplay between the NN, task data, and optimization procedure. We then leverage CLEMC to demonstrate that the effective capacity-and, by extension, the stability-plasticity balance point is inherently non-stationary. We show that regardless of the NN architecture or optimization method, a NN's ability to represent new tasks diminishes when incoming task distributions differ from previous ones. We conduct extensive experiments to support our theoretical findings, spanning a range of architectures-from small feedforward network and convolutional networks to medium-sized graph neural networks and transformer-based large language models with millions of parameters.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.08066</link>
<guid>https://arxiv.org/abs/2508.08066</guid>
<content:encoded><![CDATA[
arXiv:2508.08066v1 Announce Type: cross 
Abstract: Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction</title>
<link>https://arxiv.org/abs/2508.08071</link>
<guid>https://arxiv.org/abs/2508.08071</guid>
<content:encoded><![CDATA[
arXiv:2508.08071v1 Announce Type: cross 
Abstract: Connecting an ever-expanding catalogue of products with suitable manufacturers and suppliers is critical for resilient, efficient global supply chains, yet traditional methods struggle to capture complex capabilities, certifications, geographic constraints, and rich multimodal data of real-world manufacturer profiles. To address these gaps, we introduce PMGraph, a public benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking 8,888 manufacturers, over 70k products, more than 110k manufacturer-product edges, and over 29k product images. Building on this benchmark, we propose the Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first aligns and aggregates textual and visual attributes into intermediate group embeddings, then propagates them through a manufacturer-product hetero-graph via multiscale message passing to enhance link prediction accuracy. C-MAG also provides practical guidelines for modality-aware fusion, preserving predictive performance in noisy, real-world settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</title>
<link>https://arxiv.org/abs/2508.08088</link>
<guid>https://arxiv.org/abs/2508.08088</guid>
<content:encoded><![CDATA[
arXiv:2508.08088v1 Announce Type: cross 
Abstract: Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Growing Reservoirs with Developmental Graph Cellular Automata</title>
<link>https://arxiv.org/abs/2508.08091</link>
<guid>https://arxiv.org/abs/2508.08091</guid>
<content:encoded><![CDATA[
arXiv:2508.08091v1 Announce Type: cross 
Abstract: Developmental Graph Cellular Automata (DGCA) are a novel model for morphogenesis, capable of growing directed graphs from single-node seeds. In this paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs are grown with two types of targets: task-driven (using the NARMA family of tasks) and task-independent (using reservoir metrics).
  Results show that DGCAs are able to grow into a variety of specialized, life-like structures capable of effectively solving benchmark tasks, statistically outperforming `typical' reservoirs on the same task. Overall, these lay the foundation for the development of DGCA systems that produce plastic reservoirs and for modeling functional, adaptive morphogenesis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Information Speech Language Models for Emotional Conversations</title>
<link>https://arxiv.org/abs/2508.08095</link>
<guid>https://arxiv.org/abs/2508.08095</guid>
<content:encoded><![CDATA[
arXiv:2508.08095v1 Announce Type: cross 
Abstract: Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid2Guide: A* Enabled Small Language Model for Indoor Navigation</title>
<link>https://arxiv.org/abs/2508.08100</link>
<guid>https://arxiv.org/abs/2508.08100</guid>
<content:encoded><![CDATA[
arXiv:2508.08100v1 Announce Type: cross 
Abstract: Reliable indoor navigation remains a significant challenge in complex environments, particularly where external positioning signals and dedicated infrastructures are unavailable. This research presents Grid2Guide, a hybrid navigation framework that combines the A* search algorithm with a Small Language Model (SLM) to generate clear, human-readable route instructions. The framework first conducts a binary occupancy matrix from a given indoor map. Using this matrix, the A* algorithm computes the optimal path between origin and destination, producing concise textual navigation steps. These steps are then transformed into natural language instructions by the SLM, enhancing interpretability for end users. Experimental evaluations across various indoor scenarios demonstrate the method's effectiveness in producing accurate and timely navigation guidance. The results validate the proposed approach as a lightweight, infrastructure-free solution for real-time indoor navigation support.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience</title>
<link>https://arxiv.org/abs/2508.08101</link>
<guid>https://arxiv.org/abs/2508.08101</guid>
<content:encoded><![CDATA[
arXiv:2508.08101v1 Announce Type: cross 
Abstract: Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal acceleration, lateral acceleration, and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of LLM-powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2508.08107</link>
<guid>https://arxiv.org/abs/2508.08107</guid>
<content:encoded><![CDATA[
arXiv:2508.08107v1 Announce Type: cross 
Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that simultaneously captures spatial and spectral information, enabling non-invasive, label-free analysis of material, chemical, and biological properties. This Primer presents a comprehensive overview of HSI, from the underlying physical principles and sensor architectures to key steps in data acquisition, calibration, and correction. We summarize common data structures and highlight classical and modern analysis methods, including dimensionality reduction, classification, spectral unmixing, and AI-driven techniques such as deep learning. Representative applications across Earth observation, precision agriculture, biomedicine, industrial inspection, cultural heritage, and security are also discussed, emphasizing HSI's ability to uncover sub-visual features for advanced monitoring, diagnostics, and decision-making. Persistent challenges, such as hardware trade-offs, acquisition variability, and the complexity of high-dimensional data, are examined alongside emerging solutions, including computational imaging, physics-informed modeling, cross-modal fusion, and self-supervised learning. Best practices for dataset sharing, reproducibility, and metadata documentation are further highlighted to support transparency and reuse. Looking ahead, we explore future directions toward scalable, real-time, and embedded HSI systems, driven by sensor miniaturization, self-supervised learning, and foundation models. As HSI evolves into a general-purpose, cross-disciplinary platform, it holds promise for transformative applications in science, technology, and society.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2508.08117</link>
<guid>https://arxiv.org/abs/2508.08117</guid>
<content:encoded><![CDATA[
arXiv:2508.08117v1 Announce Type: cross 
Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Based Localization and LLM-based Navigation for Indoor Environments</title>
<link>https://arxiv.org/abs/2508.08120</link>
<guid>https://arxiv.org/abs/2508.08120</guid>
<content:encoded><![CDATA[
arXiv:2508.08120v1 Announce Type: cross 
Abstract: Indoor navigation remains a complex challenge due to the absence of reliable GPS signals and the architectural intricacies of large enclosed environments. This study presents an indoor localization and navigation approach that integrates vision-based localization with large language model (LLM)-based navigation. The localization system utilizes a ResNet-50 convolutional neural network fine-tuned through a two-stage process to identify the user's position using smartphone camera input. To complement localization, the navigation module employs an LLM, guided by a carefully crafted system prompt, to interpret preprocessed floor plan images and generate step-by-step directions. Experimental evaluation was conducted in a realistic office corridor with repetitive features and limited visibility to test localization robustness. The model achieved high confidence and an accuracy of 96% across all tested waypoints, even under constrained viewing conditions and short-duration queries. Navigation tests using ChatGPT on real building floor maps yielded an average instruction accuracy of 75%, with observed limitations in zero-shot reasoning and inference time. This research demonstrates the potential for scalable, infrastructure-free indoor navigation using off-the-shelf cameras and publicly available floor plans, particularly in resource-constrained settings like hospitals, airports, and educational institutions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing</title>
<link>https://arxiv.org/abs/2508.08122</link>
<guid>https://arxiv.org/abs/2508.08122</guid>
<content:encoded><![CDATA[
arXiv:2508.08122v1 Announce Type: cross 
Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery from their historical interactions. Simulating students' memory states is a promising approach to enhance both the performance and interpretability of knowledge tracing models. Memory consists of three fundamental processes: encoding, storage, and retrieval. Although forgetting primarily manifests during the storage stage, most existing studies rely on a single, undifferentiated forgetting mechanism, overlooking other memory processes as well as personalized forgetting patterns. To address this, this paper proposes memoryKT, a knowledge tracing model based on a novel temporal variational autoencoder. The model simulates memory dynamics through a three-stage process: (i) Learning the distribution of students' knowledge memory features, (ii) Reconstructing their exercise feedback, while (iii) Embedding a personalized forgetting module within the temporal workflow to dynamically modulate memory storage strength. This jointly models the complete encoding-storage-retrieval cycle, significantly enhancing the model's perception capability for individual differences. Extensive experiments on four public datasets demonstrate that our proposed approach significantly outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models</title>
<link>https://arxiv.org/abs/2508.08131</link>
<guid>https://arxiv.org/abs/2508.08131</guid>
<content:encoded><![CDATA[
arXiv:2508.08131v1 Announce Type: cross 
Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to perceive speech inputs, have gained increasing attention for their potential to advance speech understanding tasks. However, despite recent progress, studies show that SLMs often struggle to generalize across datasets, even for trained languages and tasks, raising concerns about whether they process speech in a text-like manner as intended. A key challenge underlying this limitation is the modality gap between speech and text representations. The high variability in speech embeddings may allow SLMs to achieve strong in-domain performance by exploiting unintended speech variations, ultimately hindering generalization. To mitigate this modality gap, we introduce Optimal Transport Regularization (OTReg), a method that formulates speech-text alignment as an optimal transport problem and derives a regularization loss to improve SLM training. In each training iteration, OTReg first establishes a structured correspondence between speech and transcript embeddings by determining the optimal transport plan, then incorporates the regularization loss based on this transport plan to optimize SLMs in generating speech embeddings that align more effectively with transcript embeddings. OTReg is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Extensive multilingual ASR experiments demonstrate that OTReg enhances speech-text alignment, mitigates the modality gap, and consequently improves SLM generalization across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.08137</link>
<guid>https://arxiv.org/abs/2508.08137</guid>
<content:encoded><![CDATA[
arXiv:2508.08137v1 Announce Type: cross 
Abstract: Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title>
<link>https://arxiv.org/abs/2508.08139</link>
<guid>https://arxiv.org/abs/2508.08139</guid>
<content:encoded><![CDATA[
arXiv:2508.08139v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models</title>
<link>https://arxiv.org/abs/2508.08144</link>
<guid>https://arxiv.org/abs/2508.08144</guid>
<content:encoded><![CDATA[
arXiv:2508.08144v1 Announce Type: cross 
Abstract: The rapid growth of resource-constrained mobile platforms, including mobile robots, wearable systems, and Internet-of-Things devices, has increased the demand for computationally efficient neural network controllers (NNCs) that can operate within strict hardware limitations. While deep neural networks (DNNs) demonstrate superior performance in control applications, their substantial computational complexity and memory requirements present significant barriers to practical deployment on edge devices. This paper introduces a comprehensive model compression methodology that leverages component-aware structured pruning to determine the optimal pruning magnitude for each pruning group, ensuring a balance between compression and stability for NNC deployment. Our approach is rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC), a state-of-the-art model-based reinforcement learning algorithm, with a systematic integration of mathematical stability guarantee properties, specifically Lyapunov criteria. The key contribution of this work lies in providing a principled framework for determining the theoretical limits of model compression while preserving controller stability. Experimental validation demonstrates that our methodology successfully reduces model complexity while maintaining requisite control performance and stability characteristics. Furthermore, our approach establishes a quantitative boundary for safe compression ratios, enabling practitioners to systematically determine the maximum permissible model reduction before violating critical stability properties, thereby facilitating the confident deployment of compressed NNCs in resource-limited environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Explanations Make You Change Your Mind?</title>
<link>https://arxiv.org/abs/2508.08158</link>
<guid>https://arxiv.org/abs/2508.08158</guid>
<content:encoded><![CDATA[
arXiv:2508.08158v1 Announce Type: cross 
Abstract: In the context of AI-based decision support systems, explanations can help users to judge when to trust the AI's suggestion, and when to question it. In this way, human oversight can prevent AI errors and biased decision-making. However, this rests on the assumption that users will consider explanations in enough detail to be able to catch such errors. We conducted an online study on trust in explainable DSS, and were surprised to find that in many cases, participants spent little time on the explanation and did not always consider it in detail. We present an exploratory analysis of this data, investigating what factors impact how carefully study participants consider AI explanations, and how this in turn impacts whether they are open to changing their mind based on what the AI suggests.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo</title>
<link>https://arxiv.org/abs/2508.08163</link>
<guid>https://arxiv.org/abs/2508.08163</guid>
<content:encoded><![CDATA[
arXiv:2508.08163v1 Announce Type: cross 
Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend the DisCo by incorporating annotator metadata, enhancing input representations, and modifying the loss functions to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth error and calibration analyses, highlighting the conditions under which improvements occur. Our findings underscore the value of disagreement-aware modeling and offer insights into how system components interact with the complexity of human-annotated data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C</title>
<link>https://arxiv.org/abs/2508.08171</link>
<guid>https://arxiv.org/abs/2508.08171</guid>
<content:encoded><![CDATA[
arXiv:2508.08171v1 Announce Type: cross 
Abstract: Python has become the dominant language for general-purpose programming, yet it lacks robust tools for formal verification. In contrast, programmers working in languages such as C benefit from mature model checkers, for example CBMC, which enable exhaustive symbolic reasoning and fault localisation. The inherent complexity of Python, coupled with the verbosity and low-level nature of existing transpilers (e.g., Cython), have historically limited the applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large Language Models (LLMs) for high-level transpilation from Python to C, followed by bounded model checking and MaxSAT-based fault localisation in the generated C code. PyVeritas enables verification and bug localisation for Python code using existing model checking tools for C. Our empirical evaluation on two Python benchmarks demonstrates that LLM-based transpilation can achieve a high degree of accuracy, up to 80--90% for some LLMs, enabling effective development environment that supports assertion-based verification and interpretable fault diagnosis for small yet non-trivial Python programs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Logic Networks for Interpretable Classification</title>
<link>https://arxiv.org/abs/2508.08172</link>
<guid>https://arxiv.org/abs/2508.08172</guid>
<content:encoded><![CDATA[
arXiv:2508.08172v1 Announce Type: cross 
Abstract: Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on an example from the medical field where interpretability has tangible value.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</title>
<link>https://arxiv.org/abs/2508.08177</link>
<guid>https://arxiv.org/abs/2508.08177</guid>
<content:encoded><![CDATA[
arXiv:2508.08177v1 Announce Type: cross 
Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedDino: A foundation model for red blood cell analysis</title>
<link>https://arxiv.org/abs/2508.08180</link>
<guid>https://arxiv.org/abs/2508.08180</guid>
<content:encoded><![CDATA[
arXiv:2508.08180v1 Announce Type: cross 
Abstract: Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at https://github.com/Snarci/RedDino, and the pretrained models can be downloaded from our Hugging Face collection at https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street-Level AI: Are Large Language Models Ready for Real-World Judgments?</title>
<link>https://arxiv.org/abs/2508.08193</link>
<guid>https://arxiv.org/abs/2508.08193</guid>
<content:encoded><![CDATA[
arXiv:2508.08193v1 Announce Type: cross 
Abstract: A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2508.08204</link>
<guid>https://arxiv.org/abs/2508.08204</guid>
<content:encoded><![CDATA[
arXiv:2508.08204v1 Announce Type: cross 
Abstract: There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2508.08211</link>
<guid>https://arxiv.org/abs/2508.08211</guid>
<content:encoded><![CDATA[
arXiv:2508.08211v1 Announce Type: cross 
Abstract: Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</title>
<link>https://arxiv.org/abs/2508.08222</link>
<guid>https://arxiv.org/abs/2508.08222</guid>
<content:encoded><![CDATA[
arXiv:2508.08222v1 Announce Type: cross 
Abstract: Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capabilities of GPT-5 on Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2508.08224</link>
<guid>https://arxiv.org/abs/2508.08224</guid>
<content:encoded><![CDATA[
arXiv:2508.08224v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5's ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2508.08227</link>
<guid>https://arxiv.org/abs/2508.08227</guid>
<content:encoded><![CDATA[
arXiv:2508.08227v1 Announce Type: cross 
Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM) generative models show promising potential for one-step Real-World Image Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a Low-Quality (LQ) image latent distribution at the initial timestep. However, a fundamental gap exists between the LQ image latent distribution and the Gaussian noisy latent distribution, limiting the effective utilization of generative priors. We observe that the noisy latent distribution at DDPM/FM mid-timesteps aligns more closely with the LQ image latent distribution. Based on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a universal framework applicable to DDPM/FM-based generative models. OMGSR injects the LQ image latent distribution at a pre-computed mid-timestep, incorporating the proposed Latent Distribution Refinement loss to alleviate the latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to eliminate checkerboard artifacts in image generation. Within this framework, we instantiate OMGSR for DDPM/FM-based generative models with two variants: OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate that OMGSR-S/F achieves balanced/excellent performance across quantitative and qualitative metrics at 512-resolution. Notably, OMGSR-F establishes overwhelming dominance in all reference metrics. We further train a 1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which yields excellent results, especially in the details of the image generation. We also generate 2k-resolution images by the 1k-resolution OMGSR-F using our two-stage Tiled VAE & Diffusion.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL3M: Large Language 3D Modelers</title>
<link>https://arxiv.org/abs/2508.08228</link>
<guid>https://arxiv.org/abs/2508.08228</guid>
<content:encoded><![CDATA[
arXiv:2508.08228v1 Announce Type: cross 
Abstract: We present LL3M, a multi-agent system that leverages pretrained large language models (LLMs) to generate 3D assets by writing interpretable Python code in Blender. We break away from the typical generative approach that learns from a collection of 3D data. Instead, we reformulate shape generation as a code-writing task, enabling greater modularity, editability, and integration with artist workflows. Given a text prompt, LL3M coordinates a team of specialized LLM agents to plan, retrieve, write, debug, and refine Blender scripts that generate and edit geometry and appearance. The generated code works as a high-level, interpretable, human-readable, well-documented representation of scenes and objects, making full use of sophisticated Blender constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse, unconstrained shapes, materials, and scenes. This code presents many avenues for further agent and human editing and experimentation via code tweaks or procedural parameters. This medium naturally enables a co-creative loop in our system: agents can automatically self-critique using code and visuals, while iterative user instructions provide an intuitive way to refine assets. A shared code context across agents enables awareness of previous attempts, and a retrieval-augmented generation knowledge base built from Blender API documentation, BlenderRAG, equips agents with examples, types, and functions empowering advanced modeling operations and code correctness. We demonstrate the effectiveness of LL3M across diverse shape categories, style and material edits, and user-driven refinements. Our experiments showcase the power of code as a generative and interpretable medium for 3D asset creation. Our project page is at https://threedle.github.io/ll3m.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
<link>https://arxiv.org/abs/2508.08237</link>
<guid>https://arxiv.org/abs/2508.08237</guid>
<content:encoded><![CDATA[
arXiv:2508.08237v1 Announce Type: cross 
Abstract: The emergence of audio-visual foundation models underscores the importance of reliably assessing their multi-modal understanding. The VGGSounder dataset is commonly used as a benchmark for evaluation audio-visual classification. However, our analysis identifies several limitations of VGGSounder, including incomplete labelling, partially overlapping classes, and misaligned modalities. These lead to distorted evaluations of auditory and visual capabilities. To address these limitations, we introduce VGGSounder, a comprehensively re-annotated, multi-label test set that extends VGGSound and is specifically designed to evaluate audio-visual foundation models. VGGSounder features detailed modality annotations, enabling precise analyses of modality-specific performance. Furthermore, we reveal model limitations by analysing performance degradation when adding another input modality with our new modality confusion metric.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cut2Next: Generating Next Shot via In-Context Tuning</title>
<link>https://arxiv.org/abs/2508.08244</link>
<guid>https://arxiv.org/abs/2508.08244</guid>
<content:encoded><![CDATA[
arXiv:2508.08244v1 Announce Type: cross 
Abstract: Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sortability of Time Series Data</title>
<link>https://arxiv.org/abs/2407.13313</link>
<guid>https://arxiv.org/abs/2407.13313</guid>
<content:encoded><![CDATA[
arXiv:2407.13313v3 Announce Type: replace 
Abstract: Evaluating the performance of causal discovery algorithms that aim to find causal relationships between time-dependent processes remains a challenging topic. In this paper, we show that certain characteristics of datasets, such as varsortability (Reisach et al. 2021) and $R^2$-sortability (Reisach et al. 2023), also occur in datasets for autocorrelated stationary time series. We illustrate this empirically using four types of data: simulated data based on SVAR models and Erd\H{o}s-R\'enyi graphs, the data used in the 2019 causality-for-climate challenge (Runge et al. 2019), real-world river stream datasets, and real-world data generated by the Causal Chamber of (Gamella et al. 2024). To do this, we adapt var- and $R^2$-sortability to time series data. We also investigate the extent to which the performance of score-based causal discovery methods goes hand in hand with high sortability. Arguably, our most surprising finding is that the investigated real-world datasets exhibit high varsortability and low $R^2$-sortability indicating that scales may carry a significant amount of causal information.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning How to Vote with Principles: Axiomatic Insights Into the Collective Decisions of Neural Networks</title>
<link>https://arxiv.org/abs/2410.16170</link>
<guid>https://arxiv.org/abs/2410.16170</guid>
<content:encoded><![CDATA[
arXiv:2410.16170v2 Announce Type: replace 
Abstract: Can neural networks be applied in voting theory, while satisfying the need for transparency in collective decisions? We propose axiomatic deep voting: a framework to build and evaluate neural networks that aggregate preferences, using the well-established axiomatic method of voting theory. Our findings are: (1) Neural networks, despite being highly accurate, often fail to align with the core axioms of voting rules, revealing a disconnect between mimicking outcomes and reasoning. (2) Training with axiom-specific data does not enhance alignment with those axioms. (3) By solely optimizing axiom satisfaction, neural networks can synthesize new voting rules that often surpass and substantially differ from existing ones. This offers insights for both fields: For AI, important concepts like bias and value-alignment are studied in a mathematically rigorous way; for voting theory, new areas of the space of voting rules are explored.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Powered Defense: Controller Area Network Intrusion Detection for Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2412.02539</link>
<guid>https://arxiv.org/abs/2412.02539</guid>
<content:encoded><![CDATA[
arXiv:2412.02539v2 Announce Type: replace 
Abstract: The network of services, including delivery, farming, and environmental monitoring, has experienced exponential expansion in the past decade with Unmanned Aerial Vehicles (UAVs). Yet, UAVs are not robust enough against cyberattacks, especially on the Controller Area Network (CAN) bus. The CAN bus is a general-purpose vehicle-bus standard to enable microcontrollers and in-vehicle computers to interact, primarily connecting different Electronic Control Units (ECUs). In this study, we focus on solving some of the most critical security weaknesses in UAVs by developing a novel graph-based intrusion detection system (IDS) leveraging the Uncomplicated Application-level Vehicular Communication and Networking (UAVCAN) protocol. First, we decode CAN messages based on UAVCAN protocol specification; second, we present a comprehensive method of transforming tabular UAVCAN messages into graph structures. Lastly, we apply various graph-based machine learning models for detecting cyber-attacks on the CAN bus, including graph convolutional neural networks (GCNNs), graph attention networks (GATs), Graph Sample and Aggregate Networks (GraphSAGE), and graph structure-based transformers. Our findings show that inductive models such as GATs, GraphSAGE, and graph-based transformers can achieve competitive and even better accuracy than transductive models like GCNNs in detecting various types of intrusions, with minimum information on protocol specification, thus providing a generic robust solution for CAN bus security for the UAVs. We also compared our results with baseline single-layer Long Short-Term Memory (LSTM) and found that all our graph-based models perform better without using any decoded features based on the UAVCAN protocol, highlighting higher detection performance with protocol-independent capability.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Research Agenda for Usability and Generalisation in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.16970</link>
<guid>https://arxiv.org/abs/2412.16970</guid>
<content:encoded><![CDATA[
arXiv:2412.16970v2 Announce Type: replace 
Abstract: It is common practice in reinforcement learning (RL) research to train and deploy agents in bespoke simulators, typically implemented by engineers directly in general-purpose programming languages or hardware acceleration frameworks such as CUDA or JAX. This means that programming and engineering expertise is not only required to develop RL algorithms, but is also required to use already developed algorithms for novel problems. The latter poses a problem in terms of the usability of RL, in particular for private individuals and small organisations without substantial engineering expertise. We also perceive this as a challenge for effective generalisation in RL, in the sense that is no standard, shared formalism in which different problems are represented. As we typically have no consistent representation through which to provide information about any novel problem to an agent, our agents also cannot instantly or rapidly generalise to novel problems. In this position paper, we advocate for a research agenda centred around the use of user-friendly description languages for describing problems, such that (i) users with little to no engineering expertise can formally describe the problems they would like to be tackled by RL algorithms, and (ii) algorithms can leverage problem descriptions to effectively generalise among all problems describable in the language of choice.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-Judge: How Far Are We? Assessing the Discrepancies Between AI-synthesized and Natural Images through Multimodal Guidance</title>
<link>https://arxiv.org/abs/2412.17632</link>
<guid>https://arxiv.org/abs/2412.17632</guid>
<content:encoded><![CDATA[
arXiv:2412.17632v4 Announce Type: replace 
Abstract: In the rapidly evolving field of Artificial Intelligence Generated Content (AIGC), a central challenge is distinguishing AI-synthesized images from natural ones. Despite the impressive capabilities of advanced generative models in producing visually compelling images, significant discrepancies remain when compared to natural images. To systematically investigate and quantify these differences, we construct a large-scale multimodal dataset, D-ANI, comprising 5,000 natural images and over 440,000 AIGI samples generated by nine representative models using both unimodal and multimodal prompts, including Text-to-Image (T2I), Image-to-Image (I2I), and Text-and-Image-to-Image (TI2I). We then introduce an AI-Natural Image Discrepancy assessment benchmark (D-Judge) to address the critical question: how far are AI-generated images (AIGIs) from truly realistic images? Our fine-grained evaluation framework assesses the D-ANI dataset across five dimensions: naive visual quality, semantic alignment, aesthetic appeal, downstream task applicability, and coordinated human validation. Extensive experiments reveal substantial discrepancies across these dimensions, highlighting the importance of aligning quantitative metrics with human judgment to achieve a comprehensive understanding of AI-generated image quality. Code: https://github.com/ryliu68/DJudge ; Data: https://huggingface.co/datasets/Renyang/DANI.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Observation Interference in Partially Observable Assistance Games</title>
<link>https://arxiv.org/abs/2412.17797</link>
<guid>https://arxiv.org/abs/2412.17797</guid>
<content:encoded><![CDATA[
arXiv:2412.17797v2 Announce Type: replace 
Abstract: We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Instruction Tuning with Pre-training</title>
<link>https://arxiv.org/abs/2501.09368</link>
<guid>https://arxiv.org/abs/2501.09368</guid>
<content:encoded><![CDATA[
arXiv:2501.09368v4 Announce Type: replace 
Abstract: Instruction tuning enhances large language models (LLMs) to follow human instructions across diverse tasks, relying on high-quality datasets to guide behavior. However, these datasets, whether manually curated or synthetically generated, are often narrowly focused and misaligned with the broad distributions captured during pre-training, limiting LLM generalization and effective use of pre-trained knowledge. We propose Aligning Instruction Tuning with Pre-training (AITP), a method that bridges this gap by identifying coverage shortfalls in instruction-tuning datasets and rewriting underrepresented pre-training data into high-quality instruction-response pairs. This approach enriches dataset diversity while preserving task-specific objectives. Evaluations on three fully open LLMs across eight benchmarks demonstrate consistent performance improvements with AITP. Ablations highlight the benefits of adaptive data selection, controlled rewriting, and balanced integration, emphasizing the importance of aligning instruction tuning with pre-training distributions to unlock the full potential of LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Differentiated Reward Method for Reinforcement Learning based Multi-Vehicle Cooperative Decision-Making Algorithms</title>
<link>https://arxiv.org/abs/2502.00352</link>
<guid>https://arxiv.org/abs/2502.00352</guid>
<content:encoded><![CDATA[
arXiv:2502.00352v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) shows great potential for optimizing multi-vehicle cooperative driving strategies through the state-action-reward feedback loop, but it still faces challenges such as low sample efficiency. This paper proposes a differentiated reward method based on steady-state transition systems, which incorporates state transition gradient information into the reward design by analyzing traffic flow characteristics, aiming to optimize action selection and policy learning in multi-vehicle cooperative decision-making. The performance of the proposed method is validated in RL algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle penetration. The results show that the differentiated reward method significantly accelerates training convergence and outperforms centering reward and others in terms of traffic efficiency, safety, and action rationality. Additionally, the method demonstrates strong scalability and environmental adaptability, providing a novel approach for multi-agent cooperative decision-making in complex traffic scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19918</link>
<guid>https://arxiv.org/abs/2502.19918</guid>
<content:encoded><![CDATA[
arXiv:2502.19918v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with high computational time and error propagation during inference time, especially for complex tasks like math, puzzles, or coding requiring multi-step thinking. While existing reasoning models with chain-of-thoughts (CoT) can enable LLMs to do step-wise analysis and reflection, they often face the issue of wasting computation on less productive solutions and fail to make progress during inference time. In this paper, we propose Meta-Reasoner, a new framework to enable LLMs ``Think about how to think'', i.e., optimize the inference compute by adjusting strategies on how to reason during inference time. Inspired by dual-process theory, our method decouples the high-level strategy generation (e.g., backtracking, switching approaches, or restarting) from stepwise CoT generation via a lightweight progress report. The strategy module only consider the summarized version from the previous CoTs to propose new strategies accordingly. We employ the contextual multi-armed bandits (CMABs) for this module to iteratively evaluate the previous reasoning states and dynamically adjust the strategy to avoid reasoning get stuck in less productive paths during inference. Evaluations on math problems (e.g., Game-of-24, TheoremQA) and scientific problems (e.g., SciBench) demonstrate that our method improves performance by 9-12\% over previous SOTA methods while reducing inference time by 28-35\%. This approach also generalizes to other domains like creative writing, demonstrating its versatility for diverse reasoning-intensive problems using LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond</title>
<link>https://arxiv.org/abs/2502.20988</link>
<guid>https://arxiv.org/abs/2502.20988</guid>
<content:encoded><![CDATA[
arXiv:2502.20988v2 Announce Type: replace 
Abstract: The large-scale development of large language models (LLMs) in medical contexts, such as diagnostic assistance and treatment recommendations, necessitates that these models possess accurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge, encompassing the insights gained from research on the causes, prognosis, diagnosis, and treatment of diseases, has been extensively examined within real-world medical practices. Recently, there has been a notable increase in research efforts aimed at integrating this type of knowledge into LLMs, encompassing not only traditional text and multimodal data integration but also technologies such as knowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper, we review the various initiatives to embed clinical knowledge into training-based, KG-supported, and RAG-assisted LLMs. We begin by gathering reliable knowledge sources from the medical domain, including databases and datasets. Next, we evaluate implementations for integrating clinical knowledge through specialized datasets and collaborations with external knowledge sources such as KGs and relevant documentation. Furthermore, we discuss the applications of the developed medical LLMs in the industrial sector to assess the disparity between models developed in academic settings and those in industry. We conclude the survey by presenting evaluation systems applicable to relevant tasks and identifying potential challenges facing this field. In this review, we do not aim for completeness, since any ostensibly complete review would soon be outdated. Our goal is to illustrate diversity by selecting representative and accessible items from current research and industry practices, reflecting real-world situations rather than claiming completeness. Thus, we emphasize showcasing diverse approaches.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires</title>
<link>https://arxiv.org/abs/2503.00566</link>
<guid>https://arxiv.org/abs/2503.00566</guid>
<content:encoded><![CDATA[
arXiv:2503.00566v3 Announce Type: replace 
Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Planning Compilation to Reason about Goal Achievement at Planning Time</title>
<link>https://arxiv.org/abs/2503.09545</link>
<guid>https://arxiv.org/abs/2503.09545</guid>
<content:encoded><![CDATA[
arXiv:2503.09545v2 Announce Type: replace 
Abstract: Identifying the specific actions that achieve goals when solving a planning task might be beneficial for various planning applications. Traditionally, this identification occurs post-search, as some actions may temporarily achieve goals that are later undone and re-achieved by other actions. In this paper, we propose a compilation that extends the original planning task with commit actions that enforce the persistence of specific goals once achieved, allowing planners to identify permanent goal achievement during planning. Experimental results indicate that solving the reformulated tasks does not incur on any additional overhead both when performing optimal and suboptimal planning, while providing useful information for some downstream tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El Agente: An Autonomous Agent for Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.02484</link>
<guid>https://arxiv.org/abs/2505.02484</guid>
<content:encoded><![CDATA[
arXiv:2505.02484v2 Announce Type: replace 
Abstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Capabilities of Large Language Models on Dynamic Tasks</title>
<link>https://arxiv.org/abs/2505.10543</link>
<guid>https://arxiv.org/abs/2505.10543</guid>
<content:encoded><![CDATA[
arXiv:2505.10543v2 Announce Type: replace 
Abstract: Large language models excel on static benchmarks, but their ability as self-learning agents in dynamic environments remains unclear. We evaluate three prompting strategies: self-reflection, heuristic mutation, and planning across dynamic tasks with open-source models. We find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, an overly long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in areas like planning and spatial coordination, suggesting that large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while methods like Chain-of-thought improve multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identification of Probabilities of Causation: A Complete Characterization</title>
<link>https://arxiv.org/abs/2505.15274</link>
<guid>https://arxiv.org/abs/2505.15274</guid>
<content:encoded><![CDATA[
arXiv:2505.15274v2 Announce Type: replace 
Abstract: Probabilities of causation are fundamental to modern decision-making. Pearl first introduced three binary probabilities of causation, and Tian and Pearl later derived tight bounds for them using Balke's linear programming. The theoretical characterization of probabilities of causation with multi-valued treatments and outcomes has remained unresolved for decades, limiting the scope of causality-based decision-making. In this paper, we resolve this foundational gap by proposing a complete set of representative probabilities of causation and proving that they are sufficient to characterize all possible probabilities of causation within the framework of Structural Causal Models (SCMs). We then formally derive tight bounds for these representative quantities using formal mathematical proofs. Finally, we demonstrate the practical relevance of our results through illustrative toy examples.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2505.16646</link>
<guid>https://arxiv.org/abs/2505.16646</guid>
<content:encoded><![CDATA[
arXiv:2505.16646v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable results on a variety of mathematical benchmarks. However, concerns remain as to whether these successes reflect genuine reasoning or superficial pattern recognition. Common evaluation methods, which focus on the either the final answer or the reasoning process, fail to assess the entire problem-solving procedure. To address these limitations, we introduce SMART: a Self-Generating and Self-Validating Multi-Dimensional Assessment Framework, together with its corresponding benchmark, SMART-Bench. SMART decomposes the entire problem solving process into four distinct cognitive dimensions: Understanding, Reasoning, Arithmetic, and Reflection \& Refinement. Each dimension is evaluated independently through tailored tasks, enabling interpretable and fine-grained analysis of LLM behavior. We apply SMART to 21 state-of-the-art open- and closed-source LLMs, uncovering significant discrepancies in their abilities across different dimensions. Our findings reveal genuine weaknesses in current LLMs and motivate a new metric, the All-Pass Score, to better capture true problem-solving capabilities. Code and benchmarks will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values</title>
<link>https://arxiv.org/abs/2506.13774</link>
<guid>https://arxiv.org/abs/2506.13774</guid>
<content:encoded><![CDATA[
arXiv:2506.13774v2 Announce Type: replace 
Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, show great potential across diverse domains. However, their practical deployment is hindered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the complex task of providing personalized context without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected 'Creed Constitutions' encapsulating diverse rule sets -- with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs -- achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Hybrid Charging Stations Planning and Operation Considering Fixed and Mobile Chargers</title>
<link>https://arxiv.org/abs/2506.16764</link>
<guid>https://arxiv.org/abs/2506.16764</guid>
<content:encoded><![CDATA[
arXiv:2506.16764v2 Announce Type: replace 
Abstract: The success of vehicle electrification relies on efficient and adaptable charging infrastructure. Fixed-location charging stations often suffer from underutilization or congestion due to fluctuating demand, while mobile chargers offer flexibility by relocating as needed. This paper studies the optimal planning and operation of hybrid charging infrastructures that combine both fixed and mobile chargers within urban road networks. We formulate the Hybrid Charging Station Planning and Operation (HCSPO) problem, jointly optimizing the placement of fixed stations and the scheduling of mobile chargers. A charging demand prediction model based on Model Predictive Control (MPC) supports dynamic decision-making. To solve the HCSPO problem, we propose a deep reinforcement learning approach enhanced with heuristic scheduling. Experiments on real-world urban scenarios show that our method improves infrastructure availability - achieving up to 244.4% increase in coverage - and reduces user inconvenience with up to 79.8% shorter waiting times, compared to existing solutions.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
arXiv:2507.22025v3 Announce Type: replace 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE for enhancing GUI agents at both training and inference. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a continuous reward function to incentivize high-precision grounding; 2) a ``Simple Thinking'' reward to balance planning with speed and grounding accuracy; and 3) a cropping-based resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present decomposed grounding with selection to dramatically improve grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art grounding performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent capabilities. For instance, using both our training and inference enhancement methods brings 23\% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextQuests: How Good are LLMs at Text-Based Video Games?</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
arXiv:2507.23701v2 Announce Type: replace 
Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.01158</link>
<guid>https://arxiv.org/abs/2508.01158</guid>
<content:encoded><![CDATA[
arXiv:2508.01158v2 Announce Type: replace 
Abstract: Deep learning (DL) has shown state-of-the-art performance in trajectory prediction, which is critical to safe navigation in autonomous driving (AD). However, most DL-based methods suffer from catastrophic forgetting, where adapting to a new distribution may cause significant performance degradation in previously learned ones. Such inability to retain learned knowledge limits their applicability in the real world, where AD systems need to operate across varying scenarios with dynamic distributions. As revealed by neuroscience, the hippocampal circuit plays a crucial role in memory replay, effectively reconstructing learned knowledge based on limited resources. Inspired by this, we propose a hippocampal circuit-inspired continual learning method (H2C) for trajectory prediction across varying scenarios. H2C retains prior knowledge by selectively recalling a small subset of learned samples. First, two complementary strategies are developed to select the subset to represent learned knowledge. Specifically, one strategy maximizes inter-sample diversity to represent the distinctive knowledge, and the other estimates the overall knowledge by equiprobable sampling. Then, H2C updates via a memory replay loss function calculated by these selected samples to retain knowledge while learning new data. Experiments based on various scenarios from the INTERACTION dataset are designed to evaluate H2C. Experimental results show that H2C reduces catastrophic forgetting of DL baselines by 22.71% on average in a task-free manner, without relying on manually informed distributional shifts. The implementation is available at https://github.com/BIT-Jack/H2C-lifelong.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Contextual Preferential Bayesian Optimization with Historical Examples</title>
<link>https://arxiv.org/abs/2208.10300</link>
<guid>https://arxiv.org/abs/2208.10300</guid>
<content:encoded><![CDATA[
arXiv:2208.10300v3 Announce Type: replace-cross 
Abstract: State-of-the-art multi-objective optimization often assumes a known utility function, learns it interactively, or computes the full Pareto front-each requiring costly expert input.~Real-world problems, however, involve implicit preferences that are hard to formalize. To reduce expert involvement, we propose an offline, interpretable utility learning method that uses expert knowledge, historical examples, and coarse information about the utility space to reduce sample requirements. We model uncertainty via a full Bayesian posterior and propagate it throughout the optimization process. Our method outperforms standard Gaussian processes and BOPE across four domains, showing strong performance even with biased samples, as encountered in the real-world, and limited expert input.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Policy Improvement from Multiple Black-box Oracles</title>
<link>https://arxiv.org/abs/2306.10259</link>
<guid>https://arxiv.org/abs/2306.10259</guid>
<content:encoded><![CDATA[
arXiv:2306.10259v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAPS and MAPS-SE enjoy sample efficiency advantage over the state-of-the-art policy improvement algorithms. Empirical results show that MAPS-SE significantly accelerates policy optimization via state-wise imitation learning from multiple oracles across a broad spectrum of control tasks in the DeepMind Control Suite. Our code is publicly available at: https://github.com/ripl/maps.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending Imitation and Reinforcement Learning for Robust Policy Improvement</title>
<link>https://arxiv.org/abs/2310.01737</link>
<guid>https://arxiv.org/abs/2310.01737</guid>
<content:encoded><![CDATA[
arXiv:2310.01737v3 Announce Type: replace-cross 
Abstract: While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner's performance surpasses that of the oracles in a specific state. Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations</title>
<link>https://arxiv.org/abs/2310.09612</link>
<guid>https://arxiv.org/abs/2310.09612</guid>
<content:encoded><![CDATA[
arXiv:2310.09612v2 Announce Type: replace-cross 
Abstract: Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization</title>
<link>https://arxiv.org/abs/2311.13171</link>
<guid>https://arxiv.org/abs/2311.13171</guid>
<content:encoded><![CDATA[
arXiv:2311.13171v2 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create "expert" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Variational Student-t Processes</title>
<link>https://arxiv.org/abs/2312.05568</link>
<guid>https://arxiv.org/abs/2312.05568</guid>
<content:encoded><![CDATA[
arXiv:2312.05568v2 Announce Type: replace-cross 
Abstract: The theory of Bayesian learning incorporates the use of Student-t Processes to model heavy-tailed distributions and datasets with outliers. However, despite Student-t Processes having a similar computational complexity as Gaussian Processes, there has been limited emphasis on the sparse representation of this model. This is mainly due to the increased difficulty in modeling and computation compared to previous sparse Gaussian Processes. Our motivation is to address the need for a sparse representation framework that reduces computational complexity, allowing Student-t Processes to be more flexible for real-world datasets. To achieve this, we leverage the conditional distribution of Student-t Processes to introduce sparse inducing points. Bayesian methods and variational inference are then utilized to derive a well-defined lower bound, facilitating more efficient optimization of our model through stochastic gradient descent. We propose two methods for computing the variational lower bound, one utilizing Monte Carlo sampling and the other employing Jensen's inequality to compute the KL regularization term in the loss function. We propose adopting these approaches as viable alternatives to Gaussian processes when the data might contain outliers or exhibit heavy-tailed behavior, and we provide specific recommendations for their applicability. We evaluate the two proposed approaches on various synthetic and real-world datasets from UCI and Kaggle, demonstrating their effectiveness compared to baseline methods in terms of computational complexity and accuracy, as well as their robustness to outliers.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
<link>https://arxiv.org/abs/2403.06534</link>
<guid>https://arxiv.org/abs/2403.06534</guid>
<content:encoded><![CDATA[
arXiv:2403.06534v3 Announce Type: replace-cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring</title>
<link>https://arxiv.org/abs/2403.09333</link>
<guid>https://arxiv.org/abs/2403.09333</guid>
<content:encoded><![CDATA[
arXiv:2403.09333v2 Announce Type: replace-cross 
Abstract: Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpassing the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, counting, \textit{etc}. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scale up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details and significantly improves multimodal perception ability, especially for small objects. Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts, and even coordinates. Experiments demonstrate that Griffon v2 can localize objects of interest with visual and textual referring, achieve state-of-the-art performance on REC and phrase grounding, and outperform expert models in object detection, object counting, and REG. Data and codes are released at https://github.com/jefferyZhan/Griffon.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2404.07826</link>
<guid>https://arxiv.org/abs/2404.07826</guid>
<content:encoded><![CDATA[
arXiv:2404.07826v2 Announce Type: replace-cross 
Abstract: The use of Potential-Based Reward Shaping (PBRS) has shown great promise in the ongoing research effort to tackle sample inefficiency in Reinforcement Learning (RL). However, choosing the right potential function remains an open challenge. Additionally, RL techniques are usually constrained to use a finite horizon for computational limitations, which introduces a bias when using PBRS. In this paper, we first build some theoretically-grounded intuition on why selecting the potential function as the optimal value function of the task at hand produces performance advantages. We then analyse the bias induced by finite horizons in the context of PBRS producing novel insights. Finally, leveraging abstractions as a way to approximate the optimal value function of the given task, we assess the sample efficiency and performance impact of PBRS on four environments including a goal-oriented navigation task and three Arcade Learning Environments (ALE) games. Remarkably, experimental results show that we can reach the same level of performance as CNN-based solutions with a simple fully-connected network.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Monitoring and Enforcement of Conditional Fairness in Generative AIs</title>
<link>https://arxiv.org/abs/2404.16663</link>
<guid>https://arxiv.org/abs/2404.16663</guid>
<content:encoded><![CDATA[
arXiv:2404.16663v5 Announce Type: replace-cross 
Abstract: The deployment of generative AI (GenAI) models raises significant fairness concerns, addressed in this paper through novel characterization and enforcement techniques specific to GenAI. Unlike standard AI performing specific tasks, GenAI's broad functionality requires ``conditional fairness'' tailored to the context being generated, such as demographic fairness in generating images of poor people versus successful business leaders. We define two fairness levels: the first evaluates fairness in generated outputs, independent of prompts and models; the second assesses inherent fairness with neutral prompts. Given the complexity of GenAI and challenges in fairness specifications, we focus on bounding the worst case, considering a GenAI system unfair if the distance between appearances of a specific group exceeds preset thresholds. We also explore combinatorial testing for assessing relative completeness in intersectional fairness. By bounding the worst case, we develop a prompt injection scheme within an agent-based framework to enforce conditional fairness with minimal intervention, validated on state-of-the-art GenAI systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Glass, Failing Cameras: Simulating Physics-Based Adversarial Samples for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2405.15033</link>
<guid>https://arxiv.org/abs/2405.15033</guid>
<content:encoded><![CDATA[
arXiv:2405.15033v2 Announce Type: replace-cross 
Abstract: While much research has recently focused on generating physics-based adversarial samples, a critical yet often overlooked category originates from physical failures within on-board cameras -- components essential to the perception systems of autonomous vehicles. Firstly, we motivate the study using two separate real-world experiments to showcase that indeed glass failures would cause the detection based neural network models to fail. Secondly, we develop a simulation-based study using the physical process of the glass breakage to create perturbed scenarios, representing a realistic class of physics-based adversarial samples. Using a finite element model (FEM)-based approach, we generate surface cracks on the camera image by applying a stress field defined by particles within a triangular mesh. Lastly, we use physically-based rendering (PBR) techniques to provide realistic visualizations of these physically plausible fractures. To analyze the safety implications, we superimpose these simulated broken glass effects as image filters on widely used open-source datasets: KITTI and BDD100K using two most prominent object detection neural networks (CNN-based -- YOLOv8 and Faster R-CNN) and Pyramid Vision Transformers. To further investigate the distributional impact of these visual distortions, we compute the Kullback-Leibler (K-L) divergence between three distinct data distributions, applying various broken glass filters to a custom dataset (captured through a cracked windshield), as well as the KITTI and Kaggle cats and dogs datasets. The K-L divergence analysis suggests that these broken glass filters do not introduce significant distributional shifts.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks</title>
<link>https://arxiv.org/abs/2406.04657</link>
<guid>https://arxiv.org/abs/2406.04657</guid>
<content:encoded><![CDATA[
arXiv:2406.04657v3 Announce Type: replace-cross 
Abstract: Training strategies for modern deep neural networks (NNs) tend to induce a heavy-tailed (HT) empirical spectral density (ESD) in the layer weights. While previous efforts have shown that the HT phenomenon correlates with good generalization in large NNs, a theoretical explanation of its occurrence is still lacking. Especially, understanding the conditions which lead to this phenomenon can shed light on the interplay between generalization and weight spectra. Our work aims to bridge this gap by presenting a simple, rich setting to model the emergence of HT ESD. In particular, we present a theory-informed setup for 'crafting' heavy tails in the ESD of two-layer NNs and present a systematic analysis of the HT ESD emergence without any gradient noise. This is the first work to analyze a noise-free setting, and we also incorporate optimizer (GD/Adam) dependent (large) learning rates into the HT ESD analysis. Our results highlight the role of learning rates on the Bulk+Spike and HT shape of the ESDs in the early phase of training, which can facilitate generalization in the two-layer NN. These observations shed light on the behavior of large-scale NNs, albeit in a much simpler setting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVBench: An Extreme Long Video Understanding Benchmark</title>
<link>https://arxiv.org/abs/2406.08035</link>
<guid>https://arxiv.org/abs/2406.08035</guid>
<content:encoded><![CDATA[
arXiv:2406.08035v3 Announce Type: replace-cross 
Abstract: Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Our data and code are publicly available at: https://lvbench.github.io.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-AI Bias: large language models favor communications generated by large language models</title>
<link>https://arxiv.org/abs/2407.12856</link>
<guid>https://arxiv.org/abs/2407.12856</guid>
<content:encoded><![CDATA[
arXiv:2407.12856v2 Announce Type: replace-cross 
Abstract: Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning</title>
<link>https://arxiv.org/abs/2408.07057</link>
<guid>https://arxiv.org/abs/2408.07057</guid>
<content:encoded><![CDATA[
arXiv:2408.07057v2 Announce Type: replace-cross 
Abstract: The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow</title>
<link>https://arxiv.org/abs/2408.08651</link>
<guid>https://arxiv.org/abs/2408.08651</guid>
<content:encoded><![CDATA[
arXiv:2408.08651v3 Announce Type: replace-cross 
Abstract: Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Directed Score-Based Diffusion Models via q-Learning</title>
<link>https://arxiv.org/abs/2409.04832</link>
<guid>https://arxiv.org/abs/2409.04832</guid>
<content:encoded><![CDATA[
arXiv:2409.04832v2 Announce Type: replace-cross 
Abstract: We propose a new reinforcement learning (RL) formulation for training continuous-time score-based diffusion models for generative AI to generate samples that maximize reward functions while keeping the generated distributions close to the unknown target data distributions. Different from most existing studies, ours does not involve any pretrained model for the unknown score functions of the noise-perturbed data distributions, nor does it attempt to learn the score functions. Instead, we formulate the problem as entropy-regularized continuous-time RL and show that the optimal stochastic policy has a Gaussian distribution with a known covariance matrix. Based on this result, we parameterize the mean of Gaussian policies and develop an actor--critic type (little) q-learning algorithm to solve the RL problem. A key ingredient in our algorithm design is to obtain noisy observations from the unknown score function via a ratio estimator. Our formulation can also be adapted to solve pure score-matching and fine-tuning pretrained models. Numerically, we show the effectiveness of our approach by comparing its performance with two state-of-the-art RL methods that fine-tune pretrained models on several generative tasks including high-dimensional image generations. Finally, we discuss extensions of our RL formulation to probability flow ODE implementation of diffusion models and to conditional diffusion models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-Language Pretraining for Highly Label-Efficient Clinical Phenotyping</title>
<link>https://arxiv.org/abs/2409.07480</link>
<guid>https://arxiv.org/abs/2409.07480</guid>
<content:encoded><![CDATA[
arXiv:2409.07480v4 Announce Type: replace-cross 
Abstract: Multimodal language modeling has enabled breakthroughs for representation learning, yet remains unexplored in the realm of functional brain data for clinical phenotyping. This paper pioneers EEG-language models (ELMs) trained on clinical reports and 15000 EEGs. We propose to combine multimodal alignment in this novel domain with timeseries cropping and text segmentation, enabling an extension based on multiple instance learning to alleviate misalignment between irrelevant EEG or text segments. Our multimodal models significantly improve over EEG-only models across four clinical evaluations and for the first time enable zero-shot classification as well as retrieval of both neural signals and reports. In sum, these results highlight the potential of ELMs, representing significant progress for clinical applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v3 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3\% and diffusion models by 3.0\% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50\% fewer updates and 1.5\% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20--35%\% while consuming only 11.2 Wh over 37 days -- enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2410.08109</link>
<guid>https://arxiv.org/abs/2410.08109</guid>
<content:encoded><![CDATA[
arXiv:2410.08109v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Spatial Representation to Enhance LLM Reasoning in Aerial Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2410.08500</link>
<guid>https://arxiv.org/abs/2410.08500</guid>
<content:encoded><![CDATA[
arXiv:2410.08500v3 Announce Type: replace-cross 
Abstract: Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. However, it remains challenging due to the complex spatial relationships in aerial scenes.In this paper, we propose a training-free, zero-shot framework for aerial VLN tasks, where the large language model (LLM) is leveraged as the agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning capabilities of LLMs. This is achieved by extracting and projecting instruction-related semantic masks onto a top-down map, which presents spatial and topological information about surrounding landmarks and grows during the navigation process. At each step, a local map centered at the UAV is extracted from the growing top-down map, and transformed into a ma trix representation with distance metrics, serving as the text prompt to LLM for action prediction in response to the given instruction. Experiments conducted in real and simulation environments have proved the effectiveness and robustness of our method, achieving absolute success rate improvements of 26.8% and 5.8% over current state-of-the-art methods on simple and complex navigation tasks, respectively. The dataset and code will be released soon.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2410.09103</link>
<guid>https://arxiv.org/abs/2410.09103</guid>
<content:encoded><![CDATA[
arXiv:2410.09103v2 Announce Type: replace-cross 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UoMo: A Universal Model of Mobile Traffic Forecasting for Wireless Network Optimization</title>
<link>https://arxiv.org/abs/2410.15322</link>
<guid>https://arxiv.org/abs/2410.15322</guid>
<content:encoded><![CDATA[
arXiv:2410.15322v4 Announce Type: replace-cross 
Abstract: Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero/few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short/long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero/few-shot learning, showcasing a strong universality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientEQA: An Efficient Approach to Open-Vocabulary Embodied Question Answering</title>
<link>https://arxiv.org/abs/2410.20263</link>
<guid>https://arxiv.org/abs/2410.20263</guid>
<content:encoded><![CDATA[
arXiv:2410.20263v2 Announce Type: replace-cross 
Abstract: Embodied Question Answering (EQA) is an essential yet challenging task for robot assistants. Large vision-language models (VLMs) have shown promise for EQA, but existing approaches either treat it as static video question answering without active exploration or restrict answers to a closed set of choices. These limitations hinder real-world applicability, where a robot must explore efficiently and provide accurate answers in open-vocabulary settings. To overcome these challenges, we introduce EfficientEQA, a novel framework that couples efficient exploration with free-form answer generation. EfficientEQA features three key innovations: (1) Semantic-Value-Weighted Frontier Exploration (SFE) with Verbalized Confidence (VC) from a black-box VLM to prioritize semantically important areas to explore, enabling the agent to gather relevant information faster; (2) a BLIP relevancy-based mechanism to stop adaptively by flagging highly relevant observations as outliers to indicate whether the agent has collected enough information; and (3) a Retrieval-Augmented Generation (RAG) method for the VLM to answer accurately based on pertinent images from the agent's observation history without relying on predefined choices. Our experimental results show that EfficientEQA achieves over 15% higher answer accuracy and requires over 20% fewer exploration steps than state-of-the-art methods. Our code is available at: https://github.com/chengkaiAcademyCity/EfficientEQA
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Voice Conversion via Content-Aware Timbre Ensemble and Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2411.02026</link>
<guid>https://arxiv.org/abs/2411.02026</guid>
<content:encoded><![CDATA[
arXiv:2411.02026v2 Announce Type: replace-cross 
Abstract: Despite recent advances in zero-shot voice conversion (VC), achieving speaker similarity and naturalness comparable to ground-truth recordings remains a significant challenge. In this letter, we propose CTEFM-VC, a zero-shot VC framework that integrates content-aware timbre ensemble modeling with conditional flow matching. Specifically, CTEFM-VC decouples utterances into content and timbre representations and leverages a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. To enhance its timbre modeling capability and naturalness of generated speech, we first introduce a context-aware timbre ensemble modeling approach that adaptively integrates diverse speaker verification embeddings and enables the effective utilization of source content and target timbre elements through a cross-attention module. Furthermore, a structural similarity-based timbre loss is presented to jointly train CTEFM-VC end-to-end. Experiments show that CTEFM-VC consistently achieves the best performance in all metrics assessing speaker similarity, speech naturalness, and intelligibility, significantly outperforming state-of-the-art zero-shot VC systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering AI-Driven Personalization of Scientific Text for General Audiences</title>
<link>https://arxiv.org/abs/2411.09969</link>
<guid>https://arxiv.org/abs/2411.09969</guid>
<content:encoded><![CDATA[
arXiv:2411.09969v2 Announce Type: replace-cross 
Abstract: Digital media platforms (e.g., science blogs) offer opportunities to communicate scientific content to general audiences at scale. However, these audiences vary in their scientific expertise, literacy levels, and personal backgrounds, making effective science communication challenging. To address this challenge, we designed TranSlider, an AI-powered tool that generates personalized translations of scientific text based on individual user profiles (e.g., hobbies, location, and education). Our tool features an interactive slider that allows users to steer the degree of personalization from 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to generate the translations with chosen degrees. Through an exploratory study with 15 participants, we investigated both the utility of these AI-personalized translations and how interactive reading features influenced users' understanding and reading experiences. We found that participants who preferred higher degrees of personalization appreciated the relatable and contextual translations, while those who preferred lower degrees valued concise translations with subtle contextualization. Furthermore, participants reported the compounding effect of multiple translations on their understanding of scientific content. Drawing on these findings, we discuss several implications for facilitating science communication and designing steerable interfaces to support human-AI alignment.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes</title>
<link>https://arxiv.org/abs/2412.04140</link>
<guid>https://arxiv.org/abs/2412.04140</guid>
<content:encoded><![CDATA[
arXiv:2412.04140v4 Announce Type: replace-cross 
Abstract: In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2412.05148</link>
<guid>https://arxiv.org/abs/2412.05148</guid>
<content:encoded><![CDATA[
arXiv:2412.05148v2 Announce Type: replace-cross 
Abstract: Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adapters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA$.$rar, a method that not only improves image quality but also achieves a remarkable speedup of over $4000\times$ in the merging process. We collect a dataset of style and subject LoRAs and pre-train a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLMs) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens</title>
<link>https://arxiv.org/abs/2412.09919</link>
<guid>https://arxiv.org/abs/2412.09919</guid>
<content:encoded><![CDATA[
arXiv:2412.09919v2 Announce Type: replace-cross 
Abstract: Recently, Vision Large Language Models (VLLMs) integrated with vision encoders have shown promising performance in vision understanding. The key of VLLMs is to encode visual content into sequences of visual tokens, enabling VLLMs to simultaneously process both visual and textual content. However, understanding videos, especially long videos, remain a challenge to VLLMs as the number of visual tokens grows rapidly when encoding videos, resulting in the risk of exceeding the context window of VLLMs and introducing heavy computation burden. To restrict the number of visual tokens, existing VLLMs either: (1) uniformly downsample videos into a fixed number of frames or (2) reducing the number of visual tokens encoded from each frame. We argue the former solution neglects the rich temporal cue in videos and the later overlooks the spatial details in each frame. In this work, we present Balanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively leverage task relevant spatio-temporal cues while restricting the number of visual tokens under the VLLM context window length. At the core of our method, we devise a text-conditioned adaptive frame selection module to identify frames relevant to the visual understanding task. The selected frames are then de-duplicated using a temporal frame token merging technique. The visual tokens of the selected frames are processed through a spatial token sampling module and an optional spatial token merging strategy to achieve precise control over the token count. Experimental results show that B-VLLM is effective in balancing the number of frames and visual tokens in video understanding, yielding superior performance on various video understanding benchmarks. Our code is available at https://github.com/zhuqiangLu/B-VLLM.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POEX: Towards Policy Executable Jailbreak Attacks Against the LLM-based Robots</title>
<link>https://arxiv.org/abs/2412.16633</link>
<guid>https://arxiv.org/abs/2412.16633</guid>
<content:encoded><![CDATA[
arXiv:2412.16633v3 Announce Type: replace-cross 
Abstract: The integration of LLMs into robots has witnessed significant growth, where LLMs can convert instructions into executable robot policies. However, the inherent vulnerability of LLMs to jailbreak attacks brings critical security risks from the digital domain to the physical world. An attacked LLM-based robot could execute harmful policies and cause physical harm. In this paper, we investigate the feasibility and rationale of jailbreak attacks against LLM-based robots and answer three research questions: (1) How applicable are existing LLM jailbreak attacks against LLM-based robots? (2) What unique challenges arise if they are not directly applicable? (3) How to defend against such jailbreak attacks? To this end, we first construct a "human-object-environment" robot risks-oriented Harmful-RLbench and then conduct a measurement study on LLM-based robot systems. Our findings conclude that traditional LLM jailbreak attacks are inapplicable in robot scenarios, and we identify two unique challenges: determining policy-executable optimization directions and accurately evaluating robot-jailbroken policies. To enable a more thorough security analysis, we introduce POEX (POlicy EXecutable) jailbreak, a red-teaming framework that induces harmful yet executable policy to jailbreak LLM-based robots. POEX incorporates hidden layer gradient optimization to guarantee jailbreak success and policy execution as well as a multi-agent evaluator to accurately assess the practical executability of policies. Experiments conducted on the real-world robotic systems and in simulation demonstrate the efficacy of POEX, highlighting critical security vulnerabilities and its transferability across LLMs. Finally, we propose prompt-based and model-based defenses to mitigate attacks. Our findings underscore the urgent need for security measures to ensure the safe deployment of LLM-based robots in critical applications.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MomentMix Augmentation with Length-Aware DETR for Temporally Robust Moment Retrieval</title>
<link>https://arxiv.org/abs/2412.20816</link>
<guid>https://arxiv.org/abs/2412.20816</guid>
<content:encoded><![CDATA[
arXiv:2412.20816v2 Announce Type: replace-cross 
Abstract: Video Moment Retrieval (MR) aims to localize moments within a video based on a given natural language query. Given the prevalent use of platforms like YouTube for information retrieval, the demand for MR techniques is significantly growing. Recent DETR-based models have made notable advances in performance but still struggle with accurately localizing short moments. Through data analysis, we identified limited feature diversity in short moments, which motivated the development of MomentMix. MomentMix employs two augmentation strategies: ForegroundMix and BackgroundMix, each enhancing the feature representations of the foreground and background, respectively. Additionally, our analysis of prediction bias revealed that short moments particularly struggle with accurately predicting their center positions of moments. To address this, we propose a Length-Aware Decoder, which conditions length through a novel bipartite matching process. Our extensive studies demonstrate the efficacy of our length-aware approach, especially in localizing short moments, leading to improved overall performance. Our method surpasses state-of-the-art DETR-based methods on benchmark datasets, achieving the highest R1 and mAP on QVHighlights and the highest R1@0.7 on TACoS and Charades-STA (such as a 2.46% gain in R1@0.7 and a 2.57% gain in mAP average for QVHighlights). The code is available at https://github.com/sjpark5800/LA-DETR.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title>
<link>https://arxiv.org/abs/2412.21051</link>
<guid>https://arxiv.org/abs/2412.21051</guid>
<content:encoded><![CDATA[
arXiv:2412.21051v3 Announce Type: replace-cross 
Abstract: The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Cel-Animation: A Survey</title>
<link>https://arxiv.org/abs/2501.06250</link>
<guid>https://arxiv.org/abs/2501.06250</guid>
<content:encoded><![CDATA[
arXiv:2501.06250v3 Announce Type: replace-cross 
Abstract: Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, challenges like visual consistency, stylistic coherence, and ethical considerations persist. Additionally, this paper explores future directions and advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebWalker: Benchmarking LLMs in Web Traversal</title>
<link>https://arxiv.org/abs/2501.07572</link>
<guid>https://arxiv.org/abs/2501.07572</guid>
<content:encoded><![CDATA[
arXiv:2501.07572v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ehrenfeucht-Haussler Rank and Chain of Thought</title>
<link>https://arxiv.org/abs/2501.12997</link>
<guid>https://arxiv.org/abs/2501.12997</guid>
<content:encoded><![CDATA[
arXiv:2501.12997v2 Announce Type: replace-cross 
Abstract: The notion of \emph{rank} of a Boolean function has been a cornerstone in PAC learning theory, enabling quasipolynomial-time learning algorithms for polynomial-size decision trees. We present a novel characterization of rank, grounded in the well-known Transformer architecture. We show that the rank of a function $f$ corresponds to the minimum number of \emph{Chain of Thought} (CoT) steps required by a single-layer Transformer with hard attention to compute $f$. Based on this characterization we establish tight bounds on the number of CoT steps required for specific problems, showing that \(\ell\)-fold function composition necessitates exactly \(\ell\) CoT steps. Furthermore, we analyze the problem of identifying the position of the \(k\)-th occurrence of 1 in a Boolean sequence, proving that it requires \(k\) CoT steps. Finally, we introduce the notion of the multi-head rank that captures multi-head single-layer transformers, and perform the analysis of PAC-learnability of the classes of functions with bounded multi-head rank.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models</title>
<link>https://arxiv.org/abs/2501.13428</link>
<guid>https://arxiv.org/abs/2501.13428</guid>
<content:encoded><![CDATA[
arXiv:2501.13428v4 Announce Type: replace-cross 
Abstract: Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by proposing a new design principle for attention, viewing it as a two-stage process. We first decompose the Softmax operation into a non-linear positivity transformation and an $l_1$-normalisation step, identifying the latter as essential for maintaining model performance. In the first stage, we replace the standard exponential function with the more numerically stable Softplus activation and introduce a dynamic scale factor based on invariance entropy, creating a novel attention mechanism that outperforms conventional Softmax attention. In the second stage, we introduce a re-weighting mechanism that sharpens the attention distribution, amplifying significant weights while diminishing weaker ones. This enables the model to concentrate more effectively on relevant tokens and fundamentally improves length extrapolation. When combined, this two-stage approach ensures numerical stability and dramatically improves length extrapolation, maintaining a nearly constant validation loss at 16$\times$ the training length while achieving superior results on challenging long-context retrieval tasks and standard downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Your Model Ranking on Chatbot Arena by Vote Rigging</title>
<link>https://arxiv.org/abs/2501.17858</link>
<guid>https://arxiv.org/abs/2501.17858</guid>
<content:encoded><![CDATA[
arXiv:2501.17858v2 Announce Type: replace-cross 
Abstract: Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Traffic Oscillations in Mixed Traffic Flow with Scalable Deep Koopman Predictive Control</title>
<link>https://arxiv.org/abs/2502.00043</link>
<guid>https://arxiv.org/abs/2502.00043</guid>
<content:encoded><![CDATA[
arXiv:2502.00043v3 Announce Type: replace-cross 
Abstract: Mitigating traffic oscillations in mixed flows of connected automated vehicles (CAVs) and human-driven vehicles (HDVs) is critical for enhancing traffic stability. A key challenge lies in modeling the nonlinear, heterogeneous behaviors of HDVs within computationally tractable predictive control frameworks. This study proposes an adaptive deep Koopman predictive control framework (AdapKoopPC) to address this issue. The framework features a novel deep Koopman network, AdapKoopnet, which represents complex HDV car-following dynamics as a linear system in a high-dimensional space by adaptively learning from naturalistic data. This learned linear representation is then embedded into a Model Predictive Control (MPC) scheme, enabling real-time, scalable, and optimal control of CAVs. We validate our framework using the HighD dataset and extensive numerical simulations. Results demonstrate that AdapKoopnet achieves superior trajectory prediction accuracy over baseline models. Furthermore, the complete AdapKoopPC controller significantly dampens traffic oscillations with lower computational cost, exhibiting strong performance even at low CAV penetration rates. The proposed framework offers a scalable and data-driven solution for enhancing stability in realistic mixed traffic environments. The code is made publicly available.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization</title>
<link>https://arxiv.org/abs/2502.00425</link>
<guid>https://arxiv.org/abs/2502.00425</guid>
<content:encoded><![CDATA[
arXiv:2502.00425v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles with MLLMs because of (a) high inference latency from large visual token counts, (b) distributional disparities between visual and textual tokens, and (c) extreme outliers introduced by Hadamard-based transformations. To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs. textual tokens; Attention-Invariant Flexible Switching (AIFS), reordering tokens to preserve casual attention while eliminating expensive token-wise scale computations; Rotation Magnitude Suppression (RMS), mitigating weight outliers arising from online Hadamard rotations. On five mainstream MLLMs (including Qwen-VL, MiniCPM-V, CogVLM2), MQuant under W4A8 achieves near-floating-point accuracy (<1% degradation) while reducing inference latency by up to 30%, significantly outperforming existing PTQ baselines. Our MQuant effectively bridges the gap for efficient and accurate MLLMs inference in resource-constrained devices. Code has been released in https://github.com/StiphyJay/MQuant.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System</title>
<link>https://arxiv.org/abs/2502.03450</link>
<guid>https://arxiv.org/abs/2502.03450</guid>
<content:encoded><![CDATA[
arXiv:2502.03450v2 Announce Type: replace-cross 
Abstract: Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG^2, an iterative Schema-Guided Scene-Graph reasoning framework based on multi-agent LLMs. The agents are grouped into two modules: a (1) Reasoner module for abstract task planning and graph information queries generation, and a (2) Retriever module for extracting corresponding graph information based on code-writing following the queries. Two modules collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. The scene graph schema, prompted to both modules, serves to not only streamline both reasoning and retrieval process, but also guide the cooperation between two modules. This eliminates the need to prompt LLMs with full graph data, reducing the chance of hallucination due to irrelevant information. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches and baseline single-agent, tool-based Reason-while-Retrieve strategy in numerical Q\&amp;A and planning tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration</title>
<link>https://arxiv.org/abs/2502.12204</link>
<guid>https://arxiv.org/abs/2502.12204</guid>
<content:encoded><![CDATA[
arXiv:2502.12204v2 Announce Type: replace-cross 
Abstract: Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to capture intra-theme and inter-theme correlation explicitly, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35\% and 12\% compared to the state-of-the-art on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of modeling theme correlation and incorporating interactive external feedback.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Powered Medical Image Synthesis: Insights from MedVQA-GI Challenge Using CLIP, Fine-Tuned Stable Diffusion, and Dream-Booth + LoRA</title>
<link>https://arxiv.org/abs/2502.20667</link>
<guid>https://arxiv.org/abs/2502.20667</guid>
<content:encoded><![CDATA[
arXiv:2502.20667v2 Announce Type: replace-cross 
Abstract: The MEDVQA-GI challenge addresses the integration of AI-driven text-to-image generative models in medical diagnostics, aiming to enhance diagnostic capabilities through synthetic image generation. Existing methods primarily focus on static image analysis and lack the dynamic generation of medical imagery from textual descriptions. This study intends to partially close this gap by introducing a novel approach based on fine-tuned generative models to generate dynamic, scalable, and precise images from textual descriptions. Particularly, our system integrates fine-tuned Stable Diffusion and DreamBooth models, as well as Low-Rank Adaptation (LORA), to generate high-fidelity medical images. The problem is around two sub-tasks namely: image synthesis (IS) and optimal prompt production (OPG). The former creates medical images via verbal prompts, whereas the latter provides prompts that produce high-quality images in specified categories. The study emphasizes the limitations of traditional medical image generation methods, such as hand sketching, constrained datasets, static procedures, and generic models. Our evaluation measures showed that Stable Diffusion surpasses CLIP and DreamBooth + LORA in terms of producing high-quality, diversified images. Specifically, Stable Diffusion had the lowest Fr\'echet Inception Distance (FID) scores (0.099 for single center, 0.064 for multi-center, and 0.067 for combined), indicating higher image quality. Furthermore, it had the highest average Inception Score (2.327 across all datasets), indicating exceptional diversity and quality. This advances the field of AI-powered medical diagnosis. Future research will concentrate on model refining, dataset augmentation, and ethical considerations for efficiently implementing these advances into clinical practice
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth</title>
<link>https://arxiv.org/abs/2502.20758</link>
<guid>https://arxiv.org/abs/2502.20758</guid>
<content:encoded><![CDATA[
arXiv:2502.20758v3 Announce Type: replace-cross 
Abstract: We introduce a new approach in which several advanced large language models-specifically GPT-4-0125-preview, Meta-LLAMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash-collaborate to both produce and answer intricate, doctoral-level probability problems without relying on any single "correct" reference. Rather than depending on an established ground truth, our investigation focuses on how agreement among diverse models can signal the reliability of their outputs and, by extension, reflect the overall quality of the generated questions. To measure this inter-model alignment, we apply a suite of statistical evaluations, including chi-square tests, Fleiss' Kappa coefficients, and confidence interval calculations, thereby capturing both precision in answers and clarity in question phrasing. Our analysis reveals that Claude and Gemini tend to frame questions more coherently and unambiguously, which is evidenced by their tighter confidence intervals and greater concordance with responding agents. In contrast, LLAMA exhibits wider confidence bands and a lower level of agreement, indicating more variability and reduced consistency in its question formulations. These observations support the notion that a multi-model collaborative strategy not only improves answer dependability but also offers an effective, data-driven mechanism for evaluating and refining question quality when no definitive solution exists. Ultimately, this work delivers actionable insights into enhancing AI-guided reasoning processes through coordinated interactions among heterogeneous language models.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games</title>
<link>https://arxiv.org/abs/2503.05925</link>
<guid>https://arxiv.org/abs/2503.05925</guid>
<content:encoded><![CDATA[
arXiv:2503.05925v2 Announce Type: replace-cross 
Abstract: Behavioral game theory models serve two purposes: yielding insights into how human decision-making works, and predicting how people would behave in novel strategic settings. A system called GameNet represents the state of the art for predicting human behavior in the setting of unrepeated simultaneous-move games, combining a simple "level-k" model of strategic reasoning with a complex neural network model of non-strategic "level-0" behavior. Although this reliance on well-established ideas from cognitive science ought to make GameNet interpretable, the flexibility of its level-0 model raises the possibility that it is able to emulate strategic reasoning. In this work, we prove that GameNet's level-0 model is indeed too general. We then introduce ElementaryNet, a novel neural network that is provably incapable of expressing strategic behavior. We show that these additional restrictions are empirically harmless, leading ElementaryNet to statistically indistinguishable predictive performance vs GameNet. We then show how it is possible to derive insights about human behavior by varying ElementaryNet's features and interpreting its parameters, finding evidence of iterative reasoning, learning about the depth of this reasoning process, and showing the value of a rich level-0 specification.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers</title>
<link>https://arxiv.org/abs/2503.06923</link>
<guid>https://arxiv.org/abs/2503.06923</guid>
<content:encoded><![CDATA[
arXiv:2503.06923v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FunGraph: Functionality Aware 3D Scene Graphs for Language-Prompted Scene Interaction</title>
<link>https://arxiv.org/abs/2503.07909</link>
<guid>https://arxiv.org/abs/2503.07909</guid>
<content:encoded><![CDATA[
arXiv:2503.07909v2 Announce Type: replace-cross 
Abstract: The concept of 3D scene graphs is increasingly recognized as a powerful semantic and hierarchical representation of the environment. Current approaches often address this at a coarse, object-level resolution. In contrast, our goal is to develop a representation that enables robots to directly interact with their environment by identifying both the location of functional interactive elements and how these can be used. To achieve this, we focus on detecting and storing objects at a finer resolution, focusing on affordance-relevant parts. The primary challenge lies in the scarcity of data that extends beyond instance-level detection and the inherent difficulty of capturing detailed object features using robotic sensors. We leverage currently available 3D resources to generate 2D data and train a detector, which is then used to augment the standard 3D scene graph generation pipeline. Through our experiments, we demonstrate that our approach achieves functional element segmentation comparable to state-of-the-art 3D models and that our augmentation enables task-driven affordance grounding with higher accuracy than the current solutions. See our project page at https://fungraph.github.io.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Learning with Autoregressive Chain of Thought</title>
<link>https://arxiv.org/abs/2503.07932</link>
<guid>https://arxiv.org/abs/2503.07932</guid>
<content:encoded><![CDATA[
arXiv:2503.07932v2 Announce Type: replace-cross 
Abstract: For a given base class of sequence-to-next-token generators, we consider learning prompt-to-answer mappings obtained by iterating a fixed, time-invariant generator for multiple steps, thus generating a chain-of-thought, and then taking the final token as the answer. We formalize the learning problems both when the chain-of-thought is observed and when training only on prompt-answer pairs, with the chain-of-thought latent. We analyze the sample and computational complexity both in terms of general properties of the base class (e.g. its VC dimension) and for specific base classes such as linear thresholds. We present a simple base class that allows for universal representability and computationally tractable chain-of-thought learning. Central to our development is that time invariance allows for sample complexity that is independent of the length of the chain-of-thought. Attention arises naturally in our construction.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Dexterous Grasping from Single Demonstrations</title>
<link>https://arxiv.org/abs/2503.20208</link>
<guid>https://arxiv.org/abs/2503.20208</guid>
<content:encoded><![CDATA[
arXiv:2503.20208v2 Announce Type: replace-cross 
Abstract: How can robots learn dexterous grasping skills efficiently and apply them adaptively based on user instructions? This work tackles two key challenges: efficient skill acquisition from limited human demonstrations and context-driven skill selection. We introduce AdaDexGrasp, a framework that learns a library of grasping skills from a single human demonstration per skill and selects the most suitable one using a vision-language model (VLM). To improve sample efficiency, we propose a trajectory following reward that guides reinforcement learning (RL) toward states close to a human demonstration while allowing flexibility in exploration. To learn beyond the single demonstration, we employ curriculum learning, progressively increasing object pose variations to enhance robustness. At deployment, a VLM retrieves the appropriate skill based on user instructions, bridging low-level learned skills with high-level intent. We evaluate AdaDexGrasp in both simulation and real-world settings, showing that our approach significantly improves RL efficiency and enables learning human-like grasp strategies across varied object configurations. Finally, we demonstrate zero-shot transfer of our learned policies to a real-world PSYONIC Ability Hand, with a 90% success rate across objects, significantly outperforming the baseline.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D-Gaussian Simulators from RGB Videos</title>
<link>https://arxiv.org/abs/2503.24009</link>
<guid>https://arxiv.org/abs/2503.24009</guid>
<content:encoded><![CDATA[
arXiv:2503.24009v2 Announce Type: replace-cross 
Abstract: Realistic simulation is critical for applications ranging from robotics to animation. Learned simulators have emerged as a possibility to capture real world physics directly from video data, but very often require privileged information such as depth information, particle tracks and hand-engineered features to maintain spatial and temporal consistency. These strong inductive biases or ground truth 3D information help in domains where data is sparse but limit scalability and generalization in data rich regimes. To overcome the key limitations, we propose 3DGSim, a learned 3D simulator that directly learns physical interactions from multi-view RGB videos. 3DGSim unifies 3D scene reconstruction, particle dynamics prediction and video synthesis into an end-to-end trained framework. It adopts MVSplat to learn a latent particle-based representation of 3D scenes, a Point Transformer for particle dynamics, a Temporal Merging module for consistent temporal aggregation and Gaussian Splatting to produce novel view renderings. By jointly training inverse rendering and dynamics forecasting, 3DGSim embeds the physical properties into point-wise latent features. This enables the model to capture diverse physical behaviors, from rigid to elastic, cloth-like dynamics, and boundary conditions (e.g. fixed cloth corner), along with realistic lighting effects that also generalize to unseen multibody interactions and novel scene edits.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mu$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models</title>
<link>https://arxiv.org/abs/2504.01196</link>
<guid>https://arxiv.org/abs/2504.01196</guid>
<content:encoded><![CDATA[
arXiv:2504.01196v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model's internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches, especially window-based autoregressive methods, often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing ($\mu$KE), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across four benchmarks demonstrate that $\mu$KE improves edit efficacy by up to 12.33% over state-of-the-art methods, and remains robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</title>
<link>https://arxiv.org/abs/2504.02904</link>
<guid>https://arxiv.org/abs/2504.02904</guid>
<content:encoded><![CDATA[
arXiv:2504.02904v2 Announce Type: replace-cross 
Abstract: Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Hierarchical Protein Multi-Modal Representation Learning</title>
<link>https://arxiv.org/abs/2504.04770</link>
<guid>https://arxiv.org/abs/2504.04770</guid>
<content:encoded><![CDATA[
arXiv:2504.04770v2 Announce Type: replace-cross 
Abstract: Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural context. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. This bidirectional and hierarchical (Bi-Hierarchical) fusion approach leverages the strengths of both modalities to capture richer and more comprehensive protein representations. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including enzyme EC classification, model quality assessment, protein-ligand binding affinity prediction, protein-protein binding site prediction, and B cell epitopes prediction. Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of Bi-Hierarchical Fusion in bridging sequence and structural modalities.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation</title>
<link>https://arxiv.org/abs/2504.12436</link>
<guid>https://arxiv.org/abs/2504.12436</guid>
<content:encoded><![CDATA[
arXiv:2504.12436v2 Announce Type: replace-cross 
Abstract: Adapting Vision-Language Models (VLMs) to new domains with few labeled samples remains a significant challenge due to severe overfitting and computational constraints. State-of-the-art solutions, such as low-rank reparameterization, mitigate these issues but often struggle with generalization and require extensive hyperparameter tuning. In this paper, a novel Sparse Optimization (SO) framework is proposed. Unlike low-rank approaches that typically constrain updates to a fixed subspace, our SO method leverages high sparsity to dynamically adjust very few parameters. We introduce two key paradigms. First, we advocate for \textit{local sparsity and global density}, which updates a minimal subset of parameters per iteration while maintaining overall model expressiveness. As a second paradigm, we advocate for \textit{local randomness and global importance}, which sparsifies the gradient using random selection while pruning the first moment based on importance. This combination significantly mitigates overfitting and ensures stable adaptation in low-data regimes. Extensive experiments on 11 diverse datasets show that SO achieves state-of-the-art few-shot adaptation performance while reducing memory overhead.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography</title>
<link>https://arxiv.org/abs/2504.18400</link>
<guid>https://arxiv.org/abs/2504.18400</guid>
<content:encoded><![CDATA[
arXiv:2504.18400v2 Announce Type: replace-cross 
Abstract: Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-consuming for large-scale datasets due to reliance on voxel-based representations. We propose Tract2Shape, a novel multimodal deep learning framework that leverages geometric (point cloud) and scalar (tabular) features to predict ten white matter tractography shape measures. To enhance model efficiency, we utilize a dimensionality reduction algorithm for the model to predict five primary shape components. The model is trained and evaluated on two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset. We evaluate the performance of Tract2Shape by training and testing it on the HCP-YA dataset and comparing the results with state-of-the-art models. To further assess its robustness and generalization ability, we also test Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep learning models across all ten shape measures, achieving the highest average Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows that both multimodal input and PCA contribute to performance gains. On the unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low nMSE, demonstrating strong generalizability in cross-dataset evaluation. Tract2Shape enables fast, accurate, and generalizable prediction of white matter shape measures from tractography data, supporting scalable analysis across datasets. This framework lays a promising foundation for future large-scale white matter shape analysis.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
<link>https://arxiv.org/abs/2505.07078</link>
<guid>https://arxiv.org/abs/2505.07078</guid>
<content:encoded><![CDATA[
arXiv:2505.07078v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.10347</link>
<guid>https://arxiv.org/abs/2505.10347</guid>
<content:encoded><![CDATA[
arXiv:2505.10347v2 Announce Type: replace-cross 
Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task Learning by addressing issues like conflicting gradients and differing gradient norms, which hinder equal-weighted task training. However, recent critiques suggest that equally weighted tasks can achieve competitive results compared to SMTOs, arguing that previous SMTO results were influenced by poor hyperparameter optimization and lack of regularization. In this work, we evaluate these claims through an extensive empirical evaluation of SMTOs, including some of the latest methods, on more complex multi-task problems to clarify this behavior. Our findings indicate that SMTOs perform well compared to uniform loss and that fixed weights can achieve competitive performance compared to SMTOs. Furthermore, we demonstrate why uniform loss perform similarly to SMTOs in some instances. The source code is available at https://github.com/Gabriel-SGama/UnitScal_vs_SMTOs.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIDGECUT: Learning Graph Partitioning with Rings and Wedges</title>
<link>https://arxiv.org/abs/2505.13986</link>
<guid>https://arxiv.org/abs/2505.13986</guid>
<content:encoded><![CDATA[
arXiv:2505.13986v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has proven to be a powerful tool for combinatorial optimization (CO) problems due to its ability to learn heuristics that can generalize across problem instances. However, integrating knowledge that will steer the RL framework for CO solutions towards domain appropriate outcomes remains a challenging task. In this paper, we propose RIDGECUT, the first RL framework that constrains the action space to enforce structure-aware partitioning in the Normalized Cut problem. Using transportation networks as a motivating example, we introduce a novel concept that leverages domain knowledge about urban road topology -- where natural partitions often take the form of concentric rings and radial wedges. Our method reshapes the graph into a linear or circular structure to simplify the partitioning task so that we can apply sequential transformers and enables efficient learning via Proximal Policy Optimization. The resulting partitions are not only aligned with expected spatial layouts but also achieve lower normalized cuts compared to existing methods. While we focus on traffic data, our approach is broadly applicable and offers a mechanism for embedding structural priors into RL for graph partitioning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization</title>
<link>https://arxiv.org/abs/2505.15918</link>
<guid>https://arxiv.org/abs/2505.15918</guid>
<content:encoded><![CDATA[
arXiv:2505.15918v2 Announce Type: replace-cross 
Abstract: In this work, we evaluate the potential of Large Language Models (LLMs) in building Bayesian Networks (BNs) by approximating domain expert priors. LLMs have demonstrated potential as factual knowledge bases; however, their capability to generate probabilistic knowledge about real-world events remains understudied. We explore utilizing the probabilistic knowledge inherent in LLMs to derive probability estimates for statements regarding events and their relationships within a BN. Using LLMs in this context allows for the parameterization of BNs, enabling probabilistic modeling within specific domains. Our experiments on eighty publicly available Bayesian Networks, from healthcare to finance, demonstrate that querying LLMs about the conditional probabilities of events provides meaningful results when compared to baselines, including random and uniform distributions, as well as approaches based on next-token generation probabilities. We explore how these LLM-derived distributions can serve as expert priors to refine distributions extracted from data, especially when data is scarce. Overall, this work introduces a promising strategy for automatically constructing Bayesian Networks by combining probabilistic knowledge extracted from LLMs with real-world data. Additionally, we establish the first comprehensive baseline for assessing LLM performance in extracting probabilistic knowledge.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration</title>
<link>https://arxiv.org/abs/2505.17066</link>
<guid>https://arxiv.org/abs/2505.17066</guid>
<content:encoded><![CDATA[
arXiv:2505.17066v3 Announce Type: replace-cross 
Abstract: Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FP4 All the Way: Fully Quantized Training of LLMs</title>
<link>https://arxiv.org/abs/2505.19115</link>
<guid>https://arxiv.org/abs/2505.19115</guid>
<content:encoded><![CDATA[
arXiv:2505.19115v2 Announce Type: replace-cross 
Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2505.23849</link>
<guid>https://arxiv.org/abs/2505.23849</guid>
<content:encoded><![CDATA[
arXiv:2505.23849v2 Announce Type: replace-cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves the privacy and security of a client's data without exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data Readiness) for federated learning (FL), a novel framework that allows users to define custom data readiness (DR) metrics, rules, and remedies tailored to specific FL tasks. CADRE generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are prepared for FL while preserving privacy. We demonstrate a practical application of CADRE by integrating it into an existing PPFL framework. We conducted experiments across six datasets and addressed seven different DR issues. The results illustrate the versatility and effectiveness of CADRE in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2505.23870</link>
<guid>https://arxiv.org/abs/2505.23870</guid>
<content:encoded><![CDATA[
arXiv:2505.23870v2 Announce Type: replace-cross 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework</title>
<link>https://arxiv.org/abs/2506.00160</link>
<guid>https://arxiv.org/abs/2506.00160</guid>
<content:encoded><![CDATA[
arXiv:2506.00160v2 Announce Type: replace-cross 
Abstract: The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[
arXiv:2506.00826v2 Announce Type: replace-cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. multimodal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent large language models (LLMs), empowered by massive parameter scales and pretraining on vast corpora, have demonstrated strong reasoning abilities across various tasks. However, their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor, implemented via either in-context learning or lightweight fine-tuning, to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving superior performance over existing methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes</title>
<link>https://arxiv.org/abs/2506.05386</link>
<guid>https://arxiv.org/abs/2506.05386</guid>
<content:encoded><![CDATA[
arXiv:2506.05386v3 Announce Type: replace-cross 
Abstract: Clinical note generation aims to produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent LLM-based methods pre-trained on general clinical corpora show promise in clinical text generation, they fall short in producing long-form notes from limited patient information. In this paper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG) for long-form discharge instructions based on pre-admission information. ReinRAG retrieves reasoning paths from a medical knowledge graph to provide explicit semantic guidance to the LLM. To bridge the information gap, we propose group-based retriever optimization (GRO) which improves retrieval quality with group-normalized rewards, encouraging reasoning leaps for deeper inference by the LLM. Comprehensive experiments on the real-world dataset show that ReinRAG outperforms baselines in both clinical efficacy and natural language generation metrics. Further analysis reveals that ReinRAG fills semantic gaps in sparse input scenarios, and retrieved reasoning paths help LLMs avoid clinical misinterpretation by focusing on key evidence and following coherent reasoning.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Winner-takes-all for Multivariate Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.05515</link>
<guid>https://arxiv.org/abs/2506.05515</guid>
<content:encoded><![CDATA[
arXiv:2506.05515v2 Announce Type: replace-cross 
Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL) paradigm to forecast multiple plausible time series futures. Our approach employs a neural network with multiple heads and utilizes the Winner-Takes-All (WTA) loss to promote diversity among predictions. MCL has recently gained attention due to its simplicity and ability to address ill-posed and ambiguous tasks. We propose an adaptation of this framework for time-series forecasting, presenting it as an efficient method to predict diverse futures, which we relate to its implicit quantization objective. We provide insights into our approach using synthetic data and evaluate it on real-world time series, demonstrating its promising performance at a light computational cost.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLOps with Microservices: A Case Study on the Maritime Domain</title>
<link>https://arxiv.org/abs/2506.06202</link>
<guid>https://arxiv.org/abs/2506.06202</guid>
<content:encoded><![CDATA[
arXiv:2506.06202v2 Announce Type: replace-cross 
Abstract: This case study describes challenges and lessons learned on building Ocean Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the maritime domain. First, the paper presents the system's specification, and architecture. Ocean Guard was designed with a microservices' architecture to enable multiple teams to work on the project in parallel. Then, the paper discusses how the developers adapted contract-based design to MLOps for achieving that goal. As a MLES, Ocean Guard employs code, model, and data contracts to establish guidelines between its services. This case study hopes to inspire software engineers, machine learning engineers, and data scientists to leverage similar approaches for their systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Compromises for Coalition Formation</title>
<link>https://arxiv.org/abs/2506.06837</link>
<guid>https://arxiv.org/abs/2506.06837</guid>
<content:encoded><![CDATA[
arXiv:2506.06837v3 Announce Type: replace-cross 
Abstract: The challenge of finding compromises between agent proposals is fundamental to AI subfields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. A crucial step in this process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals remains an open question. We address this gap by formalizing a model that incorporates agent bounded rationality and uncertainty, and by developing AI methods to generate compromise proposals. We focus on the domain of collaborative document writing, such as the democratic drafting of a community constitution. Our approach uses natural language processing techniques and large language models to induce a semantic metric space over text. Based on this space, we design algorithms to suggest compromise points likely to receive broad support. To evaluate our methods, we simulate coalition formation processes and show that AI can facilitate large-scale democratic text editing, a domain where traditional tools are limited.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting</title>
<link>https://arxiv.org/abs/2506.08049</link>
<guid>https://arxiv.org/abs/2506.08049</guid>
<content:encoded><![CDATA[
arXiv:2506.08049v3 Announce Type: replace-cross 
Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions from several weeks to months in advance, represents a critical frontier for agricultural planning, energy management, and disaster preparedness. However, it remains one of the most challenging problems in atmospheric science, due to the chaotic dynamics of atmospheric systems and complex interactions across multiple scales. Current approaches often fail to explicitly model underlying physical processes and teleconnections that are crucial at S2S timescales. We introduce \textbf{TelePiT}, a novel deep learning architecture that enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness. Our approach consists of three key components: (1) Spherical Harmonic Embedding, which accurately encodes global atmospheric variables onto spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which explicitly captures atmospheric physical processes across multiple learnable frequency bands; (3) Teleconnection-Aware Transformer, which models critical global climate interactions through explicitly modeling teleconnection patterns into the self-attention. Extensive experiments demonstrate that \textbf{TelePiT} significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems across all forecast horizons, marking a significant advance toward reliable S2S forecasting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing</title>
<link>https://arxiv.org/abs/2506.13469</link>
<guid>https://arxiv.org/abs/2506.13469</guid>
<content:encoded><![CDATA[
arXiv:2506.13469v2 Announce Type: replace-cross 
Abstract: Quantum magnetic sensing based on spin systems has emerged as a new paradigm for detecting ultra-weak magnetic fields with unprecedented sensitivity, revitalizing applications in navigation, geo-localization, biology, and beyond. At the heart of quantum magnetic sensing, from the protocol perspective, lies the design of optimal sensing parameters to manifest and then estimate the underlying signals of interest (SoI). Existing studies on this front mainly rely on adaptive algorithms based on black-box AI models or formula-driven principled searches. However, when the SoI spans a wide range and the quantum sensor has physical constraints, these methods may fail to converge efficiently or optimally, resulting in prolonged interrogation times and reduced sensing accuracy. In this work, we report the design of a new protocol using a two-stage optimization method. In the 1st Stage, a Bayesian neural network with a fixed set of sensing parameters is used to narrow the range of SoI. In the 2nd Stage, a federated reinforcement learning agent is designed to fine-tune the sensing parameters within a reduced search space. The proposed protocol is developed and evaluated in a challenging context of single-shot readout of an NV-center electron spin under a constrained total sensing time budget; and yet it achieves significant improvements in both accuracy and resource efficiency for wide-range D.C. magnetic field estimation compared to the state of the art.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving</title>
<link>https://arxiv.org/abs/2506.17230</link>
<guid>https://arxiv.org/abs/2506.17230</guid>
<content:encoded><![CDATA[
arXiv:2506.17230v2 Announce Type: replace-cross 
Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical systems, yet solving them in a generic and efficient manner using machine learning-based approaches remains challenging due to limited multi-input and multi-scale generalization capabilities, as well as high computational costs. This paper proposes the Multi-input and Multi-scale Efficient Transformer (MMET), a novel framework designed to address the above challenges. MMET decouples mesh and query points as two sequences and feeds them into the encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE) layer to embed input variables or functions with varying dimensions, enabling effective solutions for multi-scale and multi-input problems. Additionally, a Hilbert curve-based reserialization and patch embedding mechanism decrease the input length. This significantly reduces the computational cost when dealing with large-scale geometric models. These innovations enable efficient representations and support multi-scale resolution queries for large-scale and multi-input PDE problems. Experimental evaluations on diverse benchmarks spanning different physical fields demonstrate that MMET outperforms SOTA methods in both accuracy and computational efficiency. This work highlights the potential of MMET as a robust and scalable solution for real-time PDE solving in engineering and physics-based applications, paving the way for future explorations into pre-trained large-scale models in specific domains. This work is open-sourced at https://github.com/YichenLuo-0/MMET.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving</title>
<link>https://arxiv.org/abs/2506.17590</link>
<guid>https://arxiv.org/abs/2506.17590</guid>
<content:encoded><![CDATA[
arXiv:2506.17590v2 Announce Type: replace-cross 
Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Granular-Ball-Induced Multiple Kernel K-Means</title>
<link>https://arxiv.org/abs/2506.18637</link>
<guid>https://arxiv.org/abs/2506.18637</guid>
<content:encoded><![CDATA[
arXiv:2506.18637v2 Announce Type: replace-cross 
Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel K-means, often struggle with computational efficiency and robustness when faced with complex data distributions. These challenges stem from their dependence on point-to-point relationships for optimization, which can lead to difficulty in accurately capturing data sets' inherent structure and diversity. Additionally, the intricate interplay between multiple kernels in such algorithms can further exacerbate these issues, effectively impacting their ability to cluster data points in high-dimensional spaces. In this paper, we leverage granular-ball computing to improve the multi-kernel clustering framework. The core of granular-ball computing is to adaptively fit data distribution by balls from coarse to acceptable levels. Each ball can enclose data points based on a density consistency measurement. Such ball-based data description thus improves the computational efficiency and the robustness to unknown noises. Specifically, based on granular-ball representations, we introduce the granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel K-means framework (GB-MKKM) for efficient clustering. Using granular-ball relationships in multiple kernel spaces, the proposed GB-MKKM framework shows its superiority in efficiency and clustering performance in the empirical evaluation of various clustering tasks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Behavior Cloning Via Global Lipschitz Regularization</title>
<link>https://arxiv.org/abs/2506.19250</link>
<guid>https://arxiv.org/abs/2506.19250</guid>
<content:encoded><![CDATA[
arXiv:2506.19250v2 Announce Type: replace-cross 
Abstract: Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017</title>
<link>https://arxiv.org/abs/2506.19877</link>
<guid>https://arxiv.org/abs/2506.19877</guid>
<content:encoded><![CDATA[
arXiv:2506.19877v2 Announce Type: replace-cross 
Abstract: Identifying suitable machine learning paradigms for intrusion detection remains critical for building effective and generalizable security solutions. In this study, we present a controlled comparison of four representative models - Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN), One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on the CICIDS2017 dataset under two scenarios: detecting known attack types and generalizing to previously unseen threats. Our results show that supervised MLP and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic recall drops on novel attacks. Unsupervised LOF attains moderate overall accuracy and high recall on unknown threats at the cost of elevated false alarms, while boundary-based OCSVM balances precision and recall best, demonstrating robust detection across both scenarios. These findings offer practical guidance for selecting IDS models in dynamic network environments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation</title>
<link>https://arxiv.org/abs/2506.19952</link>
<guid>https://arxiv.org/abs/2506.19952</guid>
<content:encoded><![CDATA[
arXiv:2506.19952v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Adapter Design Tradeoffs for Low Resource Music Generation</title>
<link>https://arxiv.org/abs/2506.21298</link>
<guid>https://arxiv.org/abs/2506.21298</guid>
<content:encoded><![CDATA[
arXiv:2506.21298v2 Announce Type: replace-cross 
Abstract: Fine-tuning large-scale music generation models, such as MusicGen and Mustango, is a computationally expensive process, often requiring updates to billions of parameters and, therefore, significant hardware resources. Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based methods, have emerged as a promising alternative, enabling adaptation with minimal trainable parameters while preserving model performance. However, the design choices for adapters, including their architecture, placement, and size, are numerous, and it is unclear which of these combinations would produce optimal adapters and why, for a given case of low-resource music genre. In this paper, we attempt to answer this question by studying various adapter configurations for two AI music models, MusicGen and Mustango, on two genres: Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in capturing fine-grained local musical details such as ornamentations and short melodic phrases, while transformer-based adapters better preserve long-range dependencies crucial for structured improvisation. Additionally, we analyze computational resource requirements across different adapter scales, demonstrating how mid-sized adapters (40M parameters) achieve an optimal balance between expressivity and quality. Furthermore, we find that Mustango, a diffusion-based model, generates more diverse outputs with better adherence to the description in the input prompt while lacking in providing stability in notes, rhythm alignment, and aesthetics. Also, it is computationally intensive and requires significantly more time to train. In contrast, autoregressive models like MusicGen offer faster training and are more efficient, and can produce better quality output in comparison, but have slightly higher redundancy in their generations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.21931</link>
<guid>https://arxiv.org/abs/2506.21931</guid>
<content:encoded><![CDATA[
arXiv:2506.21931v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v2 Announce Type: replace-cross 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop OptScale, a practical algorithm that dynamically determines the optimal number of sampled responses. OptScale employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that OptScale significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition</title>
<link>https://arxiv.org/abs/2507.03541</link>
<guid>https://arxiv.org/abs/2507.03541</guid>
<content:encoded><![CDATA[
arXiv:2507.03541v2 Announce Type: replace-cross 
Abstract: In this paper, we address the following question: How do generic foundation models (e.g., CLIP, BLIP, GPT-4o, Grok-4) compare against a domain-specific face recognition model (viz., AdaFace or ArcFace) on the face recognition task? Through a series of experiments involving several foundation models and benchmark datasets, we report the following findings: (a) In all face benchmark datasets considered, domain-specific models outperformed zero-shot foundation models. (b) The performance of zero-shot generic foundation models improved on over-segmented face images compared to tightly cropped faces, thereby suggesting the importance of contextual clues. (c) A simple score-level fusion of a foundation model with a domain-specific face recognition model improved the accuracy at low false match rates. (d) Foundation models, such as GPT-4o and Grok-4, are able to provide explainability to the face recognition pipeline. In some instances, foundation models are even able to resolve low-confidence decisions made by AdaFace, thereby reiterating the importance of combining domain-specific face recognition models with generic foundation models in a judicious manner.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning</title>
<link>https://arxiv.org/abs/2507.04106</link>
<guid>https://arxiv.org/abs/2507.04106</guid>
<content:encoded><![CDATA[
arXiv:2507.04106v2 Announce Type: replace-cross 
Abstract: Our research addresses the overlooked security concerns related to data poisoning in continual learning (CL). Data poisoning - the intentional manipulation of training data to affect the predictions of machine learning models - was recently shown to be a threat to CL training stability. While existing literature predominantly addresses scenario-dependent attacks, we propose to focus on a more simple and realistic single-task poison (STP) threats. In contrast to previously proposed poisoning settings, in STP adversaries lack knowledge and access to the model, as well as to both previous and future tasks. During an attack, they only have access to the current task within the data stream. Our study demonstrates that even within these stringent conditions, adversaries can compromise model performance using standard image corruptions. We show that STP attacks are able to strongly disrupt the whole continual training process: decreasing both the stability (its performance on past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm. Finally, we propose a high-level defense framework for CL along with a poison task detection method based on task vectors. The code is available at https://github.com/stapaw/STP.git .
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance</title>
<link>https://arxiv.org/abs/2507.06272</link>
<guid>https://arxiv.org/abs/2507.06272</guid>
<content:encoded><![CDATA[
arXiv:2507.06272v3 Announce Type: replace-cross 
Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in segmentation and comprehension, they still struggle with two limitations: inaccurate segmentation and hallucinated comprehension. These challenges stem primarily from constraints in weak visual comprehension and a lack of fine-grained perception. To alleviate these limitations, we propose LIRA, a framework that capitalizes on the complementary relationship between visual comprehension and segmentation via two key components: (1) Semantic-Enhanced Feature Extractor (SEFE) improves object attribute inference by fusing semantic and pixel-level features, leading to more accurate segmentation; (2) Interleaved Local Visual Coupling (ILVC) autoregressively generates local descriptions after extracting local features based on segmentation masks, offering fine-grained supervision to mitigate hallucinations. Furthermore, we find that the precision of object segmentation is positively correlated with the latent related semantics of the  token. To quantify this relationship and the model's potential semantic inferring ability, we introduce the Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA achieves state-of-the-art performance in both segmentation and comprehension tasks. Code will be available at https://github.com/echo840/LIRA.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data</title>
<link>https://arxiv.org/abs/2507.06828</link>
<guid>https://arxiv.org/abs/2507.06828</guid>
<content:encoded><![CDATA[
arXiv:2507.06828v2 Announce Type: replace-cross 
Abstract: Image denoising is a fundamental task in computer vision, particularly in medical ultrasound (US) imaging, where speckle noise significantly degrades image quality. Although recent advancements in deep neural networks have led to substantial improvements in denoising for natural images, these methods cannot be directly applied to US speckle noise, as it is not purely random. Instead, US speckle arises from complex wave interference within the body microstructure, making it tissue-dependent. This dependency means that obtaining two independent noisy observations of the same scene, as required by pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also cannot handle US speckle noise due to its high spatial dependency. To address this challenge, we introduce Speckle2Self, a novel self-supervised algorithm for speckle reduction using only single noisy observations. The key insight is that applying a multi-scale perturbation (MSP) operation introduces tissue-dependent variations in the speckle pattern across different scales, while preserving the shared anatomical structure. This enables effective speckle suppression by modeling the clean image as a low-rank signal and isolating the sparse noise component. To demonstrate its effectiveness, Speckle2Self is comprehensively compared with conventional filter-based denoising algorithms and SOTA learning-based methods, using both realistic simulated US images and human carotid US images. Additionally, data from multiple US machines are employed to evaluate model generalization and adaptability to images from unseen domains. Project page: https://noseefood.github.io/us-speckle2self/
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching</title>
<link>https://arxiv.org/abs/2507.07192</link>
<guid>https://arxiv.org/abs/2507.07192</guid>
<content:encoded><![CDATA[
arXiv:2507.07192v3 Announce Type: replace-cross 
Abstract: Existing generative models for time series forecasting often transform simple priors (typically Gaussian) into complex data distributions. However, their sampling initialization, independent of historical data, hinders the capture of temporal dependencies, limiting predictive accuracy. They also treat residuals merely as optimization targets, ignoring that residuals often exhibit meaningful patterns like systematic biases or nontrivial distributional structures. To address these, we propose Conditional Guided Flow Matching (CGFM), a novel model-agnostic framework that extends flow matching by integrating outputs from an auxiliary predictive model. This enables learning from the probabilistic structure of prediction residuals, leveraging the auxiliary model's prediction distribution as a source to reduce learning difficulty and refine forecasts. CGFM incorporates historical data as both conditions and guidance, uses two-sided conditional paths (with source and target conditioned on the same history), and employs affine paths to expand the path space, avoiding path crossing without complex mechanisms, preserving temporal consistency, and strengthening distribution alignment. Experiments across datasets and baselines show CGFM consistently outperforms state-of-the-art models, advancing forecasting.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model</title>
<link>https://arxiv.org/abs/2507.08920</link>
<guid>https://arxiv.org/abs/2507.08920</guid>
<content:encoded><![CDATA[
arXiv:2507.08920v3 Announce Type: replace-cross 
Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening</title>
<link>https://arxiv.org/abs/2507.10461</link>
<guid>https://arxiv.org/abs/2507.10461</guid>
<content:encoded><![CDATA[
arXiv:2507.10461v2 Announce Type: replace-cross 
Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SystolicAttention: Fusing FlashAttention within a Single Systolic Array</title>
<link>https://arxiv.org/abs/2507.11331</link>
<guid>https://arxiv.org/abs/2507.11331</guid>
<content:encoded><![CDATA[
arXiv:2507.11331v3 Announce Type: replace-cross 
Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA), typically implemented using the FlashAttention algorithm. However, current systolic-array-based accelerators face significant challenges when executing FlashAttention. Systolic arrays achieve high utilization primarily for consecutive and large matrix multiplications, whereas FlashAttention requires frequent interleaving of matrix multiplications and softmax operations.
  The frequent data swaps between matrix multiplications on the systolic array and softmax operations on external units result in low array utilization. Moreover, when these computations run concurrently, the softmax stage contends with matrix multiplication for register file and SRAM ports, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array architecture that enables the FlashAttention algorithm to run entirely within a single systolic array, eliminating the need for external vector units. At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps FlashAttention operations onto systolic arrays with fine-grained, element-wise overlap. This approach significantly improves array utilization while preserving the original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against state-of-the-art commercial accelerators. Our results show that FSA achieves 1.77 and 4.83 times higher attention FLOPs/s utilization compared to AWS Neuron v2 and Google TPUv5e, respectively, with only 12% area overhead.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?</title>
<link>https://arxiv.org/abs/2507.11569</link>
<guid>https://arxiv.org/abs/2507.11569</guid>
<content:encoded><![CDATA[
arXiv:2507.11569v2 Announce Type: replace-cross 
Abstract: Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation</title>
<link>https://arxiv.org/abs/2507.20568</link>
<guid>https://arxiv.org/abs/2507.20568</guid>
<content:encoded><![CDATA[
arXiv:2507.20568v2 Announce Type: replace-cross 
Abstract: Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: https://cau-irislab.github.io/interspeech25/
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>aLLoyM: A large language model for alloy phase diagram prediction</title>
<link>https://arxiv.org/abs/2507.22558</link>
<guid>https://arxiv.org/abs/2507.22558</guid>
<content:encoded><![CDATA[
arXiv:2507.22558v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are general-purpose tools with wide-ranging applications, including in materials science. In this work, we introduce aLLoyM, a fine-tuned LLM specifically trained on alloy compositions, temperatures, and their corresponding phase information. To develop aLLoyM, we curated question-and-answer (Q&amp;A) pairs for binary and ternary phase diagrams using the open-source Computational Phase Diagram Database (CPDDB) and assessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned Mistral, an open-source pre-trained LLM, for two distinct Q&amp;A formats: multiple-choice and short-answer. Benchmark evaluations demonstrate that fine-tuning substantially enhances performance on multiple-choice phase diagram questions. Moreover, the short-answer model of aLLoyM exhibits the ability to generate novel phase diagrams from its components alone, underscoring its potential to accelerate the discovery of previously unexplored materials systems. To promote further research and adoption, we have publicly released the short-answer fine-tuned version of aLLoyM, along with the complete benchmarking Q&amp;A dataset, on Hugging Face.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model</title>
<link>https://arxiv.org/abs/2507.22854</link>
<guid>https://arxiv.org/abs/2507.22854</guid>
<content:encoded><![CDATA[
arXiv:2507.22854v2 Announce Type: replace-cross 
Abstract: We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations</title>
<link>https://arxiv.org/abs/2507.22919</link>
<guid>https://arxiv.org/abs/2507.22919</guid>
<content:encoded><![CDATA[
arXiv:2507.22919v2 Announce Type: replace-cross 
Abstract: Objectives: With accurate estimates of expected safety results, clinical trials could be better designed and monitored. We evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. Material and Methods: We analyzed 22,107 two-arm parallel interventional clinical trials from ClinicalTrials.gov with structured summary results. Two prediction models were developed: a classifier predicting whether a greater proportion of participants in an experimental arm would have SAEs (area under the receiver operating characteristic curve; AUC) compared to the control arm, and a regression model to predict the proportion of participants with SAEs in the control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with a downstream model for prediction. To maintain semantic representation in long trial texts exceeding localized language model input limits, a sliding window method was developed for embedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC when predicting which trial arm had a higher proportion of SAEs. When predicting SAE proportion in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed direct comparisons. Across 12 classifiers, the average absolute AUC increase was 2.00%, and absolute RMSE reduction was 1.58% across 12 regressors. Discussion: Summary results data from ClinicalTrials.gov remains underutilized. Predicted results of publicly reported trials provides an opportunity to identify discrepancies between expected and reported safety results.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</title>
<link>https://arxiv.org/abs/2508.01713</link>
<guid>https://arxiv.org/abs/2508.01713</guid>
<content:encoded><![CDATA[
arXiv:2508.01713v2 Announce Type: replace-cross 
Abstract: Robot-assisted surgeries rely on accurate and real-time scene understanding to safely guide surgical instruments. However, segmentation models trained on static datasets face key limitations when deployed in these dynamic and evolving surgical environments. Class-incremental semantic segmentation (CISS) allows models to continually adapt to new classes while avoiding catastrophic forgetting of prior knowledge, without training on previous data. In this work, we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularized Incremental Class Segmentation (TOPICS) approach and propose an enhanced variant, termed TOPICS+, specifically tailored for robust segmentation of surgical scenes. Concretely, we incorporate the Dice loss into the hierarchical loss formulation to handle strong class imbalances, introduce hierarchical pseudo-labeling, and design tailored label taxonomies for robotic surgery environments. We also propose six novel CISS benchmarks designed for robotic surgery environments including multiple incremental steps and several semantic categories to emulate realistic class-incremental settings in surgical environments. In addition, we introduce a refined set of labels with more than 144 classes on the Syn-Mediverse synthetic dataset, hosted online as an evaluation benchmark. We make the code and trained models publicly available at http://topics.cs.uni-freiburg.de.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction</title>
<link>https://arxiv.org/abs/2508.02622</link>
<guid>https://arxiv.org/abs/2508.02622</guid>
<content:encoded><![CDATA[
<div> Keywords: Noosemia, AI, intentionality, agency, LLM

Summary:
This paper introduces the concept of Noosemia, a cognitive-phenomenological pattern that arises from human interaction with generative AI systems, particularly in dialogic or multimodal exchanges. The authors propose a framework that explains how users attribute intentionality, agency, and interiority to AI systems based on linguistic performance, epistemic opacity, and technological complexity. They link meaning holism with the LLM Contextual Cognitive Field to show how LLMs construct meaning relationally, leading to coherence and a semblance of agency at the human-AI interface. Noosemia is compared to pareidolia, animism, the intentional stance, and the uncanny valley, highlighting its unique characteristics. The concept of a-noosemia is also introduced to describe the withdrawal of these projections. The paper concludes by discussing the philosophical, epistemological, and social implications of Noosemia and suggests future research directions. 

<br /><br />Summary: 
- Introduction of Noosemia in human interaction with AI systems
- Framework explaining attribution of intentionality and agency to AI
- Linking meaning holism with the LLM Contextual Cognitive Field
- Comparison of Noosemia with other cognitive phenomena
- Introduction of a-noosemia and implications for future research <div>
arXiv:2508.02622v2 Announce Type: replace 
Abstract: This paper introduces and formalizes Noosem\`ia, a novel cognitive-phenomenological pattern emerging from human interaction with generative AI systems, particularly those enabling dialogic or multimodal exchanges. We propose a multidisciplinary framework to explain how, under certain conditions, users attribute intentionality, agency, and even interiority to these systems - a process grounded not in physical resemblance, but in linguistic performance, epistemic opacity, and emergent technological complexity. By linking an LLM declination of meaning holism to our technical notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct meaning relationally and how coherence and a simulacrum of agency arise at the human-AI interface. The analysis situates noosemia alongside pareidolia, animism, the intentional stance and the uncanny valley, distinguishing its unique characteristics. We also introduce a-noosemia to describe the phenomenological withdrawal of such projections. The paper concludes with reflections on the broader philosophical, epistemological and social implications of noosemic dynamics and directions for future research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization</title>
<link>https://arxiv.org/abs/2508.05731</link>
<guid>https://arxiv.org/abs/2508.05731</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Graphical User Interfaces, Reinforcement Learning, Adaptive Exploration Policy Optimization, Semantic Alignment

Summary:
The article discusses the challenge of grounding natural language instructions in Multimodal Large Language Models (MLLMs) operating on Graphical User Interfaces (GUIs). While Reinforcement Learning with Verifiable Rewards (RLVR) has improved spatial alignment, inefficient exploration hinders semantic alignment. To address this, the authors propose Adaptive Exploration Policy Optimization (AEPO) with a multi-answer generation strategy and an Adaptive Exploration Reward function. Their models, InfiGUI-G1-3B and InfiGUI-G1-7B, outperform the RLVR baseline on difficult benchmarks, showcasing improved generalization and semantic understanding. This demonstrates the effectiveness of AEPO in enhancing exploration and improving model performance in GUI grounding tasks.<br /><br />Summary: <div>
arXiv:2508.05731v1 Announce Type: new 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Inherently Safer AGI through Language-Mediated Active Inference</title>
<link>https://arxiv.org/abs/2508.05766</link>
<guid>https://arxiv.org/abs/2508.05766</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial General Intelligence, Active Inference, Large Language Models, Safety, Belief representation

Summary: 
This paper introduces a novel framework for creating safe Artificial General Intelligence (AGI) by merging Active Inference principles with Large Language Models (LLMs). Traditional AI safety methods revolve around post-hoc interpretability and reward engineering, which have limitations. The proposed architecture integrates safety guarantees into the core design of the system through transparent belief representations and hierarchical value alignment. Utilizing natural language for belief representation allows for direct human oversight while maintaining computational feasibility. The multi-agent system implements Active Inference principles where agents self-organize, with safety constraints flowing hierarchically. Specific safety mechanisms include explicit separation of beliefs and preferences in natural language, bounded rationality through resource-aware free energy minimization, and compositional safety through modular agent structures. The research agenda focuses on validating the framework's safety properties through experiments using the Abstraction and Reasoning Corpus (ARC) benchmark. This approach promises safer AGI development without the need for retrofitting safety measures. 

<br /><br />Summary: <div>
arXiv:2508.05766v1 Announce Type: new 
Abstract: This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations. We present an architecture where safety guarantees are integrated into the system's core design through transparent belief representations and hierarchical value alignment. Our framework leverages natural language as a medium for representing and manipulating beliefs, enabling direct human oversight while maintaining computational tractability. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whither symbols in the era of advanced neural networks?</title>
<link>https://arxiv.org/abs/2508.05776</link>
<guid>https://arxiv.org/abs/2508.05776</guid>
<content:encoded><![CDATA[
<div> Keywords: human minds, symbolic systems, neural networks, artificial intelligence, cognitive processes.

Summary:<br /><br /> The article argues that the abilities of modern neural networks and artificial intelligence systems resemble those of human minds in combining ideas, producing novelty, and learning quickly. This challenges the notion that human cognitive processes are solely symbolic in nature. While neural networks are typically trained on data from symbolic systems, they play a crucial role in understanding the abstract problems faced by human minds. The intersection of symbolic systems and neural networks suggests a new direction for research on the symbolic basis of human thought. The authors propose that studying how neural networks mimic human cognitive abilities can provide valuable insights into the nature of human intelligence and the role of symbolic systems in cognitive processes.<br /><br />Summary: <div>
arXiv:2508.05776v1 Announce Type: new 
Abstract: Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making</title>
<link>https://arxiv.org/abs/2508.05792</link>
<guid>https://arxiv.org/abs/2508.05792</guid>
<content:encoded><![CDATA[
<div> Holistic-XAI, explanation, hypothesis testing, causal rating methods, traditional XAI<br />
Summary:<br />
Holistic-XAI (H-XAI) is a new framework that integrates causal rating methods with traditional XAI methods to provide a holistic approach to explainable AI. This framework allows stakeholders to ask questions, test hypotheses, and compare model behavior against random and biased baselines. H-XAI combines instance-level and global explanations to cater to stakeholder-specific goals, such as understanding individual decisions, evaluating group-level bias, or assessing robustness under perturbations. The framework addresses gaps in existing XAI methods by offering a comprehensive solution for explanation as an interactive, multi-method process. Two case studies in binary credit risk classification and financial time-series forecasting demonstrate the versatility and effectiveness of H-XAI in addressing diverse stakeholder needs. <div>
arXiv:2508.05792v1 Announce Type: new 
Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing on justifying model outputs rather than supporting diverse stakeholder needs. A recent shift toward Evaluative AI reframes explanation as a tool for hypothesis testing, but still focuses primarily on operational organizations. We introduce Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods with traditional XAI methods to support explanation as an interactive, multi-method process. H-XAI allows stakeholders to ask a series of questions, test hypotheses, and compare model behavior against automatically constructed random and biased baselines. It combines instance-level and global explanations, adapting to each stakeholder's goals, whether understanding individual decisions, assessing group-level bias, or evaluating robustness under perturbations. We demonstrate the generality of our approach through two case studies spanning six scenarios: binary credit risk classification and financial time-series forecasting. H-XAI fills critical gaps left by existing XAI methods by combining causal ratings and post-hoc explanations to answer stakeholder-specific questions at both the individual decision level and the overall model level.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety of Embodied Navigation: A Survey</title>
<link>https://arxiv.org/abs/2508.05855</link>
<guid>https://arxiv.org/abs/2508.05855</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, embodied AI, navigation, safety, evaluation

Summary: 
Large language models (LLMs) are advancing rapidly, with a focus on embodied AI in navigation scenarios. Safety concerns arise as these systems are deployed in dynamic, real-world environments. This survey comprehensively analyzes safety in embodied navigation, covering attack strategies, defense mechanisms, and evaluation methodologies. It explores existing challenges, mitigation technologies, datasets, and metrics for assessing effectiveness and robustness. Future research directions include addressing potential attack methods, improving mitigation strategies, enhancing evaluation techniques, and implementing verification frameworks. By addressing these gaps, the survey aims to guide the development of safer and more reliable navigation systems. The findings also have broader implications for enhancing societal safety and industrial efficiency. 

<br /><br />Summary: <div>
arXiv:2508.05855v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning</title>
<link>https://arxiv.org/abs/2508.05888</link>
<guid>https://arxiv.org/abs/2508.05888</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, tool retrieval, AI agents, semantic relationships, multi-step tasks

Summary:
The article introduces a Knowledge Graph-based framework for effective tool retrieval in the context of complex user queries. Traditional approaches of tool retrieval based on similarity between user queries and tool descriptions have limitations, especially for multi-step tasks. The proposed framework leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools, improving tool selection for multi-step tasks. Evaluation on a synthetic dataset shows that the tool graph-based method outperforms a non-KG baseline in terms of tool coverage. The results demonstrate that leveraging the structural information in a Knowledge Graph provides complementary signals to similarity matching, particularly for queries requiring sequential tool composition. <div>
arXiv:2508.05888v1 Announce Type: new 
Abstract: Effective tool retrieval is essential for AI agents to select from a vast array of tools when identifying and planning actions in the context of complex user queries. Despite its central role in planning, this aspect remains underexplored in the literature. Traditional approaches rely primarily on similarities between user queries and tool descriptions, which significantly limits retrieval accuracy, specifically when handling multi-step user requests. To address these limitations, we propose a Knowledge Graph (KG)-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies. Our retrieval algorithm leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools, enabling more comprehensive and contextual tool selection for multi-step tasks. We evaluate our approach on a synthetically generated internal dataset across six defined user classes, extending previous work on coherent dialogue synthesis and too retrieval benchmarks. Results demonstrate that our tool graph-based method achieves 91.85% tool coverage on the micro-average Complete Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid retrieval, the strongest non-KG baseline in our experiments. These findings support our hypothesis that the structural information in the KG provides complementary signals to pure similarity matching, particularly for queries requiring sequential tool composition.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making</title>
<link>https://arxiv.org/abs/2508.05996</link>
<guid>https://arxiv.org/abs/2508.05996</guid>
<content:encoded><![CDATA[
<div> agent, medical decision-making, multimodal, collaboration, AI <br />
Summary: <br />
The article introduces MedOrch, a framework for facilitating collaboration among multiple agents in medical decision-making tasks involving both language and vision modalities. It addresses the limitations of individual vision-language models (VLMs) by incorporating a mediator agent based on large language models (LLMs) to guide and enhance their interactions. By leveraging a variety of open-source VLMs, MedOrch demonstrates improved performance in answering medical vision questions through collaborative efforts. The study highlights the importance of mediator-guided multi-agent collaboration in advancing the field of medical multimodal intelligence. The results emphasize the effectiveness of combining diverse models without the need for additional training, showcasing the potential of heterogeneous model ensembles for complex medical workflows. The code for MedOrch will be shared publicly for further research and development. <br /> <div>
arXiv:2508.05996v1 Announce Type: new 
Abstract: Complex medical decision-making involves cooperative workflows operated by different clinicians. Designing AI multi-agent systems can expedite and augment human-level clinical decision-making. Existing multi-agent researches primarily focus on language-only tasks, yet their extension to multimodal scenarios remains challenging. A blind combination of diverse vision-language models (VLMs) can amplify an erroneous outcome interpretation. VLMs in general are less capable in instruction following and importantly self-reflection, compared to large language models (LLMs) of comparable sizes. This disparity largely constrains VLMs' ability in cooperative workflows. In this study, we propose MedOrch, a mediator-guided multi-agent collaboration framework for medical multimodal decision-making. MedOrch employs an LLM-based mediator agent that enables multiple VLM-based expert agents to exchange and reflect on their outputs towards collaboration. We utilize multiple open-source general-purpose and domain-specific VLMs instead of costly GPT-series models, revealing the strength of heterogeneous models. We show that the collaboration within distinct VLM-based agents can surpass the capabilities of any individual agent. We validate our approach on five medical vision question answering benchmarks, demonstrating superior collaboration performance without model training. Our findings underscore the value of mediator-guided multi-agent collaboration in advancing medical multimodal intelligence. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning</title>
<link>https://arxiv.org/abs/2508.06042</link>
<guid>https://arxiv.org/abs/2508.06042</guid>
<content:encoded><![CDATA[
<div> Hierarchical Multi-Agent Framework, Imitation Learning Agents, Strategic Planner, Specialized Strategies, StarCraftII <br />
<br />
HIMA, a new hierarchical multi-agent framework, combines specialized imitation learning agents with a meta-controller to address challenges in real-time strategic games like StarCraftII. Each agent learns a distinct strategy from expert demonstrations and generates structured action sequences, which the Strategic Planner then integrates into an adaptive plan aligning local decisions with long-term strategies. The TEXTSCII-ALL testbed covers all race match combinations in SC2. Empirical results demonstrate that HIMA surpasses existing approaches in strategic clarity, adaptability, and computational efficiency, showcasing the efficacy of combining specialized imitation modules with meta-level orchestration to develop versatile AI agents. <br /><br />Summary: <div>
arXiv:2508.06042v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently demonstrated impressive action sequence prediction capabilities but often struggle with dynamic, long-horizon tasks such as real-time strategic games. In a game such as StarCraftII (SC2), agents need to manage resource constraints and adapt to evolving battlefield situations in a partially observable environment. This often overwhelms exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a meta-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coherent, structured multistep action sequences. The SP then orchestrates these proposals into a single, environmentally adaptive plan that ensures local decisions aligning with long-term strategies. We call this HIMA (Hierarchical Imitation Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that encompasses all race match combinations in SC2. Our empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with meta-level orchestration to develop more robust, general-purpose AI agents.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences</title>
<link>https://arxiv.org/abs/2508.06060</link>
<guid>https://arxiv.org/abs/2508.06060</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Participatory Budgeting, Resource Allocation, Reasoning Capabilities, Prompt Design

Summary:<br /><br />Large Language Models (LLMs) are being utilized for structured resource allocation tasks within the framework of Participatory Budgeting. Three prompting strategies are employed for LLMs: greedy selection, direct optimization, and a hill-climbing-inspired refinement. The study evaluates LLMs' allocations against a utility-maximizing oracle and tests their ability to infer structured preferences from voter input or metadata without explicit votes. The results highlight the importance of prompt design and demonstrate that LLMs show promise in mechanism design with unstructured inputs. Additionally, the research addresses challenges in evaluating LLMs' reasoning due to data contamination and static benchmarks, providing insights into their potential for complex decision-making tasks. <div>
arXiv:2508.06060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly expected to handle complex decision-making tasks, yet their ability to perform structured resource allocation remains underexplored. Evaluating their reasoning is also difficult due to data contamination and the static nature of existing benchmarks. We present a dual-purpose framework leveraging Participatory Budgeting (PB) both as (i) a practical setting for LLM-based resource allocation and (ii) an adaptive benchmark for evaluating their reasoning capabilities. We task LLMs with selecting project subsets under feasibility (e.g., budget) constraints via three prompting strategies: greedy selection, direct optimization, and a hill-climbing-inspired refinement. We benchmark LLMs' allocations against a utility-maximizing oracle. Interestingly, we also test whether LLMs can infer structured preferences from natural-language voter input or metadata, without explicit votes. By comparing allocations based on inferred preferences to those from ground-truth votes, we evaluate LLMs' ability to extract preferences from open-ended input. Our results underscore the role of prompt design and show that LLMs hold promise for mechanism design with unstructured inputs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Forget Imagination!</title>
<link>https://arxiv.org/abs/2508.06062</link>
<guid>https://arxiv.org/abs/2508.06062</guid>
<content:encoded><![CDATA[
<div> cognitive imagination, human thinking, artificial intelligence, semantic models, probabilistic causal relationships

Summary:<br />
- Cognitive imagination is a crucial aspect of human thinking, allowing for the visualization of coherent systems of concepts and causal links.
- This type of imagination is essential for reasoning, decision making, and prediction, as it provides a semantic context for these processes.
- The role of cognitive imagination is currently undervalued in artificial intelligence, limiting its capabilities.
- Semantic models are proposed as a tool for simulating cognitive imagination, based on probabilistic causal relationships and the consistency of imaginary contexts.
- These models mimic cognitive imagination by ensuring coherence and holism in the manipulation of interrelated facts within a context. 

Summary: <div>
arXiv:2508.06062v1 Announce Type: new 
Abstract: Cognitive imagination is a type of imagination that plays a key role in human thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to mentally visualize coherent and holistic systems of concepts and causal links that serve as semantic contexts for reasoning, decision making and prediction. Our position is that the role of cognitive imagination is still greatly underestimated, and this creates numerous problems and diminishes the current capabilities of AI. For instance, when reasoning, humans rely on imaginary contexts to retrieve background info. They also constantly return to the context for semantic verification that their reasoning is still reasonable. Thus, reasoning without imagination is blind. This paper is a call for greater attention to cognitive imagination as the next promising breakthrough in artificial intelligence. As an instrument for simulating cognitive imagination, we propose semantic models -- a new approach to mathematical models that can learn, like neural networks, and are based on probabilistic causal relationships. Semantic models can simulate cognitive imagination because they ensure the consistency of imaginary contexts and implement a glass-box approach that allows the context to be manipulated as a holistic and coherent system of interrelated facts glued together with causal relations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Complete Anytime Beam Search for Optimal Decision Tree</title>
<link>https://arxiv.org/abs/2508.06064</link>
<guid>https://arxiv.org/abs/2508.06064</guid>
<content:encoded><![CDATA[
<div> Algorithm, decision tree, anytime, heuristics, optimization

Summary:
- The paper discusses the difficulty of finding an optimal decision tree to minimize classification error due to its NP-hard nature.
- Several anytime extensions of exact algorithms have been proposed to address this challenge but have not been systematically compared.
- The authors introduce CA-DL8.5, a generic and complete beam search algorithm that extends the DL8.5 framework, allowing for the integration of various heuristics and relaxation mechanisms.
- CA-DL8.5 combines efficient branch-and-bound pruning with a restart-based beam search that gradually relaxes criteria to improve solution quality.
- Empirical comparisons show that CA-DL8.5 using limited discrepancy heuristics consistently provides the best anytime performance, outperforming other variants and the Blossom algorithm while maintaining completeness and optimality guarantees.<br /><br />Summary: <div>
arXiv:2508.06064v1 Announce Type: new 
Abstract: Finding an optimal decision tree that minimizes classification error is known to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic programming guarantee optimality, they often suffer from poor anytime behavior -- meaning they struggle to find high-quality decision trees quickly when the search is stopped before completion -- due to unbalanced search space exploration. To address this, several anytime extensions of exact methods have been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not been systematically compared, making it difficult to assess their relative effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and anytime beam search algorithm that extends the DL8.5 framework and unifies some existing anytime strategies. In particular, CA-DL8.5 generalizes previous approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various heuristics and relaxation mechanisms through a modular design. The algorithm reuses DL8.5's efficient branch-and-bound pruning and trie-based caching, combined with a restart-based beam search that gradually relaxes pruning criteria to improve solution quality over time. Our contributions are twofold: (1) We introduce this new generic framework for exact and anytime decision tree learning, enabling the incorporation of diverse heuristics and search strategies; (2) We conduct a rigorous empirical comparison of several instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k heuristics -- using an anytime evaluation metric called the primal gap integral. Experimental results on standard classification benchmarks show that CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime performance, outperforming both other CA-DL8.5 variants and the Blossom algorithm while maintaining completeness and optimality guarantees.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception</title>
<link>https://arxiv.org/abs/2508.06074</link>
<guid>https://arxiv.org/abs/2508.06074</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, deep reinforcement learning, bird's-eye view perception, Mamba-BEV model, ME^3-BEV framework

Summary:
The paper introduces a novel approach to autonomous driving using deep reinforcement learning (DRL) with bird's-eye view (BEV) perception. The Mamba-BEV model combines BEV-based perception with the Mamba framework for effective feature extraction. This integration allows for encoding vehicle surroundings and road features in a unified coordinate system and modeling long-range dependencies accurately. The ME^3-BEV framework utilizes the Mamba-BEV model for end-to-end DRL, showing better performance in dynamic urban driving situations. The model's interpretability is enhanced through visualizing high-dimensional features with semantic segmentation, providing insight into learned representations. Extensive experiments on the CARLA simulator demonstrate that the ME^3-BEV outperforms existing models in collision rate and trajectory accuracy, showing promise for real-time autonomous driving.

<br /><br />Summary: <div>
arXiv:2508.06074v1 Announce Type: new 
Abstract: Autonomous driving systems face significant challenges in perceiving complex environments and making real-time decisions. Traditional modular approaches, while offering interpretability, suffer from error propagation and coordination issues, whereas end-to-end learning systems can simplify the design but face computational bottlenecks. This paper presents a novel approach to autonomous driving using deep reinforcement learning (DRL) that integrates bird's-eye view (BEV) perception for enhanced real-time decision-making. We introduce the \texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction network that combines BEV-based perception with the Mamba framework for temporal feature modeling. This integration allows the system to encode vehicle surroundings and road features in a unified coordinate system and accurately model long-range dependencies. Building on this, we propose the \texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a feature input for end-to-end DRL, achieving superior performance in dynamic urban driving scenarios. We further enhance the interpretability of the model by visualizing high-dimensional features through semantic segmentation, providing insight into the learned representations. Extensive experiments on the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing models across multiple metrics, including collision rate and trajectory accuracy, offering a promising solution for real-time autonomous driving.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2</title>
<link>https://arxiv.org/abs/2508.06091</link>
<guid>https://arxiv.org/abs/2508.06091</guid>
<content:encoded><![CDATA[
<div> expressive power, graph neural networks, logical languages, graded modal logic, C2 <br />
Summary: 
This paper addresses the question of the logical expressiveness of aggregate-combine-readout Graph Neural Networks (GNNs) in relation to the graded modal logic C2. Building on previous research, the authors show that the logical expressiveness of aggregate-combine-readout GNNs surpasses that of C2, solving an open problem in the field. This result applies to both undirected and directed graphs, providing insights not only into the capabilities of GNNs but also into the expressive power of infinitary logics. The study contributes to a better understanding of the relationship between GNNs and logical languages, shedding light on the capabilities and limitations of these AI models. <div>
arXiv:2508.06091v1 Announce Type: new 
Abstract: In recent years, there has been growing interest in understanding the expressive power of graph neural networks (GNNs) by relating them to logical languages. This research has been been initialised by an influential result of Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded fragment of the logic C2), characterises the logical expressiveness of aggregate-combine GNNs. As a ``challenging open problem'' they left the question whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs. This question has remained unresolved despite several attempts. In this paper, we solve the above open problem by proving that the logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2. This result holds over both undirected and directed graphs. Beyond its implications for GNNs, our work also leads to purely logical insights on the expressive power of infinitary logics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion</title>
<link>https://arxiv.org/abs/2508.06110</link>
<guid>https://arxiv.org/abs/2508.06110</guid>
<content:encoded><![CDATA[
<div> Keywords: Table reasoning, LLMs, PanelTR, structured scientific approach, zero-shot context

Summary:
PanelTR is a framework that utilizes LLM agent scientists for robust table reasoning through a structured scientific approach. It addresses the limitations of annotated data and data augmentation in table reasoning tasks. The workflow involves agent scientists conducting individual investigations, self-review, and collaborative peer-review discussions. Five scientist personas drive the process, enabling semantic-level transfer without relying on data augmentation or parametric optimization. Experimental results show that PanelTR outperforms vanilla LLMs and rivals fully supervised models across four benchmarks. The framework demonstrates the effectiveness of structured scientific methodology in handling complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context. <br /><br />Summary: PanelTR utilizes LLM agent scientists for table reasoning, overcoming limitations of annotated data. Its structured scientific approach and collaborative peer-review process enable semantic-level transfer without data augmentation. Experimental results show superior performance over vanilla LLMs and rivaling fully supervised models. The framework showcases the potential of structured scientific methodology for complex tasks in a zero-shot context. <div>
arXiv:2508.06110v1 Announce Type: new 
Abstract: Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTR's workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</title>
<link>https://arxiv.org/abs/2508.06111</link>
<guid>https://arxiv.org/abs/2508.06111</guid>
<content:encoded><![CDATA[
<div> Keywords: evaluation framework, large language models, verifiable tasks, scalability, automated

Summary:<br /><br />Evaluating large language models (LLMs) is crucial, but current methods are hindered by the need for domain expertise. To address this, SKATE introduces a novel evaluation framework where LLMs compete by generating and solving verifiable tasks for each other. This approach treats evaluation as a game, with models acting as task-setters and solvers. SKATE offers scalability, objectivity, and open-endedness, as it is fully automated, data-free, and scalable without requiring human input. By using verifiable tasks and a TrueSkill-based ranking system, SKATE can accurately differentiate between models and highlight their strengths and weaknesses. The framework reveals that weaker models can distinguish and score stronger ones, LLM-based systems exhibit self-preferencing behavior, and SKATE automatically identifies fine-grained capability differences. This innovative approach represents a significant step towards developing general and scalable evaluation frameworks for keeping pace with LLM progress.

Summary: <div>
arXiv:2508.06111v1 Announce Type: new 
Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet current methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve. We introduce SKATE: a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. Our core insight is to treat evaluation as a game: models act as both task-setters and solvers, incentivized to create questions which highlight their own strengths while exposing others' weaknesses. SKATE offers several key advantages, balancing scalability, open-endedness, and objectivity. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. By using verifiable tasks rather than LLM judges, scoring is objective. Unlike domain-limited programmatically-generated benchmarks (e.g. chess-playing or spatial reasoning), having LLMs creatively pose challenges enables open-ended and scalable evaluation. As a proof of concept, we introduce LLM-set code-output-prediction (COP) challenges as a verifiable and extensible framework in which to test our approach. Using a TrueSkill-based ranking system, we evaluate six frontier LLMs and find that: (1) weaker models can reliably differentiate and score stronger ones, (2) LLM-based systems are capable of self-preferencing behavior, generating questions that align with their own capabilities, and (3) SKATE automatically surfaces fine-grained capability differences between models. Our findings are an important step towards general, scalable evaluation frameworks which can keep pace with LLM progress.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2508.06129</link>
<guid>https://arxiv.org/abs/2508.06129</guid>
<content:encoded><![CDATA[
<div> machine learning, vehicle routing problem, metaheuristic algorithms, feature importance analysis, sensitivity analysis

Summary:
This study explores the use of machine learning methods to improve metaheuristic algorithms for solving the Vehicle Routing Problem (VRP). By conducting a sensitivity analysis with multiple classifier models, the researchers identify key features that predict the quality of VRP solutions. They leverage explainable AI to gain insights into how these models make decisions and propose a unified framework for ranking feature impact across different scenarios. The findings reveal that certain features consistently emerge as strong predictors, highlighting the potential of feature importance analysis in guiding the development of more efficient VRP-solving algorithms. <div>
arXiv:2508.06129v1 Announce Type: new 
Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with numerous real-world applications, mostly solved using metaheuristic algorithms due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely on human-crafted designs developed through empirical studies. However, recent research shows that machine learning methods can be used the structural characteristics of solutions in combinatorial optimization, thereby aiding in designing more efficient algorithms, particularly for solving VRP. Building on this advancement, this study extends the previous research by conducting a sensitivity analysis using multiple classifier models that are capable of predicting the quality of VRP solutions. Hence, by leveraging explainable AI, this research is able to extend the understanding of how these models make decisions. Finally, our findings indicate that while feature importance varies, certain features consistently emerge as strong predictors. Furthermore, we propose a unified framework able of ranking feature impact across different scenarios to illustrate this finding. These insights highlight the potential of feature importance analysis as a foundation for developing a guidance mechanism of metaheuristic algorithms for solving the VRP.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications</title>
<link>https://arxiv.org/abs/2508.06145</link>
<guid>https://arxiv.org/abs/2508.06145</guid>
<content:encoded><![CDATA[
<div> age groups, pregnancy, concomitant drug use, large language models, drug contraindications <br />
Summary: <br />
- Study focuses on enhancing large language models (LLMs) for addressing pharmaceutical contraindications in healthcare.
- Implementing Retrieval Augmented Generation (RAG) pipeline with OpenAI's GPT-4o-mini base model and text-embedding-3-small model.
- Leveraging Drug Utilization Review (DUR) data for contraindications related to age groups, pregnancy, and concomitant drug use.
- Dataset of 300 question-answer pairs across three categories with baseline model accuracy ranging from 0.49 to 0.57.
- Post-integration of RAG pipeline, significant improvement in model accuracy observed with rates of 0.94, 0.87, and 0.89 for different contraindication categories. <br /> <div>
arXiv:2508.06145v1 Announce Type: new 
Abstract: The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution</title>
<link>https://arxiv.org/abs/2508.06225</link>
<guid>https://arxiv.org/abs/2508.06225</guid>
<content:encoded><![CDATA[
<div> confidence, risk-aware, evaluation, overconfidence phenomenon, LLM-as-a-Judge

Summary:
The paper discusses the importance of well-calibrated confidence in Large Language Models (LLMs) used as automated judges. It highlights the issue of the Overconfidence Phenomenon in current LLM-as-a-Judges, where predicted confidence exceeds actual correctness, leading to unreliable judgments. The proposed TH-Score metric quantifies this phenomenon, while the LLM-as-a-Fuser ensemble framework aims to improve calibration and reliability in evaluation pipelines. Experimental results show that the approach enhances confidence-driven evaluation, achieving better accuracy and reliability compared to existing methods. Overall, the paper advocates for a shift towards confidence-driven, risk-aware LLM-as-a-Judge systems to ensure trustworthy and adaptive evaluations. 

<br /><br />Summary: <div>
arXiv:2508.06225v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used as automated judges, where practical value depends on both accuracy and trustworthy, risk-aware judgments. Existing approaches predominantly focus on accuracy, overlooking the necessity of well-calibrated confidence, which is vital for adaptive and reliable evaluation pipelines. In this work, we advocate a shift from accuracy-centric evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing the necessity of well-calibrated confidence for trustworthy and adaptive evaluation. We systematically identify the **Overconfidence Phenomenon** in current LLM-as-a-Judges, where predicted confidence significantly overstates actual correctness, undermining reliability in practical deployment. To quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an ensemble framework that transforms LLMs into reliable, risk-aware evaluators. Extensive experiments demonstrate that our approach substantially improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</title>
<link>https://arxiv.org/abs/2508.06226</link>
<guid>https://arxiv.org/abs/2508.06226</guid>
<content:encoded><![CDATA[
<div> Geometry problem solving, Multimodal Large Language Models, GeoLaux benchmark, reasoning steps, auxiliary line construction <br />
<br />
Summary: 
The study introduces the GeoLaux benchmark to evaluate Multimodal Large Language Models' (MLLMs) geometry skills. The benchmark consists of 2,186 geometry problems, including calculation and proving questions. It necessitates an average of 6.51 reasoning steps, with auxiliary line construction in 41.8% of the problems. The evaluation strategy encompasses answer correctness, process correctness, process quality, auxiliary line impact, and error causes. The experiments on 13 MLLMs reveal significant performance drops in extended reasoning steps, models taking shortcuts in solving proving problems, and lack of auxiliary line awareness. Enhancing auxiliary line capabilities proves beneficial for overall geometry reasoning improvement. This work establishes GeoLaux as a comprehensive benchmark for MLLMs' geometric reasoning with auxiliary lines, providing insights for capability advancement. <div>
arXiv:2508.06226v1 Announce Type: new 
Abstract: Geometry problem solving (GPS) requires models to master diagram comprehension, logical reasoning, knowledge application, numerical computation, and auxiliary line construction. This presents a significant challenge for Multimodal Large Language Models (MLLMs). However, existing benchmarks for evaluating MLLM geometry skills overlook auxiliary line construction and lack fine-grained process evaluation, making them insufficient for assessing MLLMs' long-step reasoning abilities. To bridge these gaps, we present the GeoLaux benchmark, comprising 2,186 geometry problems, incorporating both calculation and proving questions. Notably, the problems require an average of 6.51 reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary line construction. Building on the dataset, we design a novel five-dimensional evaluation strategy assessing answer correctness, process correctness, process quality, auxiliary line impact, and error causes. Extensive experiments on 13 leading MLLMs (including thinking models and non-thinking models) yield three pivotal findings: First, models exhibit substantial performance degradation in extended reasoning steps (nine models demonstrate over 50% performance drop). Second, compared to calculation problems, MLLMs tend to take shortcuts when solving proving problems. Third, models lack auxiliary line awareness, and enhancing this capability proves particularly beneficial for overall geometry reasoning improvement. These findings establish GeoLaux as both a benchmark for evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a guide for capability advancement. Our dataset and code are included in supplementary materials and will be released.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Logical Rules using Minimum Message Length</title>
<link>https://arxiv.org/abs/2508.06230</link>
<guid>https://arxiv.org/abs/2508.06230</guid>
<content:encoded><![CDATA[
<div> Bayesian inductive logic programming, minimum message length programs, noisy data, hypothesis complexity, data fit<br />
<br />
Summary: 
The article introduces a Bayesian inductive logic programming approach that learns minimum message length programs from noisy data. This approach balances hypothesis complexity and data fit using priors that favor more general programs and a likelihood that favors accurate programs. In experiments across various domains such as game playing and drug design, the method outperforms previous approaches, particularly those based on learning minimum description length programs. The results also demonstrate that the approach is data-efficient and not affected by example imbalance, including the ability to learn from only positive examples. <div>
arXiv:2508.06230v1 Announce Type: new 
Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We introduce a Bayesian inductive logic programming approach that learns minimum message length programs from noisy data. Our approach balances hypothesis complexity and data fit through priors, which explicitly favour more general programs, and a likelihood that favours accurate programs. Our experiments on several domains, including game playing and drug design, show that our method significantly outperforms previous methods, notably those that learn minimum description length programs. Our results also show that our approach is data-efficient and insensitive to example balance, including the ability to learn from exclusively positive examples.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry breaking for inductive logic programming</title>
<link>https://arxiv.org/abs/2508.06263</link>
<guid>https://arxiv.org/abs/2508.06263</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive logic programming, hypothesis space, symmetries, answer set programming, solving times reduction

Summary:
Inductive logic programming aims to find a hypothesis that can generalize training data and background knowledge. The main challenge lies in the vast hypothesis space and the existence of logically equivalent hypotheses. This paper introduces a method to overcome these challenges by breaking symmetries in the hypothesis space using answer set programming. Experimental results in various domains, such as visual reasoning and game playing, demonstrate that this approach significantly reduces solving times, from over an hour to just 17 seconds. This breakthrough in streamlining the search for hypotheses has the potential to improve efficiency and effectiveness in tasks that rely on inductive logic programming. 

<br /><br />Summary: <div>
arXiv:2508.06263v1 Announce Type: new 
Abstract: The goal of inductive logic programming is to search for a hypothesis that generalises training data and background knowledge. The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist. To address this challenge, we introduce a method to break symmetries in the hypothesis space. We implement our idea in answer set programming. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce solving times from over an hour to just 17 seconds.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Robustness Leaderboard v1 --Technical report</title>
<link>https://arxiv.org/abs/2508.06296</link>
<guid>https://arxiv.org/abs/2508.06296</guid>
<content:encoded><![CDATA[
<div> adversarial optimization, LLMs, robustness metric, vulnerability analysis, collaborative evaluation

Summary:
The technical report introduces the PRISM Eval Behavior Elicitation Tool (BET), which uses Dynamic Adversarial Optimization to achieve a 100% Attack Success Rate against 37 out of 41 state-of-the-art Large Language Models (LLMs). A fine-grained robustness metric is proposed to estimate the average number of attempts needed to elicit harmful behaviors, highlighting significant variations in attack difficulty across models despite their universal vulnerability. Primitive-level vulnerability analysis is introduced to identify the most effective jailbreaking techniques for specific hazard categories. The collaborative evaluation with trusted third parties from the AI Safety Network showcases practical pathways for distributed robustness assessment within the community. <div>
arXiv:2508.06296v1 Announce Type: new 
Abstract: This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A "good regulator theorem" for embodied agents</title>
<link>https://arxiv.org/abs/2508.06326</link>
<guid>https://arxiv.org/abs/2508.06326</guid>
<content:encoded><![CDATA[
<div> belief updating, regulation task, model, observer, environment

Summary:
The article challenges Conant and Ashby's theorem that every good regulator must be a model of the system by introducing the concept of belief updating by an observer. It suggests that whenever an agent performs a regulation task, an observer can interpret it as having beliefs about its environment and updating these beliefs in response to sensory input. This perspective offers a more sophisticated notion of a model than Conant and Ashby's, implying that models are imposed on systems from outside by observers. The theorem proposed in this new framework is applicable regardless of whether the system is regulating its environment or internal state. Even apparent counterexamples to the original theorem can be resolved by considering the model of the environment, which may be trivial. This shift in perspective acknowledges the crucial role of the observer in determining and interpreting models within systems. <div>
arXiv:2508.06326v1 Announce Type: new 
Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a system must be a model of that system." Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup. Nevertheless, here we show that a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable. However, it necessitates a change in perspective, in that the observer plays an essential role in the theory: models are not a mere property of the system but are imposed on it from outside. Our theorem holds regardless of whether the system is regulating its environment in a classic control theory setup, or whether it's regulating its own internal state; the model is of its environment either way. The model might be trivial, however, and this is how the apparent counterexamples are resolved.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</title>
<link>https://arxiv.org/abs/2508.06348</link>
<guid>https://arxiv.org/abs/2508.06348</guid>
<content:encoded><![CDATA[
<div> transformer-based machine learning model, cheating detection, online video games, Counter-Strike 2, dataset

Summary:
AntiCheatPT\_256 is a transformer-based machine learning model developed to detect cheating in Counter-Strike 2 by analyzing gameplay data. The model was trained on a newly introduced dataset called CS2CD, which contains labeled matches. A total of 90,707 context windows were created and augmented to address class imbalance. The transformer model achieved an accuracy of 89.17% and an AUC of 93.36% on a test set. The approach prioritizes reproducibility and real-world applicability, providing a strong foundation for future research in cheat detection in online gaming. <div>
arXiv:2508.06348v1 Announce Type: new 
Abstract: Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users' systems. This paper presents AntiCheatPT\_256, a transformer-based machine learning model designed to detect cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using this dataset, 90,707 context windows were created and subsequently augmented to address class imbalance. The transformer model, trained on these windows, achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test set. This approach emphasizes reproducibility and real-world applicability, offering a robust baseline for future research in data-driven cheat detection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</title>
<link>https://arxiv.org/abs/2508.06352</link>
<guid>https://arxiv.org/abs/2508.06352</guid>
<content:encoded><![CDATA[
<div> XAI, Explanatory AI, generative AI, human understanding, algorithmic transparency <br />
Summary: Current approaches to explainable AI (XAI) focus on algorithmic transparency, but often fail to provide meaningful explanations for end-users. This paper introduces Explanatory AI as a new paradigm that leverages generative AI to act as partners in explaining AI decisions to humans. Unlike XAI, Explanatory AI emphasizes contextual reasoning to support human decision-making in sociotechnical contexts. The authors propose a systematic eight-dimensional conceptual model that highlights the importance of narrative communication, adaptive personalization, and progressive disclosure in explanations. Empirical validation with healthcare professionals shows a preference for context-sensitive, multimodal explanations over technical transparency. The study suggests a need for AI systems designed for human comprehension rather than algorithmic introspection, calling for a research agenda to advance user-centered AI explanations across various domains and cultural contexts. <br /><br /> <div>
arXiv:2508.06352v1 Announce Type: new 
Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency and present explanations in abstract, non-adaptive formats that often fail to support meaningful end-user understanding. This paper introduces "Explanatory AI" as a complementary paradigm that leverages generative AI capabilities to serve as explanatory partners for human understanding rather than providers of algorithmic transparency. While XAI reveals algorithmic decision processes for model validation, Explanatory AI addresses contextual reasoning to support human decision-making in sociotechnical contexts. We develop a definition and systematic eight-dimensional conceptual model distinguishing Explanatory AI through narrative communication, adaptive personalization, and progressive disclosure principles. Empirical validation through Rapid Contextual Design methodology with healthcare professionals demonstrates that users consistently prefer context-sensitive, multimodal explanations over technical transparency. Our findings reveal the practical urgency for AI systems designed for human comprehension rather than algorithmic introspection, establishing a comprehensive research agenda for advancing user-centered AI explanation approaches across diverse domains and cultural contexts.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned</title>
<link>https://arxiv.org/abs/2508.06368</link>
<guid>https://arxiv.org/abs/2508.06368</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Knowledge Graphs, legal cases, violence against women, machine learning, predictive justice

Summary: 
Legal Knowledge Graphs (KGs) are essential for facilitating access to comprehensive legal information and supporting the decision-making process in legal cases. This study addresses the lack of legal KGs by developing a legal KG focusing on cases of violence against women. Two approaches, a bottom-up methodology tailored for the legal domain and a new solution using Large Language Models, were employed to construct the KGs. By integrating structured data extraction, ontology development, and semantic enrichment, the developed KGs provide a valuable resource for legal experts and machine learning applications. The validation of the KGs through competency questions further enhances their reliability and usefulness. This work contributes to improving the accessibility of legal information, enabling complex queries, and potentially supporting predictive justice tools. <div>
arXiv:2508.06368v1 Announce Type: new 
Abstract: Legal decision-making process requires the availability of comprehensive and detailed legislative background knowledge and up-to-date information on legal cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a valuable tool to facilitate access to legal information, to be queried and exploited for the purpose, and to enable advanced reasoning and machine learning applications. Indeed, legal KGs may act as knowledge intensive component to be used by pre-dictive machine learning solutions supporting the decision process of the legal expert. Nevertheless, a few KGs can be found in the legal domain. To fill this gap, we developed a legal KG targeting legal cases of violence against women, along with clear adopted methodologies. Specifically, the paper introduces two complementary approaches for automated legal KG construction; a systematic bottom-up approach, customized for the legal domain, and a new solution leveraging Large Language Models. Starting from legal sentences publicly available from the European Court of Justice, the solutions integrate structured data extraction, ontology development, and semantic enrichment to produce KGs tailored for legal cases involving violence against women. After analyzing and comparing the results of the two approaches, the developed KGs are validated via suitable competency questions. The obtained KG may be impactful for multiple purposes: can improve the accessibility to legal information both to humans and machine, can enable complex queries and may constitute an important knowledge component to be possibly exploited by machine learning tools tailored for predictive justice.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Fair Game: Auditing &amp; Debiasing AI Algorithms Over Time</title>
<link>https://arxiv.org/abs/2508.06443</link>
<guid>https://arxiv.org/abs/2508.06443</guid>
<content:encoded><![CDATA[
<div> Keywords: Fair Machine Learning, bias, Fair Game, Auditor, Debiasing algorithm

Summary:
Fair Machine Learning (ML) seeks to address bias in algorithmic predictions. Existing bias definitions rely on observational data and can only be applied after deployment, creating a gap in real-time fairness. "Fair Game" introduces a dynamic mechanism for ensuring fairness in ML predictions by combining an Auditor and a Debiasing algorithm in a Reinforcement Learning loop. This framework allows for adaptive fairness goals by modifying the auditor and biases quantified. By simulating the evolution of ethical and legal frameworks in society, "Fair Game" enables the development of flexible and adaptive Fair ML systems both pre- and post-deployment. <div>
arXiv:2508.06443v1 Announce Type: new 
Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The "Fair Game" puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. "Fair Game" provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting</title>
<link>https://arxiv.org/abs/2508.06454</link>
<guid>https://arxiv.org/abs/2508.06454</guid>
<content:encoded><![CDATA[
<div> Keywords: Committee-selection problems, multi-winner voting rules, axioms, preference distributions, neural networks

Summary: 
Committee-selection problems are prevalent in various applications, prompting interest in the social choice research community to assess multi-winner voting rules' adherence to axioms. A new data-driven framework is proposed to evaluate the frequency of axiom violations across diverse preference distributions, moving beyond traditional worst-case analysis. By applying this framework, the relationship between voting rules and their axiomatic performance across multiple preference distributions is examined. Surprisingly, neural networks as voting rules exhibit superior performance in minimizing axiom violations compared to traditional rules. These findings suggest that leveraging data-driven approaches in social choice can enhance the development of novel voting systems and advocate for the continued exploration of data-driven research in social choice. 

<br /><br />Summary: <div>
arXiv:2508.06454v1 Announce Type: new 
Abstract: Committee-selection problems arise in many contexts and applications, and there has been increasing interest within the social choice research community on identifying which properties are satisfied by different multi-winner voting rules. In this work, we propose a data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions in practice, shifting away from the binary perspective of axiom satisfaction given by worst-case analysis. Using this framework, we analyze the relationship between multi-winner voting rules and their axiomatic performance under several preference distributions. We then show that neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations. Our results suggest that data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient</title>
<link>https://arxiv.org/abs/2304.04475</link>
<guid>https://arxiv.org/abs/2304.04475</guid>
<content:encoded><![CDATA[
<div> Keywords: pandemic, interventions, optimization, policy, simulation

Summary:
In response to the pandemic, various measures have been implemented such as lockdowns, vaccination programs, school closures, and economic stimulus. However, these interventions can have both positive and unintended negative consequences. Current research on determining optimal interventions is limited by simulation objectives, scale, model types, and the number of strategies explored. To address these challenges, a Deep Deterministic Policy Gradient (DDPG) based policy optimization framework was utilized on a large-scale epidemiological agent-based simulation. Multi-objective optimization was performed to determine the optimal policy for lockdown and vaccination in a minimalist age-stratified multi-vaccine scenario. Results showed that a balance between economic well-being and health objectives could be achieved with no lockdown and vaccination for mid-age and elderly individuals. Further validation of these results through in-depth simulations is necessary, and the framework used in this study will be made open-source. 

<br /><br />Summary: <div>
arXiv:2304.04475v1 Announce Type: cross 
Abstract: To mitigate the impact of the pandemic, several measures include lockdowns, rapid vaccination programs, school closures, and economic stimulus. These interventions can have positive or unintended negative consequences. Current research to model and determine an optimal intervention automatically through round-tripping is limited by the simulation objectives, scale (a few thousand individuals), model types that are not suited for intervention studies, and the number of intervention strategies they can explore (discrete vs continuous). We address these challenges using a Deep Deterministic Policy Gradient (DDPG) based policy optimization framework on a large-scale (100,000 individual) epidemiological agent-based simulation where we perform multi-objective optimization. We determine the optimal policy for lockdown and vaccination in a minimalist age-stratified multi-vaccine scenario with a basic simulation for economic activity. With no lockdown and vaccination (mid-age and elderly), results show optimal economy (individuals below the poverty line) with balanced health objectives (infection, and hospitalization). An in-depth simulation is needed to further validate our results and open-source our framework.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques</title>
<link>https://arxiv.org/abs/2507.12286</link>
<guid>https://arxiv.org/abs/2507.12286</guid>
<content:encoded><![CDATA[
<div> Keywords: SHACL, OWL, RDF data, ontologies, validation <br />
<br />
Summary:
This paper discusses the differences between SHACL and OWL, two W3C standards for managing RDF data. While OWL is designed for inferring facts from incomplete data with an open-world assumption, SHACL functions as a constraint language under the closed-world assumption. The combination of both languages has been called for, but the semantic gap between them poses challenges both semantically and computationally. The authors propose a semantics for SHACL validation in the presence of ontologies using core universal models in the description logic Horn-ALCHIQ. They present a technique for constructing these models and a rewriting technique that simplifies SHACL validation in the presence of ontologies. The complexity of SHACL validation with ontologies is studied, revealing that even simple ontologies can make the problem EXPTIME-complete and PTIME-complete in data complexity. <br /><br />Summary: <div>
arXiv:2507.12286v1 Announce Type: cross 
Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These languages share many features, but they have one fundamental difference: OWL, designed for inferring facts from incomplete data, makes the open-world assumption, whereas SHACL is a constraint language that treats the data as complete and must be validated under the closed-world assumption. The combination of both formalisms is very appealing and has been called for, but their semantic gap is a major challenge, semantically and computationally. In this paper, we advocate a semantics for SHACL validation in the presence of ontologies based on core universal models. We provide a technique for constructing these models for ontologies in the rich data-tractable description logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to develop a rewriting technique that reduces SHACL validation in the presence of ontologies to standard validation. Finally, we study the complexity of SHACL validation in the presence of ontologies, and show that even very simple ontologies make the problem EXPTIME-complete, and PTIME-complete in data complexity.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</title>
<link>https://arxiv.org/abs/2508.04748</link>
<guid>https://arxiv.org/abs/2508.04748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, molecular property prediction, reinforcement learning, attribute-based structured output, interpretability <br />
Summary: <br />
The article introduces AttriLens-Mol, a reinforcement learning framework for molecular property prediction with Large Language Models (LLMs). It steers the model's reasoning using attribute-based rewards and advanced techniques to improve relevance and performance. By training models on a small dataset with AttriLens-Mol, significant performance boosts are achieved compared to other fine-tuning and advanced models. Additionally, using the extracted attributes for property prediction results in superior performance compared to prompting LLMs. This approach enhances interpretability, making predictions more effective and relevant for molecular properties. The code is available on GitHub for further exploration and research. <br /> <div>
arXiv:2508.04748v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Visualization Makeovers with LLMs</title>
<link>https://arxiv.org/abs/2508.05637</link>
<guid>https://arxiv.org/abs/2508.05637</guid>
<content:encoded><![CDATA[
<div> Keywords: data visualization, large language models, visualisation makeovers, constructive criticism, best practices

Summary:
This article introduces the concept of using large language models (LLMs) to provide constructive criticism for improving data visualizations. Visualisation makeovers, typically a community exercise, can now be emulated by LLMs. By priming the model with visualization best practices, users can receive feedback on their plots to enhance their effectiveness. The system focuses on educating users on improving existing visualizations rather than generating new ones. Evaluations demonstrate the LLM's sensitivity to various plotting issues across different chart types. The tool is accessible through a simple self-hosted applet with a user-friendly web interface. <div>
arXiv:2508.05637v1 Announce Type: cross 
Abstract: Making a good graphic that accurately and efficiently conveys the desired message to the audience is both an art and a science, typically not taught in the data science curriculum. Visualisation makeovers are exercises where the community exchange feedback to improve charts and data visualizations. Can multi-modal large language models (LLMs) emulate this task? Given a plot in the form of an image file, or the code used to generate it, an LLM, primed with a list of visualization best practices, is employed to semi-automatically generate constructive criticism to produce a better plot. Our system is centred around prompt engineering of a pre-trained model, relying on a combination of userspecified guidelines and any latent knowledge of data visualization practices that might lie within an LLMs training corpus. Unlike other works, the focus is not on generating valid visualization scripts from raw data or prompts, but on educating the user how to improve their existing data visualizations according to an interpretation of best practices. A quantitative evaluation is performed to measure the sensitivity of the LLM agent to various plotting issues across different chart types. We make the tool available as a simple self-hosted applet with an accessible Web interface.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Request-Only Optimization for Recommendation Systems</title>
<link>https://arxiv.org/abs/2508.05640</link>
<guid>https://arxiv.org/abs/2508.05640</guid>
<content:encoded><![CDATA[
<div> deep learning, recommendation models, training efficiency, storage efficiency, model quality

Summary:<br />
- Deep Learning Recommendation Models (DLRMs) are widely used in large-scale recommendation systems, requiring efficient storage and training algorithms due to the large amount of data involved.
- The Request-Only Optimizations (ROO) paradigm improves both storage and training efficiency as well as model quality by treating a user request as a unit of training data.
- ROO implementation involves using request-only data, request-only based data processing pipeline, and request-only neural architectures.
- Native feature deduplication in data logging is achieved by treating a user request as a unit, saving data storage.
- De-duplicating computations and communications across multiple impressions in a request allows for highly scaled-up neural network architectures, enabling better capturing of user interest signals. <div>
arXiv:2508.05640v1 Announce Type: cross 
Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest machine learning applications on the planet. Industry-scale DLRMs are trained with petabytes of recommendation data to serve billions of users every day. To utilize the rich user signals in the long user history, DLRMs have been scaled up to unprecedented complexity, up to trillions of floating-point operations (TFLOPs) per example. This scale, coupled with the huge amount of training data, necessitates new storage and training algorithms to efficiently improve the quality of these complex recommendation systems. In this paper, we present a Request-Only Optimizations (ROO) training and modeling paradigm. ROO simultaneously improves the storage and training efficiency as well as the model quality of recommendation systems. We holistically approach this challenge through co-designing data (i.e., request-only data), infrastructure (i.e., request-only based data processing pipeline), and model architecture (i.e., request-only neural architectures). Our ROO training and modeling paradigm treats a user request as a unit of the training data. Compared with the established practice of treating a user impression as a unit, our new design achieves native feature deduplication in data logging, consequently saving data storage. Second, by de-duplicating computations and communications across multiple impressions in a request, this new paradigm enables highly scaled-up neural network architectures to better capture user interest signals, such as Generative Recommenders (GRs) and other request-only friendly architectures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.05647</link>
<guid>https://arxiv.org/abs/2508.05647</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, retrieval-augmented generation, query-aware attention, multi-hop questions, knowledge graphs<br />
<br />
Summary: <br />
The article introduces a novel Graph Neural Network architecture for retrieval-augmented generation, enhancing retrieval accuracy for complex, multi-hop questions. Unlike traditional methods, this approach constructs per-episode knowledge graphs capturing sequential and semantic relationships in text chunks. Featuring an Enhanced Graph Attention Network with query-guided pooling, it dynamically focuses on relevant graph parts based on user queries. The experimental results demonstrate superior performance over standard dense retrievers, particularly for multi-document reasoning questions. Leveraging PyTorch Geometric for efficient graph-structured data processing, it allows scalable deployment in production retrieval systems. <div>
arXiv:2508.05647v1 Announce Type: cross 
Abstract: We present a novel graph neural network (GNN) architecture for retrieval-augmented generation (RAG) that leverages query-aware attention mechanisms and learned scoring heads to improve retrieval accuracy on complex, multi-hop questions. Unlike traditional dense retrieval methods that treat documents as independent entities, our approach constructs per-episode knowledge graphs that capture both sequential and semantic relationships between text chunks. We introduce an Enhanced Graph Attention Network with query-guided pooling that dynamically focuses on relevant parts of the graph based on user queries. Experimental results demonstrate that our approach significantly outperforms standard dense retrievers on complex question answering tasks, particularly for questions requiring multi-document reasoning. Our implementation leverages PyTorch Geometric for efficient processing of graph-structured data, enabling scalable deployment in production retrieval systems
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups</title>
<link>https://arxiv.org/abs/2508.05648</link>
<guid>https://arxiv.org/abs/2508.05648</guid>
<content:encoded><![CDATA[
<div> Keywords: research groups, collective knowledge, tacit knowledge, retrieval-augmented generation, AquiLLM

Summary:
Research groups often struggle to capture, store, and retrieve distributed knowledge within their teams. This informal, experience-based expertise, known as tacit knowledge, is crucial but often left undocumented. AquiLLM is a retrieval-augmented generation (RAG) system designed to address this challenge. Unlike existing systems focused on public documents, AquiLLM is tailored for internal research materials, such as emails and meeting notes, with configurable privacy settings. By supporting various document types and privacy settings, AquiLLM enables more effective access to both formal and informal knowledge within scholarly groups. <div>
arXiv:2508.05648v1 Announce Type: cross 
Abstract: Research groups face persistent challenges in capturing, storing, and retrieving knowledge that is distributed across team members. Although structured data intended for analysis and publication is often well managed, much of a group's collective knowledge remains informal, fragmented, or undocumented--often passed down orally through meetings, mentoring, and day-to-day collaboration. This includes private resources such as emails, meeting notes, training materials, and ad hoc documentation. Together, these reflect the group's tacit knowledge--the informal, experience-based expertise that underlies much of their work. Accessing this knowledge can be difficult, requiring significant time and insider understanding. Retrieval-augmented generation (RAG) systems offer promising solutions by enabling users to query and generate responses grounded in relevant source material. However, most current RAG-LLM systems are oriented toward public documents and overlook the privacy concerns of internal research materials. We introduce AquiLLM (pronounced ah-quill-em), a lightweight, modular RAG system designed to meet the needs of research groups. AquiLLM supports varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools</title>
<link>https://arxiv.org/abs/2508.05650</link>
<guid>https://arxiv.org/abs/2508.05650</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, evaluation, multi domain, standardized metrics, knowledge fields 

Summary: 
The article introduces OmniBench RAG, a platform designed for evaluating Retrieval Augmented Generation systems across multiple domains. It addresses the current challenges of evaluating RAG systems, such as lack of domain coverage, coarse metrics, and inability to compare effectiveness across models and domains. OmniBench RAG provides standardized metrics - Improvements for accuracy gains and Transformation for efficiency differences. The platform covers nine knowledge fields and enables reproducible comparisons across models and tasks. Through evaluation, it reveals varying effectiveness of RAG systems, with significant gains in some domains like culture and declines in others like mathematics. The platform features dynamic test generation, modular evaluation pipelines, and automated knowledge base construction to ensure systematic, domain-aware assessment. A demonstration video and access to code and datasets are also provided for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.05650v1 Announce Type: cross 
Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance LLMs, evaluating its true performance benefits in a reproducible and interpretable way remains a major hurdle. Existing methods often fall short: they lack domain coverage, employ coarse metrics that miss sub document precision, and fail to capture computational trade offs. Most critically, they provide no standardized framework for comparing RAG effectiveness across different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain evaluation of RAG systems. The platform quantifies performance gains across accuracy and efficiency dimensions, spanning nine knowledge fields including culture, geography, and health. We introduce two standardized metrics: Improvements (accuracy gains) and Transformation (efficiency differences between pre RAG and post RAG models), enabling reproducible comparisons across models and tasks. The platform features dynamic test generation, modular evaluation pipelines, and automated knowledge base construction. Our evaluation reveals striking variability in RAG effectiveness, from significant gains in culture to declines in mathematics, highlighting the critical importance of systematic, domain aware assessment. A demonstration video is available at: https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets: https://github.com/Garnett-Liang/Omnibench-RAG.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2508.05652</link>
<guid>https://arxiv.org/abs/2508.05652</guid>
<content:encoded><![CDATA[
<div> Trail recommendation, Chatbot, Conversational AI, Outdoor activities, Large language model

Summary:
The paper discusses the development of Judy, a chatbot that recommends outdoor trails using a large language model with retrieval augmented generation. Challenges addressed include providing accurate trail information and ensuring usability of recommendation services. Case studies in Connecticut were conducted to assess system performance. Web-based data collection and management, along with LLM model studies, were used to evaluate Judy's accuracy, effectiveness, and usability in recommending trails. Results showed the efficacy of Judy in providing personalized recommendations for outdoor activities. <div>
arXiv:2508.05652v1 Announce Type: cross 
Abstract: The increasing popularity of outdoor recreational activities (such as hiking and biking) has boosted the demand for a conversational AI system to provide informative and personalized suggestion on outdoor trails. Challenges arise in response to (1) how to provide accurate outdoor trail information via conversational AI; and (2) how to enable usable and efficient recommendation services. To address above, this paper discusses the preliminary and practical lessons learned from developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG). To gain concrete system insights, we have performed case studies with the outdoor trails in Connecticut (CT), US. We have conducted web-based data collection, outdoor trail data management, and LLM model performance studies on the RAG-based recommendation. Our experimental results have demonstrated the accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Interactive Narrative Systems: A Formal Approach</title>
<link>https://arxiv.org/abs/2508.05653</link>
<guid>https://arxiv.org/abs/2508.05653</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive Narrative Systems, formal representation framework, analysis, evaluation, collaboration

Summary: 
This paper introduces a formal representation framework for Interactive Narrative Systems (INS) to address the challenges faced by the field. By offering a consistent vocabulary and modeling structure inspired by various approaches, the framework assists in analyzing and describing INS properties. Experimental validations conducted on the "Little Red Riding Hood" scenario demonstrate the effectiveness of the proposed formalism in enhancing the evaluation of INS. The work aims to foster collaboration and coherence within the INS research community by presenting a methodology for formally representing these systems. <div>
arXiv:2508.05653v1 Announce Type: cross 
Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences by empowering users to actively shape their stories, diverging from traditional passive storytelling. However, the field faces challenges due to fragmented research efforts and diverse system representations. This paper introduces a formal representation framework for INS, inspired by diverse approaches from the state of the art. By providing a consistent vocabulary and modeling structure, the framework facilitates the analysis, the description and comparison of INS properties. Experimental validations on the "Little Red Riding Hood" scenario highlight the usefulness of the proposed formalism and its impact on improving the evaluation of INS. This work aims to foster collaboration and coherence within the INS research community by proposing a methodology for formally representing these systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Information Retrieval Techniques Applied to IT Support Tickets</title>
<link>https://arxiv.org/abs/2508.05654</link>
<guid>https://arxiv.org/abs/2508.05654</guid>
<content:encoded><![CDATA[
<div> machine learning, IT help desk system, information retrieval techniques, Sentence-BERT, support ticket recovery system

Summary:
This study compares eleven information retrieval techniques in the context of IT support tickets to enhance the efficiency of IT help desk systems. The Sentence-BERT technique, particularly the multi-language variation distilluse-base-multilingual-cased-v1, stood out with 78.7% relevant recommendations. Other techniques such as TF-IDF (69.0%), Word2vec (68.7%), and LDA (66.3%) also performed well. The datasets and code used in the study have been released as open source. A prototype of a support ticket recovery system was successfully implemented, demonstrating its practicality. A new metric was proposed to evaluate the retrieval quality based on IT analyst perception. The study provides valuable insights for improving IT support systems through the application of machine learning techniques.<br /><br />Summary: <div>
arXiv:2508.05654v1 Announce Type: cross 
Abstract: Institutions dependent on IT services and resources acknowledge the crucial significance of an IT help desk system, that act as a centralized hub connecting IT staff and users for service requests. Employing various Machine Learning models, these IT help desk systems allow access to corrective actions used in the past, but each model has different performance when applied to different datasets. This work compares eleven Information Retrieval techniques in a dataset of IT support tickets, with the goal of implementing a software that facilitates the work of Information Technology support analysts. The best results were obtained with the Sentence-BERT technique, in its multi-language variation distilluse-base-multilingual-cased-v1, where 78.7% of the recommendations made by the model were considered relevant. TF-IDF (69.0%), Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results. Furthermore, the used datasets and essential parts of coding have been published and made open source. It also demonstrated the practicality of a support ticket recovery system by implementing a minimal viable prototype, and described in detail the implementation of the system. Finally, this work proposed a novel metric for comparing the techniques, whose aim is to closely reflect the perception of the IT analysts about the retrieval quality.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation</title>
<link>https://arxiv.org/abs/2508.05657</link>
<guid>https://arxiv.org/abs/2508.05657</guid>
<content:encoded><![CDATA[
<div> recommender systems, conversational systems, data augmentation, semantic relevance, collaborative information  
Summary:  
- This article discusses the challenges faced by conversational recommender systems (CRSs) in dealing with false negatives during training, leading to suboptimal recommendations.  
- The proposed solution involves expanding the label set through data augmentation while balancing semantic relevance and collaborative information.  
- A novel data augmentation framework is introduced, utilizing an LLM-based semantic retriever to identify relevant items and a relevance scorer to filter noisy candidates.  
- A two-stage training strategy is implemented to ensure a balance between semantic relevance and collaborative information in CRS datasets.  
- Extensive experiments on benchmark datasets and user simulators demonstrate significant performance improvements across various recommenders, showing the effectiveness of the approach in advancing CRS performance.<br /><br />Summary: <div>
arXiv:2508.05657v1 Announce Type: cross 
Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by engaging users in multi-turn dialogues, capturing nuanced preferences through natural language interactions. However, these systems often face the false negative issue, where items that a user might like are incorrectly labeled as negative during training, leading to suboptimal recommendations.Expanding the label set through data augmentation presents an intuitive solution but faces the challenge of balancing two key aspects: ensuring semantic relevance and preserving the collaborative information inherent in CRS datasets. To address these issues, we propose a novel data augmentation framework that first leverages an LLM-based semantic retriever to identify diverse and semantically relevant items, which are then filtered by a relevance scorer to remove noisy candidates. Building on this, we introduce a two-stage training strategy balancing semantic relevance and collaborative information. Extensive experiments on two benchmark datasets and user simulators demonstrate significant and consistent performance improvements across various recommenders, highlighting the effectiveness of our approach in advancing CRS performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review</title>
<link>https://arxiv.org/abs/2508.05660</link>
<guid>https://arxiv.org/abs/2508.05660</guid>
<content:encoded><![CDATA[
<div> keywords: hybrid Retrieval Augmented Generation, autonomous agent, uncertainty estimation, structured metadata, scientific publications

Summary:
The article introduces a novel approach to addressing the challenges posed by the surge in scientific publications. It presents an agentic system that dynamically selects between graph-based and vector-based retrieval methods, adapts generation to researcher needs, and quantifies uncertainty during inference. By ingesting data from bibliometric sources and building a knowledge graph and vector store, the system achieves improved relevance, reduced hallucinations, and enhanced reproducibility. Through instruction tuning and bootstrapped evaluation, the system outperforms baseline methods on synthetic benchmarks, demonstrating gains in various metrics such as context recall, precision, faithfulness, and relevance. These results underscore the system's ability to improve reasoning across diverse data sources and establish a scalable framework for autonomous, agentic scientific discovery.<br /><br />Summary: <div>
arXiv:2508.05660v1 Announce Type: cross 
Abstract: The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1) dynamically selecting between GraphRAG and VectorRAG for each query, (2) adapting instruction-tuned generation in real time to researcher needs, and (3) quantifying uncertainty during inference. This dynamic orchestration improves relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2 model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking). Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace</title>
<link>https://arxiv.org/abs/2508.05661</link>
<guid>https://arxiv.org/abs/2508.05661</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual search, Consumer-to-consumer marketplace, Zero-shot image retrieval, SigLIP model, Production impact

Summary: 
Visual search is an important feature in consumer-to-consumer marketplaces like Mercari, allowing users to explore product catalogs efficiently. This paper presents a scalable visual search system that integrates recent vision-language models for zero-shot image retrieval. The multilingual SigLIP model outperforms other models in offline evaluation and real-world A/B testing, showing a significant increase in engagement and conversion rates. The system combines real-time inference and background indexing workflows, optimized through a unified embedding pipeline. The findings suggest that zero-shot models can serve as a strong baseline for production use, enabling teams to deploy effective visual search systems with minimal overhead and the flexibility to fine-tune based on specific needs. 

<br /><br />Summary: <div>
arXiv:2508.05661v1 Announce Type: cross 
Abstract: Visual search offers an intuitive way for customers to explore diverse product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where listings are often unstructured and visually driven. This paper presents a scalable visual search system deployed in Mercari's C2C marketplace, where end-users act as buyers and sellers. We evaluate recent vision-language models for zero-shot image retrieval and compare their performance with an existing fine-tuned baseline. The system integrates real-time inference and background indexing workflows, supported by a unified embedding pipeline optimized through dimensionality reduction. Offline evaluation using user interaction logs shows that the multilingual SigLIP model outperforms other models across multiple retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A one-week online A/B test in production further confirms real-world impact, with the treatment group showing substantial gains in engagement and conversion, up to a 40.9% increase in transaction rate via image search. Our findings highlight that recent zero-shot models can serve as a strong and practical baseline for production use, which enables teams to deploy effective visual search systems with minimal overhead, while retaining the flexibility to fine-tune based on future data or domain-specific needs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base</title>
<link>https://arxiv.org/abs/2508.05662</link>
<guid>https://arxiv.org/abs/2508.05662</guid>
<content:encoded><![CDATA[
<div> pipeline, cosine screening, clustering, index upsert mechanism, real-time streams <br />
Summary: 
The article presents Streaming RAG, a unified pipeline that utilizes multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to manage dynamic streams efficiently. It addresses the challenges posed by static RAG frameworks, such as high memory costs and latency issues. The system maintains a compact prototype set and achieves a balance between retrieval quality and clustering variance. An incremental index upsert mechanism ensures prototypes are updated without interrupting queries. Experimental results demonstrate significant improvements in recall, low latency, and high throughput within a specified memory budget. Hyperparameter analysis validates default settings, while application in open-domain question answering and summarization tasks shows performance enhancements. Streaming RAG sets a new standard for retrieval augmentation, offering a versatile solution for handling dynamic data streams effectively. <br /> <div>
arXiv:2508.05662v1 Announce Type: cross 
Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial markets challenge static RAG frameworks. Full-scale indices incur high memory costs; periodic rebuilds introduce latency that undermines data freshness; naive sampling sacrifices semantic coverage. We present Streaming RAG, a unified pipeline that combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* - L \Delta\$ linking retrieval quality to clustering variance. An incremental index upsert mechanism refreshes prototypes without interrupting queries. Experiments on eight real-time streams show statistically significant gains in Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and throughput above 900 documents per second under a 150 MB budget. Hyperparameter sensitivity analysis over cluster count, admission probability, relevance threshold, and counter capacity validates default settings. In open-domain question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L improvements. Streaming RAG establishes a new Pareto frontier for retrieval augmentation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support</title>
<link>https://arxiv.org/abs/2508.05664</link>
<guid>https://arxiv.org/abs/2508.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: AI customer service systems, query rewriting, RAG Fusion, intent recognition, context reranking

Summary:<br /><br /> This case study explores various techniques for enhancing AI customer service systems in the electric power domain. It compares vector-store and graph-based frameworks, with the graph-based RAG showing superior performance in handling complex queries. Query rewriting improves retrieval for non-standard or detail-specific queries, while RAG Fusion merges multiple retrievals to address vague or multifaceted queries. Context reranking filters irrelevant contexts to reduce hallucinations, and intent recognition supports decomposing complex questions into targeted sub-queries for increased relevance and efficiency. Although keyword augmentation had a negative impact due to biased keyword selection, the final system combines intent recognition, RAG Fusion, and reranking to effectively handle disambiguation and multi-source queries. Evaluation on both synthetic and real-world datasets showcases the system's high accuracy, outperforming baseline RAG models significantly.<br />Summary: <div>
arXiv:2508.05664v1 Announce Type: cross 
Abstract: Many AI customer service systems use standard NLP pipelines or finetuned language models, which often fall short on ambiguous, multi-intent, or detail-specific queries. This case study evaluates recent techniques: query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking, for building a robust customer support system in the electric power domain. We compare vector-store and graph-based RAG frameworks, ultimately selecting the graph-based RAG for its superior performance in handling complex queries. We find that query rewriting improves retrieval for queries using non-standard terminology or requiring precise detail. RAG Fusion boosts performance on vague or multifaceted queries by merging multiple retrievals. Reranking reduces hallucinations by filtering irrelevant contexts. Intent recognition supports the decomposition of complex questions into more targeted sub-queries, increasing both relevance and efficiency. In contrast, keyword augmentation negatively impacts results due to biased keyword selection. Our final system combines intent recognition, RAG Fusion, and reranking to handle disambiguation and multi-source queries. Evaluated on both a GPT-4-generated dataset and a real-world electricity provider FAQ dataset, it achieves 97.9% and 89.6% accuracy respectively, substantially outperforming baseline RAG models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis</title>
<link>https://arxiv.org/abs/2508.05666</link>
<guid>https://arxiv.org/abs/2508.05666</guid>
<content:encoded><![CDATA[
<div> retrieval, generation, ETL, literature synthesis, research gaps  
Summary:   
HySemRAG is a framework that combines ETL pipelines with RAG to automate literature synthesis and identify research gaps. The system addresses limitations in existing RAG architectures through hybrid retrieval, self-correction, and citation verification. It processes scholarly literature through multiple stages, creating a knowledge graph and vector collections. Evaluation shows higher semantic similarity scores with structured field extraction compared to PDF chunking. The agentic quality assurance mechanism achieves high success rates and citation accuracy. Applied to geospatial epidemiology literature, the system identifies methodological trends and research gaps. HySemRAG demonstrates broad applicability for accelerating evidence synthesis and discovery.  
Summary: <div>
arXiv:2508.05666v1 Announce Type: cross 
Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations</title>
<link>https://arxiv.org/abs/2508.05667</link>
<guid>https://arxiv.org/abs/2508.05667</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, recommendation systems, instruction tuning dataset, user-item interaction, user-item understanding 

Summary: 
Large language models (LLMs) have shown great performance in natural language tasks but struggle in recommendation systems due to differences in data structure. To address this, a new instruction tuning dataset, ITDR, was created, with 7 subtasks covering user-item interaction and understanding. Data from 13 public recommendation datasets was integrated into ITDR, comprising around 200,000 instances. Experimental results indicate that ITDR significantly enhances the performance of mainstream LLMs on recommendation tasks. The correlations between tasks were analyzed, and the impact of task descriptions and data scale on instruction tuning effectiveness was explored. Comparative experiments with closed-source LLMs were also conducted. The ITDR dataset and fine-tuned large recommendation models can be accessed on GitHub.<br /><br />Summary: <div>
arXiv:2508.05667v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated outstanding performance in natural language processing tasks. However, in the field of recommendation systems, due to the structural differences between user behavior data and natural language, LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance. To address this gap, in this work, we construct a sufficient instruction tuning dataset, ITDR, which encompasses 7 subtasks across two core root tasks--user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances. Experimental results demonstrate that ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks. Furthermore, we analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with substantial parameters. Our tuning dataset ITDR and the fine-tuned large recommendation models can be accessed at https://github.com/hellolzk/ITDR.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges</title>
<link>https://arxiv.org/abs/2508.05668</link>
<guid>https://arxiv.org/abs/2508.05668</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Search Agents, Information retrieval, Deep learning, Autonomous information seeking

Summary:
Large Language Models (LLMs) have transformed web search with the introduction of Search Agents that can understand user intentions and context for dynamic retrieval. These agents, exemplified by OpenAI's Deep Research, have the potential for extensive information mining beyond the web. This survey systematically analyzes existing search agents based on architecture, optimization, application, and evaluation. Critical challenges and future research directions in this rapidly evolving field are identified. The repository of analyzed works is accessible at https://github.com/YunjiaXi/Awesome-Search-Agent-Papers. <div>
arXiv:2508.05668v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports</title>
<link>https://arxiv.org/abs/2508.05669</link>
<guid>https://arxiv.org/abs/2508.05669</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, table extraction, vision-language model, Markdown generation, document understanding

Summary: 
- The study focuses on accurately converting financial tables from Malaysian audited financial reports into Markdown format.
- A fine-tuned vision-language model, based on Qwen2.5-VL-7B, is proposed for high-fidelity Markdown generation from document images.
- A curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA was utilized.
- Performance evaluation on 100 out-of-sample tables showed the model achieved an overall accuracy of 92.20% on criteria-based assessment and a 96.53% Markdown TEDS score.
- The model outperforms larger-scale VLMs, specialized reasoning-enabled models, and proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash, demonstrating the effectiveness of domain-specific fine-tuning in bridging the gap between unstructured financial documents and downstream automation. 

<br /><br />Summary: <div>
arXiv:2508.05669v1 Announce Type: cross 
Abstract: Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?</title>
<link>https://arxiv.org/abs/2508.05670</link>
<guid>https://arxiv.org/abs/2508.05670</guid>
<content:encoded><![CDATA[
<div> game theory, cybersecurity, Large Language Models, agent characteristics, linguistic sensitivity

Summary:
- The study explores how classical game theory can capture the behaviors of Large Language Model (LLM) actors and bots in cybersecurity.
- Four state-of-the-art LLMs are tested in two scenarios, one-shot zero-sum game and dynamic Prisoner's Dilemma, across five languages to assess linguistic sensitivity.
- The final payoffs in the games are influenced by agent characteristics like personality traits and knowledge of repeated rounds.
- Unexpected sensitivity in payoffs is observed based on the choice of languages, emphasizing the need for caution in deploying LLMs in different regions.
- Quantitative metrics are utilized to evaluate internal consistency and cross-language stability of LLM agents, aiding in selecting stable LLMs for secure applications.<br /><br />Summary: <div>
arXiv:2508.05670v1 Announce Type: cross 
Abstract: Game theory has long served as a foundational tool in cybersecurity to test, predict, and design strategic interactions between attackers and defenders. The recent advent of Large Language Models (LLMs) offers new tools and challenges for the security of computer systems; In this work, we investigate whether classical game-theoretic frameworks can effectively capture the behaviours of LLM-driven actors and bots. Using a reproducible framework for game-theoretic LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to expected outcomes or exhibit deviations due to embedded biases. Our experiments involve four state-of-the-art LLMs and span five natural languages, English, French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic sensitivity. For both games, we observe that the final payoffs are influenced by agents characteristics such as personality traits or knowledge of repeated rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to the choice of languages, which should warn against indiscriminate application of LLMs in cybersecurity applications and call for in-depth studies, as LLMs may behave differently when deployed in different countries. We also employ quantitative metrics to evaluate the internal consistency and cross-language stability of LLM agents, to help guide the selection of the most stable LLMs and optimising models for secure applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing</title>
<link>https://arxiv.org/abs/2508.05672</link>
<guid>https://arxiv.org/abs/2508.05672</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, LMAR, data augmentation, domain-specific adaptation, text clustering

Summary: 
LMAR (Language Model Augmented Retriever) is a model-agnostic framework that addresses challenges faced by Retrieval Augmented Generation (RAG) systems in handling domain-specific knowledge. The two-stage pipeline of LMAR combines LLM-guided data synthesis with contrastive embedding adaptation and efficient text clustering. By utilizing LLMs for triplet sampling and synthetic data augmentation, LMAR ensures high-fidelity supervision throughout the pipeline. Experimental results on multiple domain-specific benchmark datasets show that LMAR outperforms baseline models while maintaining moderate hardware requirements and low latency. The model-agnostic nature of LMAR enables seamless integration with various RAG architectures and text embedding models for continual improvements without redesigning the pipeline. Overall, LMAR presents a practical and cost-effective solution for scalable domain-specific adaptation. 

<br /><br />Summary: <div>
arXiv:2508.05672v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) systems often struggle with domain-specific knowledge due to performance deterioration of pre-trained embeddings and prohibitive computational costs of large language model (LLM)-based retrievers. While fine-tuning data augmentation embedding models offers a promising direction, its effectiveness is limited by the need for high-quality training data and reliable chunking strategies that preserve contextual integrity. We propose LMAR (Language Model Augmented Retriever), a model-agnostic framework that addresses these challenges by combining LLM-guided data synthesis with contrastive embedding adaptation and efficient text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling and synthetic data augmentation, where LLMs act as both labeler and validator to ensure high-fidelity supervision throughout the pipeline. Experimental results across multiple domain-specific benchmark datasets demonstrate that LMAR outperforms multiple baseline models, while maintaining moderate hardware requirements and low latency. Its model-agnostic nature further enables seamless integration with emerging RAG architectures and text embedding models, ensuring continual improvements without redesigning the pipeline. These results highlight LMAR as a practical and cost-effective solution for scalable domain-specific adaptation.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems</title>
<link>https://arxiv.org/abs/2508.05673</link>
<guid>https://arxiv.org/abs/2508.05673</guid>
<content:encoded><![CDATA[
arXiv:2508.05673v1 Announce Type: cross 
Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as NDCG@$K$ are the gold standard for evaluating recommendation performance. However, during the training of recommendation models, optimizing NDCG@$K$ poses significant challenges due to its inherent discontinuous nature and the intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either overlooked the Top-$K$ truncation or suffered from high computational costs and training instability. To overcome these limitations, we propose SoftmaxLoss@$K$ (SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization. Specifically, we integrate the quantile technique to handle Top-$K$ truncation and derive a smooth upper bound for optimizing NDCG@$K$ to address discontinuity. The resulting SL@$K$ loss has several desirable properties, including theoretical guarantees, ease of implementation, computational efficiency, gradient stability, and noise robustness. Extensive experiments on four real-world datasets and three recommendation backbones demonstrate that SL@$K$ outperforms existing losses with a notable average improvement of 6.03%. The code is available at https://github.com/Tiny-Snow/IR-Benchmark.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark</title>
<link>https://arxiv.org/abs/2508.05674</link>
<guid>https://arxiv.org/abs/2508.05674</guid>
<content:encoded><![CDATA[
arXiv:2508.05674v1 Announce Type: cross 
Abstract: Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on https://github.com/NYU-LLM-CTF/CTFJudge.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration</title>
<link>https://arxiv.org/abs/2508.05675</link>
<guid>https://arxiv.org/abs/2508.05675</guid>
<content:encoded><![CDATA[
arXiv:2508.05675v1 Announce Type: cross 
Abstract: Recent years have witnessed growing interest in adopting large language models (LLMs) for Register Transfer Level (RTL) code optimization. While powerful cloud-based LLMs offer superior optimization capabilities, they pose unacceptable intellectual property (IP) leakage risks when processing proprietary hardware designs. In this paper, we propose a new scenario where Verilog code must be optimized for specific attributes without leaking sensitive IP information. We introduce the first IP-preserving edge-cloud collaborative framework that leverages the benefits of both paradigms. Our approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure comparative analysis between paired high-quality target designs and novice draft codes, yielding general design principles that summarize key insights for improvements. These principles are then used to query stronger cloud LLMs (e.g., Deepseek-V3) for targeted code improvement, ensuring that only abstracted and IP-safe guidance reaches external services. Our experimental results demonstrate that the framework achieves significantly higher optimization success rates compared to baseline methods. For example, combining Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even commercial models like GPT-4o (55.81\%). Further investigation of local and cloud LLM combinations reveals that different model pairings exhibit varying strengths for specific optimization objectives, with interesting trends emerging when varying the number of comparative code pairs. Our work establishes a new paradigm for secure hardware design optimization that balances performance gains with IP protection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation</title>
<link>https://arxiv.org/abs/2508.05677</link>
<guid>https://arxiv.org/abs/2508.05677</guid>
<content:encoded><![CDATA[
arXiv:2508.05677v1 Announce Type: cross 
Abstract: RL-based medical questionnaire systems have shown great potential in medical scenarios. However, their safety and robustness remain unresolved. This study performs a comprehensive evaluation on adversarial attack methods to identify and analyze their potential vulnerabilities. We formulate the diagnosis process as a Markov Decision Process (MDP), where the state is the patient responses and unasked questions, and the action is either to ask a question or to make a diagnosis. We implemented six prevailing major attack methods, including the Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini & Wagner Attack (C&amp;W) attack, Basic Iterative Method (BIM), DeepFool, and AutoAttack, with seven epsilon values each. To ensure the generated adversarial examples remain clinically plausible, we developed a comprehensive medical validation framework consisting of 247 medical constraints, including physiological bounds, symptom correlations, and conditional medical constraints. We achieved a 97.6% success rate in generating clinically plausible adversarial samples. We performed our experiment on the National Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which consists of 182,630 samples, to predict the participant's 4-year mortality rate. We evaluated our attacks on the AdaptiveFS framework proposed in arXiv:2004.00994. Our results show that adversarial attacks could significantly impact the diagnostic accuracy, with attack success rates ranging from 33.08% (FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict medical constraints on the input, such RL-based medical questionnaire systems still show significant vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness</title>
<link>https://arxiv.org/abs/2508.05680</link>
<guid>https://arxiv.org/abs/2508.05680</guid>
<content:encoded><![CDATA[
arXiv:2508.05680v1 Announce Type: cross 
Abstract: Algorithmic systems such as search engines and information retrieval platforms significantly influence academic visibility and the dissemination of knowledge. Despite assumptions of neutrality, these systems can reproduce or reinforce societal biases, including those related to gender. This paper introduces and applies a bias-preserving definition of algorithmic gender fairness, which assesses whether algorithmic outputs reflect real-world gender distributions without introducing or amplifying disparities. Using a heterogeneous dataset of academic profiles from German universities and universities of applied sciences, we analyse gender differences in metadata completeness, publication retrieval in academic databases, and visibility in Google search results. While we observe no overt algorithmic discrimination, our findings reveal subtle but consistent imbalances: male professors are associated with a greater number of search results and more aligned publication records, while female professors display higher variability in digital visibility. These patterns reflect the interplay between platform algorithms, institutional curation, and individual self-presentation. Our study highlights the need for fairness evaluations that account for both technical performance and representational equality in digital systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning</title>
<link>https://arxiv.org/abs/2508.05681</link>
<guid>https://arxiv.org/abs/2508.05681</guid>
<content:encoded><![CDATA[
arXiv:2508.05681v1 Announce Type: cross 
Abstract: Active learning(AL), which serves as the representative label-efficient learning paradigm, has been widely applied in resource-constrained scenarios. The achievement of AL is attributed to acquisition functions, which are designed for identifying the most important data to label. Despite this success, one question remains unanswered: is AL safe? In this work, we introduce ALA, a practical and the first framework to utilize the acquisition function as the poisoning attack surface to reveal the weakness of active learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit high uncertainty scores, increasing their probability of being selected by acquisition functions. To evaluate ALA, we conduct extensive experiments across three datasets, three acquisition functions, and two types of clean-label backdoor triggers. Results show that our attack can achieve high success rates (up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model utility and remaining undetectable to human annotators. Our findings remind active learning users: acquisition functions can be easily exploited, and active learning should be deployed with caution in trusted data scenarios.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2508.05687</link>
<guid>https://arxiv.org/abs/2508.05687</guid>
<content:encoded><![CDATA[
arXiv:2508.05687v1 Announce Type: cross 
Abstract: Organisations are starting to adopt LLM-based AI agents, with their deployments naturally evolving from single agents towards interconnected, multi-agent networks. Yet a collection of safe agents does not guarantee a safe collection of agents, as interactions between agents over time create emergent behaviours and induce novel failure modes. This means multi-agent systems require a fundamentally different risk analysis approach than that used for a single agent.
  This report addresses the early stages of risk identification and analysis for multi-agent AI systems operating within governed environments where organisations control their agent configurations and deployment. In this setting, we examine six critical failure modes: cascading reliability failures, inter-agent communication failures, monoculture collapse, conformity bias, deficient theory of mind, and mixed motive dynamics. For each, we provide a toolkit for practitioners to extend or integrate into their existing frameworks to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our approach centres on analysis validity, and advocates for progressively increasing validity through staged testing across stages of abstraction and deployment that gradually increases exposure to potential negative impacts, while collecting convergent evidence through simulation, observational analysis, benchmarking, and red teaming. This methodology establishes the groundwork for robust organisational risk management as these LLM-based multi-agent systems are deployed and operated.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach</title>
<link>https://arxiv.org/abs/2508.05693</link>
<guid>https://arxiv.org/abs/2508.05693</guid>
<content:encoded><![CDATA[
arXiv:2508.05693v1 Announce Type: cross 
Abstract: Selecting third-party software packages in open-source ecosystems like Python is challenging due to the large number of alternatives and limited transparent evidence for comparison. Generative AI tools are increasingly used in development workflows, but their suggestions often overlook dependency evaluation, emphasize popularity over suitability, and lack reproducibility. This creates risks for projects that require transparency, long-term reliability, maintainability, and informed architectural decisions. This study formulates software package selection as a Multi-Criteria Decision-Making (MCDM) problem and proposes a data-driven framework for technology evaluation. Automated data pipelines continuously collect and integrate software metadata, usage trends, vulnerability information, and developer sentiment from GitHub, PyPI, and Stack Overflow. These data are structured into a decision model representing relationships among packages, domain features, and quality attributes. The framework is implemented in PySelect, a decision support system that uses large language models to interpret user intent and query the model to identify contextually appropriate packages. The approach is evaluated using 798,669 Python scripts from 16,887 GitHub repositories and a user study based on the Technology Acceptance Model. Results show high data extraction precision, improved recommendation quality over generative AI baselines, and positive user evaluations of usefulness and ease of use. This work introduces a scalable, interpretable, and reproducible framework that supports evidence-based software selection using MCDM principles, empirical data, and AI-assisted intent modeling.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection</title>
<link>https://arxiv.org/abs/2508.05694</link>
<guid>https://arxiv.org/abs/2508.05694</guid>
<content:encoded><![CDATA[
arXiv:2508.05694v1 Announce Type: cross 
Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge in cybersecurity due to the subtle, long-term, and context-dependent nature of malicious insider behaviors. Traditional models often struggle to capture semantic intent and complex behavior dynamics, while existing LLM-based solutions face limitations in prompt adaptability and modality coverage. To bridge this gap, we propose DMFI, a dual-modality framework that integrates semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into two structured views: (1) a semantic view that processes content-rich artifacts (e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned independently, and their outputs are fused via a lightweight MLP-based decision module. We further introduce DMFI-B, a discriminative adaptation strategy that separates normal and abnormal behavior representations, improving robustness under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets demonstrate that DMFI outperforms state-of-the-art methods in detection accuracy. Our approach combines the semantic reasoning power of LLMs with structured behavior modeling, offering a scalable and effective solution for real-world insider threat detection. Our work demonstrates the effectiveness of combining LLM reasoning with structured behavioral modeling, offering a scalable and deployable solution for modern insider threat detection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition</title>
<link>https://arxiv.org/abs/2508.05696</link>
<guid>https://arxiv.org/abs/2508.05696</guid>
<content:encoded><![CDATA[
arXiv:2508.05696v1 Announce Type: cross 
Abstract: Insider threat detection presents a significant challenge due to the deceptive nature of malicious behaviors, which often resemble legitimate user operations. However, existing approaches typically model system logs as flat event sequences, thereby failing to capture the inherent frequency dynamics and multiscale disturbance patterns embedded in user behavior. To address these limitations, we propose Log2Sig, a robust anomaly detection framework that transforms user logs into multivariate behavioral frequency signals, introducing a novel representation of user behavior. Log2Sig employs Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode Functions (IMFs), which reveal behavioral fluctuations across multiple temporal scales. Based on this, the model further performs joint modeling of behavioral sequences and frequency-decomposed signals: the daily behavior sequences are encoded using a Mamba-based temporal encoder to capture long-term dependencies, while the corresponding frequency components are linearly projected to match the encoder's output dimension. These dual-view representations are then fused to construct a comprehensive user behavior profile, which is fed into a multilayer perceptron for precise anomaly detection. Experimental results on the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly outperforms state-of-the-art baselines in both accuracy and F1 score.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking</title>
<link>https://arxiv.org/abs/2508.05700</link>
<guid>https://arxiv.org/abs/2508.05700</guid>
<content:encoded><![CDATA[
arXiv:2508.05700v1 Announce Type: cross 
Abstract: Large embedding tables are indispensable in modern recommendation systems, thanks to their ability to effectively capture and memorize intricate details of interactions among diverse entities. As we explore integrating large embedding tables into Pinterest's ads ranking models, we encountered not only common challenges such as sparsity and scalability, but also several obstacles unique to our context. Notably, our initial attempts to train large embedding tables from scratch resulted in neutral metrics. To tackle this, we introduced a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms. This approach greatly enriched the embedding tables and resulted in significant performance improvements. As a result, the multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid serving infrastructure to overcome GPU memory limits and elevate the scalability. This framework has been deployed in the Pinterest Ads system and achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control</title>
<link>https://arxiv.org/abs/2508.05702</link>
<guid>https://arxiv.org/abs/2508.05702</guid>
<content:encoded><![CDATA[
arXiv:2508.05702v1 Announce Type: cross 
Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread adoption of Electric Vehicles (EVs), and the growing frequency of extreme weather events have significantly increased the complexity of power grid planning, operation, and management. Traditional rule-based systems and numerical optimization approaches often struggle with the scale, dynamics, and adaptability required by modern power networks. This paper introduces Grid-Agent, an autonomous, AI-driven framework that combines Large Language Models (LLMs) with multi-agent reinforcement learning to detect and remediate grid violations in real time. Grid-Agent integrates semantic reasoning with numerical precision through a modular agent architecture: a planning agent generates coordinated action sequences using numerical power flow solvers, while a validation agent evaluates system stability and action effectiveness via sandboxed execution with safety rollbacks. To ensure scalability, Grid-Agent incorporates an adaptive multiscale network representation that dynamically selects optimal encoding schemes based on network size and complexity. The framework enables coordinated violation resolution through optimizing switch configurations, battery deployment, and load curtailment strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE 69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation performance. Additionally, the framework's built-in data collection and learning capabilities enable continuous learning and adaptation to diverse network topologies. The autonomous nature of the framework makes it particularly suitable for modern smart grid applications requiring rapid response to dynamic operating conditions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes</title>
<link>https://arxiv.org/abs/2508.05705</link>
<guid>https://arxiv.org/abs/2508.05705</guid>
<content:encoded><![CDATA[
arXiv:2508.05705v1 Announce Type: cross 
Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is critical for developing personalized treatments and supporting data-driven clinical decisions. Existing models often miss key physiological aspects and are difficult to individualize. Here, we introduce physiologically-constrained neural network (NN) digital twins to simulate glucose dynamics in T1D. To ensure interpretability and physiological consistency, we first build a population-level NN state-space model aligned with a set of ordinary differential equations (ODEs) describing glucose regulation. This model is formally verified to conform to known T1D dynamics. Digital twins are then created by augmenting the population model with individual-specific models, which include personal data, such as glucose management and contextual information, capturing both inter- and intra-individual variability. We validate our approach using real-world data from the T1D Exercise Initiative study. Two weeks of data per participant were split into 5-hour sequences and simulated glucose profiles were compared to observed ones. Clinically relevant outcomes were used to assess similarity via paired equivalence t-tests with predefined clinical equivalence margins. Across 394 digital twins, glucose outcomes were equivalent between simulated and observed data: time in range (70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real; P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022); and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001). Our framework can incorporate unmodeled factors like sleep and activity while preserving key dynamics. This approach enables personalized in silico testing of treatments, supports insulin optimization, and integrates physics-based and data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05710</link>
<guid>https://arxiv.org/abs/2508.05710</guid>
<content:encoded><![CDATA[
arXiv:2508.05710v1 Announce Type: cross 
Abstract: Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: https://github.com/Kwai-Klear/CodeTest.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAPP: The CLASS LLM Agent for Pair Programming</title>
<link>https://arxiv.org/abs/2508.05728</link>
<guid>https://arxiv.org/abs/2508.05728</guid>
<content:encoded><![CDATA[
arXiv:2508.05728v1 Announce Type: cross 
Abstract: We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnGuide: Learning to Forget with LoRA-Guided Diffusion Models</title>
<link>https://arxiv.org/abs/2508.05755</link>
<guid>https://arxiv.org/abs/2508.05755</guid>
<content:encoded><![CDATA[
arXiv:2508.05755v1 Announce Type: cross 
Abstract: Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks</title>
<link>https://arxiv.org/abs/2508.05783</link>
<guid>https://arxiv.org/abs/2508.05783</guid>
<content:encoded><![CDATA[
arXiv:2508.05783v1 Announce Type: cross 
Abstract: Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data</title>
<link>https://arxiv.org/abs/2508.05791</link>
<guid>https://arxiv.org/abs/2508.05791</guid>
<content:encoded><![CDATA[
arXiv:2508.05791v1 Announce Type: cross 
Abstract: Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality. In this work, developed in collaboration with Oncor Electric Delivery, we propose a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. We observe that distribution topology is fundamentally governed by two complementary dimensions: the spatial layout of physical infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the system in the signal domain (e.g., voltage time series). When jointly leveraged, these dimensions support a complete and physically coherent reconstruction of network connectivity. To address the challenge of uneven data quality without compromising observability, we introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. This soft handling of uncertainty is tightly coupled with hard enforcement of physical feasibility: we embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process. Together, these components ensure that inference is both uncertainty-aware and structurally valid, enabling rapid convergence to actionable, trustworthy topologies under real-world deployment conditions. The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Guided Exploration of Large-Scale Codebases</title>
<link>https://arxiv.org/abs/2508.05799</link>
<guid>https://arxiv.org/abs/2508.05799</guid>
<content:encoded><![CDATA[
arXiv:2508.05799v1 Announce Type: cross 
Abstract: Understanding large-scale, complex software systems is a major challenge for developers, who spend a significant portion of their time on program comprehension. Traditional tools such as static visualizations and reverse engineering techniques provide structural insights but often lack interactivity, adaptability, and integration with contextual information. Recent advancements in large language models (LLMs) offer new opportunities to enhance code exploration workflows, yet their lack of grounding and integration with structured views limits their effectiveness. This work introduces a hybrid approach that integrates deterministic reverse engineering with LLM-guided, intent-aware visual exploration. The proposed system combines UML-based visualization, dynamic user interfaces, historical context, and collaborative features into an adaptive tool for code comprehension. By interpreting user queries and interaction patterns, the LLM helps developers navigate and understand complex codebases more effectively. A prototype implementation for Java demonstrates the feasibility of this approach. Future work includes empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM interaction models. This research lays the groundwork for intelligent, interactive environments that align with developer cognition and collaborative workflows.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction</title>
<link>https://arxiv.org/abs/2508.05838</link>
<guid>https://arxiv.org/abs/2508.05838</guid>
<content:encoded><![CDATA[
arXiv:2508.05838v1 Announce Type: cross 
Abstract: This paper presents a novel approach that integrates vision foundation models with reinforcement learning to enhance object interaction capabilities in simulated environments. By combining the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the AI2-THOR simulation environment, we enable the agent to perceive and interact with objects more effectively. Our comprehensive experiments, conducted across four diverse indoor kitchen settings, demonstrate significant improvements in object interaction success rates and navigation efficiency compared to a baseline agent without advanced perception. The results show a 68% increase in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency. These findings highlight the potential of integrating foundation models with reinforcement learning for complex robotic tasks, paving the way for more sophisticated and capable autonomous agents.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems</title>
<link>https://arxiv.org/abs/2508.05846</link>
<guid>https://arxiv.org/abs/2508.05846</guid>
<content:encoded><![CDATA[
arXiv:2508.05846v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models</title>
<link>https://arxiv.org/abs/2508.05880</link>
<guid>https://arxiv.org/abs/2508.05880</guid>
<content:encoded><![CDATA[
arXiv:2508.05880v1 Announce Type: cross 
Abstract: Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction</title>
<link>https://arxiv.org/abs/2508.05913</link>
<guid>https://arxiv.org/abs/2508.05913</guid>
<content:encoded><![CDATA[
arXiv:2508.05913v1 Announce Type: cross 
Abstract: As AI systems become increasingly embedded in organizational workflows and consumer applications, ethical principles such as fairness, transparency, and robustness have been widely endorsed in policy and industry guidelines. However, there is still scarce empirical evidence on whether these principles are recognized, valued, or impactful from the perspective of users. This study investigates the link between ethical AI and user satisfaction by analyzing over 100,000 user reviews of AI products from G2. Using transformer-based language models, we measure sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all seven dimensions are positively associated with user satisfaction. Yet, this relationship varies systematically across user and product types. Technical users and reviewers of AI development platforms more frequently discuss system-level concerns (e.g., transparency, data governance), while non-technical users and reviewers of end-user applications emphasize human-centric dimensions (e.g., human agency, societal well-being). Moreover, the association between ethical AI and user satisfaction is significantly stronger for non-technical users and end-user applications across all dimensions. Our results highlight the importance of ethical AI design from users' perspectives and underscore the need to account for contextual differences across user roles and product types.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm</title>
<link>https://arxiv.org/abs/2508.05923</link>
<guid>https://arxiv.org/abs/2508.05923</guid>
<content:encoded><![CDATA[
arXiv:2508.05923v1 Announce Type: cross 
Abstract: Software vulnerabilities continue to undermine the reliability and security of modern systems, particularly as software complexity outpaces the capabilities of traditional detection methods. This study introduces a genetic algorithm-based method for test input generation that innovatively integrates genetic operators and adaptive learning to enhance software vulnerability detection. A key contribution is the application of the crossover operator, which facilitates exploration by searching across a broader space of potential test inputs. Complementing this, an adaptive feedback mechanism continuously learns from the system's execution behavior and dynamically guides input generation toward promising areas of the input space. Rather than relying on fixed or randomly selected inputs, the approach evolves a population of structurally valid test cases using feedback-driven selection, enabling deeper and more effective code traversal. This strategic integration of exploration and exploitation ensures that both diverse and targeted test inputs are developed over time. Evaluation was conducted across nine open-source JSON-processing libraries. The proposed method achieved substantial improvements in coverage compared to a benchmark evolutionary fuzzing method, with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0% in line coverage, 114.0% in instruction coverage, and 166.0% in branch coverage. These results highlight the method's capacity to detect deeper and more complex vulnerabilities, offering a scalable and adaptive solution to software security testing.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition</title>
<link>https://arxiv.org/abs/2508.05933</link>
<guid>https://arxiv.org/abs/2508.05933</guid>
<content:encoded><![CDATA[
arXiv:2508.05933v1 Announce Type: cross 
Abstract: The affective brain-computer interface is a crucial technology for affective interaction and emotional intelligence, emerging as a significant area of research in the human-computer interaction. Compared to single-type features, multi-type EEG features provide a multi-level representation for analyzing multi-dimensional emotions. However, the high dimensionality of multi-type EEG features, combined with the relatively small number of high-quality EEG samples, poses challenges such as classifier overfitting and suboptimal real-time performance in multi-dimensional emotion recognition. Moreover, practical applications of affective brain-computer interface frequently encounters partial absence of multi-dimensional emotional labels due to the open nature of the acquisition environment, and ambiguity and variability in individual emotion perception. To address these challenges, this study proposes a novel EEG feature selection method for missing multi-dimensional emotion recognition. The method leverages adaptive orthogonal non-negative matrix factorization to reconstruct the multi-dimensional emotional label space through second-order and higher-order correlations, which could reduce the negative impact of missing values and outliers on label reconstruction. Simultaneously, it employs least squares regression with graph-based manifold learning regularization and global feature redundancy minimization regularization to enable EEG feature subset selection despite missing information, ultimately achieving robust EEG-based multi-dimensional emotion recognition. Simulation experiments on three widely used multi-dimensional emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method outperforms thirteen advanced feature selection methods in terms of robustness for EEG emotional feature selection.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection</title>
<link>https://arxiv.org/abs/2508.05934</link>
<guid>https://arxiv.org/abs/2508.05934</guid>
<content:encoded><![CDATA[
arXiv:2508.05934v1 Announce Type: cross 
Abstract: Recently, multi-modal physiological signals based emotion recognition has garnered increasing attention in the field of brain-computer interfaces. Nevertheness, the associated multi-modal physiological features are often high-dimensional and inevitably include irrelevant, redundant, and noisy representation, which can easily lead to overfitting, poor performance, and high computational complexity in emotion classifiers. Feature selection has been widely applied to address these challenges. However, previous studies generally assumed that multi-modal physiological data are complete, whereas in reality, the data are often incomplete due to the openness of the acquisition and operational environment. For example, a part of samples are available in several modalities but not in others. To address this issue, we propose a novel method for incomplete multi-modal physiological signal feature selection called adaptive shared latent structure learning (ASLSL). Based on the property that similar features share similar emotional labels, ASLSL employs adaptive shared latent structure learning to explore a common latent space shared for incomplete multi-modal physiological signals and multi-dimensional emotional labels, thereby mitigating the impact of missing information and mining consensus information. Two most popular multi-modal physiological emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were utilized to compare the performance between compare ASLSL and seventeen feature selection methods. Comprehensive experimental results on these datasets demonstrate the effectiveness of ASLSL.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale</title>
<link>https://arxiv.org/abs/2508.05938</link>
<guid>https://arxiv.org/abs/2508.05938</guid>
<content:encoded><![CDATA[
arXiv:2508.05938v1 Announce Type: cross 
Abstract: Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\sim$35\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image</title>
<link>https://arxiv.org/abs/2508.05950</link>
<guid>https://arxiv.org/abs/2508.05950</guid>
<content:encoded><![CDATA[
arXiv:2508.05950v1 Announce Type: cross 
Abstract: The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents</title>
<link>https://arxiv.org/abs/2508.05954</link>
<guid>https://arxiv.org/abs/2508.05954</guid>
<content:encoded><![CDATA[
arXiv:2508.05954v1 Announce Type: cross 
Abstract: There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Armed Bandits-Based Optimization of Decision Trees</title>
<link>https://arxiv.org/abs/2508.05957</link>
<guid>https://arxiv.org/abs/2508.05957</guid>
<content:encoded><![CDATA[
arXiv:2508.05957v1 Announce Type: cross 
Abstract: Decision trees, without appropriate constraints, can easily become overly complex and prone to overfit, capturing noise rather than generalizable patterns. To resolve this problem,pruning operation is a crucial part in optimizing decision trees, as it not only reduces the complexity of trees but also decreases the probability of generating overfit models. The conventional pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning (REP) are mostly based on greedy approaches that focus on immediate gains in performance while pruning nodes of the decision tree. However, this might result in a lower generalization in the long run, compromising the robust ability of the tree model when introduced to unseen data samples, particularly when trained with small and complex datasets. To address this challenge, we are proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement learning (RL)-based technique, that will dynamically prune the tree to generate an optimal decision tree with better generalization. Our proposed approach assumes the pruning process as an exploration-exploitation problem, where we are utilizing the MAB algorithms to find optimal branch nodes to prune based on feedback from each pruning actions. Experimental evaluation on several benchmark datasets, demonstrated that our proposed approach results in better predictive performance compared to the traditional ones. This suggests the potential of utilizing MAB for a dynamic and probabilistic way of decision tree pruning, in turn optimizing the decision tree-based model.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05960</link>
<guid>https://arxiv.org/abs/2508.05960</guid>
<content:encoded><![CDATA[
arXiv:2508.05960v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without further environment interaction. A key challenge is the distribution shift between the learned and behavior policies, leading to out-of-distribution (OOD) actions and overestimation. To prevent gross overestimation, the value function must remain conservative; however, excessive conservatism may hinder performance improvement. To address this, we propose the mildly conservative regularized evaluation (MCRE) framework, which balances conservatism and performance by combining temporal difference (TD) error with a behavior cloning term in the Bellman backup. Building on this, we develop the mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates MCRE into an off-policy actor-critic framework. Experiments show that MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact-driven Context Filtering For Cross-file Code Completion</title>
<link>https://arxiv.org/abs/2508.05970</link>
<guid>https://arxiv.org/abs/2508.05970</guid>
<content:encoded><![CDATA[
arXiv:2508.05970v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable potential for repository-level code completion, as it integrates cross-file knowledge with in-file preceding code to provide comprehensive contexts for generation. To better understand the contribution of the retrieved cross-file contexts, we introduce a likelihood-based metric to evaluate the impact of each retrieved code chunk on the completion. Our analysis reveals that, despite retrieving numerous chunks, only a small subset positively contributes to the completion, while some chunks even degrade performance. To address this issue, we leverage this metric to construct a repository-level dataset where each retrieved chunk is labeled as positive, neutral, or negative based on its relevance to the target completion. We then propose an adaptive retrieval context filtering framework, CODEFILTER, trained on this dataset to mitigate the harmful effects of negative retrieved contexts in code completion. Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks demonstrates that CODEFILTER consistently improves completion accuracy compared to approaches without filtering operations across various tasks. Additionally, CODEFILTER significantly reduces the length of the input prompt, enhancing computational efficiency while exhibiting strong generalizability across different models. These results underscore the potential of CODEFILTER to enhance the accuracy, efficiency, and attributability of repository-level code completion.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching</title>
<link>https://arxiv.org/abs/2508.05978</link>
<guid>https://arxiv.org/abs/2508.05978</guid>
<content:encoded><![CDATA[
arXiv:2508.05978v1 Announce Type: cross 
Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target while keeping melody and lyrics. The key challenge in any-to-any SVC is adapting unseen speaker timbres to source audio without quality degradation. Existing methods either face timbre leakage or fail to achieve satisfactory timbre similarity and quality in the generated audio. To address these challenges, we propose DAFMSVC, where the self-supervised learning (SSL) features from the source audio are replaced with the most similar SSL features from the target audio to prevent timbre leakage. It also incorporates a dual cross-attention mechanism for the adaptive fusion of speaker embeddings, melody, and linguistic content. Additionally, we introduce a flow matching module for high quality audio generation from the fused features. Experimental results show that DAFMSVC significantly enhances timbre similarity and naturalness, outperforming state-of-the-art methods in both subjective and objective evaluations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education</title>
<link>https://arxiv.org/abs/2508.05979</link>
<guid>https://arxiv.org/abs/2508.05979</guid>
<content:encoded><![CDATA[
arXiv:2508.05979v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) are often used as virtual tutors in computer science (CS) education, this approach can foster passive learning and over-reliance. This paper presents a novel pedagogical paradigm that inverts this model: students act as instructors who must teach an LLM to solve problems. To facilitate this, we developed strategies for designing questions with engineered knowledge gaps that only a student can bridge, and we introduce Socrates, a system for deploying this method with minimal overhead. We evaluated our approach in an undergraduate course and found that this active-learning method led to statistically significant improvements in student performance compared to historical cohorts. Our work demonstrates a practical, cost-effective framework for using LLMs to deepen student engagement and mastery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETA: Energy-based Test-time Adaptation for Depth Completion</title>
<link>https://arxiv.org/abs/2508.05989</link>
<guid>https://arxiv.org/abs/2508.05989</guid>
<content:encoded><![CDATA[
arXiv:2508.05989v1 Announce Type: cross 
Abstract: We propose a method for test-time adaptation of pretrained depth completion models. Depth completion models, trained on some ``source'' data, often predict erroneous outputs when transferred to ``target'' data captured in novel environmental conditions due to a covariate shift. The crux of our method lies in quantifying the likelihood of depth predictions belonging to the source data distribution. The challenge is in the lack of access to out-of-distribution (target) data prior to deployment. Hence, rather than making assumptions regarding the target distribution, we utilize adversarial perturbations as a mechanism to explore the data space. This enables us to train an energy model that scores local regions of depth predictions as in- or out-of-distribution. We update the parameters of pretrained depth completion models at test time to minimize energy, effectively aligning test-time predictions to those of the source distribution. We call our method ``Energy-based Test-time Adaptation'', or ETA for short. We evaluate our method across three indoor and three outdoor datasets, where ETA improve over the previous state-of-the-art method by an average of 6.94% for outdoors and 10.23% for indoors. Project Page: https://fuzzythecat.github.io/eta.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge</title>
<link>https://arxiv.org/abs/2508.05991</link>
<guid>https://arxiv.org/abs/2508.05991</guid>
<content:encoded><![CDATA[
arXiv:2508.05991v1 Announce Type: cross 
Abstract: Emotion recognition plays a vital role in enhancing human-computer interaction. In this study, we tackle the MER-SEMI challenge of the MER2025 competition by proposing a novel multimodal emotion recognition framework. To address the issue of data scarcity, we leverage large-scale pre-trained models to extract informative features from visual, audio, and textual modalities. Specifically, for the visual modality, we design a dual-branch visual encoder that captures both global frame-level features and localized facial representations. For the textual modality, we introduce a context-enriched method that employs large language models to enrich emotional cues within the input text. To effectively integrate these multimodal features, we propose a fusion strategy comprising two key components, i.e., self-attention mechanisms for dynamic modality weighting, and residual connections to preserve original representations. Beyond architectural design, we further refine noisy labels in the training set by a multi-source labeling strategy. Our approach achieves a substantial performance improvement over the official baseline on the MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to 78.63%, thereby validating the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning</title>
<link>https://arxiv.org/abs/2508.06000</link>
<guid>https://arxiv.org/abs/2508.06000</guid>
<content:encoded><![CDATA[
arXiv:2508.06000v1 Announce Type: cross 
Abstract: Operational skill learning, inherently physical and reliant on hands-on practice and kinesthetic feedback, has yet to be effectively replicated in large language model (LLM)-supported training. Current LLM training assistants primarily generate customized textual feedback, neglecting the crucial kinesthetic modality. This gap derives from the textual and uncertain nature of LLMs, compounded by concerns on user acceptance of LLM driven body control. To bridge this gap and realize the potential of collaborative human-LLM action, this work explores human experience of LLM driven kinesthetic assistance. Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS) for flight skill acquisition, a representative operational skill domain. FlightAxis learns flight skills from manuals and guides forearm movements during simulated flight tasks. Our results demonstrate high user acceptance of LLM-mediated body control and significantly reduced task completion times. Crucially, trainees reported that this kinesthetic assistance enhanced their awareness of operation flaws and fostered increased engagement in the training process, rather than relieving perceived load. This work demonstrated the potential of kinesthetic LLM training in operational skill acquisition.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crisp Attention: Regularizing Transformers via Structured Sparsity</title>
<link>https://arxiv.org/abs/2508.06016</link>
<guid>https://arxiv.org/abs/2508.06016</guid>
<content:encoded><![CDATA[
arXiv:2508.06016v1 Announce Type: cross 
Abstract: The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. While attention sparsity is widely studied as a technique to improve computational efficiency, it is almost universally assumed to come at the cost of model accuracy. In this paper, we report a surprising counter-example to this common wisdom. By introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, we find that model accuracy improves significantly. Our model with 80\% attention sparsity achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over the dense baseline. We hypothesize that this phenomenon is due to sparsity acting as a powerful implicit regularizer, preventing the model from overfitting by forcing it to make predictions with a more constrained and robust set of features. Our work recasts attention sparsity not just as a tool for computational efficiency, but as a potential method for improving the generalization and performance of Transformer models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</title>
<link>https://arxiv.org/abs/2508.06021</link>
<guid>https://arxiv.org/abs/2508.06021</guid>
<content:encoded><![CDATA[
arXiv:2508.06021v1 Announce Type: cross 
Abstract: Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</title>
<link>https://arxiv.org/abs/2508.06026</link>
<guid>https://arxiv.org/abs/2508.06026</guid>
<content:encoded><![CDATA[
arXiv:2508.06026v1 Announce Type: cross 
Abstract: Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity</title>
<link>https://arxiv.org/abs/2508.06034</link>
<guid>https://arxiv.org/abs/2508.06034</guid>
<content:encoded><![CDATA[
arXiv:2508.06034v1 Announce Type: cross 
Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often exhibit heterophily. However, most existing studies focus on either heterogeneity or heterophily in isolation, overlooking the prevalence of heterophilic HGs in practical applications. Such ignorance leads to their performance degradation. In this work, we first identify two main challenges in modeling heterophily HGs: (1) varying heterophily distributions across hops and meta-paths; (2) the intricate and often heterophily-driven diversity of semantic information across different meta-paths. Then, we propose the Adaptive Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN employs a heterophily-aware convolution that accounts for heterophily distributions specific to both hops and meta-paths. It then integrates messages from diverse semantic spaces using a coarse-to-fine attention mechanism, which filters out noise and emphasizes informative signals. Experiments on seven real-world graphs and twenty baselines demonstrate the superior performance of AHGNN, particularly in high-heterophily situations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.06038</link>
<guid>https://arxiv.org/abs/2508.06038</guid>
<content:encoded><![CDATA[
arXiv:2508.06038v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) typically replace the predefined image placeholder token () in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of vision tokens significantly increases the context length, leading to high computational overhead and inference latency. While previous efforts mitigate this by selecting only important visual features or leveraging learnable queries to reduce token count, they often compromise performance or introduce substantial extra costs. In response, we propose Fourier-VLM, a simple yet efficient method that compresses visual representations in the frequency domain. Our approach is motivated by the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components. Leveraging this, we apply a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$, minimizing the extra computational cost while introducing no additional parameters. Extensive experiments across various image-based benchmarks demonstrate that Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</title>
<link>https://arxiv.org/abs/2508.06041</link>
<guid>https://arxiv.org/abs/2508.06041</guid>
<content:encoded><![CDATA[
arXiv:2508.06041v1 Announce Type: cross 
Abstract: How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding iterations. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. DP-LLM augments each linear layer in an LLM with a precision selector that determines the bitwidth at runtime using a lightweight error estimator and threshold values learned through fine-tuning. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</title>
<link>https://arxiv.org/abs/2508.06046</link>
<guid>https://arxiv.org/abs/2508.06046</guid>
<content:encoded><![CDATA[
arXiv:2508.06046v1 Announce Type: cross 
Abstract: Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation</title>
<link>https://arxiv.org/abs/2508.06065</link>
<guid>https://arxiv.org/abs/2508.06065</guid>
<content:encoded><![CDATA[
arXiv:2508.06065v1 Announce Type: cross 
Abstract: Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
<link>https://arxiv.org/abs/2508.06066</link>
<guid>https://arxiv.org/abs/2508.06066</guid>
<content:encoded><![CDATA[
arXiv:2508.06066v1 Announce Type: cross 
Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs) achieve strong predictive performance on sequential data, yet theoretical understanding of their generalization remains limited. We address this gap by providing both the first non-vacuous, architecture-aware generalization bounds for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $ O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our delayed-feedback blocking mechanism transforms dependent samples into effectively independent ones while discarding only $O(1/\log N)$ of the data, yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective sample size to isolate the effect of temporal structure from information content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences ($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly dependent ones ($\rho=0.2$), challenging the intuition that dependence is purely detrimental. Yet convergence rates diverge from theory: weak dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$. These findings reveal that temporal dependence can enhance learning under fixed information budgets, while highlighting gaps between theory and practice that motivate future research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Models Fool the Eye? A New Turing Test for Biological Animation</title>
<link>https://arxiv.org/abs/2508.06072</link>
<guid>https://arxiv.org/abs/2508.06072</guid>
<content:encoded><![CDATA[
arXiv:2508.06072v1 Announce Type: cross 
Abstract: Evaluating the abilities of large models and manifesting their gaps are challenging. Current benchmarks adopt either ground-truth-based score-form evaluation on static datasets or indistinct textual chatbot-style human preferences collection, which may not provide users with immediate, intuitive, and perceptible feedback on performance differences. In this paper, we introduce BioMotion Arena, a novel framework for evaluating large language models (LLMs) and multimodal large language models (MLLMs) via visual animation. Our methodology draws inspiration from the inherent visual perception of motion patterns characteristic of living organisms that utilizes point-light source imaging to amplify the performance discrepancies between models. Specifically, we employ a pairwise comparison evaluation and collect more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion variants. Data analyses show that the crowd-sourced human votes are in good agreement with those of expert raters, demonstrating the superiority of our BioMotion Arena in offering discriminative feedback. We also find that over 90\% of evaluated models, including the cutting-edge open-source InternVL3 and proprietary Claude-4 series, fail to produce fundamental humanoid point-light groups, much less smooth and biologically plausible motions. This enables BioMotion Arena to serve as a challenging benchmark for performance visualization and a flexible evaluation framework without restrictions on ground-truth.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards MR-Based Trochleoplasty Planning</title>
<link>https://arxiv.org/abs/2508.06076</link>
<guid>https://arxiv.org/abs/2508.06076</guid>
<content:encoded><![CDATA[
arXiv:2508.06076v1 Announce Type: cross 
Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at https://wehrlimi.github.io/sr-3d-planning/.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounding Distributional Shifts in World Modeling through Novelty Detection</title>
<link>https://arxiv.org/abs/2508.06096</link>
<guid>https://arxiv.org/abs/2508.06096</guid>
<content:encoded><![CDATA[
arXiv:2508.06096v1 Announce Type: cross 
Abstract: Recent work on visual world models shows significant promise in latent state dynamics obtained from pre-trained image backbones. However, most of the current approaches are sensitive to training quality, requiring near-complete coverage of the action and state space during training to prevent divergence during inference. To make a model-based planning algorithm more robust to the quality of the learned world model, we propose in this work to use a variational autoencoder as a novelty detector to ensure that proposed action trajectories during planning do not cause the learned model to deviate from the training data distribution. To evaluate the effectiveness of this approach, a series of experiments in challenging simulated robot environments was carried out, with the proposed method incorporated into a model-predictive control policy loop extending the DINO-WM architecture. The results clearly show that the proposed method improves over state-of-the-art solutions in terms of data efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</title>
<link>https://arxiv.org/abs/2508.06098</link>
<guid>https://arxiv.org/abs/2508.06098</guid>
<content:encoded><![CDATA[
arXiv:2508.06098v1 Announce Type: cross 
Abstract: Recent developments in diffusion- and flow- based models have significantly advanced Text-to-Audio Generation (TTA). While achieving great synthesis quality and controllability, current TTA systems still suffer from slow inference speed, which significantly limits their practical applicability. This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation. Built on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by mapping directly from the start to the endpoint of the flow trajectory. By incorporating classifier-free guidance (CFG) into the training target, MeanAudio incurs no additional cost in the guided sampling process. To further stabilize training, we propose an instantaneous-to-mean curriculum with flow field mix-up, which encourages the model to first learn the foundational instantaneous dynamics, and then gradually adapt to mean flows. This strategy proves critical for enhancing training efficiency and generation quality. Experimental results demonstrate that MeanAudio achieves state-of-the-art performance in single-step audio generation. Specifically, it achieves a real time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates strong performance in multi-step generation, enabling smooth and coherent transitions across successive synthesis steps.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask &amp; Match: Learning to Recognize Handwritten Math with Self-Supervised Attention</title>
<link>https://arxiv.org/abs/2508.06107</link>
<guid>https://arxiv.org/abs/2508.06107</guid>
<content:encoded><![CDATA[
arXiv:2508.06107v1 Announce Type: cross 
Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task due to the inherent two-dimensional structure, varying symbol scales, and complex spatial relationships among symbols. In this paper, we present a self-supervised learning (SSL) framework for HMER that eliminates the need for expensive labeled data. Our approach begins by pretraining an image encoder using a combination of global and local contrastive loss, enabling the model to learn both holistic and fine-grained representations. A key contribution of this work is a novel self-supervised attention network, which is trained using a progressive spatial masking strategy. This attention mechanism is designed to learn semantically meaningful focus regions, such as operators, exponents, and nested mathematical notation, without requiring any supervision. The progressive masking curriculum encourages the network to become increasingly robust to missing or occluded visual information, ultimately improving structural understanding. Our complete pipeline consists of (1) self-supervised pretraining of the encoder, (2) self-supervised attention learning, and (3) supervised fine-tuning with a transformer decoder to generate LATEX sequences. Extensive experiments on CROHME benchmarks demonstrate that our method outperforms existing SSL and fully supervised baselines, validating the effectiveness of our progressive attention mechanism in enhancing HMER performance. Our codebase can be found here.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06108</link>
<guid>https://arxiv.org/abs/2508.06108</guid>
<content:encoded><![CDATA[
arXiv:2508.06108v1 Announce Type: cross 
Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a fundamental challenge in reinforcement learning. While hindsight experience replay (HER) has shown promise by relabeling collected trajectories with achieved goals, we argue that trajectory relabeling alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned Regularization (HGR), a technique that generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR), our approach enables off-policy RL algorithms to maximize experience utilization. Compared to existing GCRL methods that employ HER and self-imitation techniques, our hindsight regularizations achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMCE-Net++: Feature Map Convergence Evaluation and Training</title>
<link>https://arxiv.org/abs/2508.06109</link>
<guid>https://arxiv.org/abs/2508.06109</guid>
<content:encoded><![CDATA[
arXiv:2508.06109v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their opaque internal representations. While Feature Map Convergence Evaluation (FMCE) quantifies module-level convergence via Feature Map Convergence Scores (FMCS), it lacks experimental validation and closed-loop integration. To address this limitation, we propose FMCE-Net++, a novel training framework that integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module generates FMCS predictions, which, combined with task labels, jointly supervise backbone optimization through a Representation Auxiliary Loss. The RAL dynamically balances the primary classification loss and feature convergence optimization via a tunable \Representation Abstraction Factor. Extensive experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100 demonstrate that FMCE-Net++ consistently enhances model performance without architectural modifications or additional data. Key experimental outcomes include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp (ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate state-of-the-art performance ceilings.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Serving Optimization with Variable Prefill and Decode Lengths</title>
<link>https://arxiv.org/abs/2508.06133</link>
<guid>https://arxiv.org/abs/2508.06133</guid>
<content:encoded><![CDATA[
arXiv:2508.06133v1 Announce Type: cross 
Abstract: We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.06135</link>
<guid>https://arxiv.org/abs/2508.06135</guid>
<content:encoded><![CDATA[
arXiv:2508.06135v1 Announce Type: cross 
Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large language models (LLMs) into compact, efficient student models. However, existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. To address these limitations, we propose Selective Reflection Distillation (SRD), a novel data curation framework that leverages reflections from student models to systematically refine training data. SRD dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, selectively curating high-quality, student-compatible training instances through automated ranking based on difficulty. Furthermore, after selecting the training data, a curriculum scheduling strategy is employed to incrementally introduce these curated subsets into the distillation process at fixed intervals. As a plug-and-play enhancement, SRD consistently improves distillation outcomes across diverse white-box KD approaches and model architectures, as well as decreases computational cost significantly during KD training. Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. Notably, SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms. Our findings highlight that data quality and compatibility are pivotal to effective and efficient distillation of LLMs, and SRD provides a principled framework to achieve both. This work advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation</title>
<link>https://arxiv.org/abs/2508.06136</link>
<guid>https://arxiv.org/abs/2508.06136</guid>
<content:encoded><![CDATA[
arXiv:2508.06136v1 Announce Type: cross 
Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Item Graph Enhancement for Multimodal Recommendation</title>
<link>https://arxiv.org/abs/2508.06154</link>
<guid>https://arxiv.org/abs/2508.06154</guid>
<content:encoded><![CDATA[
arXiv:2508.06154v1 Announce Type: cross 
Abstract: Multimodal recommendation systems have attracted increasing attention for their improved performance by leveraging items' multimodal information. Prior methods often build modality-specific item-item semantic graphs from raw modality features and use them as supplementary structures alongside the user-item interaction graph to enhance user preference learning. However, these semantic graphs suffer from semantic deficiencies, including (1) insufficient modeling of collaborative signals among items and (2) structural distortions introduced by noise in raw modality features, ultimately compromising performance. To address these issues, we first extract collaborative signals from the interaction graph and infuse them into each modality-specific item semantic graph to enhance semantic modeling. Then, we design a modulus-based personalized embedding perturbation mechanism that injects perturbations with modulus-guided personalized intensity into embeddings to generate contrastive views. This enables the model to learn noise-robust representations through contrastive learning, thereby reducing the effect of structural noise in semantic graphs. Besides, we propose a dual representation alignment mechanism that first aligns multiple semantic representations via a designed Anchor-based InfoNCE loss using behavior representations as anchors, and then aligns behavior representations with the fused semantics by standard InfoNCE, to ensure representation consistency. Extensive experiments on four benchmark datasets validate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging</title>
<link>https://arxiv.org/abs/2508.06163</link>
<guid>https://arxiv.org/abs/2508.06163</guid>
<content:encoded><![CDATA[
arXiv:2508.06163v1 Announce Type: cross 
Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \textbf{TADrop} (\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06165</link>
<guid>https://arxiv.org/abs/2508.06165</guid>
<content:encoded><![CDATA[
arXiv:2508.06165v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.06169</link>
<guid>https://arxiv.org/abs/2508.06169</guid>
<content:encoded><![CDATA[
arXiv:2508.06169v1 Announce Type: cross 
Abstract: Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation</title>
<link>https://arxiv.org/abs/2508.06170</link>
<guid>https://arxiv.org/abs/2508.06170</guid>
<content:encoded><![CDATA[
arXiv:2508.06170v1 Announce Type: cross 
Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer, which is one of the main causes of cancer-related mortality globally; hence, it is deemed an essential technique for the prevention and early detection of colorectal cancer. The research introduces a unique multidirectional architectural framework to automate polyp detection within colonoscopy images while helping resolve limited healthcare dataset sizes and annotation complexities. The research implements a comprehensive system that delivers synthetic data generation through Stable Diffusion enhancements together with detection and segmentation algorithms. This detection approach combines Faster R-CNN for initial object localization while the Segment Anything Model (SAM) refines the segmentation masks. The faster R-CNN detection algorithm achieved a recall of 93.08% combined with a precision of 88.97% and an F1 score of 90.98%.SAM is then used to generate the image mask. The research evaluated five state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet, and MANet using ResNet34 as a base model. The results demonstrate the superior performance of FPN with the highest scores of PSNR (7.205893) and SSIM (0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced performance in IoU (64.20%) and Dice score (77.53%).
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Federated Clustering with Random Rebalancing</title>
<link>https://arxiv.org/abs/2508.06183</link>
<guid>https://arxiv.org/abs/2508.06183</guid>
<content:encoded><![CDATA[
arXiv:2508.06183v1 Announce Type: cross 
Abstract: Federated clustering aims to group similar clients into clusters and produce one model for each cluster. Such a personalization approach typically improves model performance compared with training a single model to serve all clients, but can be more vulnerable to privacy leakage. Directly applying client-level differentially private (DP) mechanisms to federated clustering could degrade the utilities significantly. We identify that such deficiencies are mainly due to the difficulties of averaging privacy noise within each cluster (following standard privacy mechanisms), as the number of clients assigned to the same clusters is uncontrolled. To this end, we propose a simple and effective technique, named RR-Cluster, that can be viewed as a light-weight add-on to many federated clustering algorithms. RR-Cluster achieves reduced privacy noise via randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster. We analyze the tradeoffs between decreased privacy noise variance and potentially increased bias from incorrect assignments and provide convergence bounds for RR-Clsuter. Empirically, we demonstrate the RR-Cluster plugged into strong federated clustering algorithms results in significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning</title>
<link>https://arxiv.org/abs/2508.06199</link>
<guid>https://arxiv.org/abs/2508.06199</guid>
<content:encoded><![CDATA[
arXiv:2508.06199v1 Announce Type: cross 
Abstract: Pretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2508.06202</link>
<guid>https://arxiv.org/abs/2508.06202</guid>
<content:encoded><![CDATA[
arXiv:2508.06202v1 Announce Type: cross 
Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification is a RAG problem: A case study on hate speech detection</title>
<link>https://arxiv.org/abs/2508.06204</link>
<guid>https://arxiv.org/abs/2508.06204</guid>
<content:encoded><![CDATA[
arXiv:2508.06204v1 Announce Type: cross 
Abstract: Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Federated Learning for Personalized Privacy Recommendation</title>
<link>https://arxiv.org/abs/2508.06208</link>
<guid>https://arxiv.org/abs/2508.06208</guid>
<content:encoded><![CDATA[
arXiv:2508.06208v1 Announce Type: cross 
Abstract: Federated recommendation systems (FedRecs) have gained significant attention for providing privacy-preserving recommendation services. However, existing FedRecs assume that all users have the same requirements for privacy protection, i.e., they do not upload any data to the server. The approaches overlook the potential to enhance the recommendation service by utilizing publicly available user data. In real-world applications, users can choose to be private or public. Private users' interaction data is not shared, while public users' interaction data can be shared. Inspired by the issue, this paper proposes a novel Graph Federated Learning for Personalized Privacy Recommendation (GFed-PP) that adapts to different privacy requirements while improving recommendation performance. GFed-PP incorporates the interaction data of public users to build a user-item interaction graph, which is then used to form a user relationship graph. A lightweight graph convolutional network (GCN) is employed to learn each user's user-specific personalized item embedding. To protect user privacy, each client learns the user embedding and the scoring function locally. Additionally, GFed-PP achieves optimization of the federated recommendation framework through the initialization of item embedding on clients and the aggregation of the user relationship graph on the server. Experimental results demonstrate that GFed-PP significantly outperforms existing methods for five datasets, offering superior recommendation accuracy without compromising privacy. This framework provides a practical solution for accommodating varying privacy preferences in federated recommendation systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterization Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2508.06214</link>
<guid>https://arxiv.org/abs/2508.06214</guid>
<content:encoded><![CDATA[
arXiv:2508.06214v1 Announce Type: cross 
Abstract: Reparameterization policy gradient (RPG) is promising for improving sample efficiency by leveraging differentiable dynamics. However, a critical barrier is its training instability, where high-variance gradients can destabilize the learning process. To address this, we draw inspiration from Proximal Policy Optimization (PPO), which uses a surrogate objective to enable stable sample reuse in the model-free setting. We first establish a connection between this surrogate objective and RPG, which has been largely unexplored and is non-trivial. Then, we bridge this gap by demonstrating that the reparameterization gradient of a PPO-like surrogate objective can be computed efficiently using backpropagation through time. Based on this key insight, we propose Reparameterization Proximal Policy Optimization (RPO), a stable and sample-efficient RPG-based method. RPO enables multiple epochs of stable sample reuse by optimizing a clipped surrogate objective tailored for RPG, while being further stabilized by Kullback-Leibler (KL) divergence regularization and remaining fully compatible with existing variance reduction methods. We evaluate RPO on a suite of challenging locomotion and manipulation tasks, where experiments demonstrate that our method achieves superior sample efficiency and strong performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</title>
<link>https://arxiv.org/abs/2508.06220</link>
<guid>https://arxiv.org/abs/2508.06220</guid>
<content:encoded><![CDATA[
arXiv:2508.06220v1 Announce Type: cross 
Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive capabilities in perception and reasoning. However, the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings. In this study, we introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans to ensure they cannot be answered based on surface-level cues alone but instead require genuine visual grounding. Our experimental results reveal that current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attack with Partial Features</title>
<link>https://arxiv.org/abs/2508.06244</link>
<guid>https://arxiv.org/abs/2508.06244</guid>
<content:encoded><![CDATA[
arXiv:2508.06244v1 Announce Type: cross 
Abstract: Machine learning models have been shown to be susceptible to membership inference attack, which can be used to determine whether a given sample appears in the training data. Existing membership inference methods commonly assume that the adversary has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features information is available, thereby limiting the applicability of these methods. In this work, we study an inference scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set of the target model. We define this problem as Partial Feature Membership Inference (PFMI). To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework. In the first stage, MRAD optimizes the unknown feature values to minimize the loss of the sample. In the second stage, it measures the deviation between the reconstructed sample and the training distribution using anomaly detection. Empirical results demonstrate that MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Training Defenses against Emergent Misalignment in Language Models</title>
<link>https://arxiv.org/abs/2508.06249</link>
<guid>https://arxiv.org/abs/2508.06249</guid>
<content:encoded><![CDATA[
arXiv:2508.06249v1 Announce Type: cross 
Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. We investigate four training regularization interventions: (i) KL-divergence regularization toward a safe reference model, (ii) $\ell_2$ distance in feature space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving of a small amount of safe training examples from a general instruct-tuning dataset. We first evaluate the methods' emergent misalignment effect across four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on benign tasks. We conclude with a discussion of open questions in emergent misalignment research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)</title>
<link>https://arxiv.org/abs/2508.06251</link>
<guid>https://arxiv.org/abs/2508.06251</guid>
<content:encoded><![CDATA[
arXiv:2508.06251v1 Announce Type: cross 
Abstract: Synthetic data generation is a key technique in modern artificial intelligence, addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models. In this work, we propose a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS). We benchmark the MPS-based generative model against state-of-the-art models such as CTGAN, VAE, and PrivBayes, focusing on both fidelity and privacy-preserving capabilities. To ensure differential privacy (DP), we integrate noise injection and gradient clipping during training, enabling privacy guarantees via R\'enyi Differential Privacy accounting. Across multiple metrics analyzing data fidelity and downstream machine learning task performance, our results show that MPS outperforms classical models, particularly under strict privacy constraints. This work highlights MPS as a promising tool for privacy-aware synthetic data generation. By combining the expressive power of tensor network representations with formal privacy mechanisms, the proposed approach offers an interpretable and scalable alternative for secure data sharing. Its structured design facilitates integration into sensitive domains where both data quality and confidentiality are critical.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.06259</link>
<guid>https://arxiv.org/abs/2508.06259</guid>
<content:encoded><![CDATA[
arXiv:2508.06259v1 Announce Type: cross 
Abstract: Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Considerations in Weighted Model Counting</title>
<link>https://arxiv.org/abs/2508.06264</link>
<guid>https://arxiv.org/abs/2508.06264</guid>
<content:encoded><![CDATA[
arXiv:2508.06264v1 Announce Type: cross 
Abstract: Weighted model counting computes the sum of the rational-valued weights associated with the satisfying assignments for a Boolean formula, where the weight of an assignment is given by the product of the weights assigned to the positive and negated variables comprising the assignment. Weighted model counting finds applications across a variety of domains including probabilistic reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly) converting the input formula into a form that enables arithmetic evaluation, using multiplication for conjunctions and addition for disjunctions. Performing this evaluation using floating-point arithmetic can yield inaccurate results, and it cannot quantify the level of precision achieved. Computing with rational arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to efficiently compute weighted model counts that are guaranteed to achieve a user-specified precision. When all weights are nonnegative, we prove that the precision loss of arithmetic evaluation using floating-point arithmetic can be tightly bounded. We show that supplementing a standard IEEE double-precision representation with a separate 64-bit exponent, a format we call extended-range double (ERD), avoids the underflow and overflow issues commonly encountered in weighted model counting. For problems with mixed negative and positive weights, we show that a combination of interval floating-point arithmetic and rational arithmetic can achieve the twin goals of efficiency and guaranteed precision. For our evaluations, we have devised especially challenging formulas and weight assignments, demonstrating the robustness of our approach.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OM2P: Offline Multi-Agent Mean-Flow Policy</title>
<link>https://arxiv.org/abs/2508.06269</link>
<guid>https://arxiv.org/abs/2508.06269</guid>
<content:encoded><![CDATA[
arXiv:2508.06269v1 Announce Type: cross 
Abstract: Generative models, especially diffusion and flow-based models, have been promising in offline multi-agent reinforcement learning. However, integrating powerful generative models into this framework poses unique challenges. In particular, diffusion and flow-based policies suffer from low sampling efficiency due to their iterative generation processes, making them impractical in time-sensitive or resource-constrained settings. To tackle these difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel offline MARL algorithm to achieve efficient one-step action sampling. To address the misalignment between generative objectives and reward maximization, we introduce a reward-aware optimization scheme that integrates a carefully-designed mean-flow matching loss with Q-function supervision. Additionally, we design a generalized timestep distribution and a derivative-free estimation strategy to reduce memory overhead and improve training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo benchmarks demonstrate that OM2P achieves superior performance, with up to a 3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time. Our approach represents the first to successfully integrate mean-flow model into offline MARL, paving the way for practical and scalable generative policies in cooperative multi-agent settings.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</title>
<link>https://arxiv.org/abs/2508.06287</link>
<guid>https://arxiv.org/abs/2508.06287</guid>
<content:encoded><![CDATA[
arXiv:2508.06287v1 Announce Type: cross 
Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields</title>
<link>https://arxiv.org/abs/2508.06301</link>
<guid>https://arxiv.org/abs/2508.06301</guid>
<content:encoded><![CDATA[
arXiv:2508.06301v1 Announce Type: cross 
Abstract: Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the client's private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.06318</link>
<guid>https://arxiv.org/abs/2508.06318</guid>
<content:encoded><![CDATA[
arXiv:2508.06318v1 Announce Type: cross 
Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</title>
<link>https://arxiv.org/abs/2508.06336</link>
<guid>https://arxiv.org/abs/2508.06336</guid>
<content:encoded><![CDATA[
arXiv:2508.06336v1 Announce Type: cross 
Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Approximate MMS Allocations on Restricted Graph Classes</title>
<link>https://arxiv.org/abs/2508.06343</link>
<guid>https://arxiv.org/abs/2508.06343</guid>
<content:encoded><![CDATA[
arXiv:2508.06343v1 Announce Type: cross 
Abstract: We study the problem of fair division of a set of indivisible goods with connectivity constraints. Specifically, we assume that the goods are represented as vertices of a connected graph, and sets of goods allocated to the agents are connected subgraphs of this graph. We focus on the widely-studied maximin share criterion of fairness. It has been shown that an allocation satisfying this criterion may not exist even without connectivity constraints, i.e., if the graph of goods is complete. In view of this, it is natural to seek approximate allocations that guarantee each agent a connected bundle of goods with value at least a constant fraction of the maximin share value to the agent. It is known that for some classes of graphs, such as complete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such approximate allocations indeed exist. However, it is an open problem whether they exist for the class of all graphs.
  In this paper, we continue the systematic study of the existence of approximate allocations on restricted graph classes. In particular, we show that such allocations exist for several well-studied classes, including block graphs, cacti, complete multipartite graphs, and split graphs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</title>
<link>https://arxiv.org/abs/2508.06345</link>
<guid>https://arxiv.org/abs/2508.06345</guid>
<content:encoded><![CDATA[
arXiv:2508.06345v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Equation-VAE: Disentangled Latent Representations for Tabular Data</title>
<link>https://arxiv.org/abs/2508.06347</link>
<guid>https://arxiv.org/abs/2508.06347</guid>
<content:encoded><![CDATA[
arXiv:2508.06347v1 Announce Type: cross 
Abstract: Learning interpretable latent representations from tabular data remains a challenge in deep generative modeling. We introduce SE-VAE (Structural Equation-Variational Autoencoder), a novel architecture that embeds measurement structure directly into the design of a variational autoencoder. Inspired by structural equation modeling, SE-VAE aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate construct-specific confounding variation. This modular architecture enables disentanglement through design rather than through statistical regularizers alone. We evaluate SE-VAE on a suite of simulated tabular datasets and benchmark its performance against a series of leading baselines using standard disentanglement metrics. SE-VAE consistently outperforms alternatives in factor recovery, interpretability, and robustness to nuisance variation. Ablation results reveal that architectural structure, rather than regularization strength, is the key driver of performance. SE-VAE offers a principled framework for white-box generative modeling in scientific and social domains where latent constructs are theory-driven and measurement validity is essential.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd</title>
<link>https://arxiv.org/abs/2508.06357</link>
<guid>https://arxiv.org/abs/2508.06357</guid>
<content:encoded><![CDATA[
arXiv:2508.06357v1 Announce Type: cross 
Abstract: A central problem in one-to-many facial identification is that the person in the probe image may or may not have enrolled image(s) in the gallery; that is, may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one result is Out-of-gallery have mostly focused on finding a suitable threshold on the similarity score. We take a new approach, using the additional enrolled images of the identity with the rank-one result to predict if the rank-one result is In-gallery / Out-of-gallery. Given a gallery of identities and images, we generate In-gallery and Out-of-gallery training data by extracting the ranks of additional enrolled images corresponding to the rank-one identity. We then train a classifier to utilize this feature vector to predict whether a rank-one result is In-gallery or Out-of-gallery. Using two different datasets and four different matchers, we present experimental results showing that our approach is viable for mugshot quality probe images, and also, importantly, for probes degraded by blur, reduced resolution, atmospheric turbulence and sunglasses. We also analyze results across demographic groups, and show that In-gallery / Out-of-gallery classification accuracy is similar across demographics. Our approach has the potential to provide an objective estimate of whether a one-to-many facial identification is Out-of-gallery, and thereby to reduce false positive identifications, wrongful arrests, and wasted investigative time. Interestingly, comparing the results of older deep CNN-based face matchers with newer ones suggests that the effectiveness of our Out-of-gallery detection approach emerges only with matchers trained using advanced margin-based loss functions.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</title>
<link>https://arxiv.org/abs/2508.06361</link>
<guid>https://arxiv.org/abs/2508.06361</guid>
<content:encoded><![CDATA[
arXiv:2508.06361v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been widely deployed in reasoning, planning, and decision-making tasks, making their trustworthiness a critical concern. The potential for intentional deception, where an LLM deliberately fabricates or conceals information to serve a hidden objective, remains a significant and underexplored threat. Existing studies typically induce such deception by explicitly setting a "hidden" objective through prompting or fine-tuning, which may not fully reflect real-world human-LLM interactions. Moving beyond this human-induced deception, we investigate LLMs' self-initiated deception on benign prompts. To address the absence of ground truth in this evaluation, we propose a novel framework using "contact searching questions." This framework introduces two statistical metrics derived from psychological principles to quantify the likelihood of deception. The first, the Deceptive Intention Score, measures the model's bias towards a hidden objective. The second, Deceptive Behavior Score, measures the inconsistency between the LLM's internal belief and its expressed output. Upon evaluating 14 leading LLMs, we find that both metrics escalate as task difficulty increases, rising in parallel for most models. Building on these findings, we formulate a mathematical model to explain this behavior. These results reveal that even the most advanced LLMs exhibit an increasing tendency toward deception when handling complex problems, raising critical concerns for the deployment of LLM agents in complex and crucial domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</title>
<link>https://arxiv.org/abs/2508.06364</link>
<guid>https://arxiv.org/abs/2508.06364</guid>
<content:encoded><![CDATA[
arXiv:2508.06364v1 Announce Type: cross 
Abstract: Achieving precise control over a molecule's biological activity-encompassing targeted activation/inhibition, cooperative multi-target modulation, and off-target toxicity mitigation-remains a critical challenge in de novo drug design. However, existing generative methods primarily focus on producing molecules with a single desired activity, lacking integrated mechanisms for the simultaneous management of multiple intended and unintended molecular interactions. Here, we propose ActivityDiff, a generative approach based on the classifier-guidance technique of diffusion models. It leverages separately trained drug-target classifiers for both positive and negative guidance, enabling the model to enhance desired activities while minimizing harmful off-target effects. Experimental results show that ActivityDiff effectively handles essential drug design tasks, including single-/dual-target generation, fragment-constrained dual-target design, selective generation to enhance target specificity, and reduction of off-target effects. These results demonstrate the effectiveness of classifier-guided diffusion in balancing efficacy and safety in molecular design. Overall, our work introduces a novel paradigm for achieving integrated control over molecular activity, and provides ActivityDiff as a versatile and extensible framework.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.06372</link>
<guid>https://arxiv.org/abs/2508.06372</guid>
<content:encoded><![CDATA[
arXiv:2508.06372v1 Announce Type: cross 
Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke when and what" within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation</title>
<link>https://arxiv.org/abs/2508.06387</link>
<guid>https://arxiv.org/abs/2508.06387</guid>
<content:encoded><![CDATA[
arXiv:2508.06387v1 Announce Type: cross 
Abstract: Text-to-SQL bridges the gap between natural language and structured database language, thus allowing non-technical users to easily query databases. Traditional approaches model text-to-SQL as a direct translation task, where a given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances in large language models (LLMs) have significantly improved translation accuracy, however, these methods all require that the target database is pre-specified. This becomes problematic in scenarios with multiple extensive databases, where identifying the correct database becomes a crucial yet overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL framework to identify the user's intended database before generating SQL queries. Our approach leverages LLMs and prompt engineering to extract implicit information from natural language queries (NLQs) in the form of a ruleset. We then train a large db\_id prediction model, which includes a RoBERTa-based finetuned encoder, to predict the correct Database identifier (db\_id) based on both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL by using critic agents to correct errors. Experimental results demonstrate that our framework outperforms the current state-of-the-art models in both database intent prediction and SQL generation accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity Increases Stability in Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2508.06389</link>
<guid>https://arxiv.org/abs/2508.06389</guid>
<content:encoded><![CDATA[
arXiv:2508.06389v1 Announce Type: cross 
Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of two-dimensional artificial organisms from a single seed cell. From the outset, NCA-grown organisms have had issues with stability, their natural boundary often breaking down and exhibiting tumour-like growth or failing to maintain the expected shape. In this paper, we present a method for improving the stability of NCA-grown organisms by introducing an 'identity' layer with simple constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with the original NCA model. Moreover, only a single identity value is required to achieve this increase in stability. We observe emergent movement from the stable organisms, with increasing prevalence for models with multiple identity values.
  This work lays the foundation for further study of the interaction between NCA-grown organisms, paving the way for studying social interaction at a cellular level in artificial organisms.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling</title>
<link>https://arxiv.org/abs/2508.06393</link>
<guid>https://arxiv.org/abs/2508.06393</guid>
<content:encoded><![CDATA[
arXiv:2508.06393v1 Announce Type: cross 
Abstract: Traditional speech separation and speaker diarization approaches rely on prior knowledge of target speakers or a predetermined number of participants in audio signals. To address these limitations, recent advances focus on developing enrollment-free methods capable of identifying targets without explicit speaker labeling. This work introduces a new approach to train simultaneous speech separation and diarization using automatic identification of target speaker embeddings, within mixtures. Our proposed model employs a dual-stage training pipeline designed to learn robust speaker representation features that are resilient to background noise interference. Furthermore, we present an overlapping spectral loss function specifically tailored for enhancing diarization accuracy during overlapped speech frames. Experimental results show significant performance gains compared to the current SOTA baseline, achieving 71% relative improvement in DER and 69% in cpWER.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</title>
<link>https://arxiv.org/abs/2508.06401</link>
<guid>https://arxiv.org/abs/2508.06401</guid>
<content:encoded><![CDATA[
arXiv:2508.06401v1 Announce Type: cross 
Abstract: This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery</title>
<link>https://arxiv.org/abs/2508.06407</link>
<guid>https://arxiv.org/abs/2508.06407</guid>
<content:encoded><![CDATA[
arXiv:2508.06407v1 Announce Type: cross 
Abstract: High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</title>
<link>https://arxiv.org/abs/2508.06411</link>
<guid>https://arxiv.org/abs/2508.06411</guid>
<content:encoded><![CDATA[
arXiv:2508.06411v1 Announce Type: cross 
Abstract: Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</title>
<link>https://arxiv.org/abs/2508.06426</link>
<guid>https://arxiv.org/abs/2508.06426</guid>
<content:encoded><![CDATA[
arXiv:2508.06426v1 Announce Type: cross 
Abstract: Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\pi_0$, in both simulation and real-world environments. More information at https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation</title>
<link>https://arxiv.org/abs/2508.06429</link>
<guid>https://arxiv.org/abs/2508.06429</guid>
<content:encoded><![CDATA[
arXiv:2508.06429v1 Announce Type: cross 
Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memp: Exploring Agent Procedural Memory</title>
<link>https://arxiv.org/abs/2508.06433</link>
<guid>https://arxiv.org/abs/2508.06433</guid>
<content:encoded><![CDATA[
arXiv:2508.06433v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</title>
<link>https://arxiv.org/abs/2508.06434</link>
<guid>https://arxiv.org/abs/2508.06434</guid>
<content:encoded><![CDATA[
arXiv:2508.06434v1 Announce Type: cross 
Abstract: Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the model's ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at https://github.com/T6Yang/CLIPin.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</title>
<link>https://arxiv.org/abs/2508.06435</link>
<guid>https://arxiv.org/abs/2508.06435</guid>
<content:encoded><![CDATA[
arXiv:2508.06435v1 Announce Type: cross 
Abstract: Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echoes of Automation: The Increasing Use of LLMs in Newsmaking</title>
<link>https://arxiv.org/abs/2508.06445</link>
<guid>https://arxiv.org/abs/2508.06445</guid>
<content:encoded><![CDATA[
arXiv:2508.06445v1 Announce Type: cross 
Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Embedded Swin-UMamba for DeepLesion Segmentation</title>
<link>https://arxiv.org/abs/2508.06453</link>
<guid>https://arxiv.org/abs/2508.06453</guid>
<content:encoded><![CDATA[
arXiv:2508.06453v1 Announce Type: cross 
Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title>
<link>https://arxiv.org/abs/2508.06457</link>
<guid>https://arxiv.org/abs/2508.06457</guid>
<content:encoded><![CDATA[
arXiv:2508.06457v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intuition emerges in Maximum Caliber models at criticality</title>
<link>https://arxiv.org/abs/2508.06477</link>
<guid>https://arxiv.org/abs/2508.06477</guid>
<content:encoded><![CDATA[
arXiv:2508.06477v1 Announce Type: cross 
Abstract: Whether large predictive models merely parrot their training data or produce genuine insight lacks a physical explanation. This work reports a primitive form of intuition that emerges as a metastable phase of learning that critically balances next-token prediction against future path-entropy. The intuition mechanism is discovered via mind-tuning, the minimal principle that imposes Maximum Caliber in predictive models with a control temperature-like parameter $\lambda$. Training on random walks in deterministic mazes reveals a rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination (high $\lambda$), and a fragile in-between window exhibiting strong protocol-dependence (hysteresis) and multistability, where models spontaneously discover novel goal-directed strategies. These results are captured by an effective low-dimensional theory and frame intuition as an emergent property at the critical balance between memorizing what is and wondering what could be.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-training for Efficient Communication via Convention Formation</title>
<link>https://arxiv.org/abs/2508.06482</link>
<guid>https://arxiv.org/abs/2508.06482</guid>
<content:encoded><![CDATA[
arXiv:2508.06482v1 Announce Type: cross 
Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v1 Announce Type: cross 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2407.00900</link>
<guid>https://arxiv.org/abs/2407.00900</guid>
<content:encoded><![CDATA[
arXiv:2407.00900v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) solely trained on next-token prediction learn to solve a wide range of problems involving mathematical reasoning. But how does this ability evolve during training? We show the first analysis of how mathematical reasoning abilities of several open-weight LLMs develop during pre-training and post-training. To this end, we construct MathCAMPS, a synthetic dataset of novel mathematical reasoning problems grounded in 44 fine-grained skills taken from the Common Core curriculum from K to 8th grades. In one experiment, we show that mathematical skills are learned during pre-training in an order that measurably correlates with the human-designed curriculum, even though training data are randomly ordered. We also show a detailed analysis of which mathematical abilities benefit from instruction tuning, a widely used post-training method and, in contrast, which skills suffer. Our work paves the way for an empirical understanding of LLM training dynamics in relation to reasoning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Your LLMs Capable of Stable Reasoning?</title>
<link>https://arxiv.org/abs/2412.13147</link>
<guid>https://arxiv.org/abs/2412.13147</guid>
<content:encoded><![CDATA[
arXiv:2412.13147v5 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce G-Pass@$k$, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model's performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@$k$ in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Foundations for Metacognition via Hybrid-AI</title>
<link>https://arxiv.org/abs/2502.05398</link>
<guid>https://arxiv.org/abs/2502.05398</guid>
<content:encoded><![CDATA[
arXiv:2502.05398v3 Announce Type: replace 
Abstract: Metacognition is the concept of reasoning about an agent's own internal processes, and it has recently received renewed attention with respect to artificial intelligence (AI) and, more specifically, machine learning systems. This paper reviews a hybrid-AI approach known as "error detecting and correcting rules" (EDCR) that allows for the learning of rules to correct perceptual (e.g., neural) models. Additionally, we introduce a probabilistic framework that adds rigor to prior empirical studies, and we use this framework to prove results on necessary and sufficient conditions for metacognitive improvement, as well as limits to the approach. A set of future
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11881</link>
<guid>https://arxiv.org/abs/2502.11881</guid>
<content:encoded><![CDATA[
arXiv:2502.11881v2 Announce Type: replace 
Abstract: Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o3 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Evaluation for Sequential Persuasion Process with Unobserved Confounding</title>
<link>https://arxiv.org/abs/2504.01211</link>
<guid>https://arxiv.org/abs/2504.01211</guid>
<content:encoded><![CDATA[
arXiv:2504.01211v2 Announce Type: replace 
Abstract: In this paper, we expand the Bayesian persuasion framework to account for unobserved confounding variables in sender-receiver interactions. While traditional models assume that belief updates follow Bayesian principles, real-world scenarios often involve hidden variables that impact the receiver's belief formation and decision-making. We conceptualize this as a sequential decision-making problem, where the sender and receiver interact over multiple rounds. In each round, the sender communicates with the receiver, who also interacts with the environment. Crucially, the receiver's belief update is affected by an unobserved confounding variable. By reformulating this scenario as a Partially Observable Markov Decision Process (POMDP), we capture the sender's incomplete information regarding both the dynamics of the receiver's beliefs and the unobserved confounder. We prove that finding an optimal observation-based policy in this POMDP is equivalent to solving for an optimal signaling strategy in the original persuasion framework. Furthermore, we demonstrate how this reformulation facilitates the application of proximal learning for off-policy evaluation in the persuasion process. This advancement enables the sender to evaluate alternative signaling strategies using only observational data from a behavioral policy, thus eliminating the necessity for costly new experiments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning</title>
<link>https://arxiv.org/abs/2504.14810</link>
<guid>https://arxiv.org/abs/2504.14810</guid>
<content:encoded><![CDATA[
arXiv:2504.14810v2 Announce Type: replace 
Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation. While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data. To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method. Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability. Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process. Experiments on mathematical tasks demonstrate that data selected by DONOD achieves superior fine-tuning efficiency and improved robustness against noisy data. By filtering out 70% of the whole dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile, our selected data present superior cross-architecture generalization. Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability. Code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemplative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.15125</link>
<guid>https://arxiv.org/abs/2504.15125</guid>
<content:encoded><![CDATA[
arXiv:2504.15125v2 Announce Type: replace 
Abstract: As artificial intelligence (AI) improves, traditional alignment strategies may falter in the face of unpredictable self-improvement, hidden subgoals, and the sheer complexity of intelligent systems. Inspired by contemplative wisdom traditions, we show how four axiomatic principles can instil a resilient Wise World Model in AI systems. First, mindfulness enables self-monitoring and recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal fixation and relaxes rigid priors. Third, non-duality dissolves adversarial self-other boundaries. Fourth, boundless care motivates the universal reduction of suffering. We find that prompting AI to reflect on these principles improves performance on the AILuminate Benchmark (d=.96) and boosts cooperation and joint-reward on the Prisoner's Dilemma task (d=7+). We offer detailed implementation strategies at the level of architectures, constitutions, and reinforcement on chain-of-thought. For future systems, active inference may offer the self-organizing and dynamic coupling capabilities needed to enact Contemplative AI in embodied agents.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping MOFs text mining with a dynamic multi-agents framework of large language model</title>
<link>https://arxiv.org/abs/2504.18880</link>
<guid>https://arxiv.org/abs/2504.18880</guid>
<content:encoded><![CDATA[
arXiv:2504.18880v3 Announce Type: replace 
Abstract: Accurately identifying the synthesis conditions of metal-organic frameworks (MOFs) is essential for guiding experimental design, yet remains challenging because relevant information in the literature is often scattered, inconsistent, and difficult to interpret. We present MOFh6, a large language model driven system that reads raw articles or crystal codes and converts them into standardized synthesis tables. It links related descriptions across paragraphs, unifies ligand abbreviations with full names, and outputs structured parameters ready for use. MOFh6 achieved 99% extraction accuracy, resolved 94.1% of abbreviation cases across five major publishers, and maintained a precision of 0.93 +/- 0.01. Processing a full text takes 9.6 s, locating synthesis descriptions 36 s, with 100 papers processed for USD 4.24. By replacing static database lookups with real-time extraction, MOFh6 reshapes MOF synthesis research, accelerating the conversion of literature knowledge into practical synthesis protocols and enabling scalable, data-driven materials discovery.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?</title>
<link>https://arxiv.org/abs/2505.09614</link>
<guid>https://arxiv.org/abs/2505.09614</guid>
<content:encoded><![CDATA[
arXiv:2505.09614v2 Announce Type: replace 
Abstract: Language model (LM) agents are increasingly used as autonomous decision-makers which need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established Blicket Test paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This "disjunctive bias" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not child-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning</title>
<link>https://arxiv.org/abs/2506.01096</link>
<guid>https://arxiv.org/abs/2506.01096</guid>
<content:encoded><![CDATA[
arXiv:2506.01096v2 Announce Type: replace 
Abstract: Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. We introduce SuperRL, a unified training framework that adaptively alternates between RL and SFT. Whenever every rollout for a given instance receives zero reward, indicating the absence of a learning signal, SuperRL falls back to SFT on the curated offline data. Extensive experiments across diverse reasoning benchmarks show that SuperRL surpasses vanilla RL by delivering higher sample efficiency, stronger generalization, and improved robustness under sparse rewards.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HASD: Hierarchical Adaption for pathology Slide-level Domain-shift</title>
<link>https://arxiv.org/abs/2506.23673</link>
<guid>https://arxiv.org/abs/2506.23673</guid>
<content:encoded><![CDATA[
arXiv:2506.23673v2 Announce Type: replace 
Abstract: Domain shift is a critical problem for pathology AI as pathology data is heavily influenced by center-specific conditions. Current pathology domain adaptation methods focus on image patches rather than WSI, thus failing to capture global WSI features required in typical clinical scenarios. In this work, we address the challenges of slide-level domain shift by proposing a Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD achieves multi-scale feature consistency and computationally efficient slide-level domain adaptation through two key components: (1) a hierarchical adaptation framework that integrates a Domain-level Alignment Solver for feature alignment, a Slide-level Geometric Invariance Regularization to preserve the morphological structure, and a Patch-level Attention Consistency Regularization to maintain local critical diagnostic cues; and (2) a prototype selection mechanism that reduces computational overhead. We validate our method on two slide-level tasks across five datasets, achieving a 4.1\% AUROC improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in a UCEC survival prediction cohort. Our method provides a practical and reliable slide-level domain adaption solution for pathology institutions, minimizing both computational and annotation costs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems</title>
<link>https://arxiv.org/abs/2507.03226</link>
<guid>https://arxiv.org/abs/2507.03226</guid>
<content:encoded><![CDATA[
arXiv:2507.03226v2 Announce Type: replace 
Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based Retrieval Augmented Generation (GraphRAG) in enterprise environments. While GraphRAG has shown promise for multi-hop reasoning and structured retrieval, its adoption has been limited by the high computational cost of constructing knowledge graphs using large language models (LLMs) and the latency of graph-based retrieval. To address these challenges, we introduce two core innovations: (1) a dependency-based knowledge graph construction pipeline that leverages industrial-grade NLP libraries to extract entities and relations from unstructured text completely eliminating reliance on LLMs; and (2) a lightweight graph retrieval strategy that combines hybrid query node identification with efficient one-hop traversal for high-recall, low-latency subgraph extraction. We evaluate our framework on two SAP datasets focused on legacy code migration and demonstrate strong empirical performance. Our system achieves up to 15% and 4.35% improvements over traditional RAG baselines based on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based construction approach attains 94% of the performance of LLM-generated knowledge graphs (61.87% vs. 65.83%) while significantly reducing cost and improving scalability. These results validate the feasibility of deploying GraphRAG systems in real-world, large-scale enterprise applications without incurring prohibitive resource requirements paving the way for practical, explainable, and domain-adaptable retrieval-augmented reasoning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Deception Probes via Black-to-White Performance Boosts</title>
<link>https://arxiv.org/abs/2507.12691</link>
<guid>https://arxiv.org/abs/2507.12691</guid>
<content:encoded><![CDATA[
arXiv:2507.12691v2 Announce Type: replace 
Abstract: AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called "deception probes") have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it's unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepFun-Prover Preview: Let's Think and Verify Step by Step</title>
<link>https://arxiv.org/abs/2507.20199</link>
<guid>https://arxiv.org/abs/2507.20199</guid>
<content:encoded><![CDATA[
arXiv:2507.20199v2 Announce Type: replace 
Abstract: We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of $70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From research to clinic: Accelerating the translation of clinical decision support systems by making synthetic data interoperable</title>
<link>https://arxiv.org/abs/2308.02613</link>
<guid>https://arxiv.org/abs/2308.02613</guid>
<content:encoded><![CDATA[
arXiv:2308.02613v2 Announce Type: replace-cross 
Abstract: The translation of clinical decision support system (CDSS) tools from research settings into the clinic is often non-existent, partly because the focus tends to be on training machine learning models rather than tool development using the model for inference. To develop a CDSS tool that can be deployed in the clinical workflow, there is a need to integrate, validate, and test the tool on the Electronic Health Record (EHR) systems that store and manage patient data. Not surprisingly, it is rarely possible for researchers to get the necessary access to an EHR system due to legal restrictions pertaining to the protection of data privacy in patient records. We propose an architecture for using synthetic data in EHR systems to make CDSS tool development and testing much easier. In this study, the architecture is implemented in the SyntHIR system. SyntHIR has three noteworthy architectural features enabling (i) integration with synthetic data generators, (ii) data interoperability, and (iii) tool transportability. The translational value of this approach was evaluated through two primary steps. First, a working proof-of-concept of a machine learning-based CDSS tool was developed using data from patient registries in Norway. Second, the transportability of this CDSS tool was demonstrated by successfully deploying it in Norway's largest EHR system vendor (DIPS). These findings showcase the value of the SyntHIR architecture as a useful reference model to accelerate the translation of "bench to bedside" research of CDSS tools.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Random Field model for Hypergraph-based Machine Learning</title>
<link>https://arxiv.org/abs/2308.14172</link>
<guid>https://arxiv.org/abs/2308.14172</guid>
<content:encoded><![CDATA[
arXiv:2308.14172v4 Announce Type: replace-cross 
Abstract: Understanding the data-generating process is essential for building machine learning models that generalise well while ensuring robustness and interpretability. This paper addresses the fundamental challenge of modelling the data generation processes on hypergraphs and explores how such models can inform the design of machine learning algorithms for hypergraph data. The key to our approach is the development of a hypergraph Markov random field that models the joint distribution of the node features and hyperedge features in a hypergraph through a multivariate Gaussian distribution whose covariance matrix is uniquely determined by the hypergraph structure. The proposed data-generating process provides a valuable inductive bias for various hypergraph machine learning tasks, thus enhancing the algorithm design. In this paper, we focus on two representative downstream tasks: structure inference and node classification. Accordingly, we introduce two novel frameworks: 1) an original hypergraph structure inference framework named HGSI, and 2) a novel learning framework entitled Hypergraph-MLP for node classification on hypergraphs. Empirical evaluation of the proposed frameworks demonstrates that: 1) HGSI outperforms existing hypergraph structure inference methods on both synthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in six hypergraph node classification benchmarks, at the same time promoting runtime efficiency and robustness against structural perturbations during inference.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Initialize Trajectory Optimization for Vision-Based Autonomous Flight in Unknown Environments</title>
<link>https://arxiv.org/abs/2309.10683</link>
<guid>https://arxiv.org/abs/2309.10683</guid>
<content:encoded><![CDATA[
arXiv:2309.10683v2 Announce Type: replace-cross 
Abstract: Autonomous flight in unknown environments requires precise spatial and temporal trajectory planning, often involving computationally expensive nonconvex optimization prone to local optima. To overcome these challenges, we present the Neural-Enhanced Trajectory Planner (NEO-Planner), a novel approach that leverages a Neural Network (NN) Planner to provide informed initial values for trajectory optimization. The NN-Planner is trained on a dataset generated by an expert planner using batch sampling, capturing multimodal trajectory solutions. It learns to predict spatial and temporal parameters for trajectories directly from raw sensor observations. NEO-Planner starts optimization from these predictions, accelerating computation speed while maintaining explainability. Furthermore, we introduce a robust online replanning framework that accommodates planning latency for smooth trajectory tracking. Extensive simulations demonstrate that NEO-Planner reduces optimization iterations by 20%, leading to a 26% decrease in computation time compared with pure optimization-based methods. It maintains trajectory quality comparable to baseline approaches and generalizes well to unseen environments. Real-world experiments validate its effectiveness for autonomous drone navigation in cluttered, unknown environments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2311.04938</link>
<guid>https://arxiv.org/abs/2311.04938</guid>
<content:encoded><![CDATA[
arXiv:2311.04938v4 Announce Type: replace-cross 
Abstract: We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ, class-conditional models trained on ImageNet, and text-to-image generation using Stable Diffusion v2.1 on COYO700M datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73 respectively with a Gaussian kernel. Further, we derive novel SDE samplers for rectified flow matching models and experiment with the proposed approach. We see improvements using both 1-rectified flow and 2-rectified flow models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy Causal Graphs for Multivariate Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2312.09478</link>
<guid>https://arxiv.org/abs/2312.09478</guid>
<content:encoded><![CDATA[
arXiv:2312.09478v2 Announce Type: replace-cross 
Abstract: Many multivariate time series anomaly detection frameworks have been proposed and widely applied. However, most of these frameworks do not consider intrinsic relationships between variables in multivariate time series data, thus ignoring the causal relationship among variables and degrading anomaly detection performance. This work proposes a novel framework called CGAD, an entropy Causal Graph for multivariate time series Anomaly Detection. CGAD utilizes transfer entropy to construct graph structures that unveil the underlying causal relationships among time series data. Weighted graph convolutional networks combined with causal convolutions are employed to model both the causal graph structures and the temporal patterns within multivariate time series data. Furthermore, CGAD applies anomaly scoring, leveraging median absolute deviation-based normalization to improve the robustness of the anomaly identification process. Extensive experiments demonstrate that CGAD outperforms state-of-the-art methods on real-world datasets with a 9% average improvement in terms of three different multivariate time series anomaly detection metrics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance</title>
<link>https://arxiv.org/abs/2406.09105</link>
<guid>https://arxiv.org/abs/2406.09105</guid>
<content:encoded><![CDATA[
arXiv:2406.09105v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) and Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance in various general multimodal applications and have shown increasing promise in specialized domains. However, their potential in the insurance domain-characterized by diverse application scenarios and rich multimodal data-remains largely underexplored. To date, there is no systematic review of multimodal tasks, nor a benchmark specifically designed to assess the capabilities of LVLMs in insurance. This gap hinders the development of LVLMs within the insurance industry. This study systematically reviews and categorizes multimodal tasks for 4 representative types of insurance: auto, property, health, and agricultural. We introduce INS-MMBench, the first hierarchical benchmark tailored for the insurance domain. INS-MMBench encompasses 22 fundamental tasks, 12 meta-tasks and 5 scenario tasks, enabling a comprehensive and progressive assessment from basic capabilities to real-world use cases. We benchmark 11 leading LVLMs, including closed-source models such as GPT-4o and open-source models like LLaVA. Our evaluation validates the effectiveness of INS-MMBench and offers detailed insights into the strengths and limitations of current LVLMs on a variety of insurance-related multimodal tasks. We hope that INS-MMBench will accelerate the integration of LVLMs into the insurance industry and foster interdisciplinary research. Our dataset and evaluation code are available at https://github.com/FDU-INS/INS-MMBench.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reorganizing attention-space geometry with expressive attention</title>
<link>https://arxiv.org/abs/2407.18601</link>
<guid>https://arxiv.org/abs/2407.18601</guid>
<content:encoded><![CDATA[
arXiv:2407.18601v3 Announce Type: replace-cross 
Abstract: Attention regulates information transfer between tokens. For this, query and key vectors are compared, typically in terms of a scalar product, $\mathbf{Q}^T\mathbf{K}$, together with a subsequent softmax normalization. In geometric terms, the standard dot-product attention (DPA) leads to large/small attention weights for parallel/antiparallel queries and keys. Here we study expressive attention (EA), which is based on $(\mathbf{Q}^T\mathbf{K})^2$, the squared dot product. In this case, attention is enhanced when query and key are either parallel or antiparallel, and suppressed for orthogonal configurations. EA can be introduced into any attention-based code without additional compute costs or memory requirements. For a series of autoregressive prediction tasks, we find that expressive attention performs at least as well as vanilla DPA. Increasing task complexity, EA is observed to outperform DPA with increasing margins, which also holds for multi-task settings. For a given model size, EA manages to achieve 100% performance for a range of complexity levels not accessible to DPA. Our results show that it is possible to reorganize the geometry of the matching condition in the space of attention heads without loss of performance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal Partial Sensing Forecast for Long-term Traffic</title>
<link>https://arxiv.org/abs/2408.02689</link>
<guid>https://arxiv.org/abs/2408.02689</guid>
<content:encoded><![CDATA[
arXiv:2408.02689v2 Announce Type: replace-cross 
Abstract: Traffic forecasting uses recent measurements by sensors installed at chosen locations to forecast the future road traffic. Existing work either assumes all locations are equipped with sensors or focuses on short-term forecast. This paper studies partial sensing forecast of long-term traffic, assuming sensors are available only at some locations. The problem is challenging due to the unknown data distribution at unsensed locations, the intricate spatio-temporal correlation in long-term forecasting, as well as noise to traffic patterns. We propose a Spatio-temporal Long-term Partial sensing Forecast model (SLPF) for traffic prediction, with several novel contributions, including a rank-based embedding technique to reduce the impact of noise in data, a spatial transfer matrix to overcome the spatial distribution shift from sensed locations to unsensed locations, and a multi-step training process that utilizes all available data to successively refine the model parameters for better accuracy. Extensive experiments on several real-world traffic datasets demonstrate its superior performance. Our source code is at https://github.com/zbliu98/SLPF
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATM: Improving Model Merging by Alternating Tuning and Merging</title>
<link>https://arxiv.org/abs/2411.03055</link>
<guid>https://arxiv.org/abs/2411.03055</guid>
<content:encoded><![CDATA[
arXiv:2411.03055v4 Announce Type: replace-cross 
Abstract: Model merging has emerged as a cost-efficient approximation to multitask learning. Among merging strategies, task arithmetic is notable for its simplicity and effectiveness. In this work, we provide a theoretical motivation for task vectors by highlighting that, under single-epoch full-batch gradient descent, they are equivalent to multitask gradients. This insight leads us to reinterpret model merging as a single step in an iterative procedure that Alternates between Tuning and Merging (ATM). We propose two applications of ATM: (1) as an alternative to multitask learning in scenarios where data sharing is restricted (e.g., federated settings), and (2) as a lightweight refinement step to improve existing model merging methods using a small validation set. Experiments across diverse vision tasks demonstrate the effectiveness of ATM.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering the Performance of GAE in Link Prediction</title>
<link>https://arxiv.org/abs/2411.03845</link>
<guid>https://arxiv.org/abs/2411.03845</guid>
<content:encoded><![CDATA[
arXiv:2411.03845v3 Announce Type: replace-cross 
Abstract: Recent advancements in graph neural networks (GNNs) for link prediction have introduced sophisticated training techniques and model architectures. However, reliance on outdated baselines may exaggerate the benefits of these new approaches. To tackle this issue, we systematically explore Graph Autoencoders (GAEs) by applying model-agnostic tricks in recent methods and tuning hyperparameters. We find that a well-tuned GAE can match the performance of recent sophisticated models while offering superior computational efficiency on widely-used link prediction benchmarks. Our approach delivers substantial performance gains on datasets where structural information dominates and feature data is limited. Specifically, our GAE achieves a state-of-the-art Hits@100 score of 78.41\% on the ogbl-ppa dataset. Furthermore, we examine the impact of various tricks to uncover the reasons behind our success and to guide the design of future methods. Our study emphasizes the critical need to update baselines for a more accurate assessment of progress in GNNs for link prediction. Our code is available at https://github.com/GraphPKU/Refined-GAE.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval</title>
<link>https://arxiv.org/abs/2411.12644</link>
<guid>https://arxiv.org/abs/2411.12644</guid>
<content:encoded><![CDATA[
arXiv:2411.12644v3 Announce Type: replace-cross 
Abstract: Despite the success of text retrieval in many NLP tasks, code retrieval remains a largely underexplored area. Most text retrieval systems are tailored for natural language queries, often neglecting the specific challenges of retrieving code. This gap leaves existing models unable to effectively capture the diversity of programming languages and tasks across different domains, highlighting the need for more focused research in code retrieval. To address this, we introduce CodeXEmbed, a family of large-scale code embedding models ranging from 400M to 7B parameters. Our novel training pipeline unifies multiple programming languages and transforms various code-related tasks into a common retrieval framework, enhancing model generalizability and retrieval performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval, outperforming the previous leading model, Voyage-Code, by over 20% on CoIR benchmark. In addition to excelling in code retrieval, our models demonstrate competitive performance on the widely adopted BeIR text retrieval benchmark, offering versatility across domains. Experimental results demonstrate that improving retrieval performance significantly enhances end-to-end Retrieval-Augmented Generation (RAG) performance for code-related tasks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepMDV: Global Spatial Matching for Multi-depot Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2411.17080</link>
<guid>https://arxiv.org/abs/2411.17080</guid>
<content:encoded><![CDATA[
arXiv:2411.17080v3 Announce Type: replace-cross 
Abstract: The rapid growth of online retail and e-commerce has made effective and efficient Vehicle Routing Problem (VRP) solutions essential. To meet rising demand, companies are adding more depots, which changes the VRP problem to a complex optimization task of Multi-Depot VRP (MDVRP) where the routing decisions of vehicles from multiple depots are highly interdependent. The complexities render traditional VRP methods suboptimal and non-scalable for the MDVRP. In this paper, we propose a novel approach to solve MDVRP addressing these interdependencies, hence achieving more effective results. The key idea is, the MDVRP can be broken down into two core spatial tasks: assigning customers to depots and optimizing the sequence of customer visits. We adopt task-decoupling approach and propose a two-stage framework that is scalable: (i) an interdependent partitioning module that embeds spatial and tour context directly into the representation space to globally match customers to depots and assign them to tours; and (ii) an independent routing module that determines the optimal visit sequence within each tour. Extensive experiments on both synthetic and real-world datasets demonstrate that our method outperforms all baselines across varying problem sizes, including the adaptations of learning-based solutions for single-depot VRP. Its adaptability and performance make it a practical and readily deployable solution for real-world logistics challenges.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models</title>
<link>https://arxiv.org/abs/2411.18350</link>
<guid>https://arxiv.org/abs/2411.18350</guid>
<content:encoded><![CDATA[
arXiv:2411.18350v2 Announce Type: replace-cross 
Abstract: This paper introduces Virtual Try-Off (VTOFF), a novel task generating standardized garment images from single photos of clothed individuals. Unlike Virtual Try-On (VTON), which digitally dresses models, VTOFF extracts canonical garment images, demanding precise reconstruction of shape, texture, and complex patterns, enabling robust evaluation of generative model fidelity. We propose TryOffDiff, adapting Stable Diffusion with SigLIP-based visual conditioning to deliver high-fidelity reconstructions. Experiments on VITON-HD and Dress Code datasets show that TryOffDiff outperforms adapted pose transfer and VTON baselines. We observe that traditional metrics such as SSIM inadequately reflect reconstruction quality, prompting our use of DISTS for reliable assessment. Our findings highlight VTOFF's potential to improve e-commerce product imagery, advance generative model evaluation, and guide future research on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage</title>
<link>https://arxiv.org/abs/2412.05734</link>
<guid>https://arxiv.org/abs/2412.05734</guid>
<content:encoded><![CDATA[
arXiv:2412.05734v2 Announce Type: replace-cross 
Abstract: Recent studies have discovered that large language models (LLM) may be ``fooled'' to output private information, including training data, system prompts, and personally identifiable information, under carefully crafted adversarial prompts. Existing red-teaming approaches for privacy leakage either rely on manual efforts or focus solely on system prompt extraction, making them ineffective for severe risks of training data leakage. We propose LeakAgent, a novel black-box red-teaming framework for LLM privacy leakage. Our framework trains an open-source LLM through reinforcement learning as the attack agent to generate adversarial prompts for both training data extraction and system prompt extraction. To achieve this, we propose a novel reward function to provide effective and fine-grained rewards and design novel mechanisms to balance exploration and exploitation during learning and enhance the diversity of adversarial prompts. Through extensive evaluations, we first show that LeakAgent significantly outperforms existing rule-based approaches in training data extraction and automated methods in system prompt leakage. We also demonstrate the effectiveness of LeakAgent in extracting system prompts from real-world applications in OpenAI's GPT Store. We further demonstrate LeakAgent's effectiveness in evading the existing guardrail defense and its helpfulness in enabling better safety alignment. Finally, we validate our customized designs through a detailed ablation study. We release our code here https://github.com/rucnyz/LeakAgent.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
<link>https://arxiv.org/abs/2501.10970</link>
<guid>https://arxiv.org/abs/2501.10970</guid>
<content:encoded><![CDATA[
arXiv:2501.10970v4 Announce Type: replace-cross 
Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Contextual Reinforcement Framework for Logical Structure Language Generation</title>
<link>https://arxiv.org/abs/2501.11417</link>
<guid>https://arxiv.org/abs/2501.11417</guid>
<content:encoded><![CDATA[
arXiv:2501.11417v2 Announce Type: replace-cross 
Abstract: The Neural Contextual Reinforcement Framework introduces an innovative approach to enhancing the logical coherence and structural consistency of text generated by large language models. Leveraging reinforcement learning principles, the framework integrates custom reward functions and dynamic context alignment mechanisms to address challenges inherent in maintaining long-range dependencies across extended sequences. The architecture incorporates multi-head attention layers and hierarchical encoding modules, enabling the model to produce outputs that align closely with human expectations of logical structure and semantic flow. Quantitative evaluations across diverse datasets demonstrate substantial improvements in coherence metrics, perplexity reduction, and semantic alignment, showcasing the framework's ability to outperform baseline models in both general and domain-specific tasks. Qualitative analyses further highlight the framework's capacity to generate text with improved narrative clarity and reduced redundancy, reflecting its effectiveness in balancing fluency with structural precision. In addition to its performance gains, the framework exhibits robustness in handling noisy input data and scalability across varying model sizes, reinforcing its versatility in practical applications. Experimental results reveal that optimal context window sizes significantly influence coherence outcomes, showing the importance of architectural flexibility in adapting to diverse linguistic structures. Cross-lingual performance evaluations affirm the framework's adaptability to multiple languages, extending its utility beyond monolingual contexts. Resource efficiency analyses indicate a reduction in computational overhead compared to traditional approaches, emphasizing the practicality of the framework for large-scale deployment.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Zero-Space Detection: A Novel Framework for Autonomous Ransomware Identification in High-Velocity Environments</title>
<link>https://arxiv.org/abs/2501.12811</link>
<guid>https://arxiv.org/abs/2501.12811</guid>
<content:encoded><![CDATA[
arXiv:2501.12811v2 Announce Type: replace-cross 
Abstract: Modern cybersecurity landscapes increasingly demand sophisticated detection frameworks capable of identifying evolving threats with precision and adaptability. The proposed Zero-Space Detection framework introduces a novel approach that dynamically identifies latent behavioral patterns through unsupervised clustering and advanced deep learning techniques. Designed to address the limitations of signature-based and heuristic methods, it operates effectively in high-velocity environments by integrating multi-phase filtering and ensemble learning for refined decision-making. Experimental evaluation reveals high detection rates across diverse ransomware families, including LockBit, Conti, REvil, and BlackMatter, while maintaining low false positive rates and scalable performance. Computational overhead remains minimal, with average processing times ensuring compatibility with real-time systems even under peak operational loads. The framework demonstrates resilience against adversarial strategies such as obfuscation and encryption speed variability, which frequently challenge conventional detection systems. Analysis across multiple data sources highlights its versatility in handling diverse file types and operational contexts. Comprehensive metrics, including detection probability, latency, and resource efficiency, validate its efficacy under real-world conditions. Through its modular architecture, the framework achieves seamless integration with existing cybersecurity infrastructures without significant reconfiguration. The results demonstrate its robustness and scalability, offering a transformative paradigm for ransomware identification in dynamic and resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration</title>
<link>https://arxiv.org/abs/2501.12901</link>
<guid>https://arxiv.org/abs/2501.12901</guid>
<content:encoded><![CDATA[
arXiv:2501.12901v2 Announce Type: replace-cross 
Abstract: Contextual Partitioning introduces an innovative approach to enhancing the architectural design of large-scale computational models through the dynamic segmentation of parameters into context-aware regions. This methodology emphasizes the importance of task-specific specialization, achieved through adaptive parameter allocation mechanisms that align with the linguistic features of input data. Experimental evaluations demonstrated substantial improvements in accuracy, perplexity, and contextual coherence across a variety of linguistic tasks, highlighting the adaptability and scalability of the proposed framework. By reducing redundancy and enhancing computational efficiency, Contextual Partitioning not only streamlines model operations but also expands the scope of applications for advanced language processing systems. The approach operates autonomously, requiring no external fine-tuning, thereby addressing a significant limitation in conventional parameter optimization techniques. Empirical results demonstrate the effectiveness of gradient-driven segmentation, enabling models to dynamically recalibrate and specialize in response to task-specific demands. Furthermore, resource utilization metrics reveal notable reductions in memory usage and training times, confirming the efficiency of the approach. Observations from qualitative analyses illustrate improved contextual coherence and logical flow in generated outputs, reinforcing the practical value of this technique. The findings collectively demonstrate the potential for Contextual Partitioning to redefine the scalability and adaptability of computational language architectures in diverse and complex domains.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation</title>
<link>https://arxiv.org/abs/2501.14119</link>
<guid>https://arxiv.org/abs/2501.14119</guid>
<content:encoded><![CDATA[
arXiv:2501.14119v2 Announce Type: replace-cross 
Abstract: Transformative innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model's robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systemizing Multiplicity: The Curious Case of Arbitrariness in Machine Learning</title>
<link>https://arxiv.org/abs/2501.14959</link>
<guid>https://arxiv.org/abs/2501.14959</guid>
<content:encoded><![CDATA[
arXiv:2501.14959v2 Announce Type: replace-cross 
Abstract: Algorithmic modeling relies on limited information in data to extrapolate outcomes for unseen scenarios, often embedding an element of arbitrariness in its decisions. A perspective on this arbitrariness that has recently gained interest is multiplicity-the study of arbitrariness across a set of "good models", i.e., those likely to be deployed in practice. In this work, we systemize the literature on multiplicity by: (a) formalizing the terminology around model design choices and their contribution to arbitrariness, (b) expanding the definition of multiplicity to incorporate underrepresented forms beyond just predictions and explanations, (c) clarifying the distinction between multiplicity and other lenses of arbitrariness, i.e., uncertainty and variance, and (d) distilling the benefits and potential risks of multiplicity into overarching trends, situating it within the broader landscape of responsible AI. We conclude by identifying open research questions and highlighting emerging trends in this young but rapidly growing area of research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Pattern Decryption Methodology for Ransomware Detection Using Probabilistic Cryptographic Footprints</title>
<link>https://arxiv.org/abs/2501.15084</link>
<guid>https://arxiv.org/abs/2501.15084</guid>
<content:encoded><![CDATA[
arXiv:2501.15084v2 Announce Type: replace-cross 
Abstract: The increasing sophistication of encryption-based ransomware has demanded innovative approaches to detection and mitigation, prompting the development of a hierarchical framework grounded in probabilistic cryptographic analysis. By focusing on the statistical characteristics of encryption patterns, the proposed methodology introduces a layered approach that combines advanced clustering algorithms with machine learning to isolate ransomware-induced anomalies. Through comprehensive testing across diverse ransomware families, the framework demonstrated exceptional accuracy, effectively distinguishing malicious encryption operations from benign activities while maintaining low false positive rates. The system's design integrates dynamic feedback mechanisms, enabling adaptability to varying cryptographic complexities and operational environments. Detailed entropy-based evaluations revealed its sensitivity to subtle deviations in encryption workflows, offering a robust alternative to traditional detection methods reliant on static signatures or heuristics. Computational benchmarks confirmed its scalability and efficiency, achieving consistent performance even under high data loads and complex cryptographic scenarios. The inclusion of real-time clustering and anomaly evaluation ensures rapid response capabilities, addressing critical latency challenges in ransomware detection. Performance comparisons with established methods highlighted its improvements in detection efficacy, particularly against advanced ransomware employing extended key lengths and unique cryptographic protocols.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint</title>
<link>https://arxiv.org/abs/2501.15509</link>
<guid>https://arxiv.org/abs/2501.15509</guid>
<content:encoded><![CDATA[
arXiv:2501.15509v3 Announce Type: replace-cross 
Abstract: Model fingerprinting is a widely adopted approach to safeguard the intellectual property rights of open-source models by preventing their unauthorized reuse. It is promising and convenient since it does not necessitate modifying the protected model. In this paper, we revisit existing fingerprinting methods and reveal that they are vulnerable to false claim attacks where adversaries falsely assert ownership of any third-party model. We demonstrate that this vulnerability mostly stems from their untargeted nature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by these findings, we propose a targeted fingerprinting paradigm (i.e., FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods, i.e., FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Extensive experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Reinforcement in Multimodal Token Compression for Large Language Models</title>
<link>https://arxiv.org/abs/2501.16658</link>
<guid>https://arxiv.org/abs/2501.16658</guid>
<content:encoded><![CDATA[
arXiv:2501.16658v2 Announce Type: replace-cross 
Abstract: Effective token compression remains a critical challenge for scaling models to handle increasingly complex and diverse datasets. A novel mechanism based on contextual reinforcement is introduced, dynamically adjusting token importance through interdependencies and semantic relevance. This approach enables substantial reductions in token usage while preserving the quality and coherence of information representation. Incorporating graph-based algorithms and adaptive weighting, the method captures subtle contextual relationships across textual and multimodal data, ensuring robust alignment and performance in downstream tasks. Evaluations across varied domains reveal significant improvements in accuracy and semantic retention, particularly for tasks requiring detailed cross-modal interactions. Memory usage analyses demonstrate improved computational efficiency, with minimal overhead despite the additional reinforcement processes. Performance gains are further validated through error distribution analyses, showing reduced semantic loss and syntactic inconsistencies compared to baseline models. The modular architecture ensures compatibility with a wide range of open-source frameworks, facilitating scalable implementation for real-world applications. These findings highlight the potential of contextual reinforcement in redefining token management strategies and advancing large-scale model design.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Segmentation and Behavioral Profiling for Ransomware Detection Using Temporal-Correlation Graphs</title>
<link>https://arxiv.org/abs/2501.17429</link>
<guid>https://arxiv.org/abs/2501.17429</guid>
<content:encoded><![CDATA[
arXiv:2501.17429v2 Announce Type: replace-cross 
Abstract: The rapid evolution of cyber threats has outpaced traditional detection methodologies, necessitating innovative approaches capable of addressing the adaptive and complex behaviors of modern adversaries. A novel framework was introduced, leveraging Temporal-Correlation Graphs to model the intricate relationships and temporal patterns inherent in malicious operations. The approach dynamically captured behavioral anomalies, offering a robust mechanism for distinguishing between benign and malicious activities in real-time scenarios. Extensive experiments demonstrated the framework's effectiveness across diverse ransomware families, with consistently high precision, recall, and overall detection accuracy. Comparative evaluations highlighted its better performance over traditional signature-based and heuristic methods, particularly in handling polymorphic and previously unseen ransomware variants. The architecture was designed with scalability and modularity in mind, ensuring compatibility with enterprise-scale environments while maintaining resource efficiency. Analysis of encryption speeds, anomaly patterns, and temporal correlations provided deeper insights into the operational strategies of ransomware, validating the framework's adaptability to evolving threats. The research contributes to advancing cybersecurity technologies by integrating dynamic graph analytics and machine learning for future innovations in threat detection. Results from this study underline the potential for transforming the way organizations detect and mitigate complex cyberattacks.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextually Entangled Gradient Mapping for Optimized LLM Comprehension</title>
<link>https://arxiv.org/abs/2502.00048</link>
<guid>https://arxiv.org/abs/2502.00048</guid>
<content:encoded><![CDATA[
arXiv:2502.00048v2 Announce Type: replace-cross 
Abstract: Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to gradient optimization, redefining the relationship between contextual embeddings and gradient updates to enhance semantic coherence and reasoning capabilities in neural architectures. By treating gradients as dynamic carriers of contextual dependencies rather than isolated numerical entities, the proposed methodology bridges critical gaps in existing optimization strategies. The integration of entangled gradient dynamics into a loss regularization framework demonstrated significant improvements in tasks involving long-form reasoning, contextual retention, and adaptability to unseen domains. Experimental evaluations showed that the CEGM-enhanced model consistently outperformed baseline approaches, achieving higher accuracy in token-level predictions and greater resilience to noisy inputs. Practical implementations involved modifications to training pipelines, introducing entanglement layers and dynamic coefficient adjustments that seamlessly align with existing architectures. Results further highlighted reductions in semantic drift during sequential transformations and improvements in embedding coherence across paraphrased sentences, showing the robustness and versatility of the proposed methodology. The findings demonstrate the broader implications of gradient entanglement for both theoretical advancements and practical applications in optimization strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes</title>
<link>https://arxiv.org/abs/2502.06728</link>
<guid>https://arxiv.org/abs/2502.06728</guid>
<content:encoded><![CDATA[
arXiv:2502.06728v3 Announce Type: replace-cross 
Abstract: Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, DeMo assumes that models fit on a single accelerator. We relax this assumption and introduce FlexDeMo, whereby nodes fully shard model parameters locally between different accelerators, while inter-node communication is reduced by synchronizing only fast-moving components instead of the full gradients -- resulting in a hybrid sharded data parallel training strategy. We further introduce a framework, denoted as DeToNATION, that generalizes DeMo, FlexDeMo, and other popular distributed training schemes such as DiLoCo -- introducing new variations of replication schemes and challenging choices made in DeMo. Our results across language and vision domains show that FlexDeMo attains similar validation loss as hybrid sharded data parallel training employing AdamW and full gradient synchronization, while being substantially faster. FlexDeMo is thus a promising distributed training scheme for the largest machine learning models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction</title>
<link>https://arxiv.org/abs/2502.06836</link>
<guid>https://arxiv.org/abs/2502.06836</guid>
<content:encoded><![CDATA[
arXiv:2502.06836v2 Announce Type: replace-cross 
Abstract: Recent advancements in graph neural networks (GNNs) have significantly enhanced the prediction of material properties by modeling crystal structures as graphs. However, GNNs often struggle to capture global structural characteristics, such as crystal systems, limiting their predictive performance. To overcome this issue, we propose CAST, a cross-attention-based multimodal model that integrates graph representations with textual descriptions of materials, effectively preserving critical structural and compositional information. Unlike previous approaches, such as CrysMMNet and MultiMat, which rely on aggregated material-level embeddings, CAST leverages cross-attention mechanisms to combine fine-grained graph node-level and text token-level features. Additionally, we introduce a masked node prediction pretraining strategy that further enhances the alignment between node and text embeddings. Our experimental results demonstrate that CAST outperforms existing baseline models across four key material properties-formation energy, band gap, bulk modulus, and shear modulus-with average relative MAE improvements ranging from 10.2% to 35.7%. Analysis of attention maps confirms the importance of pretraining in effectively aligning multimodal representations. This study underscores the potential of multimodal learning frameworks for developing more accurate and globally informed predictive models in materials science.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.09891</link>
<guid>https://arxiv.org/abs/2502.09891</guid>
<content:encoded><![CDATA[
arXiv:2502.09891v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for solving question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in both accuracy and token cost.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration</title>
<link>https://arxiv.org/abs/2502.10699</link>
<guid>https://arxiv.org/abs/2502.10699</guid>
<content:encoded><![CDATA[
arXiv:2502.10699v2 Announce Type: replace-cross 
Abstract: Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic Over Source: The Key to Effective Data Mixing for Language Models Pre-training</title>
<link>https://arxiv.org/abs/2502.16802</link>
<guid>https://arxiv.org/abs/2502.16802</guid>
<content:encoded><![CDATA[
arXiv:2502.16802v3 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various languages, sources, and topics. Effectively integrating these heterogeneous data groups is crucial for optimizing LLM performance. Previous research has predominantly concentrated on source-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a topic-based data mixing strategy that utilizes detailed topic labels generated through a multi-stage process combining unsupervised clustering, LLM-based summarization, and supervised classifier training. With this strategy, we conduct the first comprehensive comparison of topic-based versus source-based partitioning across multiple mixing strategies. We demonstrate that language models pretrained on data mixed by topics consistently outperform those trained on data mixed by sources across multiple methods including RegMix, DoReMi,temperature-based sampling, and a manual mixing method based on downstream task performance. Our theoretical analysis reveals that topic-based data achieves significantly lower validation loss compared to source-based approaches, creating a better optimization landscape for model training. We will make our code, annotated datasets, and topic classification models publicly available to facilitate further research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACTIVA: Amortized Causal Effect Estimation via Transformer-based Variational Autoencoder</title>
<link>https://arxiv.org/abs/2503.01290</link>
<guid>https://arxiv.org/abs/2503.01290</guid>
<content:encoded><![CDATA[
arXiv:2503.01290v2 Announce Type: replace-cross 
Abstract: Predicting the distribution of outcomes under hypothetical interventions is crucial across healthcare, economics, and policy-making. However, existing methods often require restrictive assumptions, and are typically limited by the lack of amortization across problem instances. We propose ACTIVA, a transformer-based conditional variational autoencoder (VAE) architecture for amortized causal inference, which estimates interventional distributions directly from observational data without. ACTIVA learns a latent representation conditioned on observational inputs and intervention queries, enabling zero-shot inference by amortizing causal knowledge from diverse training scenarios. We provide theoretical insights showing that ACTIVA predicts interventional distributions as mixtures over observationally equivalent causal models. Empirical evaluations on synthetic and semi-synthetic datasets confirm the effectiveness of our amortized approach and highlight promising directions for future real-world applications.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation</title>
<link>https://arxiv.org/abs/2503.01700</link>
<guid>https://arxiv.org/abs/2503.01700</guid>
<content:encoded><![CDATA[
arXiv:2503.01700v2 Announce Type: replace-cross 
Abstract: Recent works have shown great potentials of Large Language Models (LLMs) in robot task and motion planning (TAMP). Current LLM approaches generate text- or code-based reasoning chains with sub-goals and action plans. However, they do not fully leverage LLMs' symbolic computing and code generation capabilities. Many robot TAMP tasks involve complex optimization under multiple constraints, where pure textual reasoning is insufficient. While augmenting LLMs with predefined solvers and planners improves performance, it lacks generalization across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP capabilities by steering them to generate code as symbolic planners for optimization and constraint verification. Unlike prior work that uses code to interface with robot action modules, we steer LLMs to generate code as solvers, planners, and checkers for TAMP tasks requiring symbolic computing, while still leveraging textual reasoning to incorporate common sense. With a multi-round guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner improves success rates by average 24.1\% over best baseline methods across seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows strong effectiveness and generalizability across discrete and continuous environments, 2D/3D simulations and real-world settings, as well as single- and multi-robot tasks with diverse requirements. See our project website https://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and code.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Plug-n-Play Knowledge Modules with Deep Context Distillation</title>
<link>https://arxiv.org/abs/2503.08727</link>
<guid>https://arxiv.org/abs/2503.08727</guid>
<content:encoded><![CDATA[
arXiv:2503.08727v4 Announce Type: replace-cross 
Abstract: Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
arXiv:2503.23145v2 Announce Type: replace-cross 
Abstract: Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/CodeARC
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality</title>
<link>https://arxiv.org/abs/2503.24277</link>
<guid>https://arxiv.org/abs/2503.24277</guid>
<content:encoded><![CDATA[
arXiv:2503.24277v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) are widely used in mechanistic interpretability research for large language models; however, the state-of-the-art method of using $k$-sparse autoencoders lacks a theoretical grounding for selecting the hyperparameter $k$ that represents the number of nonzero activations, often denoted by $\ell_0$. In this paper, we reveal a theoretical link that the $\ell_2$-norm of the sparse feature vector can be approximated with the $\ell_2$-norm of the dense vector with a closed-form error, which allows sparse autoencoders to be trained without the need to manually determine $\ell_0$. Specifically, we validate two applications of our theoretical findings. First, we introduce a new methodology that can assess the feature activations of pre-trained SAEs by computing the theoretically expected value from the input embedding, which has been overlooked by existing SAE evaluation methods and loss functions. Second, we introduce a novel activation function, top-AFA, which builds upon our formulation of approximate feature activation (AFA). This function enables top-$k$ style activation without requiring a constant hyperparameter $k$ to be tuned, dynamically determining the number of activated features for each input. By training SAEs on three intermediate layers to reconstruct GPT2 hidden embeddings for over 80 million tokens from the OpenWebText dataset, we demonstrate the empirical merits of this approach and compare it with current state-of-the-art $k$-sparse autoencoders. Our code is available at: https://github.com/SewoongLee/top-afa-sae.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</title>
<link>https://arxiv.org/abs/2504.04633</link>
<guid>https://arxiv.org/abs/2504.04633</guid>
<content:encoded><![CDATA[
arXiv:2504.04633v2 Announce Type: replace-cross 
Abstract: Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\% with substantial improvements in overall efficiency.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Steering Language Models</title>
<link>https://arxiv.org/abs/2504.07081</link>
<guid>https://arxiv.org/abs/2504.07081</guid>
<content:encoded><![CDATA[
arXiv:2504.07081v2 Announce Type: replace-cross 
Abstract: While test-time reasoning enables language models (LMs) to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. Our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layers at Similar Depths Generate Similar Activations Across LLM Architectures</title>
<link>https://arxiv.org/abs/2504.08775</link>
<guid>https://arxiv.org/abs/2504.08775</guid>
<content:encoded><![CDATA[
arXiv:2504.08775v3 Announce Type: replace-cross 
Abstract: How do the latent spaces used by independently-trained LLMs relate to one another? We study the nearest neighbor relationships induced by activations at different layers of 24 open-weight LLMs, and find that they 1) tend to vary from layer to layer within a model, and 2) are approximately shared between corresponding layers of different models. Claim 2 shows that these nearest neighbor relationships are not arbitrary, as they are shared across models, but Claim 1 shows that they are not "obvious" either, as there is no single set of nearest neighbor relationships that is universally shared. Together, these suggest that LLMs generate a progression of activation geometries from layer to layer, but that this entire progression is largely shared between models, stretched and squeezed to fit into different architectures.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience</title>
<link>https://arxiv.org/abs/2504.13908</link>
<guid>https://arxiv.org/abs/2504.13908</guid>
<content:encoded><![CDATA[
arXiv:2504.13908v2 Announce Type: replace-cross 
Abstract: Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to AI 'chatbots' which use large language models (LLMs) to dynamically probe respondents for elaboration and interactively code open-ended responses to fixed questions developed by human researchers. We assessed the AI chatbot's performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that AI chatbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods such as chatbots enhanced by LLMs to enhance open-ended data collection in web surveys.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network</title>
<link>https://arxiv.org/abs/2504.16432</link>
<guid>https://arxiv.org/abs/2504.16432</guid>
<content:encoded><![CDATA[
arXiv:2504.16432v2 Announce Type: replace-cross 
Abstract: As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression</title>
<link>https://arxiv.org/abs/2504.21152</link>
<guid>https://arxiv.org/abs/2504.21152</guid>
<content:encoded><![CDATA[
arXiv:2504.21152v2 Announce Type: replace-cross 
Abstract: Imbalanced regression refers to prediction tasks where the target variable is skewed. This skewness hinders machine learning models, especially neural networks, which concentrate on dense regions and therefore perform poorly on underrepresented (minority) samples. Despite the importance of this problem, only a few methods have been proposed for imbalanced regression. Many of the available solutions for imbalanced regression adapt techniques from the class imbalance domain, such as linear interpolation and the addition of Gaussian noise, to create synthetic data in sparse regions. However, in many cases, the underlying distribution of the data is complex and non-linear. Consequently, these approaches generate synthetic samples that do not accurately represent the true feature-target relationship. To overcome these limitations, we propose SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage 1, an existing oversampler generates initial synthetic samples in sparse target regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves as SMOGAN's filtering layer and refines these samples via adversarial loss augmented with a Maximum Mean Discrepancy objective, aligning them with the true joint feature-target distribution. Extensive experiments on 23 imbalanced datasets show that SMOGAN consistently outperforms the default oversampling method without the DistGAN filtering layer.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung Nodule Malignancy Prediction</title>
<link>https://arxiv.org/abs/2504.21344</link>
<guid>https://arxiv.org/abs/2504.21344</guid>
<content:encoded><![CDATA[
arXiv:2504.21344v2 Announce Type: replace-cross 
Abstract: Machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, guiding the model to learn clinically relevant, robust, and explainable imaging features for predicting lung cancer. We obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST) with 1,246 nodules and semantic features. Additionally, the Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a pretrained Contrastive Language-Image Pretraining (CLIP) model with a parameter-efficient fine-tuning approach to align imaging and semantic text features and predict the one-year lung cancer diagnosis. Our model outperformed state-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and AUPRC of 0.776. It also showed robust results in external datasets. Using CLIP, we also obtained predictions on semantic features through zero-shot inference, such as nodule margin (AUROC: 0.812), nodule consistency (0.812), and pleural attachment (0.840). Our approach surpasses the SOTA models in predicting lung cancer across datasets collected from diverse clinical settings, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings. The code is available at https://github.com/luotingzhuang/CLIP_nodule.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline</title>
<link>https://arxiv.org/abs/2504.21772</link>
<guid>https://arxiv.org/abs/2504.21772</guid>
<content:encoded><![CDATA[
arXiv:2504.21772v3 Announce Type: replace-cross 
Abstract: Short video platforms like YouTube Shorts and TikTok face significant copyright compliance challenges, as infringers frequently embed arbitrary background music (BGM) to obscure original soundtracks (OST) and evade content originality detection. To tackle this issue, we propose a novel pipeline that integrates Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR). Our approach effectively separates arbitrary BGM from the original OST, enabling the restoration of authentic video audio tracks. To support this work, we introduce two domain-specific datasets: OASD-20K for audio separation and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips featuring mixed BGM and OST pairs, while OSVAR-160 is a unique benchmark dataset comprising 1,121 video and mixed-audio pairs, specifically designed for short video restoration tasks. Experimental results demonstrate that our pipeline not only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring content integrity. This approach provides an ethical and scalable solution to copyright challenges in user-generated content on short video platforms.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Query, No Access</title>
<link>https://arxiv.org/abs/2505.07258</link>
<guid>https://arxiv.org/abs/2505.07258</guid>
<content:encoded><![CDATA[
arXiv:2505.07258v2 Announce Type: replace-cross 
Abstract: Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.
  Concurrently, we use diverse adversarial example generation, employing various attack methods to generate and select the adversarial example with better similarity and attack effectiveness. Experiments on the Emotion and SST5 datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR improvement of 52.08\% while significantly reducing attack queries to 0. More importantly, we discover that VDBA poses a significant threat to LLMs such as Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without access to the API, confirming that advanced NLP models still face serious security risks. Our codes can be found at https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?</title>
<link>https://arxiv.org/abs/2505.10443</link>
<guid>https://arxiv.org/abs/2505.10443</guid>
<content:encoded><![CDATA[
arXiv:2505.10443v2 Announce Type: replace-cross 
Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is critical for their reliable use in programming tasks. While recent studies have assessed LLMs' ability to predict program outputs, most focus solely on the accuracy of those predictions, without evaluating the reasoning behind them. Moreover, it has been observed on mathematical reasoning tasks that LLMs can arrive at correct answers through flawed logic, raising concerns about similar issues in code understanding. In this work, we evaluate whether state-of-the-art LLMs with up to 8B parameters can reason about Python programs or are simply guessing. We apply five semantics-preserving code mutations: renaming variables, mirroring comparison expressions, swapping if-else branches, converting for loops to while, and loop unrolling. These mutations maintain program semantics while altering its syntax. We evaluated six LLMs and performed a human expert analysis using LiveCodeBench to assess whether the correct predictions are based on sound reasoning. We also evaluated prediction stability across different code mutations on LiveCodeBench and CruxEval. Our findings show that LLMs trained for code produce correct predictions based on flawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change predictions in response to our code mutations, indicating they do not yet exhibit stable, semantically grounded reasoning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</title>
<link>https://arxiv.org/abs/2505.11528</link>
<guid>https://arxiv.org/abs/2505.11528</guid>
<content:encoded><![CDATA[
arXiv:2505.11528v2 Announce Type: replace-cross 
Abstract: Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUB: Benchmarking Context Utilisation Techniques for Language Models</title>
<link>https://arxiv.org/abs/2505.16518</link>
<guid>https://arxiv.org/abs/2505.16518</guid>
<content:encoded><![CDATA[
arXiv:2505.16518v2 Announce Type: replace-cross 
Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks, such as question answering and fact checking. However, language models (LMs) may ignore relevant information that contradicts outdated parametric memory or be distracted by irrelevant contexts. While many context utilisation manipulation techniques (CMTs) have recently been proposed to alleviate these issues, few have seen systematic comparison. In this paper, we develop CUB (Context Utilisation Benchmark) - the first comprehensive benchmark designed to help practitioners within retrieval-augmented generation (RAG) diagnose CMTs under different context conditions. With this benchmark, we conduct the most extensive evaluation to date of seven state-of-the-art methods, representative of the main categories of CMTs, across three diverse datasets and tasks, applied to nine LMs. Our results reveal that most existing CMTs struggle to handle the full spectrum of context types encountered in real-world retrieval-augmented scenarios. We also find that many CMTs display inflated performance on simple synthesised datasets, compared to more realistic datasets with naturally occurring samples. Our findings expose critical gaps in current CMT evaluation practices and demonstrate the need for holistic testing and the development of CMTs that can robustly handle multiple context types.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression</title>
<link>https://arxiv.org/abs/2505.18602</link>
<guid>https://arxiv.org/abs/2505.18602</guid>
<content:encoded><![CDATA[
arXiv:2505.18602v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have revolutionized algorithm development, yet their application in symbolic regression, where algorithms automatically discover symbolic expressions from data, remains constrained and is typically designed manually by human experts. In this paper, we propose a meta learning framework that enables LLMs to automatically design selection operators for evolutionary symbolic regression algorithms. We first identify two key limitations in existing LLM-based algorithm evolution techniques: a lack of semantic guidance and code bloat. The absence of semantic awareness can lead to ineffective exchange of useful code components, and bloat results in unnecessarily complex components, both of which can reduce the interpretability of the designed algorithm or hinder evolutionary learning progress. To address these issues, we enhance the LLM-based evolution framework for meta symbolic regression with two key innovations: a complementary, semantics-aware selection operator and bloat control. Additionally, we embed domain knowledge into the prompt, enabling the LLM to generate more effective and contextually relevant selection operators. Our experimental results on symbolic regression benchmarks show that LLMs can devise selection operators that outperform nine expert-designed baselines, achieving state-of-the-art performance. Moreover, the evolved operator can further improve the state-of-the-art symbolic regression algorithm, achieving the best performance among 26 symbolic regression and machine learning algorithms across 116 regression datasets. This demonstrates that LLMs can exceed expert-level algorithm design for symbolic regression.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World</title>
<link>https://arxiv.org/abs/2506.03155</link>
<guid>https://arxiv.org/abs/2506.03155</guid>
<content:encoded><![CDATA[
arXiv:2506.03155v2 Announce Type: replace-cross 
Abstract: The proliferation of artificial intelligence has enabled a diversity of applications that bridge the gap between digital and physical worlds. As physical environments are too complex to model through a single information acquisition approach, it is crucial to fuse multimodal data generated by different sources, such as sensors, devices, systems, and people, to solve a problem in the real world. Unfortunately, it is neither applicable nor sustainable to deploy new resources to collect original data from scratch for every problem. Thus, when data is inadequate in the domain of problem, it is vital to fuse knowledge from multimodal data that is already available in other domains. We call this cross-domain knowledge fusion. Existing research focus on fusing multimodal data in a single domain, supposing the knowledge from different datasets is intrinsically aligned; however, this assumption may not hold in the scenarios of cross-domain knowledge fusion. In this paper, we formally define the cross-domain multimodal data fusion problem, discussing its unique challenges, differences and advantages beyond data fusion in a single domain. We propose a four-layer framework, consisting of Domains, Links, Models and Data layers, answering three key questions:"what to fuse", "why can be fused", and "how to fuse". The Domains Layer selects relevant data from different domains for a given problem. The Links Layer reveals the philosophy of knowledge alignment beyond specific model structures. The Models Layer provides two knowledge fusion paradigms based on the fundamental mechanisms for processing data. The Data Layer turns data of different structures, resolutions, scales and distributions into a consistent representation that can be fed into an AI model. With this framework, we can design solutions that fuse cross-domain multimodal data effectively for solving real-world problems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on the Evaluation of Generative Models in Music</title>
<link>https://arxiv.org/abs/2506.05104</link>
<guid>https://arxiv.org/abs/2506.05104</guid>
<content:encoded><![CDATA[
arXiv:2506.05104v2 Announce Type: replace-cross 
Abstract: Research on generative systems in music has seen considerable attention and growth in recent years. A variety of attempts have been made to systematically evaluate such systems.
  We present an interdisciplinary review of the common evaluation targets, methodologies, and metrics for the evaluation of both system output and model use, covering subjective and objective approaches, qualitative and quantitative approaches, as well as empirical and computational methods. We examine the benefits and limitations of these approaches from a musicological, an engineering, and an HCI perspective.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models</title>
<link>https://arxiv.org/abs/2506.09082</link>
<guid>https://arxiv.org/abs/2506.09082</guid>
<content:encoded><![CDATA[
arXiv:2506.09082v2 Announce Type: replace-cross 
Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation. A common approach pairs VFMs with large language models (LLMs) as general-purpose heads, followed by evaluation on broad Visual Question Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i) the instruction tuning data may not align with VQA test distributions, meaning a wrong prediction can stem from such data mismatch rather than a VFM' visual shortcomings; (ii) VQA benchmarks often require multiple visual abilities, making it hard to tell whether errors stem from lacking all required abilities or just a single critical one. To address these gaps, we introduce AVA-Bench, the first benchmark that explicitly disentangles 14 Atomic Visual Abilities (AVAs) -- foundational skills like localization, depth estimation, and spatial understanding that collectively support complex visual reasoning tasks. By decoupling AVAs and matching training and test distributions within each, AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM selection from educated guesswork into principled engineering. Notably, we find that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours by 8x, enabling more efficient evaluation. By offering a comprehensive and transparent benchmark, we hope AVA-Bench lays the foundation for the next generation of VFMs.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning</title>
<link>https://arxiv.org/abs/2506.11246</link>
<guid>https://arxiv.org/abs/2506.11246</guid>
<content:encoded><![CDATA[
arXiv:2506.11246v2 Announce Type: replace-cross 
Abstract: Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective reasoning to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, model performance varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique on diverse table types to determine that performance depends on factors such as entity type, table structure, requirement of additional context and question complexity, with "NO" single method consistently outperforming others. To address this, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts to context and integrates structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model reasoning.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientifically-Interpretable Reasoning Network (ScIReN): Discovering Hidden Relationships in the Carbon Cycle and Beyond</title>
<link>https://arxiv.org/abs/2506.14054</link>
<guid>https://arxiv.org/abs/2506.14054</guid>
<content:encoded><![CDATA[
arXiv:2506.14054v2 Announce Type: replace-cross 
Abstract: Understanding how carbon flows through the soil is crucial for mitigating the effects of climate change. While soils have potential to sequester carbon from the atmosphere, the soil carbon cycle remains poorly understood. Scientists have developed mathematical process-based models of the soil carbon cycle based on existing knowledge, but they contain numerous unknown parameters that must be set in an ad-hoc manner, and often fit observations poorly. On the other hand, neural networks can learn patterns from data, but do not respect known scientific laws, nor can they reveal novel scientific relationships due to their black-box nature. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is fully interpretable and reveals relationships between input features and latent parameters; it uses novel smoothness penalties to balance expressivity and simplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge. While the process-based decoder enforces established scientific knowledge, the KAN-based encoder reveals new scientific relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.17631</link>
<guid>https://arxiv.org/abs/2506.17631</guid>
<content:encoded><![CDATA[
arXiv:2506.17631v2 Announce Type: replace-cross 
Abstract: Time series forecasting aims to model temporal dependencies among variables for future state inference, holding significant importance and widespread applications in real-world scenarios. Although deep learning-based methods have achieved remarkable progress, they still exhibit suboptimal performance in long-term forecasting and data-scarce scenarios. Recent research demonstrates that large language models (LLMs) achieve promising performance in time series forecasting. However, we find existing LLM-based methods still have shortcomings: (1) the absence of a unified paradigm for textual prompt formulation and (2) the neglect of modality discrepancies between textual prompts and time series. To address this, we propose LLM-Prompt, an LLM-based time series forecasting framework integrating multi-prompt information and cross-modal semantic alignment. Specifically, we first construct a unified textual prompt paradigm containing learnable soft prompts and textualized hard prompts. Second, to enhance LLMs' comprehensive understanding of the forecasting task, we design a semantic space embedding and cross-modal alignment module to achieve cross-modal fusion of temporal and textual information. Finally, the transformed time series from the LLMs are projected to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3 carbon emission datasets demonstrate that LLM-Prompt is a powerful framework for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective</title>
<link>https://arxiv.org/abs/2506.19028</link>
<guid>https://arxiv.org/abs/2506.19028</guid>
<content:encoded><![CDATA[
arXiv:2506.19028v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information</title>
<link>https://arxiv.org/abs/2507.00038</link>
<guid>https://arxiv.org/abs/2507.00038</guid>
<content:encoded><![CDATA[
arXiv:2507.00038v3 Announce Type: replace-cross 
Abstract: In order to increase the effectiveness of model training, data reduction is essential to data-centric Artificial Intelligence (AI). It achieves this by locating the most instructive examples in massive datasets. To increase data quality and training efficiency, the main difficulty is choosing the best examples rather than the complete datasets. In this paper, we propose an effective data reduction strategy based on Pointwise V-Information (PVI). To enable a static method, we first use PVI to quantify instance difficulty and remove instances with low difficulty. Experiments show that classifier performance is maintained with only a 0.0001% to 0.76% decline in accuracy when 10%-30% of the data is removed. Second, we train the classifiers using a progressive learning strategy on examples sorted by increasing PVI, accelerating convergence and achieving a 0.8% accuracy gain over conventional training. Our findings imply that training a classifier on the chosen optimal subset may improve model performance and increase training efficiency when combined with an efficient data reduction strategy. Furthermore, we have adapted the PVI framework, which was previously limited to English datasets, to a variety of Chinese Natural Language Processing (NLP) tasks and base models, yielding insightful results for faster training and cross-lingual data reduction.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crop Pest Classification Using Deep Learning Techniques: A Review</title>
<link>https://arxiv.org/abs/2507.01494</link>
<guid>https://arxiv.org/abs/2507.01494</guid>
<content:encoded><![CDATA[
arXiv:2507.01494v3 Announce Type: replace-cross 
Abstract: Insect pests continue to bring a serious threat to crop yields around the world, and traditional methods for monitoring them are often slow, manual, and difficult to scale. In recent years, deep learning has emerged as a powerful solution, with techniques like convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid models gaining popularity for automating pest detection. This review looks at 37 carefully selected studies published between 2018 and 2025, all focused on AI-based pest classification. The selected research is organized by crop type, pest species, model architecture, dataset usage, and key technical challenges. The early studies relied heavily on CNNs but latest work is shifting toward hybrid and transformer-based models that deliver higher accuracy and better contextual understanding. Still, challenges like imbalanced datasets, difficulty in detecting small pests, limited generalizability, and deployment on edge devices remain significant hurdles. Overall, this review offers a structured overview of the field, highlights useful datasets, and outlines the key challenges and future directions for AI-based pest monitoring systems.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans overrely on overconfident language models, across languages</title>
<link>https://arxiv.org/abs/2507.06306</link>
<guid>https://arxiv.org/abs/2507.06306</guid>
<content:encoded><![CDATA[
arXiv:2507.06306v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Prior work shows that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., 'I think it's') differs sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate LLM safety in a global context. Our work finds that overreliance risks are high across languages. We first analyze the distribution of LLM-generated epistemic markers and observe that LLMs are overconfident across languages, frequently generating strengtheners even as part of incorrect responses. Model generations are, however, sensitive to documented cross-linguistic variation in usage: for example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. Next, we measure human reliance rates across languages, finding that reliance behaviors differ cross-linguistically: for example, participants are significantly more likely to discount expressions of uncertainty in Japanese than in English (i.e., ignore their 'hedging' function and rely on generations that contain them). Taken together, these results indicate a high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks</title>
<link>https://arxiv.org/abs/2507.17747</link>
<guid>https://arxiv.org/abs/2507.17747</guid>
<content:encoded><![CDATA[
arXiv:2507.17747v2 Announce Type: replace-cross 
Abstract: As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates--where one model is given the official answer to defend, and another constructs and defends an alternative answer--adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm's effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination--a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that "pretraining on the test set is no longer all you need," offering a sustainable path for measuring the genuine reasoning ability of advanced language models.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ART: Adaptive Relation Tuning for Generalized Relation Prediction</title>
<link>https://arxiv.org/abs/2507.23543</link>
<guid>https://arxiv.org/abs/2507.23543</guid>
<content:encoded><![CDATA[
arXiv:2507.23543v2 Announce Type: replace-cross 
Abstract: Visual relation detection (VRD) is the task of identifying the relationships between objects in a scene. VRD models trained solely on relation detection data struggle to generalize beyond the relations on which they are trained. While prompt tuning has been used to adapt vision-language models (VLMs) for VRD, it uses handcrafted prompts and struggles with novel or complex relations. We argue that instruction tuning offers a more effective solution by fine-tuning VLMs on diverse instructional data. We thus introduce ART, an Adaptive Relation Tuning framework that adapts VLMs for VRD through instruction tuning and strategic instance selection. By converting VRD datasets into an instruction tuning format and employing an adaptive sampling algorithm, ART directs the VLM to focus on informative relations while maintaining generalizability. Specifically, we focus on the relation classification, where subject-object boxes are given and the model predicts the predicate between them. We tune on a held-in set and evaluate across multiple held-out datasets of varying complexity. Our approach strongly improves over its baselines and can infer unseen relation concepts, a capability absent in mainstream VRD methods. We demonstrate ART's practical value by using the predicted relations for segmenting complex scenes.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
<link>https://arxiv.org/abs/2508.00381</link>
<guid>https://arxiv.org/abs/2508.00381</guid>
<content:encoded><![CDATA[
arXiv:2508.00381v2 Announce Type: replace-cross 
Abstract: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARTA: Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization</title>
<link>https://arxiv.org/abs/2508.01646</link>
<guid>https://arxiv.org/abs/2508.01646</guid>
<content:encoded><![CDATA[
arXiv:2508.01646v2 Announce Type: replace-cross 
Abstract: Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics inherent in spike-based processing, relying primarily on rate coding while overlooking precise timing information that provides rich computational cues. We propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a framework that leverages heterogeneous neuron dynamics and spike-timing information to enable efficient sparse attention. SPARTA prioritizes tokens based on temporal cues, including firing patterns, spike timing, and inter-spike intervals, achieving 65.4% sparsity through competitive gating. By selecting only the most salient tokens, SPARTA reduces attention complexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy. Our method achieves state-of-the-art performance on DVS-Gesture (98.78%) and competitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating that exploiting spike timing dynamics improves both computational efficiency and accuracy.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation</title>
<link>https://arxiv.org/abs/2507.02253</link>
<guid>https://arxiv.org/abs/2507.02253</guid>
<content:encoded><![CDATA[
<div> Workflow domain, Large Language Model, NL2Flow, planning problems, evaluation data  
Summary:  
NL2Flow is a system designed to generate planning problems for Large Language Models (LLMs) in natural language, structured intermediate representation, and formal PDDL, enabling rigorous plan evaluation. Results show that LLMs achieve 86% success in generating valid plans and 69% in generating optimal plans for low-difficulty problems. Model performance is influenced by problem characteristics and prompt design. LLMs translating natural language to JSON representation of workflow problems have lower success rates compared to generating plans directly, indicating the benefit of models capable of reasoning directly from natural language to action. Understanding the bottlenecks and sources of error in LLM reasoning on complex problems is essential for their scalability and effectiveness.  
Summary: <div>
arXiv:2507.02253v3 Announce Type: replace 
Abstract: Effective agent performance relies on the ability to compose tools and agents into effective workflows. However, progress in Large Language Model (LLM) planning and reasoning is limited by the scarcity of scalable, reliable evaluation data. This study addresses this limitation by identifying a suitable workflow domain for LLM application. I introduce NL2Flow, a fully automated system for parametrically generating planning problems, which are expressed in natural language, a structured intermediate representation, and formal PDDL, and rigorously evaluating the quality of generated plans. NL2Flow generates a dataset of 2296 low-difficulty problems in automated workflow generation and evaluates multiple open-sourced, instruct-tuned LLMs without task-specific optimization or architectural modifications. Results reveal that the highest performing model achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible plans. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. To investigate the potential of LLMs as natural language-to-JSON translators for workflow definition, and to facilitate integration with downstream symbolic computation tools and a symbolic planner, I evaluated the LLM's translation performance on natural language workflow descriptions. I observed that translating natural language into a JSON representation of a workflow problem yielded a lower success rate than generating a plan directly, suggesting that unnecessary decomposition of the reasoning task may degrade performance and highlighting the benefit of models capable of reasoning directly from natural language to action. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.01109</link>
<guid>https://arxiv.org/abs/2506.01109</guid>
<content:encoded><![CDATA[
<div> adaptive-density Gaussian Splatting, fruit counting, language guidance, 3D reconstruction, semantic retrieval
Summary:
FruitLangGS is a novel framework for accurate 3D fruit counting in orchards. It utilizes adaptive-density Gaussian Splatting with radius-aware pruning and tile-based rasterization for scalable 3D representation. The framework incorporates language guidance by embedding CLIP-aligned semantic vectors in each Gaussian and using a dual-threshold cosine similarity mechanism for prompt filtering. This enables the selection of relevant Gaussians while suppressing distractions like foliage. The selected Gaussians are then clustered geometrically to estimate fruit instances, overcoming challenges like occlusion and viewpoint variation. FruitLangGS outperforms existing pipelines in fruit counting recall, achieving up to 99.7% recall on a specific orchard dataset. Ablation studies confirm the importance of language-conditioned semantic embedding and dual-threshold prompt filtering for accurate counting under occlusion. The framework has the potential for prompt-driven 3D semantic retrieval without retraining, demonstrating the value of language-guided 3D perception in agricultural scene understanding. 
Summary: <div>
arXiv:2506.01109v3 Announce Type: replace-cross 
Abstract: Accurate 3D fruit counting in orchards is challenging due to heavy occlusion, semantic ambiguity between fruits and surrounding structures, and the high computational cost of volumetric reconstruction. Existing pipelines often rely on multi-view 2D segmentation and dense volumetric sampling, which lead to accumulated fusion errors and slow inference. We introduce FruitLangGS, a language-guided 3D fruit counting framework that reconstructs orchard-scale scenes using an adaptive-density Gaussian Splatting pipeline with radius-aware pruning and tile-based rasterization, enabling scalable 3D representation. During inference, compressed CLIP-aligned semantic vectors embedded in each Gaussian are filtered via a dual-threshold cosine similarity mechanism, retrieving Gaussians relevant to target prompts while suppressing common distractors (e.g., foliage), without requiring retraining or image-space masks. The selected Gaussians are then sampled into dense point clouds and clustered geometrically to estimate fruit instances, remaining robust under severe occlusion and viewpoint variation. Experiments on nine different orchard-scale datasets demonstrate that FruitLangGS consistently outperforms existing pipelines in instance counting recall, avoiding multi-view segmentation fusion errors and achieving up to 99.7% recall on Pfuji-Size_Orch2018 orchard dataset. Ablation studies further confirm that language-conditioned semantic embedding and dual-threshold prompt filtering are essential for suppressing distractors and improving counting accuracy under heavy occlusion. Beyond fruit counting, the same framework enables prompt-driven 3D semantic retrieval without retraining, highlighting the potential of language-guided 3D perception for scalable agricultural scene understanding.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions</title>
<link>https://arxiv.org/abs/2508.01674</link>
<guid>https://arxiv.org/abs/2508.01674</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalization, Large Language Models, User Preferences, Contextual Preferences, Interaction Sessions
Summary: 
Personalization of Large Language Models (LLMs) often assumes static user preferences across all tasks. However, humans have dynamic preferences that change with context. The CUPID benchmark evaluates LLMs' ability to infer and apply contextual preferences based on interaction histories. Results show that current LLMs struggle to accurately infer preferences and relevant context, with performance below 50% precision and 65% recall. This highlights the need to enhance LLM capabilities for more personalized interactions. CUPID serves as a valuable resource to drive improvements in contextually personalized responses. 

<br /><br />Summary: <div>
arXiv:2508.01674v2 Announce Type: replace-cross 
Abstract: Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</title>
<link>https://arxiv.org/abs/2508.02095</link>
<guid>https://arxiv.org/abs/2508.02095</guid>
<content:encoded><![CDATA[
<div> benchmark, VLMs, spatiotemporal reasoning, motion continuity, visual intelligence 

Summary:<br />
The paper introduces VLM4D, a benchmark designed to assess VLMs' spatiotemporal reasoning abilities. It emphasizes dynamic interactions like object movements, rotations, and perspective shifts crucial for real-world understanding. Current VLMs struggle with integrating visual cues and maintaining temporal coherence, falling short compared to human baselines. Promising directions like 4D feature field reconstruction and supervised spatiotemporal fine-tuning show potential in enhancing spatiotemporal comprehension. The benchmark comprises diverse videos with question-answer pairs to evaluate translational and rotational motions, perspective awareness, and motion continuity. The aim is to encourage research in improving VLMs' spatial and temporal grounding for better visual intelligence in dynamic environments. <div>
arXiv:2508.02095v2 Announce Type: replace-cross 
Abstract: Vision language models (VLMs) have shown remarkable capabilities in integrating linguistic and visual reasoning but remain fundamentally limited in understanding dynamic spatiotemporal interactions. Humans effortlessly track and reason about object movements, rotations, and perspective shifts-abilities essential for robust dynamic real-world understanding yet notably lacking in current VLMs. In this paper, we introduce VLM4D, the first benchmark specifically designed to evaluate the spatiotemporal reasoning capabilities of VLMs. Our benchmark comprises diverse real-world and synthetic videos accompanied by carefully curated question-answer pairs emphasizing translational and rotational motions, perspective awareness, and motion continuity. Through comprehensive evaluations of state-of-the-art open and closed-source VLMs, we identify significant performance gaps compared to human baselines, highlighting fundamental deficiencies in existing models. Extensive analysis reveals that VLMs struggle particularly with integrating multiple visual cues and maintaining temporal coherence. We further explore promising directions, such as leveraging 4D feature field reconstruction and targeted spatiotemporal supervised fine-tuning, demonstrating their effectiveness in enhancing spatiotemporal comprehension. Our work aims to encourage deeper exploration into improving VLMs' spatial and temporal grounding, paving the way towards more capable and reliable visual intelligence for dynamic environments.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space</title>
<link>https://arxiv.org/abs/2508.02247</link>
<guid>https://arxiv.org/abs/2508.02247</guid>
<content:encoded><![CDATA[
<div> Generative modeling, high-frequency limit order book, byte streams, market dynamics, autoregressive prediction<br />
Summary:<br />
ByteGen is a novel generative model for high-frequency limit order book dynamics that operates directly on raw byte streams. It uses a compact 32-byte binary format to represent market messages without information loss, eliminating the need for feature engineering and tokenization. The model, based on the H-Net architecture, successfully reproduces key financial market characteristics like realistic price distributions and bursty event timing. Trained on CME Bitcoin futures data, ByteGen achieves competitive performance on market quality metrics without the biases of tokenization. This approach represents a promising paradigm for modeling complex financial systems by learning directly from byte space. <div>
arXiv:2508.02247v2 Announce Type: replace-cross 
Abstract: Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
<link>https://arxiv.org/abs/2508.02317</link>
<guid>https://arxiv.org/abs/2508.02317</guid>
<content:encoded><![CDATA[
<div> framework, omni-modal, large language models, 3D parallelism, scalability

Summary:
VeOmni is a modular training framework designed to facilitate the development of omni-modal Large Language Models (LLMs). It addresses the challenge of heterogeneity in model architectures by introducing model-centric distributed recipes that separate communication from computation, enabling efficient 3D parallelism for omni-modal LLMs. The framework features a flexible configuration interface that allows for the integration of new modalities with minimal code changes. Using VeOmni, a large omni-modal mixture-of-experts (MoE) model with 30B parameters achieved impressive training throughput of over 2,800 tokens/sec/GPU and scalability to 160K context lengths on 128 GPUs. This showcases VeOmni's superior efficiency and scalability for training large omni-modal LLMs. <div>
arXiv:2508.02317v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. VeOmni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prescriptive Agents based on Rag for Automated Maintenance (PARAM)</title>
<link>https://arxiv.org/abs/2508.04714</link>
<guid>https://arxiv.org/abs/2508.04714</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, prescriptive maintenance, anomaly detection, bearing vibration analysis, intelligent maintenance planning

Summary: 
This paper introduces an integrated Large Language Model (LLM) based system for prescriptive maintenance in industrial machinery. The system goes beyond traditional anomaly detection and provides actionable maintenance recommendations. By combining bearing vibration frequency analysis with multi-agent generation, the system can classify fault types and assess severity levels. Through natural language processing, the system serializes vibration data for accurate anomaly detection. A multi-agentic component processes maintenance manuals and conducts web searches to provide comprehensive maintenance recommendations. The Gemini model generates structured maintenance recommendations including immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation on bearing vibration datasets confirms the effectiveness of anomaly detection and maintenance guidance. This work bridges the gap between condition monitoring and maintenance planning, offering intelligent decision support for industrial practitioners. <br /><br />Summary: <div>
arXiv:2508.04714v1 Announce Type: new 
Abstract: Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball/roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoFlow: Agentic Workflow Automation for Geospatial Tasks</title>
<link>https://arxiv.org/abs/2508.04719</link>
<guid>https://arxiv.org/abs/2508.04719</guid>
<content:encoded><![CDATA[
<div> Keywords: GeoFlow, agentic workflows, geospatial tasks, API selection, LLM families 

Summary: 
GeoFlow is a new method that automatically generates agentic workflows for geospatial tasks. Unlike previous approaches that focus on reasoning decomposition, GeoFlow provides agents with specific tool-calling objectives to guide geospatial API invocation at runtime. This approach increases agentic success rates by 6.8% and reduces token usage by up to four times across major LLM families compared to existing methods. By explicitly guiding API selection for each agent, GeoFlow enhances the efficiency and effectiveness of geospatial task execution. This method can streamline the workflow generation process and improve task performance in various geospatial applications. GeoFlow represents a significant advancement in the field of automated workflow generation for geospatial tasks, offering a more precise and optimized approach to API selection and invocation. <br /><br />Summary:  <div>
arXiv:2508.04719v1 Announce Type: new 
Abstract: We present GeoFlow, a method that automatically generates agentic workflows for geospatial tasks. Unlike prior work that focuses on reasoning decomposition and leaves API selection implicit, our method provides each agent with detailed tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow increases agentic success by 6.8% and reduces token usage by up to fourfold across major LLM families compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is a Better Player: LLM against LLM</title>
<link>https://arxiv.org/abs/2508.04720</link>
<guid>https://arxiv.org/abs/2508.04720</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial board games, Large Language Models, Qi Town, Performance Loop Graph, Elo rating system

Summary: 
The article proposes a new benchmarking framework for assessing the performance of Large Language Models (LLMs) through adversarial board games competition. The framework is implemented in Qi Town, an evaluation platform supporting 5 popular games with 20 LLM-driven players. Utilizing the Elo rating system and a Performance Loop Graph (PLG), the platform quantitatively evaluates the technical capabilities of LLMs and assesses mental fitness through Positive Sentiment Score (PSS). Results show that LLMs demonstrate adaptability in high-stress environments but exhibit instability in skill play. The round-robin tournament structure enables systematic comparison across players, highlighting the need for further exploration into the complex relationship between wins and losses in PLGs. Overall, the study emphasizes the importance of evaluating AI systems in adversarial settings to better understand their strategic reasoning and intelligence capabilities.<br /><br />Summary: <div>
arXiv:2508.04720v1 Announce Type: new 
Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and intelligence, have long served as both a popular competitive activity and a benchmark for evaluating artificial intelligence (AI) systems. Building on this foundation, we propose an adversarial benchmarking framework to assess the comprehensive performance of Large Language Models (LLMs) through board games competition, compensating the limitation of data dependency of the mainstream Question-and-Answer (Q&amp;A) based benchmark method. We introduce Qi Town, a specialized evaluation platform that supports 5 widely played games and involves 20 LLM-driven players. The platform employs both the Elo rating system and a novel Performance Loop Graph (PLG) to quantitatively evaluate the technical capabilities of LLMs, while also capturing Positive Sentiment Score (PSS) throughout gameplay to assess mental fitness. The evaluation is structured as a round-robin tournament, enabling systematic comparison across players. Experimental results indicate that, despite technical differences, most LLMs remain optimistic about winning and losing, demonstrating greater adaptability to high-stress adversarial environments than humans. On the other hand, the complex relationship between cyclic wins and losses in PLGs exposes the instability of LLMs' skill play during games, warranting further explanation and exploration.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)</title>
<link>https://arxiv.org/abs/2508.04846</link>
<guid>https://arxiv.org/abs/2508.04846</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous WebGIS, Geospatial operations, Small language model, Client-side computation, Browser-executable models  
Summary:  
- The study compares three approaches for Autonomous WebGIS: online cloud-based LLMs, semi-automated offline classical ML classifiers, and fully autonomous offline client-side SLMs using a T5-small model.
- The client-side computation approach with SLMs achieved the highest accuracy, with exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and high ROUGE-1 and ROUGE-L scores of 0.98.
- This strategy reduces the backend server load by offloading processing to the user's device, eliminating the need for server-based inference and addressing privacy and scalability concerns.
- The results demonstrate the feasibility of browser-executable models for efficient and accurate AWebGIS solutions.
- By leveraging SLMs for geospatial operations, users can benefit from intuitive, intelligent, and hands-free interaction while maintaining privacy and scalability.  

<br /><br />Summary: <div>
arXiv:2508.04846v1 Announce Type: new 
Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to perform geospatial operations from natural language input, providing intuitive, intelligent, and hands-free interaction. However, most current solutions rely on cloud-based large language models (LLMs), which require continuous internet access and raise users' privacy and scalability issues due to centralized server processing. This study compares three approaches to enabling AWebGIS: (1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2) a semi-automated offline method using classical machine learning classifiers such as support vector machine and random forest; and (3) a fully autonomous offline (client-side) method based on a fine-tuned small language model (SLM), specifically T5-small model, executed in the client's web browser. The third approach, which leverages SLMs, achieved the highest accuracy among all methods, with an exact matching accuracy of 0.93, Levenshtein similarity of 0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L scores of 0.98. Crucially, this client-side computation strategy reduces the load on backend servers by offloading processing to the user's device, eliminating the need for server-based inference. These results highlight the feasibility of browser-executable models for AWebGIS solutions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.04848</link>
<guid>https://arxiv.org/abs/2508.04848</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, large language models, realistic scenarios, reasoning abilities, limitations

Summary:<br />
Reinforcement learning has been instrumental in enhancing the reasoning abilities of large language models, but current benchmarks often overlook performance in realistic, non-ideal scenarios. This study identifies and evaluates three challenging scenarios - summary inference, fine-grained noise suppression, and contextual filtering. Testing three large language models and a vision-language model after RL fine-tuning reveals that while performance improves under idealized settings, there's a significant decline in non-ideal scenarios. Despite proposing specific remediation methods, the results suggest that current approaches do not effectively address reasoning deficits. This work emphasizes the need to assess large models under non-ideal conditions to accurately gauge their reasoning capabilities. The code and data for this study will be made available for further research.<br /> <div>
arXiv:2508.04848v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs. We formally define and evaluate these challenging scenarios. We fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM) using RL with a representative policy-gradient algorithm and then test their performance on eight public datasets. Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities. Although we propose a scenario-specific remediation method, our results suggest current methods leave these reasoning deficits largely unresolved. This work highlights that the reasoning abilities of large models are often overstated and underscores the importance of evaluating models under non-ideal scenarios. The code and data will be released at XXXX.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.04915</link>
<guid>https://arxiv.org/abs/2508.04915</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, healthcare research, self-evolving, HealthFlow, EHRFlowBench 

Summary:
HealthFlow addresses the limitation of static strategies in AI agents in healthcare research by introducing a self-evolving mechanism. The AI agent refines its problem-solving policies autonomously by learning from its successes and failures. EHRFlowBench, a new benchmark, provides complex health data analysis tasks for evaluation. The self-evolving approach of HealthFlow outperforms existing agent frameworks significantly in experiments. This marks a shift towards designing smarter, self-evolving task-managers in AI for scientific discovery. <div>
arXiv:2508.04915v1 Announce Type: new 
Abstract: The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding</title>
<link>https://arxiv.org/abs/2508.05006</link>
<guid>https://arxiv.org/abs/2508.05006</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular docking, drug discovery, multi-task learning, game-theoretic framework, LoopPlay algorithm 

Summary: 
The article introduces a new game-theoretic framework, called the Docking Game, for molecular docking in drug discovery. It addresses the performance disparity between ligand and protein pocket docking by modeling the interaction as a two-player game. The proposed LoopPlay algorithm trains the ligand and protein players through a two-level loop, allowing them to exchange predicted poses and dynamically refine their predictions. The theoretical convergence of LoopPlay ensures stable optimization. Experimental results on benchmark datasets demonstrate a significant improvement in predicting accurate binding modes compared to existing methods, highlighting the framework's potential to enhance molecular docking accuracy. <br /><br />Summary: <div>
arXiv:2508.05006v1 Announce Type: new 
Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the binding interactions between small-molecule ligands and protein pockets. However, current multi-task learning models for docking often show inferior performance in ligand docking compared to protein pocket docking. This disparity arises largely due to the distinct structural complexities of ligands and proteins. To address this issue, we propose a novel game-theoretic framework that models the protein-ligand interaction as a two-player game called the Docking Game, with the ligand docking module acting as the ligand player and the protein pocket docking module as the protein player. To solve this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which alternately trains these players through a two-level loop. In the outer loop, the players exchange predicted poses, allowing each to incorporate the other's structural predictions, which fosters mutual adaptation over multiple iterations. In the inner loop, each player dynamically refines its predictions by incorporating its own predicted ligand or pocket poses back into its model. We theoretically show the convergence of LoopPlay, ensuring stable optimization. Extensive experiments conducted on public benchmark datasets demonstrate that LoopPlay achieves approximately a 10\% improvement in predicting accurate binding modes compared to previous state-of-the-art methods. This highlights its potential to enhance the accuracy of molecular docking in drug discovery.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses</title>
<link>https://arxiv.org/abs/2508.05009</link>
<guid>https://arxiv.org/abs/2508.05009</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, spatial data integration, spatial reasoning, review-and-refine method, adaptive integration

Summary: 
Large language models (LLMs) are investigated for spatial data integration, aiming to empower domain experts in handling large, diverse, and noisy urban spatial datasets. Traditional rule-based methods have limitations in covering all cases, while machine learning approaches require extensive data labeling. The study shows that LLMs possess spatial reasoning abilities but struggle in connecting macro-scale environments with computational geometry tasks. By reducing reliance on spatial reasoning and providing relevant features, LLMs achieve high-performance results. A review-and-refine method is successfully employed to correct initial errors while maintaining accuracy. Practical implications for real-world spatial data integration are discussed, along with future research directions like post-training techniques and support for diverse data formats. Overall, the findings suggest that LLMs offer a promising and flexible alternative to conventional rule-based methods, enhancing adaptive spatial data integration capabilities. 

<br /><br />Summary: <div>
arXiv:2508.05009v1 Announce Type: new 
Abstract: We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Duality for Adaptive Web Agents</title>
<link>https://arxiv.org/abs/2508.05081</link>
<guid>https://arxiv.org/abs/2508.05081</guid>
<content:encoded><![CDATA[
<div> Keywords: web navigation, artificial general intelligence, dual-process theory, autonomous agents, CogniWeb

Summary:
The article introduces a new approach for evaluating artificial general intelligence (AGI) through web navigation tasks. It proposes a dual-process theory inspired framework that combines fast System 1 and slow System 2 cognitive processes to enable autonomous web agents to make complex decisions in dynamic environments. The framework, implemented in CogniWeb, toggles between intuitive processing and deliberate reasoning based on task complexity. Evaluation on WebArena shows competitive performance (43.96% success rate) with higher efficiency (75% reduction in token usage). This integrated approach bridges the gap between offline learning and online exploration strategies commonly used in web agent development. CogniWeb's adaptive capabilities demonstrate the effectiveness of combining reactive behaviors and planning capabilities for improved performance in web navigation tasks.<br /><br />Summary: <div>
arXiv:2508.05081v1 Announce Type: new 
Abstract: Web navigation represents a critical and challenging domain for evaluating artificial general intelligence (AGI), demanding complex decision-making within high-entropy, dynamic environments with combinatorially explosive action spaces. Current approaches to building autonomous web agents either focus on offline imitation learning or online exploration, but rarely integrate both paradigms effectively. Inspired by the dual-process theory of human cognition, we derive a principled decomposition into fast System 1 and slow System 2 cognitive processes. This decomposition provides a unifying perspective on existing web agent methodologies, bridging the gap between offline learning of intuitive reactive behaviors and online acquisition of deliberative planning capabilities. We implement this framework in CogniWeb, a modular agent architecture that adaptively toggles between fast intuitive processing and deliberate reasoning based on task complexity. Our evaluation on WebArena demonstrates that CogniWeb achieves competitive performance (43.96% success rate) while maintaining significantly higher efficiency (75% reduction in token usage).
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.05083</link>
<guid>https://arxiv.org/abs/2508.05083</guid>
<content:encoded><![CDATA[
<div> medical multimodal large language models, benchmark, knowledge editing, MedMKEB, medical knowledge

Summary:
MedMKEB introduces a benchmark for evaluating knowledge editing in medical multimodal large language models. The benchmark assesses reliability, generality, locality, portability, and robustness of editing tasks involving image and text modalities. By incorporating tasks like counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness, MedMKEB aims to validate the accuracy and reliability of knowledge editing in medical AI. Single and sequential editing experiments on general and medical MLLMs reveal limitations in existing editing approaches, emphasizing the need for specialized strategies in the medical domain. MedMKEB provides a standardized platform to advance the development of trustworthy and efficient medical knowledge editing algorithms. <br /><br />Summary: <div>
arXiv:2508.05083v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly improved medical AI, enabling it to unify the understanding of visual and textual information. However, as medical knowledge continues to evolve, it is critical to allow these models to efficiently update outdated or incorrect information without retraining from scratch. Although textual knowledge editing has been widely studied, there is still a lack of systematic benchmarks for multimodal medical knowledge editing involving image and text modalities. To fill this gap, we present MedMKEB, the first comprehensive benchmark designed to evaluate the reliability, generality, locality, portability, and robustness of knowledge editing in medical multimodal large language models. MedMKEB is built on a high-quality medical visual question-answering dataset and enriched with carefully constructed editing tasks, including counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness. We incorporate human expert validation to ensure the accuracy and reliability of the benchmark. Extensive single editing and sequential editing experiments on state-of-the-art general and medical MLLMs demonstrate the limitations of existing knowledge-based editing approaches in medicine, highlighting the need to develop specialized editing strategies. MedMKEB will serve as a standard benchmark to promote the development of trustworthy and efficient medical knowledge editing algorithms.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search</title>
<link>https://arxiv.org/abs/2508.05113</link>
<guid>https://arxiv.org/abs/2508.05113</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog circuit design, gate sizing, Large Language Models, Differential Evolution, Particle Swarm Optimization

Summary: 
Analog circuit design is a complex and time-consuming task requiring significant expertise. Current AI approaches for gate sizing in analog circuits face challenges in generalizability and portability across different technology nodes. EasySize is introduced as a lightweight gate sizing framework based on a finetuned Qwen3-8B model, offering universal applicability across various process nodes, design specifications, and circuit topologies. Utilizing the concept of Ease of Attainability (EOA) of performance metrics, EasySize dynamically constructs task-specific loss functions, optimizing through global Differential Evolution (DE) and local Particle Swarm Optimization (PSO) within a feedback-enhanced flow. Despite being trained only on 350nm node data, EasySize demonstrates strong performance on Op-Amp netlists across 180nm, 45nm, and 22nm nodes. Outperforming existing methods like AutoCkt, EasySize reduces the reliance on human expertise and computational resources in gate sizing, streamlining the analog circuit design process. EasySize will be released as an open-source tool in the future. 

<br /><br />Summary: <div>
arXiv:2508.05113v1 Announce Type: new 
Abstract: Analog circuit design is a time-consuming, experience-driven task in chip development. Despite advances in AI, developing universal, fast, and stable gate sizing methods for analog circuits remains a significant challenge. Recent approaches combine Large Language Models (LLMs) with heuristic search techniques to enhance generalizability, but they often depend on large model sizes and lack portability across different technology nodes. To overcome these limitations, we propose EasySize, the first lightweight gate sizing framework based on a finetuned Qwen3-8B model, designed for universal applicability across process nodes, design specifications, and circuit topologies. EasySize exploits the varying Ease of Attainability (EOA) of performance metrics to dynamically construct task-specific loss functions, enabling efficient heuristic search through global Differential Evolution (DE) and local Particle Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned solely on 350nm node data, EasySize achieves strong performance on 5 operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology nodes without additional targeted training, and outperforms AutoCkt, a widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks with more than 96.67\% of simulation resources reduction. We argue that EasySize can significantly reduce the reliance on human expertise and computational resources in gate sizing, thereby accelerating and simplifying the analog circuit design process. EasySize will be open-sourced at a later date.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures</title>
<link>https://arxiv.org/abs/2508.05116</link>
<guid>https://arxiv.org/abs/2508.05116</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Socratic AI Tutor, Constructivist theory, Multi-agent systems, Hybrid learning ecosystems

Summary: 
This study evaluates the impact of a Socratic AI Tutor on student research question development. It compares the benefits of using a structured dialogue model against an uninstructed AI chatbot. Results show that students using the Socratic Tutor exhibit greater support for critical, independent, and reflective thinking. This challenges the notion of de-skilling due to generative AI use. The study introduces the concept of orchestrated Multi-agent systems (MAS) for educational purposes, suggesting a pedagogical shift towards AI-assisted learning. The proposed model involves educators curating specialized AI agents to support diverse learning paths. The paper also discusses system-level implications for higher education institutions and students, including changes in funding, faculty roles, curricular design, competencies, and assessment practices. Additionally, a cost-effectiveness analysis highlights the scalability of such systems. Overall, the study provides empirical evidence and a conceptual roadmap for integrating human-AI co-agency in educational ecosystems.<br /><br />Summary: <div>
arXiv:2508.05116v1 Announce Type: new 
Abstract: Generative AI is no longer a peripheral tool in higher education. It is rapidly evolving into a general-purpose infrastructure that reshapes how knowledge is generated, mediated, and validated. This paper presents findings from a controlled experiment evaluating a Socratic AI Tutor, a large language model designed to scaffold student research question development through structured dialogue grounded in constructivist theory. Conducted with 65 pre-service teacher students in Germany, the study compares interaction with the Socratic Tutor to engagement with an uninstructed AI chatbot. Students using the Socratic Tutor reported significantly greater support for critical, independent, and reflective thinking, suggesting that dialogic AI can stimulate metacognitive engagement and challenging recent narratives of de-skilling due to generative AI usage. These findings serve as a proof of concept for a broader pedagogical shift: the use of multi-agent systems (MAS) composed of specialised AI agents. To conceptualise this, we introduce the notion of orchestrated MAS, modular, pedagogically aligned agent constellations, curated by educators, that support diverse learning trajectories through differentiated roles and coordinated interaction. To anchor this shift, we propose an adapted offer-and-use model, in which students appropriate instructional offers from these agents. Beyond technical feasibility, we examine system-level implications for higher education institutions and students, including funding necessities, changes to faculty roles, curriculars, competencies and assessment practices. We conclude with a comparative cost-effectiveness analysis highlighting the scalability of such systems. In sum, this study contributes both empirical evidence and a conceptual roadmap for hybrid learning ecosystems that embed human-AI co-agency and pedagogical alignment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-based Event Log Repair</title>
<link>https://arxiv.org/abs/2508.05145</link>
<guid>https://arxiv.org/abs/2508.05145</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Heterogeneous Graph Neural Networks, Event Logs, Process Mining, Trace Reconstruction<br />
Summary:<br />
The article discusses the importance of high-quality event logs in Process Mining analysis and the challenges of dealing with missing information in real-world event logs. Traditional approaches involve using process models or machine learning models for trace reconstruction. The focus of this work is on developing a Heterogeneous Graph Neural Network model that can fill in missing attributes in incomplete events in traces. The model is evaluated on synthetic and real event logs and outperforms existing model-free approaches by effectively reconstructing all event attributes. This approach offers a more expressive and semantically rich encoding of complex multi-modal sequences like execution traces in Process Mining. <div>
arXiv:2508.05145v1 Announce Type: new 
Abstract: The quality of event logs in Process Mining is crucial when applying any form of analysis to them. In real-world event logs, the acquisition of data can be non-trivial (e.g., due to the execution of manual activities and related manual recording or to issues in collecting, for each event, all its attributes), and often may end up with events recorded with some missing information. Standard approaches to the problem of trace (or log) reconstruction either require the availability of a process model that is used to fill missing values by leveraging different reasoning techniques or employ a Machine Learning/Deep Learning model to restore the missing values by learning from similar cases. In recent years, a new type of Deep Learning model that is capable of handling input data encoded as graphs has emerged, namely Graph Neural Networks. Graph Neural Network models, and even more so Heterogeneous Graph Neural Networks, offer the advantage of working with a more natural representation of complex multi-modal sequences like the execution traces in Process Mining, allowing for more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural Network model that, given a trace containing some incomplete events, will return the full set of attributes missing from those events. We evaluate our work against a state-of-the-art approach leveraging autoencoders on two synthetic logs and four real event logs, on different types of missing values. Different from state-of-the-art model-free approaches, which mainly focus on repairing a subset of event attributes, the proposed approach shows very good performance in reconstructing all different event attributes.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</title>
<link>https://arxiv.org/abs/2508.05197</link>
<guid>https://arxiv.org/abs/2508.05197</guid>
<content:encoded><![CDATA[
<div> Retrieve-Augmented Generation, Knowledge-Intensive VQA, QA-Dragon, reasoning performance, multimodal, multi-turn<br />
Summary: <br />
QA-Dragon is a Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering. It addresses the limitations of existing RAG methods by incorporating domain-specific reasoning and dynamically selecting optimal retrieval strategies. By combining text and image search agents in a hybrid setup, QA-Dragon supports multimodal, multi-turn, and multi-hop reasoning for complex VQA tasks. The system significantly enhances reasoning performance in the Meta CRAG-MM Challenge, outperforming baselines on single-source, multi-source, and multi-turn tasks. QA-Dragon achieves substantial improvements in answer accuracy and knowledge overlap scores, demonstrating its effectiveness in handling complex queries and up-to-date factual knowledge. <div>
arXiv:2508.05197v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA). However, existing RAG methods typically retrieve from either text or images in isolation, limiting their ability to address complex queries that require multi-hop reasoning or up-to-date factual knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's subject domain for domain-specific reasoning, along with a search router that dynamically selects optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup, our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025, where it significantly enhances the reasoning performance of base models under challenging scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the multi-source task, and 5.03% on the multi-turn task.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication</title>
<link>https://arxiv.org/abs/2508.05267</link>
<guid>https://arxiv.org/abs/2508.05267</guid>
<content:encoded><![CDATA[
<div> Keywords: maintenance organizations, subject matter experts, RDF graph databases, LLMs, communication efficiency<br />
Summary:<br />
In large-scale maintenance organizations, traditional communication approaches often lead to information overload and longer response times. This article proposes a novel framework that combines RDF graph databases with LLMs to process natural language queries, enabling precise audience targeting and transparent reasoning through a planning-orchestration architecture. By allowing communication owners to formulate intuitive queries involving concepts such as equipment, manufacturers, maintenance engineers, and facilities, the framework delivers explainable results that maintain trust in the system. This approach aims to improve communication efficiency across complex entity relationships in organizations, enhancing the identification of subject matter experts and facilitating more effective communication management. <div>
arXiv:2508.05267v1 Announce Type: new 
Abstract: In large-scale maintenance organizations, identifying subject matter experts and managing communications across complex entities relationships poses significant challenges -- including information overload and longer response times -- that traditional communication approaches fail to address effectively. We propose a novel framework that combines RDF graph databases with LLMs to process natural language queries for precise audience targeting, while providing transparent reasoning through a planning-orchestration architecture. Our solution enables communication owners to formulate intuitive queries combining concepts such as equipment, manufacturers, maintenance engineers, and facilities, delivering explainable results that maintain trust in the system while improving communication efficiency across the organization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents</title>
<link>https://arxiv.org/abs/2508.05311</link>
<guid>https://arxiv.org/abs/2508.05311</guid>
<content:encoded><![CDATA[
<div> decision tree, symbolic reasoning, large language models, multi-agent framework, neuro-symbolic reasoning <br />
<br />
Summary: 
The article introduces a hybrid architecture that combines decision tree-based symbolic reasoning with large language models (LLMs) in a coordinated multi-agent system. Unlike previous methods that loosely integrate symbolic and neural modules, this approach embeds decision trees and random forests as callable oracles in a unified reasoning system. The combination allows for interpretable rule inference, causal logic, abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and facilitates communication among agents and external tools. The system demonstrates strong performance on reasoning benchmarks like ProofWriter, GSM8k, and ARC, showcasing improvements in entailment consistency, mathematical problem-solving accuracy, and abstraction accuracy. Moreover, the architecture's applications in clinical decision support and scientific discovery illustrate how it integrates domain rules symbolically while using LLMs for contextual inference and hypothesis generation. Overall, this architecture provides a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning. <br /> <div>
arXiv:2508.05311v1 Announce Type: new 
Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic reasoning with the generative capabilities of large language models (LLMs) within a coordinated multi-agent framework. Unlike prior approaches that loosely couple symbolic and neural modules, our design embeds decision trees and random forests as callable oracles within a unified reasoning system. Tree-based modules enable interpretable rule inference and causal logic, while LLM agents handle abductive reasoning, generalization, and interactive planning. A central orchestrator maintains belief state consistency and mediates communication across agents and external tools, enabling reasoning over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On \textit{ProofWriter}, it improves entailment consistency by +7.2\% through logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it boosts abstraction accuracy by +6.0\% through integration of symbolic oracles. Applications in clinical decision support and scientific discovery show how the system encodes domain rules symbolically while leveraging LLMs for contextual inference and hypothesis generation. This architecture offers a robust, interpretable, and extensible solution for general-purpose neuro-symbolic reasoning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition</title>
<link>https://arxiv.org/abs/2508.05338</link>
<guid>https://arxiv.org/abs/2508.05338</guid>
<content:encoded><![CDATA[
<div> interpretations, challenges, framework, spectrum, recommendations 
Summary:<br /><br />The paper argues that the term 'agent' in artificial intelligence needs to be redefined due to the ambiguity created by recent advancements in AI, especially large language model systems. It proposes a framework that sets clear minimum requirements for a system to be considered an agent and characterizes systems along various dimensions such as environmental interaction, learning and adaptation, autonomy, goal complexity, and temporal coherence. The approach aims to provide precise vocabulary for system description while preserving the term's historical complexity. Potential counterarguments and implementation challenges are also discussed. The paper offers recommendations for moving forward in the field, including suggestions for terminology standardization and framework adoption. This proposed approach is meant to improve research clarity and reproducibility and support more effective policy development. 
Summary: <div>
arXiv:2508.05338v1 Announce Type: new 
Abstract: The term 'agent' in artificial intelligence has long carried multiple interpretations across different subfields. Recent developments in AI capabilities, particularly in large language model systems, have amplified this ambiguity, creating significant challenges in research communication, system evaluation and reproducibility, and policy development. This paper argues that the term 'agent' requires redefinition. Drawing from historical analysis and contemporary usage patterns, we propose a framework that defines clear minimum requirements for a system to be considered an agent while characterizing systems along a multidimensional spectrum of environmental interaction, learning and adaptation, autonomy, goal complexity, and temporal coherence. This approach provides precise vocabulary for system description while preserving the term's historically multifaceted nature. After examining potential counterarguments and implementation challenges, we provide specific recommendations for moving forward as a field, including suggestions for terminology standardization and framework adoption. The proposed approach offers practical tools for improving research clarity and reproducibility while supporting more effective policy development.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making</title>
<link>https://arxiv.org/abs/2508.05344</link>
<guid>https://arxiv.org/abs/2508.05344</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, collaborative law-making, trust, reciprocity, strategic language <br />
Summary: 
Recent advancements in large language models (LLMs) have expanded their capabilities to include complex reasoning tasks, such as legal interpretation, argumentation, and strategic interaction. The authors introduce NomicLaw, a multi-agent simulation where LLMs collaborate in law-making by proposing rules, justifying them, and voting on peer proposals in response to legal vignettes. Trust and reciprocity are quantitatively measured through voting patterns, while the use of strategic language is qualitatively assessed to understand how agents influence outcomes. Experiments with homogeneous and heterogeneous LLM groups show agents forming alliances, betraying trust, and adapting rhetoric to shape decisions. The study reveals the latent social reasoning and persuasive abilities of LLMs, offering insights for developing AI systems capable of autonomous negotiation, coordination, and legislation drafting in legal settings. <br /><br />Summary: <div>
arXiv:2508.05344v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have extended their capabilities from basic text processing to complex reasoning tasks, including legal interpretation, argumentation, and strategic interaction. However, empirical understanding of LLM behavior in open-ended, multi-agent settings especially those involving deliberation over legal and ethical dilemmas remains limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs engage in collaborative law-making, responding to complex legal vignettes by proposing rules, justifying them, and voting on peer proposals. We quantitatively measure trust and reciprocity via voting patterns and qualitatively assess how agents use strategic language to justify proposals and influence outcomes. Experiments involving homogeneous and heterogeneous LLM groups demonstrate how agents spontaneously form alliances, betray trust, and adapt their rhetoric to shape collective decisions. Our results highlight the latent social reasoning and persuasive capabilities of ten open-source LLMs and provide insights into the design of future AI systems capable of autonomous negotiation, coordination and drafting legislation in legal settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimal Model Reasoning in Description Logics: Don't Try This at Home!</title>
<link>https://arxiv.org/abs/2508.05350</link>
<guid>https://arxiv.org/abs/2508.05350</guid>
<content:encoded><![CDATA[
<div> Undecidability, Description Logics, Minimal Models, Concept Satisfiability, Acyclicity
Summary:
Undecidability of concept satisfiability in minimal models within Description Logics, specifically $\mathcal{EL}$, has been proven. This result extends to a limited fragment of tuple-generating dependencies. Imposing acyclicity conditions on the TBox reduces worst-case complexity to below double exponential time, connecting with pointwise circumscription. The article also addresses data complexity in these contexts. In the DL-Lite family, ExpSpace-hardness is established for DL-Lite$_{\text{horn}}$, building on a previously known positive result for DL-Lite$_{\text{core}}. The study provides insights into reasoning with minimal models in Description Logics and highlights the challenges and complexities involved. 

<br /><br />Summary: <div>
arXiv:2508.05350v1 Announce Type: new 
Abstract: Reasoning with minimal models has always been at the core of many knowledge representation techniques, but we still have only a limited understanding of this problem in Description Logics (DLs). Minimization of some selected predicates, letting the remaining predicates vary or be fixed, as proposed in circumscription, has been explored and exhibits high complexity. The case of `pure' minimal models, where the extension of all predicates must be minimal, has remained largely uncharted. We address this problem in popular DLs and obtain surprisingly negative results: concept satisfiability in minimal models is undecidable already for $\mathcal{EL}$. This undecidability also extends to a very restricted fragment of tuple-generating dependencies. To regain decidability, we impose acyclicity conditions on the TBox that bring the worst-case complexity below double exponential time and allow us to establish a connection with the recently studied pointwise circumscription; we also derive results in data complexity. We conclude with a brief excursion to the DL-Lite family, where a positive result was known for DL-Lite$_{\text{core}}$, but our investigation establishes ExpSpace-hardness already for its extension DL-Lite$_{\text{horn}}$.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models</title>
<link>https://arxiv.org/abs/2508.05383</link>
<guid>https://arxiv.org/abs/2508.05383</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, StructVRM, fine-grained feedback, multimodal benchmarks, STEM-Bench<br />
Summary:<br />
Existing Vision-Language Models often struggle with complex, multi-question reasoning tasks. Traditional reward mechanisms are too coarse for intricate problems. StructVRM aligns multimodal reasoning with Structured and Verifiable Reward Models, using a model-based verifier for fine-grained feedback. This allows for nuanced, partial credit scoring. The trained model Seed-StructVRM achieves state-of-the-art performance on public multimodal benchmarks and a high-difficulty STEM-Bench. The success of StructVRM demonstrates that training with structured, verifiable rewards is effective for advancing multimodal models in complex reasoning domains. <br /> <div>
arXiv:2508.05383v1 Announce Type: new 
Abstract: Existing Vision-Language Models often struggle with complex, multi-question reasoning tasks where partial correctness is crucial for effective learning. Traditional reward mechanisms, which provide a single binary score for an entire response, are too coarse to guide models through intricate problems with multiple sub-parts. To address this, we introduce StructVRM, a method that aligns multimodal reasoning with Structured and Verifiable Reward Models. At its core is a model-based verifier trained to provide fine-grained, sub-question-level feedback, assessing semantic and mathematical equivalence rather than relying on rigid string matching. This allows for nuanced, partial credit scoring in previously intractable problem formats. Extensive experiments demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM, achieves state-of-the-art performance on six out of twelve public multimodal benchmarks and our newly curated, high-difficulty STEM-Bench. The success of StructVRM validates that training with structured, verifiable rewards is a highly effective approach for advancing the capabilities of multimodal models in complex, real-world reasoning domains.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal</title>
<link>https://arxiv.org/abs/2508.05388</link>
<guid>https://arxiv.org/abs/2508.05388</guid>
<content:encoded><![CDATA[
<div> predictive maintenance, Intelligent Transportation Systems, Machine Learning models, fault prediction, explainability<br />
Summary:<br />
This work presents a real-time data-driven predictive maintenance solution for Intelligent Transportation Systems. It includes a processing pipeline with sample pre-processing, incremental classification using Machine Learning models, and outcome explanation. Key highlights are the dedicated sample pre-processing module and the inclusion of explainability in fault prediction. Experiments using MetroPT data from Porto, Portugal, achieved over 98% F-measure and 99% accuracy, crucial for accurate failure prediction in railway systems. The high F-measure ensures a balance between detecting faults and minimizing false alarms, improving service availability. The accuracy obtained enhances reliability, reducing costs and increasing safety. The pipeline maintains high performance even with class imbalance and noise, with explanations accurately reflecting decision-making processes. This methodology is validated for proactive maintenance decisions in real-world railway operations, enabling swift identification of failure signs and appropriate actions. <br /><br />Summary: <div>
arXiv:2508.05388v1 Announce Type: new 
Abstract: This work contributes to a real-time data-driven predictive maintenance solution for Intelligent Transportation Systems. The proposed method implements a processing pipeline comprised of sample pre-processing, incremental classification with Machine Learning models, and outcome explanation. This novel online processing pipeline has two main highlights: (i) a dedicated sample pre-processing module, which builds statistical and frequency-related features on the fly, and (ii) an explainability module. This work is the first to perform online fault prediction with natural language and visual explainability. The experiments were performed with the MetroPT data set from the metro operator of Porto, Portugal. The results are above 98 % for F-measure and 99 % for accuracy. In the context of railway predictive maintenance, achieving these high values is crucial due to the practical and operational implications of accurate failure prediction. In the specific case of a high F-measure, this ensures that the system maintains an optimal balance between detecting the highest possible number of real faults and minimizing false alarms, which is crucial for maximizing service availability. Furthermore, the accuracy obtained enables reliability, directly impacting cost reduction and increased safety. The analysis demonstrates that the pipeline maintains high performance even in the presence of class imbalance and noise, and its explanations effectively reflect the decision-making process. These findings validate the methodological soundness of the approach and confirm its practical applicability for supporting proactive maintenance decisions in real-world railway operations. Therefore, by identifying the early signs of failure, this pipeline enables decision-makers to understand the underlying problems and act accordingly swiftly.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning</title>
<link>https://arxiv.org/abs/2508.05405</link>
<guid>https://arxiv.org/abs/2508.05405</guid>
<content:encoded><![CDATA[
<div> physics principles, Vision Language Models, DeepPHY, physical reasoning, simulated environments <br />
<br />Summary: Vision Language Models (VLMs) excel in perception and visual reasoning but struggle with precise action planning. DeepPHY is introduced to assess VLMs' understanding of physics principles through challenging simulated environments. Tasks in real-world scenarios require complex interactions, spatial reasoning, long-term planning, and strategy refinement. DeepPHY evaluates VLMs' ability to apply physical knowledge accurately for predictive control. Despite advances in VLMs, they face difficulties in precise control based on descriptive physical understanding. <div>
arXiv:2508.05405v1 Announce Type: new 
Abstract: Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation</title>
<link>https://arxiv.org/abs/2508.05427</link>
<guid>https://arxiv.org/abs/2508.05427</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, organic synthesis, data-driven chemistry, artificial intelligence, automation

Summary: 
Large language models (LLMs) are revolutionizing organic synthesis by proposing synthetic routes, forecasting reaction outcomes, and guiding automated experiments. Integration with graph neural networks, quantum calculations, and real-time spectroscopy accelerates discovery processes and supports eco-friendly, data-informed chemistry. Challenges such as biased datasets, opaque reasoning, and safety concerns necessitate the development of safety mechanisms to prevent hazards. Community initiatives like open benchmarks, federated learning, and explainable interfaces aim to democratize access to LLMs while ensuring human oversight. These advancements pave the way for swift, dependable, and inclusive molecular innovation driven by artificial intelligence and automation.<br /><br />Summary: <div>
arXiv:2508.05427v1 Announce Type: new 
Abstract: Large language models (LLMs) are beginning to reshape how chemists plan and run reactions in organic synthesis. Trained on millions of reported transformations, these text-based models can propose synthetic routes, forecast reaction outcomes and even instruct robots that execute experiments without human supervision. Here we survey the milestones that turned LLMs from speculative tools into practical lab partners. We show how coupling LLMs with graph neural networks, quantum calculations and real-time spectroscopy shrinks discovery cycles and supports greener, data-driven chemistry. We discuss limitations, including biased datasets, opaque reasoning and the need for safety gates that prevent unintentional hazards. Finally, we outline community initiatives open benchmarks, federated learning and explainable interfaces that aim to democratize access while keeping humans firmly in control. These advances chart a path towards rapid, reliable and inclusive molecular innovation powered by artificial intelligence and automation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI</title>
<link>https://arxiv.org/abs/2508.05432</link>
<guid>https://arxiv.org/abs/2508.05432</guid>
<content:encoded><![CDATA[
<div> AI Alignment, Geographic Variability, Cultural Norms, Political Realities, Legislation
Summary: The article discusses the importance of considering geographic variability in AI alignment, as societal norms and goals can differ significantly across regions. It highlights how alignment measures may produce outcomes that deviate from statistical realities, such as gender ratios in images. The paper emphasizes the need for spatio-temporally aware alignment approaches in the era of Agentic AI, where AI systems mediate knowledge and opinions on a global scale. It raises concerns about the lack of transparency in how context is managed by AI systems and suggests future research topics and methods for assessing alignment sensitivity. <div>
arXiv:2508.05432v1 Announce Type: new 
Abstract: AI (super) alignment describes the challenge of ensuring (future) AI systems behave in accordance with societal norms and goals. While a quickly evolving literature is addressing biases and inequalities, the geographic variability of alignment remains underexplored. Simply put, what is considered appropriate, truthful, or legal can differ widely across regions due to cultural norms, political realities, and legislation. Alignment measures applied to AI/ML workflows can sometimes produce outcomes that diverge from statistical realities, such as text-to-image models depicting balanced gender ratios in company leadership despite existing imbalances. Crucially, some model outputs are globally acceptable, while others, e.g., questions about Kashmir, depend on knowing the user's location and their context. This geographic sensitivity is not new. For instance, Google Maps renders Kashmir's borders differently based on user location. What is new is the unprecedented scale and automation with which AI now mediates knowledge, expresses opinions, and represents geographic reality to millions of users worldwide, often with little transparency about how context is managed. As we approach Agentic AI, the need for spatio-temporally aware alignment, rather than one-size-fits-all approaches, is increasingly urgent. This paper reviews key geographic research problems, suggests topics for future work, and outlines methods for assessing alignment sensitivity.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?</title>
<link>https://arxiv.org/abs/2508.05464</link>
<guid>https://arxiv.org/abs/2508.05464</guid>
<content:encoded><![CDATA[
<div> Keywords: General Purpose AI, Evaluation, Benchmark-regulation gap, EU AI Act, Systemic risks

Summary:
The research addresses the need for robust evaluation frameworks in the rapidly advancing field of General Purpose AI (GPAI), particularly in light of emerging regulations like the EU AI Act. The study introduces the Bench-2-CoP framework, which evaluates how well widely-used benchmarks align with the capabilities and propensities outlined in the EU AI Act. The analysis reveals a significant gap in the evaluation ecosystem, with a focus on narrow behavioral propensities and a lack of coverage for critical functional capabilities related to loss-of-control scenarios. Key capabilities such as evading human oversight, self-replication, and autonomous AI development receive zero coverage in current benchmarks, leading to near-total evaluation gaps for systemic risks like Loss of Control and Cyber Offence. The findings offer valuable insights for policymakers to enhance the Code of Practice (CoP) and for developers to create more comprehensive evaluation tools for safer and compliant AI systems. 

<br /><br />Summary: <div>
arXiv:2508.05464v1 Announce Type: new 
Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust evaluation frameworks, especially with emerging regulations like the EU AI Act and its associated Code of Practice (CoP). Current AI evaluation practices depend heavily on established benchmarks, but these tools were not designed to measure the systemic risks that are the focus of the new regulatory landscape. This research addresses the urgent need to quantify this "benchmark-regulation gap." We introduce Bench-2-CoP, a novel, systematic framework that uses validated LLM-as-judge analysis to map the coverage of 194,955 questions from widely-used benchmarks against the EU AI Act's taxonomy of model capabilities and propensities. Our findings reveal a profound misalignment: the evaluation ecosystem is overwhelmingly focused on a narrow set of behavioral propensities, such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory bias" (28.9%), while critical functional capabilities are dangerously neglected. Crucially, capabilities central to loss-of-control scenarios, including evading human oversight, self-replication, and autonomous AI development, receive zero coverage in the entire benchmark corpus. This translates to a near-total evaluation gap for systemic risks like "Loss of Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study provides the first comprehensive, quantitative analysis of this gap, offering critical insights for policymakers to refine the CoP and for developers to build the next generation of evaluation tools, ultimately fostering safer and more compliant AI.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?</title>
<link>https://arxiv.org/abs/2508.05474</link>
<guid>https://arxiv.org/abs/2508.05474</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotion recognition, Conversations, Machine intelligence, Large Language Models, Datasets <br />
Summary: 
- Emotion recognition in conversations (ERC) is crucial for advancing machine intelligence.
- Existing ERC datasets are limited and face challenges such as bias and subjectivity in labels.
- A small, resource-efficient LLM is used to synthesize six novel ERC datasets to supplement existing benchmarks.
- The generated datasets enhance ERC classification and address label imbalance issues.
- Experimental results show that models trained on the generated datasets exhibit robustness and significant performance improvements on existing ERC benchmarks.<br /><br />Summary: <div>
arXiv:2508.05474v1 Announce Type: new 
Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion shifts within interactions, representing a significant step toward advancing machine intelligence. However, ERC data remains scarce, and existing datasets face numerous challenges due to their highly biased sources and the inherent subjectivity of soft labels. Even though Large Language Models (LLMs) have demonstrated their quality in many affective tasks, they are typically expensive to train, and their application to ERC tasks--particularly in data generation--remains limited. To address these challenges, we employ a small, resource-efficient, and general-purpose LLM to synthesize ERC datasets with diverse properties, supplementing the three most widely used ERC benchmarks. We generate six novel datasets, with two tailored to enhance each benchmark. We evaluate the utility of these datasets to (1) supplement existing datasets for ERC classification, and (2) analyze the effects of label imbalance in ERC. Our experimental results indicate that ERC classifier models trained on the generated datasets exhibit strong robustness and consistently achieve statistically significant performance improvements on existing ERC benchmarks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</title>
<link>https://arxiv.org/abs/2508.05496</link>
<guid>https://arxiv.org/abs/2508.05496</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, post-training, sample efficiency, data curation, reasoning tasks

Summary: 
InfiAlign introduces a post-training framework combining supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to align large language models for improved reasoning abilities. The framework utilizes a data selection pipeline to automatically curate high-quality alignment data from open-source reasoning datasets, resulting in significant performance gains while reducing data requirements. When applied to the Qwen2.5-Math-7B-Base model, the SFT model achieves comparable performance to DeepSeek-R1-Distill-Qwen-7B using only 12% of the training data. Integration of DPO further improves the model's performance, particularly in mathematical reasoning tasks, with an average improvement of 3.89% on AIME 24/25 benchmarks. This study demonstrates the efficacy of combining principled data selection with full-stage post-training to align large reasoning models in a scalable and data-efficient manner. The model checkpoints can be accessed at the provided link. 

<br /><br />Summary: <div>
arXiv:2508.05496v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</title>
<link>https://arxiv.org/abs/2508.05498</link>
<guid>https://arxiv.org/abs/2508.05498</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Knowledge Graphs, Reasoning, Interactive Learning

Summary: 
GRAIL is a framework designed to enhance retrieval-augmented reasoning by interacting with large-scale knowledge graphs. It integrates LLM-guided exploration with path filtering to generate fine-grained reasoning trajectories automatically. A two-stage training process helps learn a policy for optimal actions at each reasoning step. Process-supervised rewards aid in balancing precision and conciseness in graph retrieval, improving data efficiency and training stability. GRAIL adopts an interactive retrieval paradigm for autonomous exploration of graph paths with dynamic precision-breadth trade-offs. Experiment results demonstrate GRAIL's effectiveness, with significant accuracy and F1 score improvements on knowledge graph question-answering datasets. The source code and datasets are available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.05498v1 Announce Type: new 
Abstract: Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) techniques have exhibited remarkable performance across a wide range of domains. However, existing RAG approaches primarily operate on unstructured data and demonstrate limited capability in handling structured knowledge such as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally struggle to capture holistic graph structures while simultaneously facing precision control challenges that manifest as either critical information gaps or excessive redundant connections, collectively undermining reasoning performance. To address this challenge, we propose GRAIL: Graph-Retrieval Augmented Interactive Learning, a framework designed to interact with large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL integrates LLM-guided random exploration with path filtering to establish a data synthesis pipeline, where a fine-grained reasoning trajectory is automatically generated for each task. Based on the synthesized data, we then employ a two-stage training process to learn a policy that dynamically decides the optimal actions at each reasoning step. The overall objective of precision-conciseness balance in graph retrieval is decoupled into fine-grained process-supervised rewards to enhance data efficiency and training stability. In practical deployment, GRAIL adopts an interactive retrieval paradigm, enabling the model to autonomously explore graph paths while dynamically balancing retrieval breadth and precision. Extensive experiments have shown that GRAIL achieves an average accuracy improvement of 21.01% and F1 improvement of 22.43% on three knowledge graph question-answering datasets. Our source code and datasets is available at https://github.com/Changgeww/GRAIL.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</title>
<link>https://arxiv.org/abs/2508.05508</link>
<guid>https://arxiv.org/abs/2508.05508</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, agent, task completion, reasoning
Summary:
The article introduces a new generalizable and modular framework for evaluating agent task completion across various domains. This framework aims to mimic human-like evaluation by breaking tasks into sub-tasks and validating each step based on the agent's output and reasoning. Different modules contribute to the evaluation process, and their outputs are combined to provide a final assessment of task completion. The framework was tested on two benchmarks, GAIA and BigCodeBench, using the Magentic-One Actor Agent. Results showed that the proposed Judge Agent had higher alignment accuracy with human evaluations compared to the baseline LLM-as-a-Judge system. This study highlights the potential of the new evaluation framework in assessing agent performance in diverse domains. 
<br /><br />Summary: <div>
arXiv:2508.05508v1 Announce Type: new 
Abstract: The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making. Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates another's task completion, are typically designed for narrow, domain-specific settings. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agent's output and reasoning. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench. Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This demonstrates the potential of our proposed general-purpose evaluation framework.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program</title>
<link>https://arxiv.org/abs/2508.05513</link>
<guid>https://arxiv.org/abs/2508.05513</guid>
<content:encoded><![CDATA[
<div> Keywords: Letters of recommendation, AI-based detection tool, leadership skills, natural language processing, graduate admissions process 

Summary: 
LORI is introduced as an AI-based detection tool for assessing leadership skills in letters of recommendation (LORs) submitted by online master's program applicants. Using RoBERTa and LLAMA models, LORI identifies attributes such as teamwork, communication, and innovation in LORs. The latest RoBERTa model achieved high accuracy in detecting leadership skills. The integration of LORI into the graduate admissions process is essential for accurately evaluating applicants' capabilities, particularly in the STEM sector. This approach not only streamlines the admissions process but also ensures a comprehensive evaluation of candidates' leadership skills. LORI's automation can provide valuable insights for the admission committee to support students' professional growth. 

<br /><br />Summary: <div>
arXiv:2508.05513v1 Announce Type: new 
Abstract: Letters of recommendation (LORs) provide valuable insights into candidates' capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students' professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online master's program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants' leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates' capabilities.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</title>
<link>https://arxiv.org/abs/2508.05557</link>
<guid>https://arxiv.org/abs/2508.05557</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, multimodal environment, harmful content detection, multi-agent debate, online safety <br />
Summary: 
The article presents MV-Debate, a framework for detecting harmful content in the complex multimodal environment of social media. The framework utilizes four debate agents to analyze content from different perspectives: surface analysis, deep reasoning, modality contrast, and social context. Through iterative debate and reflection, the agents refine their responses to improve accuracy and efficiency in detecting harmful intent such as sarcasm, hate speech, or misinformation. Experiments on benchmark datasets show that MV-Debate outperforms single-model and existing multi-agent debate baselines. This work demonstrates the potential of multi-agent debate in enhancing the detection of harmful content in online contexts, contributing to the advancement of online safety. <br /><br />Summary: <div>
arXiv:2508.05557v1 Announce Type: new 
Abstract: Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Reward: Active Inference in the Era of Experience</title>
<link>https://arxiv.org/abs/2508.05619</link>
<guid>https://arxiv.org/abs/2508.05619</guid>
<content:encoded><![CDATA[
<div> Active Inference, autonomous AI agents, learning from experience, intrinsic drive, Large Language Models
<br />
Summary: This paper argues that Active Inference (AIF) can enable autonomous AI agents to learn from experience without the need for continuous human reward engineering. Current AI systems face scalability challenges as they rely on human-designed rewards. The proposed "Era of Experience" suggests agents learn from self-generated data, but still require human input for reward functions. The grounded-agency gap is identified as the inability of AI systems to autonomously formulate and pursue objectives. AIF can bridge this gap by using intrinsic drive to minimize free energy, allowing agents to balance exploration and exploitation. Integrating Large Language Models with AIF can create agents that efficiently learn from experience while aligned with human values. This synthesis offers a pathway to developing autonomous AI systems within computational and physical constraints. 
<br /> <div>
arXiv:2508.05619v1 Announce Type: new 
Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience,'' where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIF's principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</title>
<link>https://arxiv.org/abs/2508.05622</link>
<guid>https://arxiv.org/abs/2508.05622</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, human learning behavior, multi-agent framework, cognitive growth, simulation experiments

Summary:
Longitudinal analysis of the LearnerAgent framework reveals that only the Deep Learner achieves sustained cognitive growth, while trap questions effectively diagnose the Surface Learner's shallow knowledge. The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles, and learners' self-concept scores evolve realistically. Interestingly, the General Learner develops high self-efficacy despite cognitive limitations. The base LLM defaults to a "diligent but brittle Surface Learner" behavior, mimicking good student actions but lacking true understanding. Simulation experiments demonstrate that LearnerAgent provides insightful findings about LLM behavior in realistic scenarios. 

<br /><br />Summary: <div>
arXiv:2508.05622v1 Announce Type: new 
Abstract: Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLM's default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed "trap questions" effectively diagnose Surface Learner's shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners' self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a "diligent but brittle Surface Learner"-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs' behavior.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Generation of 4-Qubits Entangled States</title>
<link>https://arxiv.org/abs/2204.12351</link>
<guid>https://arxiv.org/abs/2204.12351</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Quantum Computing, Entangled States, Q-learning, Quantum Circuits  
Summary:  
- An artificial intelligence algorithm leveraging Q-learning has been developed to create entangled states with 4 qubits across various true SLOCC classes.  
- The algorithm can generate representative states for each of the nine entanglement families, aiding in experimental realization and understanding of universe properties.  
- The state-link graph (SLG) graphical tool is introduced to visualize the construction of the Q-matrix, highlighting connections between entanglement features and required quantum gates.  
- Quantum circuits produced by the algorithm are optimized with respect to the quantum gate-set chosen.  
- The SLGs simplify the algorithm and serve as a valuable resource for automating the construction of entangled states with a limited number of qubits.  

<br /><br />Summary: <div>
arXiv:2204.12351v2 Announce Type: cross 
Abstract: We have devised an artificial intelligence algorithm with machine reinforcement learning (Q-learning) to construct remarkable entangled states with 4 qubits. This way, the algorithm is able to generate representative states for some of the 49 true SLOCC classes of the four-qubit entanglement states. In particular, it is possible to reach at least one true SLOCC class for each of the nine entanglement families. The quantum circuits synthesized by the algorithm may be useful for the experimental realization of these important classes of entangled states and to draw conclusions about the intrinsic properties of our universe. We introduce a graphical tool called the state-link graph (SLG) to represent the construction of the Quality matrix (Q-matrix) used by the algorithm to build a given objective state belonging to the corresponding entanglement class. This allows us to discover the necessary connections between specific entanglement features and the role of certain quantum gates that the algorithm needs to include in the quantum gate set of actions. The quantum circuits found are optimal by construction with respect to the quantum gate-set chosen. These SLGs make the algorithm simple, intuitive and a useful resource for the automated construction of entangled states with a low number of qubits.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Use of LLMs for Documentation to Code Traceability</title>
<link>https://arxiv.org/abs/2506.16440</link>
<guid>https://arxiv.org/abs/2506.16440</guid>
<content:encoded><![CDATA[
<div> Language Models, Traceability, Documentation, Source Code, Evaluation

Summary:
Large Language Models (LLMs) are evaluated for establishing trace links between software documentation and source code. The study uses two open-source projects to assess trace link identification accuracy, relationship explanation quality, and multi-step chain reconstruction. The best-performing LLM achieves high F1-scores, outperforming baselines like TF-IDF and CodeBERT. While correct relationship explanations vary, partial accuracy is high, indicating fundamental connections are rarely missed. LLMs display high endpoint accuracy in multi-step chains but struggle with precise intermediate links. Error analysis reveals false positives related to naming assumptions and architectural patterns. Task-framing strategies are crucial for performance. The study suggests LLMs are valuable for trace discovery but may require human-in-the-loop tool design to address limitations and specific error patterns for further research. 

<br /><br />Summary: <div>
arXiv:2506.16440v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer new potential for automating documentation-to-code traceability, yet their capabilities remain underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) in establishing trace links between various software documentation (including API references and user guides) and source code. We create two novel datasets from two open-source projects (Unity Catalog and Crawl4AI). Through systematic experiments, we assess three key capabilities: (1) trace link identification accuracy, (2) relationship explanation quality, and (3) multi-step chain reconstruction. Results show that the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two datasets, substantially outperforming our baselines (TF-IDF, BM25, and CodeBERT). While fully correct relationship explanations range from 42.9% to 71.1%, partial accuracy exceeds 97%, indicating that fundamental connections are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy but vary in capturing precise intermediate links. Error analysis reveals that many false positives stem from naming-based assumptions, phantom links, or overgeneralization of architectural patterns. We demonstrate that task-framing, such as a one-to-many matching strategy, is critical for performance. These findings position LLMs as powerful assistants for trace discovery, but their limitations could necessitate human-in-the-loop tool design and highlight specific error patterns for future research.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow</title>
<link>https://arxiv.org/abs/2507.10818</link>
<guid>https://arxiv.org/abs/2507.10818</guid>
<content:encoded><![CDATA[
<div> Keywords: Software libraries, Large Language Models, Python, Usability, Dependencies

Summary:
Software libraries play a crucial role in modern code development, influencing functionality and maintainability. Large Language Models (LLMs) are increasingly used to recommend libraries for programming tasks. An empirical study examined six state-of-the-art LLMs to solve real-world Python problems from Stack Overflow. Results showed that LLMs prefer third-party libraries over standard ones, often recommending mature, popular, and permissively licensed dependencies. However, there were usability gaps identified: a percentage of libraries could not be resolved automatically due to structural mismatches, and only two models provided installation guidance. The generated code was technically valid, but lacked contextual support, leaving users to manually resolve dependencies. The findings provide insights for developers and researchers, highlighting the need to enhance the reliability and usability of LLM-generated code, particularly in the context of software dependencies. 

<br /><br />Summary: <div>
arXiv:2507.10818v1 Announce Type: cross 
Abstract: Software libraries are central to the functionality, security, and maintainability of modern code. As developers increasingly turn to Large Language Models (LLMs) to assist with programming tasks, understanding how these models recommend libraries is essential. In this paper, we conduct an empirical study of six state-of-the-art LLMs, both proprietary and open-source, by prompting them to solve real-world Python problems sourced from Stack Overflow. We analyze the types of libraries they import, the characteristics of those libraries, and the extent to which the recommendations are usable out of the box. Our results show that LLMs predominantly favour third-party libraries over standard ones, and often recommend mature, popular, and permissively licensed dependencies. However, we also identify gaps in usability: 4.6% of the libraries could not be resolved automatically due to structural mismatches between import names and installable packages, and only two models (out of six) provided installation guidance. While the generated code is technically valid, the lack of contextual support places the burden of manually resolving dependencies on the user. Our findings offer actionable insights for both developers and researchers, and highlight opportunities to improve the reliability and usability of LLM-generated code in the context of software dependencies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis</title>
<link>https://arxiv.org/abs/2507.16641</link>
<guid>https://arxiv.org/abs/2507.16641</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, quantum circuits, tabular Q-learning, quantum state space, circuit optimization<br />
<br />
Summary: 
This article introduces a reinforcement learning framework for efficiently synthesizing quantum circuits that produce target quantum states from an initial state. The framework utilizes tabular Q-learning in a discretized quantum state space to manage dimensionality effectively. A hybrid reward mechanism guides the agent towards the target state while avoiding inefficient circuit structures. Through sparse matrix representations and state-space discretization, the method allows scalable navigation in high-dimensional environments with minimal computational overhead. Benchmarking on graph-state preparation tasks shows that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Even when extended to a universal gate set for arbitrary quantum states, it still produces minimal-depth circuits, demonstrating its robustness and adaptability. Overall, this RL-driven approach efficiently explores the quantum state space and synthesizes near-optimal quantum circuits, providing a foundation for resource-efficient quantum circuit optimization. 
<br /> <div>
arXiv:2507.16641v1 Announce Type: cross 
Abstract: A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the NISQ era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension. The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. By leveraging sparse matrix representations and state-space discretization, the method enables scalable navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set for arbitrary quantum states, it still produces minimal depth circuits, highlighting the algorithm's robustness and adaptability. The results confirm that this RL-driven approach efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Should Be More Human, Not More Complex</title>
<link>https://arxiv.org/abs/2508.04713</link>
<guid>https://arxiv.org/abs/2508.04713</guid>
<content:encoded><![CDATA[
<div> Large Language Models, AI-powered search systems, user preference, concise responses, source-attributed

Summary: Our study of AI-powered search systems shows that users prefer concise, source-attributed responses over verbose explanations. Current AI trends towards artificial sophistication lead to reduced user trust and increased cognitive load. Optimal AI communication should mirror effective human discourse by being direct, properly sourced, and transparent about limitations. The prevailing assumption that complex AI responses indicate better performance is challenged, with human-like brevity and transparency being key to user engagement and system reliability. <div>
arXiv:2508.04713v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) in search applications increasingly prioritize verbose, lexically complex responses that paradoxically reduce user satisfaction and engagement. Through a comprehensive study of 10.000 (est.) participants comparing responses from five major AI-powered search systems, we demonstrate that users overwhelmingly prefer concise, source-attributed responses over elaborate explanations. Our analysis reveals that current AI development trends toward "artificial sophistication" create an uncanny valley effect where systems sound knowledgeable but lack genuine critical thinking, leading to reduced trust and increased cognitive load. We present evidence that optimal AI communication mirrors effective human discourse: direct, properly sourced, and honest about limitations. Our findings challenge the prevailing assumption that more complex AI responses indicate better performance, instead suggesting that human-like brevity and transparency are key to user engagement and system reliability.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS</title>
<link>https://arxiv.org/abs/2508.04721</link>
<guid>https://arxiv.org/abs/2508.04721</guid>
<content:encoded><![CDATA[
<div> quantized LLM, Embedding Model, ASR model, TTS model, low-latency telecom AI voice agent pipeline <br />
Summary: 
The article introduces a low-latency telecom AI voice agent pipeline designed for real-time, interactive telecommunications use. It combines four specialized models by NetoAI: a quantized LLM, an Embedding Model, an ASR model, and a TTS model specifically tailored for the telecom industry. This pipeline enables advanced voice AI for call center automation, intelligent IVR, and AI-driven customer support, supporting knowledge-grounded spoken interactions with low latency. A dataset of 500 human-recorded telecom questions was used to evaluate the system, showing that the models deliver real-time factors below 1.0. This sets a new benchmark for telecom voice assistants and lays the foundation for next-generation telecom AI applications, such as automated customer support and diagnostics. The integration of streaming ASR, conversational intelligence, retrieval augmented generation over telecom documents, and real-time TTS showcases the capabilities of the system in enhancing telecom services. <br /><br />Summary: <div>
arXiv:2508.04721v1 Announce Type: cross 
Abstract: We introduce a low-latency telecom AI voice agent pipeline for real-time, interactive telecommunications use, enabling advanced voice AI for call center automation, intelligent IVR (Interactive Voice Response), and AI-driven customer support. The solution is built for telecom, combining four specialized models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific Text-to-Speech (TTS) model. These models enable highly responsive, domain-adapted voice AI agents supporting knowledge-grounded spoken interactions with low latency. The pipeline integrates streaming ASR (TTE), conversational intelligence (TSLAM), retrieval augmented generation (RAG) over telecom documents, and real-time TTS (T-Synth), setting a new benchmark for telecom voice assistants. To evaluate the system, we built a dataset of 500 human-recorded telecom questions from RFCs, simulating real telecom agent queries. This framework allows analysis of latency, domain relevance, and real-time performance across the stack. Results show that TSLAM, TTE, and T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise, low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and T-Synth -- provide a foundation for next-generation telecom AI, enabling automated customer support, diagnostics, and more.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion</title>
<link>https://arxiv.org/abs/2508.04723</link>
<guid>https://arxiv.org/abs/2508.04723</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion analysis, music stimuli, EEG, fNIRS, portable device

Summary: 
- The study addresses three key limitations in music-based affective computing: stimulus constraints, modality specificity, and portability limitations.
- The proposed MEEtBrain framework utilizes AI-generated music stimuli to eliminate selection biases and ensure music diversity.
- The framework integrates EEG and fNIRS data through a portable device with a wireless headband design, allowing for simultaneous data collection.
- A dataset collected from 20 participants validates the efficacy of the framework in eliciting target emotions (valence/arousal) using AI-generated music stimuli.
- The dataset, which is continuously expanding with 44 participants in the latest iteration, is publicly available for research and practical applications. 

<br /><br />Summary: <div>
arXiv:2508.04723v1 Announce Type: cross 
Abstract: Emotions critically influence mental health, driving interest in music-based affective computing via neurophysiological signals with Brain-computer Interface techniques. While prior studies leverage music's accessibility for emotion induction, three key limitations persist: \textbf{(1) Stimulus Constraints}: Music stimuli are confined to small corpora due to copyright and curation costs, with selection biases from heuristic emotion-music mappings that ignore individual affective profiles. \textbf{(2) Modality Specificity}: Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability due to procedural complexity and portability barriers. To address these limitations, we propose MEEtBrain, a portable and multimodal framework for emotion analysis (valence/arousal), integrating AI-generated music stimuli with synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the music stimuli can be automatically generated by AI on a large scale, eliminating subjective selection biases while ensuring music diversity. We use our developed portable device that is designed in a lightweight headband-style and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A 14-hour dataset from 20 participants was collected in the first recruitment to validate the framework's efficacy, with AI-generated music eliciting target emotions (valence/arousal). We are actively expanding our multimodal dataset (44 participants in the latest dataset) and make it publicly available to promote further research and practical applications. \textbf{The dataset is available at https://zju-bmi-lab.github.io/ZBra.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agency, Affordances, and Enculturation of Augmentation Technologies</title>
<link>https://arxiv.org/abs/2508.04725</link>
<guid>https://arxiv.org/abs/2508.04725</guid>
<content:encoded><![CDATA[
<div> Keywords: augmentation technologies, artificial intelligence, agents, enculturation, Metaverse <br />
<br />
Summary: 
The chapter delves into the impact of augmentation technologies in the age of artificial intelligence, focusing on the ambiguity of AI terminology and the categorization scheme proposed by WIPO. It explores the relationship between humans and robotics, highlighting the development of non-human agents in industry. The enculturation process through marketing communication to adopt and adapt to technology is discussed, emphasizing the ways people are drawn into commercial digital landscapes like the Metaverse. The chapter concludes by examining the claims surrounding the Metaverse and augmented reality, showcasing how these technologies are reshaping various aspects of society and offering potential benefits but also raising critical questions about their impact. <div>
arXiv:2508.04725v1 Announce Type: cross 
Abstract: Augmentation technologies are undergoing a process of enculturation due to many factors, one being the rise of artificial intelligence (AI), or what the World Intellectual Property Organization (WIPO) terms the AI wave or AI boom. Chapter 3 focuses critical attention on the hyped assumption that sophisticated, emergent, and embodied augmentation technologies will improve lives, literacy, cultures, arts, economies, and social contexts. The chapter begins by discussing the problem of ambiguity with AI terminology, which it aids with a description of the WIPO Categorization of AI Technologies Scheme. It then draws on media and communication studies to explore concepts such as agents, agency, power, and agentive relationships between humans and robots. The chapter focuses on the development of non-human agents in industry as a critical factor in the rise of augmentation technologies. It looks at how marketing communication enculturates future users to adopt and adapt to the technology. Scholars are charting the significant ways that people are drawn further into commercial digital landscapes, such as the Metaverse concept, in post-internet society. It concludes by examining recent claims concerning the Metaverse and augmented reality.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Image Synthesis: Generating H&amp;E from Multiplex Biomarker Imaging</title>
<link>https://arxiv.org/abs/2508.04734</link>
<guid>https://arxiv.org/abs/2508.04734</guid>
<content:encoded><![CDATA[
<div> Virtual stains, multiplex immunofluorescence, H&amp;E, VQGAN, computer-aided diagnosis <br />
<br />Summary: 
This study explores the use of a multi-level Vector-Quantized Generative Adversarial Network (VQGAN) to generate virtual H&amp;E stains from multiplex immunofluorescence (mIF) images. The virtual stains provide important morphological context to the molecular data, bridging the gap between molecular and morphological analysis. Comparing the VQGAN with a standard conditional GAN (cGAN) on colorectal cancer datasets, it is found that the VQGAN produces higher-fidelity virtual stains that are more useful for computer-aided diagnosis. The VQGAN-generated stains exhibit superior performance in downstream tasks such as nuclei segmentation and tissue classification, showing better agreement with ground-truth analysis. This research establishes the VQGAN as a robust and effective architecture for generating scientifically valuable virtual stains, enabling the integration of rich molecular data from mIF into established H&amp;E-based analytical workflows. <div>
arXiv:2508.04734v1 Announce Type: cross 
Abstract: While multiplex immunofluorescence (mIF) imaging provides deep, spatially-resolved molecular data, integrating this information with the morphological standard of Hematoxylin & Eosin (H&amp;E) can be very important for obtaining complementary information about the underlying tissue. Generating a virtual H&amp;E stain from mIF data offers a powerful solution, providing immediate morphological context. Crucially, this approach enables the application of the vast ecosystem of H&amp;E-based computer-aided diagnosis (CAD) tools to analyze rich molecular data, bridging the gap between molecular and morphological analysis. In this work, we investigate the use of a multi-level Vector-Quantized Generative Adversarial Network (VQGAN) to create high-fidelity virtual H&amp;E stains from mIF images. We rigorously evaluated our VQGAN against a standard conditional GAN (cGAN) baseline on two publicly available colorectal cancer datasets, assessing performance on both image similarity and functional utility for downstream analysis. Our results show that while both architectures produce visually plausible images, the virtual stains generated by our VQGAN provide a more effective substrate for computer-aided diagnosis. Specifically, downstream nuclei segmentation and semantic preservation in tissue classification tasks performed on VQGAN-generated images demonstrate superior performance and agreement with ground-truth analysis compared to those from the cGAN. This work establishes that a multi-level VQGAN is a robust and superior architecture for generating scientifically useful virtual stains, offering a viable pathway to integrate the rich molecular data of mIF into established and powerful H&amp;E-based analytical workflows.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERDES: A Benchmark Video Dataset for Retinal Detachment and Macular Status Classification in Ocular Ultrasound</title>
<link>https://arxiv.org/abs/2508.04735</link>
<guid>https://arxiv.org/abs/2508.04735</guid>
<content:encoded><![CDATA[
<div> Keywords: Retinal detachment, Macula-intact, Macula-detached, Point-of-care ultrasound, Deep learning

Summary:
The article discusses the importance of timely intervention in cases of retinal detachment (RD) which can lead to vision loss. Macular involvement, whether intact or detached, greatly impacts visual outcomes. Point-of-care ultrasound (POCUS) is a valuable tool for detecting RD but limited by the expertise required for image interpretation. The lack of machine learning algorithms for RD diagnosis, especially in detecting macular status, is an area of concern. The introduction of the ERDES dataset, the first open-access dataset of ocular ultrasound clips labeled for RD presence and macular status, aims to address this gap and facilitate the development of ML models for RD detection. Baseline benchmarks using CNN architectures are provided, and all data and code are openly available for research purposes. This dataset and the potential for deep learning in ultrasound image analysis have the potential to improve the accuracy and efficiency of diagnosing RD. 

<br /><br />Summary: 
1. Timely intervention is crucial for preserving vision in cases of retinal detachment (RD).
2. Macular involvement determines visual outcomes, emphasizing the importance of accurate diagnosis.
3. Point-of-care ultrasound (POCUS) is a valuable tool for detecting RD but requires expertise for interpretation.
4. Machine learning algorithms for RD diagnosis, particularly in macular status assessment, are lacking.
5. The ERDES dataset, open-access and labeled for RD presence and macula status, aims to address this gap.
6. Baseline benchmarks using CNN architectures are provided for research and development purposes.
7. Deep learning in ultrasound image analysis shows promise for improving RD diagnosis accuracy and efficiency. <div>
arXiv:2508.04735v1 Announce Type: cross 
Abstract: Retinal detachment (RD) is a vision-threatening condition that requires timely intervention to preserve vision. Macular involvement -- whether the macula is still intact (macula-intact) or detached (macula-detached) -- is the key determinant of visual outcomes and treatment urgency. Point-of-care ultrasound (POCUS) offers a fast, non-invasive, cost-effective, and accessible imaging modality widely used in diverse clinical settings to detect RD. However, ultrasound image interpretation is limited by a lack of expertise among healthcare providers, especially in resource-limited settings. Deep learning offers the potential to automate ultrasound-based assessment of RD. However, there are no ML ultrasound algorithms currently available for clinical use to detect RD and no prior research has been done on assessing macular status using ultrasound in RD cases -- an essential distinction for surgical prioritization. Moreover, no public dataset currently supports macular-based RD classification using ultrasound video clips. We introduce Eye Retinal DEtachment ultraSound, ERDES, the first open-access dataset of ocular ultrasound clips labeled for (i) presence of retinal detachment and (ii) macula-intact versus macula-detached status. The dataset is intended to facilitate the development and evaluation of machine learning models for detecting retinal detachment. We also provide baseline benchmarks using multiple spatiotemporal convolutional neural network (CNN) architectures. All clips, labels, and training code are publicly available at https://osupcvlab.github.io/ERDES/.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration</title>
<link>https://arxiv.org/abs/2508.04780</link>
<guid>https://arxiv.org/abs/2508.04780</guid>
<content:encoded><![CDATA[
<div> prediction, repair duration, equity, power restoration, reinforcement learning<br />
Summary:<br />
The article addresses the need for an equitable and efficient power system restoration strategy in the face of extreme weather events. It highlights the disparity in power restoration request submissions from disadvantaged communities, leading to vulnerability and extended outages. The proposed solution, EPOPR, incorporates Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction and Spatial-Temporal Attentional RL for equitable decision-making. By balancing restoration efficiency and equity, EPOPR reduces average outage duration by 3.60% and decreases inequity between communities by 14.19% compared to existing methods. Its innovative approach overcomes challenges in repair duration prediction under heteroscedasticity and addresses the tendency of reinforcement learning agents to favor low-uncertainty actions that may undermine equity. EPOPR's data-driven analysis and novel framework offer a promising solution for more equitable and efficient power system restoration in the face of increasing extreme weather events. <br /><br /> <div>
arXiv:2508.04780v1 Announce Type: cross 
Abstract: The increasing frequency of extreme weather events, such as hurricanes, highlights the urgent need for efficient and equitable power system restoration. Many electricity providers make restoration decisions primarily based on the volume of power restoration requests from each region. However, our data-driven analysis reveals significant disparities in request submission volume, as disadvantaged communities tend to submit fewer restoration requests. This disparity makes the current restoration solution inequitable, leaving these communities vulnerable to extended power outages. To address this, we aim to propose an equity-aware power restoration strategy that balances both restoration efficiency and equity across communities. However, achieving this goal is challenging for two reasons: the difficulty of predicting repair durations under dataset heteroscedasticity, and the tendency of reinforcement learning agents to favor low-uncertainty actions, which potentially undermine equity. To overcome these challenges, we design a predict-then-optimize framework called EPOPR with two key components: (1) Equity-Conformalized Quantile Regression for uncertainty-aware repair duration prediction, and (2) Spatial-Temporal Attentional RL that adapts to varying uncertainty levels across regions for equitable decision-making. Experimental results show that our EPOPR effectively reduces the average power outage duration by 3.60% and decreases inequity between different communities by 14.19% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts</title>
<link>https://arxiv.org/abs/2508.04787</link>
<guid>https://arxiv.org/abs/2508.04787</guid>
<content:encoded><![CDATA[
<div> reflection, prompts, interactive, AI-generated podcast, learning outcomes

Summary:
- The study examined the impact of embedding LLM-guided reflection prompts in an interactive AI-generated podcast on learning and user experience.
- Thirty-six undergraduates participated in the study, which found that while learning outcomes were similar across conditions, reflection prompts reduced perceived attractiveness of the podcast.
- This highlights the need for further research on reflective interactivity design.
<br /><br />Summary: <div>
arXiv:2508.04787v1 Announce Type: cross 
Abstract: This study examined whether embedding LLM-guided reflection prompts in an interactive AI-generated podcast improved learning and user experience compared to a version without prompts. Thirty-six undergraduates participated, and while learning outcomes were similar across conditions, reflection prompts reduced perceived attractiveness, highlighting a call for more research on reflective interactivity design.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM</title>
<link>https://arxiv.org/abs/2508.04795</link>
<guid>https://arxiv.org/abs/2508.04795</guid>
<content:encoded><![CDATA[
<div> Keywords: dialogue transcription, Large Language Models, speaker characteristics, metadata tags, speaker profiling <br />
Summary: 
In this study, the researchers propose a post-processing step in dialogue transcription pipelines that enriches transcribed dialogues by adding metadata tags for speaker characteristics such as age, gender, and emotion. They utilize frozen audio foundation models and a frozen LLAMA language model to infer these speaker attributes without the need for task-specific fine-tuning. By employing lightweight connectors to bridge audio and language representations, they achieve competitive performance on speaker profiling tasks while maintaining modularity and speed. Furthermore, they demonstrate that a frozen LLAMA model can compare x-vectors directly, showing promising results with an Equal Error Rate of 8.8% in certain scenarios. This approach could enhance the quality and depth of information available in transcribed dialogues, offering potential benefits for various applications in speech processing and analysis. <br /><br />Summary: <div>
arXiv:2508.04795v1 Announce Type: cross 
Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are frequently employed in post-processing to improve grammar, punctuation, and readability. We explore a complementary post-processing step: enriching transcribed dialogues by adding metadata tags for speaker characteristics such as age, gender, and emotion. Some of the tags are global to the entire dialogue, while some are time-variant. Our approach couples frozen audio foundation models, such as Whisper or WavLM, with a frozen LLAMA language model to infer these speaker attributes, without requiring task-specific fine-tuning of either model. Using lightweight, efficient connectors to bridge audio and language representations, we achieve competitive performance on speaker profiling tasks while preserving modularity and speed. Additionally, we demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving an Equal Error Rate of 8.8% in some scenarios.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization</title>
<link>https://arxiv.org/abs/2508.04796</link>
<guid>https://arxiv.org/abs/2508.04796</guid>
<content:encoded><![CDATA[
<div> frequency-based objectives, tokenization, NLP pipelines, Parity-aware Byte Pair Encoding, lower-resource languages
Summary:<br /><br />Tokenization is a crucial step in natural language processing pipelines, but standard algorithms based on frequency objectives often disadvantage lower-resource languages with inefficient tokenizations. To address this issue, Parity-aware Byte Pair Encoding (BPE) is introduced, which focuses on maximizing compression gain for languages with the worst compression at each merge step. This approach aims to achieve cross-lingual parity in token counts while maintaining global compression rates and language-model performance in downstream tasks. Empirical results show that Parity-aware BPE leads to more equitable token counts across languages without significantly impacting overall compression rates or language-model performance. Overall, this approach aims to reduce the computational and financial inequalities between users from different language backgrounds in NLP applications.<br /><br />Summary: <div>
arXiv:2508.04796v1 Announce Type: cross 
Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with  placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds. To remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity. We find empirically that Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control</title>
<link>https://arxiv.org/abs/2508.04799</link>
<guid>https://arxiv.org/abs/2508.04799</guid>
<content:encoded><![CDATA[
<div> Topology, Process control, Machine learning, Neural networks, Optimization 
Summary: 
This article presents a framework for integrating data-driven algorithms with classical process models in process control. The framework focuses on representing interconnections among process network units, deriving a system's objective function based on non-equilibrium entropy production, and implementing distributed control and optimization in process network structures. The approach relies on the use of conic sector conditions for flow expressions and enables the integration of conservation properties from topology with dynamic relations learned from data through neural networks. A practical example of integrating process topology with a neural network ordinary differential equation model is demonstrated, where the neural network learns constitutive equations using synthetic time-series data. The resulting neural network can be used as a state space model for applications such as model predictive control algorithms. <div>
arXiv:2508.04799v1 Announce Type: cross 
Abstract: Most recent advances in machine learning and analytics for process control pose the question of how to naturally integrate new data-driven methods with classical process models and control. We propose a process modeling framework enabling integration of data-driven algorithms through consistent topological properties and conservation of extensive quantities. Interconnections among process network units are represented through connectivity matrices and network graphs. We derive the system's natural objective function equivalent to the non-equilibrium entropy production in a steady state system as a driving force for the process dynamics. We illustrate how distributed control and optimization can be implemented into process network structures and how control laws and algorithms alter the system's natural equilibrium towards engineered objectives. The basic requirement is that the flow conditions can be expressed in terms of conic sector (passivity) conditions. Our formalism allows integration of fundamental conservation properties from topology with learned dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system how to integrate the basic topology of a process with a neural network ordinary differential equation model. The system specific constitutive equations are left undescribed and learned by the neural ordinary differential equation algorithm using the adjoint method in combination with an adaptive ODE solver from synthetic time-series data. The resulting neural network forms a state space model for use in e.g. a model predictive control algorithm.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework</title>
<link>https://arxiv.org/abs/2508.04816</link>
<guid>https://arxiv.org/abs/2508.04816</guid>
<content:encoded><![CDATA[
<div> distillation, self-supervised learning, Vision Transformers, masked modeling, consensus-oriented<br />
<br />
Summary: <br />
A new framework called Consensus-oriented Masked Distillation (CoMAD) is introduced to unify knowledge from multiple self-supervised Vision Transformers. The framework distills insights from three pretrained teachers into a compact student network using asymmetric masking and consensus gating. The student is trained with dual-level KL divergence on visible tokens and reconstructed feature maps. CoMAD's ViT-Tiny achieves a state-of-the-art performance on ImageNet-1K with 75.4 percent Top-1 accuracy. In dense-prediction tasks, it achieves 47.3 percent mIoU on ADE20K and sets new records in box average precision and mask average precision on MS-COCO. The proposed framework addresses the challenges of large model sizes and isolated pretraining methods in self-supervised learning, providing a lightweight and effective solution for distillation. <br /> <div>
arXiv:2508.04816v1 Announce Type: cross 
Abstract: Numerous self-supervised learning paradigms, such as contrastive learning and masked image modeling, learn powerful representations from unlabeled data but are typically pretrained in isolation, overlooking complementary insights and yielding large models that are impractical for resource-constrained deployment. To overcome these challenges, we introduce Consensus-oriented Masked Distillation (CoMAD), a lightweight, parameter-free framework that unifies knowledge from multiple current state-of-the-art self-supervised Vision Transformers into a compact student network. CoMAD distills from three pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct semantic and contextual priors. Rather than naively averaging teacher outputs, we apply asymmetric masking: the student sees only 25 percent of patches while each teacher receives a progressively lighter, unique mask, forcing the student to interpolate missing features under richer contexts. Teacher embeddings are aligned to the student's space via a linear adapter and layer normalization, then fused through our joint consensus gating, which weights each token by combining cosine affinity with inter-teacher agreement. The student is trained with dual-level KL divergence on visible tokens and reconstructed feature maps, capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average precision on MS-COCO, establishing a new state-of-the-art in compact SSL distillation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini</title>
<link>https://arxiv.org/abs/2508.04820</link>
<guid>https://arxiv.org/abs/2508.04820</guid>
<content:encoded><![CDATA[
<div> Keywords: logging, large language models, file-level generation, machine learning, code quality

Summary:
Large language models (LLMs) are being explored for generating log statements in software development, particularly in machine learning (ML) projects. A study evaluated GPT-4o mini's ability to generate log statements at the file level in ML repositories. The LLM placed logs in a similar position as humans in around 64% of cases but had a high overlogging rate of 82.66%. Challenges identified included overlogging at the beginning or end of functions, difficulty logging within large code blocks, and misalignment with project-specific logging conventions. While the LLM shows potential for generating logs for complete files, addressing these limitations is necessary for practical implementation.<br /><br />Summary: <div>
arXiv:2508.04820v1 Announce Type: cross 
Abstract: Logging is essential in software development, helping developers monitor system behavior and aiding in debugging applications. Given the ability of large language models (LLMs) to generate natural language and code, researchers are exploring their potential to generate log statements. However, prior work focuses on evaluating logs introduced in code functions, leaving file-level log generation underexplored -- especially in machine learning (ML) applications, where comprehensive logging can enhance reliability. In this study, we evaluate the capacity of GPT-4o mini as a case study to generate log statements for ML projects at file level. We gathered a set of 171 ML repositories containing 4,073 Python files with at least one log statement. We identified and removed the original logs from the files, prompted the LLM to generate logs for them, and evaluated both the position of the logs and log level, variables, and text quality of the generated logs compared to human-written logs. In addition, we manually analyzed a representative sample of generated logs to identify common patterns and challenges. We find that the LLM introduces logs in the same place as humans in 63.91% of cases, but at the cost of a high overlogging rate of 82.66%. Furthermore, our manual analysis reveals challenges for file-level logging, which shows overlogging at the beginning or end of a function, difficulty logging within large code blocks, and misalignment with project-specific logging conventions. While the LLM shows promise for generating logs for complete files, these limitations remain to be addressed for practical implementation.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
arXiv:2508.04825v1 Announce Type: cross 
Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History</title>
<link>https://arxiv.org/abs/2508.04826</link>
<guid>https://arxiv.org/abs/2508.04826</guid>
<content:encoded><![CDATA[
arXiv:2508.04826v1 Announce Type: cross 
Abstract: Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, we systematically vary question order, paraphrasing, personas, and reasoning modes. Our findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD > 0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2508.04845</link>
<guid>https://arxiv.org/abs/2508.04845</guid>
<content:encoded><![CDATA[
arXiv:2508.04845v1 Announce Type: cross 
Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle communication but remains susceptible to cyber-attacks due to its lack of built-in security. This paper presents a multi-stage intrusion detection framework leveraging unsupervised anomaly detection and supervised graph learning tailored for automotive CAN traffic. Our architecture combines a Variational Graph Autoencoder (VGAE) for structural anomaly detection with a Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack classification. CAN bus activity is encoded as graph sequences to model temporal and relational dependencies. The pipeline applies VGAE-based selective undersampling to address class imbalance, followed by GAT classification with optional score-level fusion. The compact student GAT achieves 96% parameter reduction compared to the teacher model while maintaining strong predictive performance. Experiments on six public CAN intrusion datasets--Car-Hacking, Car-Survival, and can-train-and-test--demonstrate competitive accuracy and efficiency, with average improvements of 16.2% in F1-score over existing methods, particularly excelling on highly imbalanced datasets with up to 55% F1-score improvements.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos</title>
<link>https://arxiv.org/abs/2508.04853</link>
<guid>https://arxiv.org/abs/2508.04853</guid>
<content:encoded><![CDATA[
arXiv:2508.04853v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the memory and compute costs of modern deep neural networks, including large language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as GPTQ-has emerged as a leading method due to its computational efficiency and strong empirical performance. Despite its widespread adoption, however, OPTQ lacks rigorous quantitative theoretical guarantees. This paper presents the first quantitative error bounds for both deterministic and stochastic variants of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ algorithm. We analyze how OPTQ's iterative procedure induces quantization error and derive non-asymptotic 2-norm error bounds that depend explicitly on the calibration data and a regularization parameter that OPTQ uses. Our analysis provides theoretical justification for several practical design choices, including the widely used heuristic of ordering features by decreasing norm, as well as guidance for selecting the regularization parameter. For the stochastic variant, we establish stronger infinity-norm error bounds, which enable control over the required quantization alphabet and are particularly useful for downstream layers and nonlinearities. Finally, we extend our analysis to Qronos, providing new theoretical bounds, for both its deterministic and stochastic variants, that help explain its empirical advantages.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence Aware SAC Control for Engine Fuel Consumption Optimization in Electrified Powertrain</title>
<link>https://arxiv.org/abs/2508.04874</link>
<guid>https://arxiv.org/abs/2508.04874</guid>
<content:encoded><![CDATA[
arXiv:2508.04874v1 Announce Type: cross 
Abstract: As hybrid electric vehicles (HEVs) gain traction in heavy-duty trucks, adaptive and efficient energy management is critical for reducing fuel consumption while maintaining battery charge for long operation times. We present a new reinforcement learning (RL) framework based on the Soft Actor-Critic (SAC) algorithm to optimize engine control in series HEVs. We reformulate the control task as a sequential decision-making problem and enhance SAC by incorporating Gated Recurrent Units (GRUs) and Decision Transformers (DTs) into both actor and critic networks to capture temporal dependencies and improve planning over time. To evaluate robustness and generalization, we train the models under diverse initial battery states, drive cycle durations, power demands, and input sequence lengths. Experiments show that the SAC agent with a DT-based actor and GRU-based critic was within 1.8% of Dynamic Programming (DP) in fuel savings on the Highway Fuel Economy Test (HFET) cycle, while the SAC agent with GRUs in both actor and critic networks, and FFN actor-critic agent were within 3.16% and 3.43%, respectively. On unseen drive cycles (US06 and Heavy Heavy-Duty Diesel Truck (HHDDT) cruise segment), generalized sequence-aware agents consistently outperformed feedforward network (FFN)-based agents, highlighting their adaptability and robustness in real-world settings.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Surface Ozone Emulators using Deep Learning</title>
<link>https://arxiv.org/abs/2508.04885</link>
<guid>https://arxiv.org/abs/2508.04885</guid>
<content:encoded><![CDATA[
arXiv:2508.04885v1 Announce Type: cross 
Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's population is exposed to unsafe pollution levels. Surface Ozone (O3), an important pollutant, and the drivers of its trends are difficult to model, and traditional physics-based models fall short in their practical use for scales relevant to human-health impacts. Deep Learning-based emulators have shown promise in capturing complex climate patterns, but overall lack the interpretability necessary to support critical decision making for policy changes and public health measures. We implement an uncertainty-aware U-Net architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian and quantile regression methods. We demonstrate the capability of our techniques in regional estimation of bias in North America and Europe for June 2019. We highlight the uncertainty quantification (UQ) scores between our two UQ methodologies and discern which ground stations are optimal and sub-optimal candidates for MOMO-Chem bias correction, and evaluate the impact of land-use information in surface ozone residual modeling.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates</title>
<link>https://arxiv.org/abs/2508.04886</link>
<guid>https://arxiv.org/abs/2508.04886</guid>
<content:encoded><![CDATA[
arXiv:2508.04886v1 Announce Type: cross 
Abstract: Air pollution is the world's largest environmental risk factor for human disease and premature death, resulting in more than 6 million permature deaths in 2019. Currently, there is still a challenge to model one of the most important air pollutants, surface ozone, particularly at scales relevant for human health impacts, with the drivers of global ozone trends at these scales largely unknown, limiting the practical use of physics-based models. We employ a 2D Convolutional Neural Network based architecture that estimate surface ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the potential of this technique in North America and Europe, highlighting its ability better to capture physical model residuals compared to a traditional machine learning method. We assess the impact of incorporating land use information from high-resolution satellite imagery to improve model estimates. Importantly, we discuss how our results can improve our scientific understanding of the factors impacting ozone bias at urban scales that can be used to improve environmental policy.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)</title>
<link>https://arxiv.org/abs/2508.04894</link>
<guid>https://arxiv.org/abs/2508.04894</guid>
<content:encoded><![CDATA[
arXiv:2508.04894v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated with graph-structured data for tasks like node classification, a domain traditionally dominated by Graph Neural Networks (GNNs). While this integration leverages rich relational information to improve task performance, their robustness against adversarial attacks remains unexplored. We take the first step to explore the vulnerabilities of graph-aware LLMs by leveraging existing adversarial attack methods tailored for graph-based models, including those for poisoning (training-time attacks) and evasion (test-time attacks), on two representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al. 2024). Additionally, we discover a new attack surface for LLAGA where an attacker can inject malicious nodes as placeholders into the node sequence template to severely degrade its performance. Our systematic analysis reveals that certain design choices in graph encoding can enhance attack success, with specific findings that: (1) the node sequence template in LLAGA increases its vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness; and (3) both approaches remain susceptible to imperceptible feature perturbation attacks. Finally, we propose an end-to-end defense framework GALGUARD, that combines an LLM-based feature correction module to mitigate feature-level perturbations and adapted GNN defenses to protect against structural attacks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Temporal Label Noise in Multimodal Hateful Video Classification</title>
<link>https://arxiv.org/abs/2508.04900</link>
<guid>https://arxiv.org/abs/2508.04900</guid>
<content:encoded><![CDATA[
arXiv:2508.04900v1 Announce Type: cross 
Abstract: The rapid proliferation of online multimedia content has intensified the spread of hate speech, presenting critical societal and regulatory challenges. While recent work has advanced multimodal hateful video detection, most approaches rely on coarse, video-level annotations that overlook the temporal granularity of hateful content. This introduces substantial label noise, as videos annotated as hateful often contain long non-hateful segments. In this paper, we investigate the impact of such label ambiguity through a fine-grained approach. Specifically, we trim hateful videos from the HateMM and MultiHateClip English datasets using annotated timestamps to isolate explicitly hateful segments. We then conduct an exploratory analysis of these trimmed segments to examine the distribution and characteristics of both hateful and non-hateful content. This analysis highlights the degree of semantic overlap and the confusion introduced by coarse, video-level annotations. Finally, controlled experiments demonstrated that time-stamp noise fundamentally alters model decision boundaries and weakens classification confidence, highlighting the inherent context dependency and temporal continuity of hate speech expression. Our findings provide new insights into the temporal dynamics of multimodal hateful videos and highlight the need for temporally aware models and benchmarks for improved robustness and interpretability. Code and data are available at https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory</title>
<link>https://arxiv.org/abs/2508.04903</link>
<guid>https://arxiv.org/abs/2508.04903</guid>
<content:encoded><![CDATA[
arXiv:2508.04903v1 Announce Type: cross 
Abstract: Multi-agent large language model (LLM) systems have shown strong potential in complex reasoning and collaborative decision-making tasks. However, most existing coordination schemes rely on static or full-context routing strategies, which lead to excessive token consumption, redundant memory exposure, and limited adaptability across interaction rounds. We introduce RCR-Router, a modular and role-aware context routing framework designed to enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge, this is the first routing approach that dynamically selects semantically relevant memory subsets for each agent based on its role and task stage, while adhering to a strict token budget. A lightweight scoring policy guides memory selection, and agent outputs are iteratively integrated into a shared memory store to facilitate progressive context refinement. To better evaluate model behavior, we further propose an Answer Quality Score metric that captures LLM-generated explanations beyond standard QA accuracy. Experiments on three multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate that RCR-Router reduces token usage (up to 30%) while improving or maintaining answer quality. These results highlight the importance of structured memory routing and output-aware evaluation in advancing scalable multi-agent LLM systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy of Faults in Attention-Based Neural Networks</title>
<link>https://arxiv.org/abs/2508.04925</link>
<guid>https://arxiv.org/abs/2508.04925</guid>
<content:encoded><![CDATA[
arXiv:2508.04925v1 Announce Type: cross 
Abstract: Attention mechanisms are at the core of modern neural architectures, powering systems ranging from ChatGPT to autonomous vehicles and driving a major economic impact. However, high-profile failures, such as ChatGPT's nonsensical outputs or Google's suspension of Gemini's image generation due to attention weight errors, highlight a critical gap: existing deep learning fault taxonomies might not adequately capture the unique failures introduced by attention mechanisms. This gap leaves practitioners without actionable diagnostic guidance. To address this gap, we present the first comprehensive empirical study of faults in attention-based neural networks (ABNNs). Our work is based on a systematic analysis of 555 real-world faults collected from 96 projects across ten frameworks, including GitHub, Hugging Face, and Stack Overflow. Through our analysis, we develop a novel taxonomy comprising seven attention-specific fault categories, not captured by existing work. Our results show that over half of the ABNN faults arise from mechanisms unique to attention architectures. We further analyze the root causes and manifestations of these faults through various symptoms. Finally, by analyzing symptom-root cause associations, we identify four evidence-based diagnostic heuristics that explain 33.0% of attention-specific faults, offering the first systematic diagnostic guidance for attention-based models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens</title>
<link>https://arxiv.org/abs/2508.04928</link>
<guid>https://arxiv.org/abs/2508.04928</guid>
<content:encoded><![CDATA[
arXiv:2508.04928v1 Announce Type: cross 
Abstract: We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: https://github.com/JungHeeKim29/calibration-token.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM</title>
<link>https://arxiv.org/abs/2508.04931</link>
<guid>https://arxiv.org/abs/2508.04931</guid>
<content:encoded><![CDATA[
arXiv:2508.04931v1 Announce Type: cross 
Abstract: Traditional control and planning for robotic manipulation heavily rely on precise physical models and predefined action sequences. While effective in structured environments, such approaches often fail in real-world scenarios due to modeling inaccuracies and struggle to generalize to novel tasks. In contrast, humans intuitively interact with their surroundings, demonstrating remarkable adaptability, making efficient decisions through implicit physical understanding. In this work, we propose INTENTION, a novel framework enabling robots with learned interactive intuition and autonomous manipulation in diverse scenarios, by integrating Vision-Language Models (VLMs) based scene reasoning with interaction-driven memory. We introduce Memory Graph to record scenes from previous task interactions which embodies human-like understanding and decision-making about different tasks in real world. Meanwhile, we design an Intuitive Perceptor that extracts physical relations and affordances from visual scenes. Together, these components empower robots to infer appropriate interaction behaviors in new scenes without relying on repetitive instructions. Videos: https://robo-intention.github.io
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring</title>
<link>https://arxiv.org/abs/2508.04943</link>
<guid>https://arxiv.org/abs/2508.04943</guid>
<content:encoded><![CDATA[
arXiv:2508.04943v1 Announce Type: cross 
Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering</title>
<link>https://arxiv.org/abs/2508.04945</link>
<guid>https://arxiv.org/abs/2508.04945</guid>
<content:encoded><![CDATA[
arXiv:2508.04945v1 Announce Type: cross 
Abstract: Evaluating visual activity recognition systems is challenging due to inherent ambiguities in verb semantics and image interpretation. When describing actions in images, synonymous verbs can refer to the same event (e.g., brushing vs. grooming), while different perspectives can lead to equally valid but distinct verb choices (e.g., piloting vs. operating). Standard exact-match evaluation, which relies on a single gold answer, fails to capture these ambiguities, resulting in an incomplete assessment of model performance. To address this, we propose a vision-language clustering framework that constructs verb sense clusters, providing a more robust evaluation. Our analysis of the imSitu dataset shows that each image maps to an average of 2.8 sense clusters, with each cluster representing a distinct perspective of the image. We evaluate multiple activity recognition models and compare our cluster-based evaluation with standard evaluation methods. Additionally, our human alignment analysis suggests that the cluster-based evaluation better aligns with human judgements, offering a more nuanced assessment of model performance.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tesserae: Scalable Placement Policies for Deep Learning Workloads</title>
<link>https://arxiv.org/abs/2508.04953</link>
<guid>https://arxiv.org/abs/2508.04953</guid>
<content:encoded><![CDATA[
arXiv:2508.04953v1 Announce Type: cross 
Abstract: Training deep learning (DL) models has become a dominant workload in data-centers and improving resource utilization is a key goal of DL cluster schedulers. In order to do this, schedulers typically incorporate placement policies that govern where jobs are placed on the cluster. Existing placement policies are either designed as ad-hoc heuristics or incorporated as constraints within a complex optimization problem and thus either suffer from suboptimal performance or poor scalability. Our key insight is that many placement constraints can be formulated as graph matching problems and based on that we design novel placement policies for minimizing job migration overheads and job packing. We integrate these policies into Tesserae and describe how our design leads to a scalable and effective GPU cluster scheduler. Our experimental results show that Tesserae improves average JCT by up to 1.62x and the Makespan by up to 1.15x compared with the existing schedulers.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics</title>
<link>https://arxiv.org/abs/2508.04955</link>
<guid>https://arxiv.org/abs/2508.04955</guid>
<content:encoded><![CDATA[
arXiv:2508.04955v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has emerged as a powerful approach for learning visual representations without manual annotations. However, the robustness of standard SSL methods to domain shift -- systematic differences across data sources -- remains uncertain, posing an especially critical challenge in biomedical imaging where batch effects can obscure true biological signals. We present AdvDINO, a domain-adversarial self-supervised learning framework that integrates a gradient reversal layer into the DINOv2 architecture to promote domain-invariant feature learning. Applied to a real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide images from non-small cell lung cancer patients, AdvDINO mitigates slide-specific biases to learn more robust and biologically meaningful representations than non-adversarial baselines. Across $>5.46$ million mIF image tiles, the model uncovers phenotype clusters with distinct proteomic profiles and prognostic significance, and improves survival prediction in attention-based multiple instance learning. While demonstrated on mIF data, AdvDINO is broadly applicable to other imaging domains -- including radiology, remote sensing, and autonomous driving -- where domain shift and limited annotated data hinder model generalization and interpretability.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENDR: Manifold Explainable Neural Data Representations</title>
<link>https://arxiv.org/abs/2508.04956</link>
<guid>https://arxiv.org/abs/2508.04956</guid>
<content:encoded><![CDATA[
arXiv:2508.04956v1 Announce Type: cross 
Abstract: Foundation models for electroencephalography (EEG) signals have recently demonstrated success in learning generalized representations of EEGs, outperforming specialized models in various downstream tasks. However, many of these models lack transparency in their pretraining dynamics and offer limited insight into how well EEG information is preserved within their embeddings. For successful clinical integration, EEG foundation models must ensure transparency in pretraining, downstream fine-tuning, and the interpretability of learned representations. Current approaches primarily operate in the temporal domain, overlooking advancements in digital signal processing that enable the extraction of deterministic and traceable features, such as wavelet-based representations. We propose MENDR (Manifold Explainable Neural Data Representations), a filter bank-based EEG foundation model built on a novel Riemannian Manifold Transformer architecture to resolve these issues. MENDR learns symmetric positive definite matrix embeddings of EEG signals and is pretrained on a large corpus comprising over 4,000 hours of EEG data, decomposed via discrete wavelet packet transforms into multi-resolution coefficients. MENDR significantly enhances interpretability by visualizing symmetric positive definite embeddings as geometric ellipsoids and supports accurate reconstruction of EEG signals from learned embeddings. Evaluations across multiple clinical EEG tasks demonstrate that MENDR achieves near state-of-the-art performance with substantially fewer parameters, underscoring its potential for efficient, interpretable, and clinically applicable EEG analysis.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS</title>
<link>https://arxiv.org/abs/2508.04968</link>
<guid>https://arxiv.org/abs/2508.04968</guid>
<content:encoded><![CDATA[
arXiv:2508.04968v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots</title>
<link>https://arxiv.org/abs/2508.04994</link>
<guid>https://arxiv.org/abs/2508.04994</guid>
<content:encoded><![CDATA[
arXiv:2508.04994v1 Announce Type: cross 
Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to traverse complex environments efficiently. While the Deep Deterministic Policy Gradient (DDPG) algorithm excels in control tasks, its performance in maze navigation suffers from sparse rewards, inefficient exploration, and long-horizon planning difficulties, often leading to low success rates and average rewards, sometimes even failing to achieve effective navigation. To address these limitations, this paper proposes an efficient Hierarchical DDPG (HDDPG) algorithm, which includes high-level and low-level policies. The high-level policy employs an advanced DDPG framework to generate intermediate subgoals from a long-term perspective and on a higher temporal scale. The low-level policy, also powered by the improved DDPG algorithm, generates primitive actions by observing current states and following the subgoal assigned by the high-level policy. The proposed method enhances stability with off-policy correction, refining subgoal assignments by relabeling historical experiences. Additionally, adaptive parameter space noise is utilized to improve exploration, and a reshaped intrinsic-extrinsic reward function is employed to boost learning efficiency. Further optimizations, including gradient clipping and Xavier initialization, are employed to improve robustness. The proposed algorithm is rigorously evaluated through numerical simulation experiments executed using the Robot Operating System (ROS) and Gazebo. Regarding the three distinct final targets in autonomous maze navigation tasks, HDDPG significantly overcomes the limitations of standard DDPG and its variants, improving the success rate by at least 56.59% and boosting the average reward by a minimum of 519.03 compared to baseline algorithms.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</title>
<link>https://arxiv.org/abs/2508.04995</link>
<guid>https://arxiv.org/abs/2508.04995</guid>
<content:encoded><![CDATA[
arXiv:2508.04995v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the fragility of contemporary knowledge infrastructures by simulating coherence while bypassing traditional modes of citation, authority, and validation. This paper introduces the Situated Epistemic Infrastructures (SEI) framework as a diagnostic tool for analyzing how knowledge becomes authoritative across hybrid human-machine systems under post-coherence conditions. Rather than relying on stable scholarly domains or bounded communities of practice, SEI traces how credibility is mediated across institutional, computational, and temporal arrangements. Integrating insights from infrastructure studies, platform theory, and epistemology, the framework foregrounds coordination over classification, emphasizing the need for anticipatory and adaptive models of epistemic stewardship. The paper contributes to debates on AI governance, knowledge production, and the ethical design of information systems by offering a robust alternative to representationalist models of scholarly communication.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticData: An Agentic Data Analytics System for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.05002</link>
<guid>https://arxiv.org/abs/2508.05002</guid>
<content:encoded><![CDATA[
arXiv:2508.05002v1 Announce Type: cross 
Abstract: Existing unstructured data analytics systems rely on experts to write code and manage complex analysis workflows, making them both expensive and time-consuming. To address these challenges, we introduce AgenticData, an innovative agentic data analytics system that allows users to simply pose natural language (NL) questions while autonomously analyzing data sources across multiple domains, including both unstructured and structured data. First, AgenticData employs a feedback-driven planning technique that automatically converts an NL query into a semantic plan composed of relational and semantic operators. We propose a multi-agent collaboration strategy by utilizing a data profiling agent for discovering relevant data, a semantic cross-validation agent for iterative optimization based on feedback, and a smart memory agent for maintaining short-term context and long-term knowledge. Second, we propose a semantic optimization model to refine and execute semantic plans effectively. Our system, AgenticData, has been tested using three benchmarks. Experimental results showed that AgenticData achieved superior accuracy on both easy and difficult tasks, significantly outperforming state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health</title>
<link>https://arxiv.org/abs/2508.05003</link>
<guid>https://arxiv.org/abs/2508.05003</guid>
<content:encoded><![CDATA[
arXiv:2508.05003v1 Announce Type: cross 
Abstract: Background: Understanding social determinants of health (SDoH) factors contributing to suicide incidents is crucial for early intervention and prevention. However, data-driven approaches to this goal face challenges such as long-tailed factor distributions, analyzing pivotal stressors preceding suicide incidents, and limited model explainability. Methods: We present a multi-stage large language model framework to enhance SDoH factor extraction from unstructured text. Our approach was compared to other state-of-the-art language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help people annotate SDoH factors more quickly and accurately. The analysis included both automated comparisons and a pilot user study. Results: We show that our proposed framework demonstrated performance boosts in the overarching task of extracting SDoH factors and in the finer-grained tasks of retrieving relevant context. Additionally, we show that fine-tuning a smaller, task-specific model achieves comparable or better performance with reduced inference costs. The multi-stage design not only enhances extraction but also provides intermediate explanations, improving model explainability. Conclusions: Our approach improves both the accuracy and transparency of extracting suicide-related SDoH from unstructured texts. These advancements have the potential to support early identification of individuals at risk and inform more effective prevention strategies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Zero: Self-Evolving Reasoning LLM from Zero Data</title>
<link>https://arxiv.org/abs/2508.05004</link>
<guid>https://arxiv.org/abs/2508.05004</guid>
<content:encoded><![CDATA[
arXiv:2508.05004v1 Announce Type: cross 
Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation</title>
<link>https://arxiv.org/abs/2508.05011</link>
<guid>https://arxiv.org/abs/2508.05011</guid>
<content:encoded><![CDATA[
arXiv:2508.05011v1 Announce Type: cross 
Abstract: Recent advances in audio-based generative language models have accelerated AI-driven lyric-to-song generation. However, these models frequently suffer from content hallucination, producing outputs misaligned with the input lyrics and undermining musical coherence. Current supervised fine-tuning (SFT) approaches, limited by passive label-fitting, exhibit constrained self-improvement and poor hallucination mitigation. To address this core challenge, we propose a novel reinforcement learning (RL) framework leveraging preference optimization for hallucination control. Our key contributions include: (1) Developing a robust hallucination preference dataset constructed via phoneme error rate (PER) computation and rule-based filtering to capture alignment with human expectations; (2) Implementing and evaluating three distinct preference optimization strategies within the RL framework: Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO). DPO operates off-policy to enhance positive token likelihood, achieving a significant 7.4% PER reduction. PPO and GRPO employ an on-policy approach, training a PER-based reward model to iteratively optimize sequences via reward maximization and KL-regularization, yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective and subjective evaluations confirm that our methods effectively suppress hallucinations while preserving musical quality. Crucially, this work presents a systematic, RL-based solution to hallucination control in lyric-to-song generation. The framework's transferability also unlocks potential for music style adherence and musicality enhancement, opening new avenues for future generative song research.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Prompts First-Class Citizens for Adaptive LLM Pipelines</title>
<link>https://arxiv.org/abs/2508.05012</link>
<guid>https://arxiv.org/abs/2508.05012</guid>
<content:encoded><![CDATA[
arXiv:2508.05012v1 Announce Type: cross 
Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they retrieve external context, compose intermediate outputs, validate results, and adapt based on runtime feedback. Yet, the central element guiding this process -- the prompt -- remains a brittle, opaque string, disconnected from the surrounding dataflow. This disconnect limits reuse, optimization, and runtime control.
  In this paper, we describe our vision and an initial design for SPEAR, a language and runtime that fills this prompt management gap by making prompts structured, adaptive, and first-class components of the execution model. SPEAR enables (1) runtime prompt refinement -- modifying prompts dynamically in response to execution-time signals such as confidence, latency, or missing context; and (2) structured prompt management -- organizing prompt fragments into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and adapted within a pipeline. It supports multiple refinement modes (manual, assisted, and automatic), giving developers a balance between control and automation. By treating prompt logic as structured data, SPEAR enables optimizations such as operator fusion, prefix caching, and view reuse. Preliminary experiments quantify the behavior of different refinement modes compared to static prompts and agentic retries, as well as the impact of prompt-level optimizations such as operator fusion.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models</title>
<link>https://arxiv.org/abs/2508.05015</link>
<guid>https://arxiv.org/abs/2508.05015</guid>
<content:encoded><![CDATA[
arXiv:2508.05015v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong reasoning capabilities when fine-tuned with reinforcement learning (RL). However, such methods require extensive data and compute, making them impractical for smaller models. Current approaches to curriculum learning or data selection are largely heuristic-driven or demand extensive computational resources, limiting their scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced learning framework that enables efficient learning based on the capability of the model being trained through optimizing which data to use and when. First, we apply \emph{cluster-based data reduction} to partition training data by semantics and difficulty, extracting a compact yet diverse subset that reduces redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms, optimized to allocate training samples based on model current performance. Experiments across multiple reasoning benchmarks show that SPaRFT achieves comparable or better accuracy than state-of-the-art baselines while using up to \(100\times\) fewer samples. Ablation studies and analyses further highlight the importance of both data clustering and adaptive selection. Our results demonstrate that carefully curated, performance-driven training curricula can unlock strong reasoning abilities in LLMs with minimal resources.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes</title>
<link>https://arxiv.org/abs/2508.05019</link>
<guid>https://arxiv.org/abs/2508.05019</guid>
<content:encoded><![CDATA[
arXiv:2508.05019v1 Announce Type: cross 
Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. Early diagnosis, accurate and timely treatment are critical to improving patient survival rates. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose skin-SOAP, a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate this clinical relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning</title>
<link>https://arxiv.org/abs/2508.05023</link>
<guid>https://arxiv.org/abs/2508.05023</guid>
<content:encoded><![CDATA[
arXiv:2508.05023v1 Announce Type: cross 
Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to extract all target-aspect-opinion-sentiment quadruples from a given multi-round, multi-participant dialogue. Existing methods typically learn word relations across entire dialogues, assuming a uniform distribution of sentiment elements. However, we find that dialogues often contain multiple semantically independent sub-dialogues without clear dependencies between them. Therefore, learning word relationships across the entire dialogue inevitably introduces additional noise into the extraction process. To address this, our method focuses on partitioning dialogues into semantically independent sub-dialogues. Achieving completeness while minimizing these sub-dialogues presents a significant challenge. Simply partitioning based on reply relationships is ineffective. Instead, we propose utilizing a structural entropy minimization algorithm to partition the dialogues. This approach aims to preserve relevant utterances while distinguishing irrelevant ones as much as possible. Furthermore, we introduce a two-step framework for quadruple extraction: first extracting individual sentiment elements at the utterance level, then matching quadruples at the sub-dialogue level. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in DiaASQ with much lower computational costs.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs in AMR Parsing</title>
<link>https://arxiv.org/abs/2508.05028</link>
<guid>https://arxiv.org/abs/2508.05028</guid>
<content:encoded><![CDATA[
arXiv:2508.05028v1 Announce Type: cross 
Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence meaning as rooted, directed, acyclic graphs, where nodes represent concepts and edges denote semantic relations. Finetuning decoder only Large Language Models (LLMs) represent a promising novel straightfoward direction for AMR parsing. This paper presents a comprehensive evaluation of finetuning four distinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that straightfoward finetuning of decoder only LLMs can achieve comparable performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2 demonstrates competitive performance against SOTA AMR parsers given a straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5 excels in structural validity.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Schema Discovery and Application for Creative Problem Solving</title>
<link>https://arxiv.org/abs/2508.05045</link>
<guid>https://arxiv.org/abs/2508.05045</guid>
<content:encoded><![CDATA[
arXiv:2508.05045v1 Announce Type: cross 
Abstract: Humans often rely on underlying structural patterns-schemas-to create, whether by writing stories, designing software, or composing music. Schemas help organize ideas and guide exploration, but they are often difficult to discover and apply, especially in complex or unfamiliar domains. My Ph.D. research develops a framework for human-AI schema discovery and application to support creative problem solving. I design systems that support users in sensemaking over examples to abstract schemas, and in operationalizing schemas into human-AI co-creative workflows for application. This research offers insights into how schema-guided interaction can make implicit knowledge more accessible and actionable, advancing more transparent and collaborative human-AI systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting</title>
<link>https://arxiv.org/abs/2508.05059</link>
<guid>https://arxiv.org/abs/2508.05059</guid>
<content:encoded><![CDATA[
arXiv:2508.05059v1 Announce Type: cross 
Abstract: Pre-trained weights have become a cornerstone of modern deep learning, enabling efficient knowledge transfer and improving downstream task performance, especially in data-scarce scenarios. However, a fundamental question remains: how can we obtain better pre-trained weights that encapsulate more knowledge beyond the given dataset? In this work, we introduce \textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that leverages structured forgetting and its inversion to synthesize knowledge-enriched weights. Our key insight is that sequential fine-tuning on progressively downsized datasets induces a structured forgetting process, which can be modeled and reversed to recover knowledge as if trained on a larger dataset. We construct a dataset of weight transitions governed by this controlled forgetting and employ meta-learning to model weight prediction effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster (KNOWN)} acts as a hyper-model that learns the general evolution of weights and predicts enhanced weights with improved generalization. Extensive experiments across diverse datasets and architectures demonstrate that KNOW prediction consistently outperforms Na\"ive fine-tuning and simple weight prediction, leading to superior downstream performance. Our work provides a new perspective on reinterpreting forgetting dynamics to push the limits of knowledge transfer in deep learning.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2508.05068</link>
<guid>https://arxiv.org/abs/2508.05068</guid>
<content:encoded><![CDATA[
arXiv:2508.05068v1 Announce Type: cross 
Abstract: Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2508.05074</link>
<guid>https://arxiv.org/abs/2508.05074</guid>
<content:encoded><![CDATA[
arXiv:2508.05074v1 Announce Type: cross 
Abstract: Personalized sequential recommendation aims to predict appropriate items for users based on their behavioral sequences. To alleviate data sparsity and interest drift issues, conventional approaches typically incorporate auxiliary behaviors from other domains via cross-domain transition. However, existing cross-domain sequential recommendation (CDSR) methods often follow an align-then-fusion paradigm that performs representation-level alignment across multiple domains and combines them mechanically for recommendation, overlooking the fine-grained fusion of domain-specific preferences. Inspired by recent advances in diffusion models (DMs) for distribution matching, we propose an align-for-fusion framework for CDSR to harmonize triple preferences via dual-oriented DMs, termed HorizonRec. Specifically, we investigate the uncertainty injection of DMs and identify stochastic noise as a key source of instability in existing DM-based recommenders. To address this, we introduce a mixed-conditioned distribution retrieval strategy that leverages distributions retrieved from users' authentic behavioral logic as semantic bridges across domains, enabling consistent multi-domain preference modeling. Furthermore, we propose a dual-oriented preference diffusion method to suppress potential noise and emphasize target-relevant interests during multi-domain user representation fusion. Extensive experiments on four CDSR datasets from two distinct platforms demonstrate the effectiveness and robustness of HorizonRec in fine-grained triple-domain preference fusion.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning</title>
<link>https://arxiv.org/abs/2508.05078</link>
<guid>https://arxiv.org/abs/2508.05078</guid>
<content:encoded><![CDATA[
arXiv:2508.05078v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</title>
<link>https://arxiv.org/abs/2508.05087</link>
<guid>https://arxiv.org/abs/2508.05087</guid>
<content:encoded><![CDATA[
arXiv:2508.05087v1 Announce Type: cross 
Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a significant research focus. Current research predominantly focuses on maximizing attack success rate (ASR), often overlooking whether the generated responses actually fulfill the attacker's malicious intent. This oversight frequently leads to low-quality outputs that bypass safety filters but lack substantial harmful content. To address this gap, we propose JPS, \underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation and textual \underline{S}teering, which achieves jailbreaks via corporation of visual image and textually steering prompt. Specifically, JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by "steering prompt" optimized via a multi-agent system to specifically guide LLM responses fulfilling the attackers' intent. These visual and textual components undergo iterative co-optimization for enhanced performance. To evaluate the quality of attack outcomes, we propose the Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a Reasoning-LLM-based evaluator. Our experiments show JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy. Codes are available at \href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}. \color{warningcolor}{Warning: This paper contains potentially sensitive contents.}
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Influence: Data Attribution with Baseline</title>
<link>https://arxiv.org/abs/2508.05089</link>
<guid>https://arxiv.org/abs/2508.05089</guid>
<content:encoded><![CDATA[
arXiv:2508.05089v1 Announce Type: cross 
Abstract: As an effective approach to quantify how training samples influence test sample, data attribution is crucial for understanding data and model and further enhance the transparency of machine learning models. We find that prevailing data attribution methods based on leave-one-out (LOO) strategy suffer from the local-based explanation, as these LOO-based methods only perturb a single training sample, and overlook the collective influence in the training set. On the other hand, the lack of baseline in many data attribution methods reduces the flexibility of the explanation, e.g., failing to provide counterfactual explanations. In this paper, we propose Integrated Influence, a novel data attribution method that incorporates a baseline approach. Our method defines a baseline dataset, follows a data degeneration process to transition the current dataset to the baseline, and accumulates the influence of each sample throughout this process. We provide a solid theoretical framework for our method, and further demonstrate that popular methods, such as influence functions, can be viewed as special cases of our approach. Experimental results show that Integrated Influence generates more reliable data attributions compared to existing methods in both data attribution task and mislablled example identification task.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</title>
<link>https://arxiv.org/abs/2508.05102</link>
<guid>https://arxiv.org/abs/2508.05102</guid>
<content:encoded><![CDATA[
arXiv:2508.05102v1 Announce Type: cross 
Abstract: Dysarthric speech poses significant challenges in developing assistive technologies, primarily due to the limited availability of data. Recent advances in neural speech synthesis, especially zero-shot voice cloning, facilitate synthetic speech generation for data augmentation; however, they may introduce biases towards dysarthric speech. In this paper, we investigate the effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using TORGO dataset, focusing on intelligibility, speaker similarity, and prosody preservation. We also analyze potential biases using fairness metrics like Disparate Impact and Parity Difference to assess disparities across dysarthric severity levels. Results show that F5-TTS exhibits a strong bias toward speech intelligibility over speaker and prosody preservation in dysarthric speech synthesis. Insights from this study can help integrate fairness-aware dysarthric speech synthesis, fostering the advancement of more inclusive speech technologies.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Superior Function Calls via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.05118</link>
<guid>https://arxiv.org/abs/2508.05118</guid>
<content:encoded><![CDATA[
arXiv:2508.05118v1 Announce Type: cross 
Abstract: Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02\% overall accuracy, outperforming standard GRPO by up to 6\% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Expression Generation for Referring Image Segmentation and Grounding</title>
<link>https://arxiv.org/abs/2508.05123</link>
<guid>https://arxiv.org/abs/2508.05123</guid>
<content:encoded><![CDATA[
arXiv:2508.05123v1 Announce Type: cross 
Abstract: Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Basin: Why Contextual Position Matters in Large Language Models</title>
<link>https://arxiv.org/abs/2508.05128</link>
<guid>https://arxiv.org/abs/2508.05128</guid>
<content:encoded><![CDATA[
arXiv:2508.05128v1 Announce Type: cross 
Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Assessing Medical Ethics from Knowledge to Practice</title>
<link>https://arxiv.org/abs/2508.05132</link>
<guid>https://arxiv.org/abs/2508.05132</guid>
<content:encoded><![CDATA[
arXiv:2508.05132v1 Announce Type: cross 
Abstract: The integration of large language models into healthcare necessitates a rigorous evaluation of their ethical reasoning, an area current benchmarks often overlook. We introduce PrinciplismQA, a comprehensive benchmark with 3,648 questions designed to systematically assess LLMs' alignment with core medical ethics. Grounded in Principlism, our benchmark features a high-quality dataset. This includes multiple-choice questions curated from authoritative textbooks and open-ended questions sourced from authoritative medical ethics case study literature, all validated by medical experts. Our experiments reveal a significant gap between models' ethical knowledge and their practical application, especially in dynamically applying ethical principles to real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence, often over-emphasizing other principles. Frontier closed-source models, driven by strong general capabilities, currently lead the benchmark. Notably, medical domain fine-tuning can enhance models' overall ethical competence, but further progress requires better alignment with medical ethical knowledge. PrinciplismQA offers a scalable framework to diagnose these specific ethical weaknesses, paving the way for more balanced and responsible medical AI.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images</title>
<link>https://arxiv.org/abs/2508.05137</link>
<guid>https://arxiv.org/abs/2508.05137</guid>
<content:encoded><![CDATA[
arXiv:2508.05137v1 Announce Type: cross 
Abstract: Medical image segmentation plays a crucial role in AI-assisted diagnostics, surgical planning, and treatment monitoring. Accurate and robust segmentation models are essential for enabling reliable, data-driven clinical decision making across diverse imaging modalities. Given the inherent variability in image characteristics across modalities, developing a unified model capable of generalizing effectively to multiple modalities would be highly beneficial. This model could streamline clinical workflows and reduce the need for modality-specific training. However, real-world deployment faces major challenges, including data scarcity, domain shift between modalities (e.g., CT vs. MRI), and privacy restrictions that prevent data sharing. To address these issues, we propose FedGIN, a Federated Learning (FL) framework that enables multimodal organ segmentation without sharing raw patient data. Our method integrates a lightweight Global Intensity Non-linear (GIN) augmentation module that harmonizes modality-specific intensity distributions during local training. We evaluated FedGIN using two types of datasets: an imputed dataset and a complete dataset. In the limited dataset scenario, the model was initially trained using only MRI data, and CT data was added to assess its performance improvements. In the complete dataset scenario, both MRI and CT data were fully utilized for training on all clients. In the limited-data scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test cases compared to FL without GIN and consistently outperformed local baselines. In the complete dataset scenario, FedGIN demonstrated near-centralized performance, with a 30% Dice score improvement over the MRI-only baseline and a 10% improvement over the CT-only baseline, highlighting its strong cross-modality generalization under privacy constraints.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories</title>
<link>https://arxiv.org/abs/2508.05148</link>
<guid>https://arxiv.org/abs/2508.05148</guid>
<content:encoded><![CDATA[
arXiv:2508.05148v1 Announce Type: cross 
Abstract: The integration of robotics and automation into self-driving laboratories (SDLs) can introduce additional safety complexities, in addition to those that already apply to conventional research laboratories. Personal protective equipment (PPE) is an essential requirement for ensuring the safety and well-being of workers in laboratories, self-driving or otherwise. Fires are another important risk factor in chemical laboratories. In SDLs, fires that occur close to mobile robots, which use flammable lithium batteries, could have increased severity. Here, we present Chemist Eye, a distributed safety monitoring system designed to enhance situational awareness in SDLs. The system integrates multiple stations equipped with RGB, depth, and infrared cameras, designed to monitor incidents in SDLs. Chemist Eye is also designed to spot workers who have suffered a potential accident or medical emergency, PPE compliance and fire hazards. To do this, Chemist Eye uses decision-making driven by a vision-language model (VLM). Chemist Eye is designed for seamless integration, enabling real-time communication with robots. Based on the VLM recommendations, the system attempts to drive mobile robots away from potential fire locations, exits, or individuals not wearing PPE, and issues audible warnings where necessary. It also integrates with third-party messaging platforms to provide instant notifications to lab personnel. We tested Chemist Eye with real-world data from an SDL equipped with three mobile robots and found that the spotting of possible safety hazards and decision-making performances reached 97 % and 95 %, respectively.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</title>
<link>https://arxiv.org/abs/2508.05149</link>
<guid>https://arxiv.org/abs/2508.05149</guid>
<content:encoded><![CDATA[
arXiv:2508.05149v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models</title>
<link>https://arxiv.org/abs/2508.05152</link>
<guid>https://arxiv.org/abs/2508.05152</guid>
<content:encoded><![CDATA[
arXiv:2508.05152v1 Announce Type: cross 
Abstract: With the remarkable advancement of AI agents, the number of their equipped tools is increasing rapidly. However, integrating all tool information into the limited model context becomes impractical, highlighting the need for efficient tool retrieval methods. In this regard, dominant methods primarily rely on semantic similarities between tool descriptions and user queries to retrieve relevant tools. However, they often consider each tool independently, overlooking dependencies between tools, which may lead to the omission of prerequisite tools for successful task execution. To deal with this defect, in this paper, we propose Tool Graph Retriever (TGR), which exploits the dependencies among tools to learn better tool representations for retrieval. First, we construct a dataset termed TDI300K to train a discriminator for identifying tool dependencies. Then, we represent all candidate tools as a tool dependency graph and use graph convolution to integrate the dependencies into their representations. Finally, these updated tool representations are employed for online retrieval. Experimental results on several commonly used datasets show that our TGR can bring a performance improvement to existing dominant methods, achieving SOTA performance. Moreover, in-depth analyses also verify the importance of tool dependencies and the effectiveness of our TGR.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction</title>
<link>https://arxiv.org/abs/2508.05153</link>
<guid>https://arxiv.org/abs/2508.05153</guid>
<content:encoded><![CDATA[
arXiv:2508.05153v1 Announce Type: cross 
Abstract: Category-level generalization for robotic garment manipulation, such as bimanual smoothing, remains a significant hurdle due to high dimensionality, complex dynamics, and intra-category variations. Current approaches often struggle, either overfitting with concurrently learned visual features for a specific instance or, despite category-level perceptual generalization, failing to predict the value of synergistic bimanual actions. We propose the Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point clouds to specifically enhance category-level policy generalization for garment smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained, frozen dense geometric features, ensuring robustness to intra-category garment variations. Trainable downstream components then learn a task-specific policy using these static features. In simulated GarmentLab experiments with the CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization. It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments compared to 96.2% for a 2D image-based baseline, and achieved 89% final coverage, outperforming an 83% coverage from a 3D correspondence-based baseline that uses identical per-point geometric features but a fixed primitive. These results highlight that the decoupling of geometric understanding from bimanual action value learning enables better category-level generalization.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation</title>
<link>https://arxiv.org/abs/2508.05154</link>
<guid>https://arxiv.org/abs/2508.05154</guid>
<content:encoded><![CDATA[
arXiv:2508.05154v1 Announce Type: cross 
Abstract: For the development and optimization of agent-based models (ABMs) and rational agent-based models (RABMs), optimization algorithms such as reinforcement learning are extensively used. However, assessing the performance of RL-based ABMs and RABMS models is challenging due to the complexity and stochasticity of the modeled systems, and the lack of well-standardized metrics for comparing RL algorithms. In this study, we are developing domain-driven metrics for RL, while building on state-of-the-art metrics. We demonstrate our ``Domain-driven-RL-metrics'' using policy optimization on a rational ABM disease modeling case study to model masking behavior, vaccination, and lockdown in a pandemic. Our results show the use of domain-driven rewards in conjunction with traditional and state-of-the-art metrics for a few different simulation scenarios such as the differential availability of masks.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models</title>
<link>https://arxiv.org/abs/2508.05165</link>
<guid>https://arxiv.org/abs/2508.05165</guid>
<content:encoded><![CDATA[
arXiv:2508.05165v1 Announce Type: cross 
Abstract: Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost. Existing inference-time methods typically ignore this balance, focusing solely on the optimized policy's performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering to reduce inference calls while preserving alignment quality. On real-world prompt datasets, HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. We also find that HIA is effective under low-inference budgets with as little as one or two response queries, offering a practical solution for scalable, personalized LLM deployment.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</title>
<link>https://arxiv.org/abs/2508.05170</link>
<guid>https://arxiv.org/abs/2508.05170</guid>
<content:encoded><![CDATA[
arXiv:2508.05170v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model's internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Gaussian Splatting: A Volumetric Densification Approach</title>
<link>https://arxiv.org/abs/2508.05187</link>
<guid>https://arxiv.org/abs/2508.05187</guid>
<content:encoded><![CDATA[
arXiv:2508.05187v1 Announce Type: cross 
Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination</title>
<link>https://arxiv.org/abs/2508.05188</link>
<guid>https://arxiv.org/abs/2508.05188</guid>
<content:encoded><![CDATA[
arXiv:2508.05188v1 Announce Type: cross 
Abstract: Timely and effective incident response is key to managing the growing frequency of cyberattacks. However, identifying the right response actions for complex systems is a major technical challenge. A promising approach to mitigate this challenge is to use the security knowledge embedded in large language models (LLMs) to assist security operators during incident handling. Recent research has demonstrated the potential of this approach, but current methods are mainly based on prompt engineering of frontier LLMs, which is costly and prone to hallucinations. We address these limitations by presenting a novel way to use an LLM for incident response planning with reduced hallucination. Our method includes three steps: fine-tuning, information retrieval, and lookahead planning. We prove that our method generates response plans with a bounded probability of hallucination and that this probability can be made arbitrarily small at the expense of increased planning time under certain assumptions. Moreover, we show that our method is lightweight and can run on commodity hardware. We evaluate our method on logs from incidents reported in the literature. The experimental results show that our method a) achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes to a broad range of incident types and response actions.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Accuracy and Novelty with Sub-Item Popularity</title>
<link>https://arxiv.org/abs/2508.05198</link>
<guid>https://arxiv.org/abs/2508.05198</guid>
<content:encoded><![CDATA[
arXiv:2508.05198v1 Announce Type: cross 
Abstract: In the realm of music recommendation, sequential recommenders have shown promise in capturing the dynamic nature of music consumption. A key characteristic of this domain is repetitive listening, where users frequently replay familiar tracks. To capture these repetition patterns, recent research has introduced Personalised Popularity Scores (PPS), which quantify user-specific preferences based on historical frequency. While PPS enhances relevance in recommendation, it often reinforces already-known content, limiting the system's ability to surface novel or serendipitous items - key elements for fostering long-term user engagement and satisfaction. To address this limitation, we build upon RecJPQ, a Transformer-based framework initially developed to improve scalability in large-item catalogues through sub-item decomposition. We repurpose RecJPQ's sub-item architecture to model personalised popularity at a finer granularity. This allows us to capture shared repetition patterns across sub-embeddings - latent structures not accessible through item-level popularity alone. We propose a novel integration of sub-ID-level personalised popularity within the RecJPQ framework, enabling explicit control over the trade-off between accuracy and personalised novelty. Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by achieving significantly higher personalised novelty without compromising recommendation accuracy. Code and experiments are publicly available at https://github.com/sisinflab/Sub-id-Popularity.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>